\documentclass{speauth}

\def\RCS$#1: #2 ${\expandafter\def\csname RCS#1\endcsname{#2}}
\RCS$Revision$
\RCS$Date$

\usepackage{url}
\usepackage[bookmarksopen,urlcolor=red]{hyperref}

\begin{document}

% SPE-specific stuff ...
\SPE{1}{1}{00}{00}{2004}

\runningheads{N Gray}{Dragged from the stone-age [v\RCSRevision]}

\title{Dragged from the stone-age: autoconfing a big old project}

\author{Norman Gray\footnotemark}
\longaddress{Department of Physics and Astronomy,
  University of Glasgow, Glasgow G12 8QQ, UK; and\\
  Starlink Project, Rutherford Appleton Laboratory, UK}
\footnotetext{E-mail: norman@astro.gla.ac.uk}

\noreceived{}
\norevised{}
\noaccepted{}

\keywords{Starlink; CVS; autoconf}

% ... to here

\begin{abstract}
The Starlink software collection currently runs on multiple
Unix and POSIX platforms and contains around 100 separate software items,
totalling 2.5~million lines of code, in a mixture of languages.  We
recently changed the build system from a hand-maintained collection of
makefiles with hard-wired OS variants to a scheme involving
feature-discovery via GNU Autoconf.

We describe here the practical lessons we learned from this
experience, including the sometimes unexpected benefits and costs.

% XXX Comment out before submission
[Version \RCSRevision]
\end{abstract}



The Starlink Software Collection \cite{draper05} is a large set of
astronomical data-reduction 
applications, written over about two decades in a variety of
languages, primarily~C, C++ and Fortran.  Until recently the source
code was managed, and the
products distributed, using a system that worked, but which was
rather ad hoc and opaque, and which was correspondingly rather
expensive to maintain.  For a variety of technical, political, and financial
reasons we made the decision to overhaul this build system, resulting
in an CVS-plus-autoconf setup which is much more efficient for the
developers, as well as more familiar to the users and and the
community which will soon have to take a larger role in its maintainance.
In Section~\ref{s:background} we elaborate the 
motivations for this and the problems we faced; in
Section~\ref{s:result} we briefly describe the resulting system; and
in Section~\ref{s:lessons} we list some of the lessons we learned, in
the hope that these will be of use to other projects considering the
same important move.

We intend this article as practical advice from the front line, and
hope it will be of use to others in the same position of trying to
rationalise a large existing system.  Our system was not an ideal
starting point, as it was large, infuriatingly ad hoc in places, and
had a mass of interface commitments, at levels ranging from library
interfaces to documentation style conventions.  At the same time, it
was far from the worst, since the ad hoc features were confined within
disciplined structures; and once-irksome coding standards documents and
previous migrations meant that the code was maintainable and
portable.  This was specifically a build-system migration, not a
porting project, and we expected to keep code changes to a bare minimum.


\section{Background}
\label{s:background}

% Ought this section to be shorter -- folk are interested in the
% solutions, not the original problems.

The Starlink Project (\url{http://www.starlink.ac.uk}) was set up
originally in the late seventies, as a way of supplying UK astronomy
with hardware (`astronomers will never need more than six
VAXes\dots'), naturally along with the data-analysis software to go
with it, and the system management to make it all work smoothly.
Since then, the project has seen hardware and technology changes
(including multiple Digital/Compaq rebrandings), been fattened up,
slimmed down; it has discarded applications when it could, but
generally it has seen more and more of its products depended on by
some section or other of its user community.  By 2000, the project had
dropped its provision of hardware and sysadmins, renamed its legacy
codebase the `Classic Collection', and switched its focus to
developing new Java applications, in tune with more recent
developments in astronomical software.  Though it is not now actively
developed, the classic software is still depended upon by astronomers
world-wide, as well as being a valuable resource for pipelines
\cite{cavanagh03,currie04} and application engines in the VO
\cite{giaretta05}.  For this reason, and because, with further
enforced dieting, we expect the community will have to take a larger
role in its curation, we needed to make the classic software as
accessible as possible.

The biggest problem with the software was that there was an awful lot
of it!  Even with ongoing pruning, we have ended up with about 100
components, totalling around 1800\,kSLOC written by the project or
curated by it, in various languages
including Fortran, C, C++, Perl and Tcl/Tk.  We add to this another
700\,kSLOC of thirdparty code, some tweaked, and all built at the same
time.  Not included in this total is about 300\,kSLOC of Java,
built separately.  Just to put this in context, David A
Wheeler's \texttt{SLOCCount} would have us believe that that's worth
over \$100M and, according to his analysis of the RH7.1 distribution
\cite{wheeler02}, it appears that 1.8\,MSLOC is larger than anything in
that distribution except the kernel, Mozilla and XFree86.  We've been busy.

The build system we had was idiosyncratic, with code scattered amongst
the distributed developers, and a set of source and compiled build
products maintained by cut-and-paste of template makefiles.  This
system did work, and was a reasonable design for the early nineties,
but it required discipline and effort to maintain, and worked largely
because most users obtained their software through pre-built tape, network and
CD distributions, in many cases installed by system managers who were
also at that time employed by the Starlink Project (whose complaints
could therefore be bought off with beer).  As Starlink's
role changed, it became increasingly important that this software set
appear more `normal', and be maintainable (without medication) by a
broader range of developers.  One important source of complication in the
template makefiles was the need to cater for the collection's various
conventions about installation locations for libraries, headers,
documentation and other support files, plus the project's private
documentation and code-generation tools.  In order to avoid major
rewrites of the code base, it was necessary to duplicate these
conventions and tool support in the new build system.  Similar
problems would likely be faced by by projects of a similar size and age.

Portability was not expected to be a major problem (and wasn't in
fact), since the software was already being supported on three Unix
platforms; as well, some of the older applications had already been
through a port to Unix from VMS.  Also, the project has benefited from
generally good software practices over the years, so the code is
mostly clean, conservative, and well documented -- the project's
long-standing obsession with code-standards has certainly paid off,
with interest.  Known portability issues were isolated, though these
were historically handled by including platform-specific versions of
key routines, selected by the user at build-time; this limited the
target platforms to those explicitly supported by the project.

% Do we need to mention the age of the software -- crustiness?



\section{The Outcome}
\label{s:result}

With this project now largely complete, we have made a number of
significant improvements to the collection.

All the source code is now conveniently available in a single CVS
repository, with anonymous access
(\texttt{:pserver:anonymous@cvs.starlink.ac.uk:/cvs} with password `starlink').

The collection now uses the GNU autotools throughout.  Libtool handles the
mind-bending details of building static and shared libraries, with the
result that the collection now builds and installs the shared
libraries that were too much trouble before.  Automake generates the
makefiles, respecting all of our installation conventions and
generating support for our internal build tools.  And autoconf
generates the \texttt{./configure} scripts which test the capabilities
of the build system and adapt the makefiles and other files
appropriately.  It turned out to be impractical to use standard
autoconf and automake, and so the former was extended with new macros,
and the latter adapted with new logic.  Because we are using automake,
the amount of text we have to put into a \texttt{Makefile.am} is
substantially reduced, with very little cut-and-pasted boilerplate.

As a result of using the autotools, the entire 2.5\,MSLOC collection
now builds successfully on Linux (RHEL is our current test platform),
Solaris, Windows (using Cygwin), and Mac\,OS\,X, even though the
latter two were not explicit goals of this porting project.  At the
time of writing, we have some version-skew problems with OS\,X 10.4,
but, crucially, these will eventually be fixed once as part of the
buildsystem's maintainance.  It should
be portable to other Unix-like systems, and in particular
\textsc{posix} systems which include an X~server.



\section{Lessons and Warnings}
\label{s:lessons}

The original plan was to autoconf everything with only necessary code
changes -- in principle, we wanted to touch only makefiles.  However
it was impossible to stop ourselves refactoring and tidying, sometimes
rather extensively.  This is both a warning to other projects that
they won't be able to stop developers doing this, and a benefit, in
the sense that a lot of code-hygiene tasks that have been too boring,
confusing or risky in the past, become a lot less so if you are
committed to adjusting and re-integrating everything anyway.  Doing
this made it possible to contemplate a fresh look at the way we curate
our code.  For historical reasons, including the fact that systems
like CVS were less well-known ten years ago, the master copies of code
were held by the (distributed) developers.  As a consequence of the
audit, all the code is now in one repository, we have identified and
normalised modules of generated code and the sometimes
developer-private tools used to produce them, and we rescued some code
from backup oblivion, as developers retired or left the project.  This
is a non-trivial but obviously valuable exercise.  This consolidation
overlapped with the refactoring work, since the consequent perception
that code is owned in common meant that the developers were more
willing to criticise each others' code, and with appropriate
consultation refactor or fix it.

That is, the disruption and reorganisation caused by this project was
by itself paradoxically beneficial.  Beyond the material outcomes of a
stable repository and more robust build tools, we have the intangible
but no less real benefits that the current developers have a stake in
the design of the system, along with a more communitarian attitude to
the code.

We should have bowed to the inevitable, and started patching the
autotools earlier, since delaying this meant some functionality had to
be implemented twice.  Our initial expectation was that we could
provide all of our extra boilerplate using autoconf macros (using the
now partly deprecated \texttt{aclocal} extension mechanism); while
this works for relatively simple cases, we discovered the technique
rapidly ran out of steam.  Autoconf
works by adjusting pre-existing template files, but as soon as the
boilerplate consists of extra makefile rules, so that these templates
themselves have to be significantly adjusted, the scripting involved
in supporting this seems inevitably to become extremely complicated
and obviously fragile.

Patching automake is not a trivial undertaking, but the size of our
project, the resulting increase in robustness, and the control it gave
us over the build system, made it worthwhile.  The automake code is
undeniably complicated and rather confusing in detail, and the bulk of
the program logic is in a monolithic kernel; however that kernel is
relatively simple in overall design (suck in the input file to create
a complicated internal state, modify that state, then spit out
the result), the makefile fragments which it manages are organised in
as modular a way as seems possible, and the code is well written, so
that the tool as a whole is reasonably easy to patch.  In particular,
automake is fairly amenable to cautious hacking, spotting likely
features in existing \texttt{Makefile.in} files, and tracing them back
to the code or template which generates them.

Even if we had not patched it, it
would probably be wise, in a project of this size, to keep a
repository copy of the automake and autoconf distributions and mandate
their use when bootstrapping the source tree; apart from the increase
in long-term security, there are a number of potentially nasty
version-skew problems that this practice evades.  This admittedly adds
an extra step for a developer interested in working on CVS sources,
and an extra stage to be documented and supported,
but since the bootstrapping of the CVS sources is necessarily
elaborate anyway, and since it is only the more sophisticated
developers who would work on CVS rather than distributed sources, this
is probably not too much of a disadvantage in practice.

As a general point, we can recommend the use of automake.  Though
there is a rather steep learning curve, and one does naturally suspect
it of being dangerously clever, this turns out not to be a problem
\emph{as long as} you are willing to build your project the way
automake thinks it ought to be built.  Since the (GNU) coding
standards it implements are both conventional and reasonably sensible,
this is not the imposition it appears.  However it follows that
adapting a pre-existing makefile to automake can be frustrating and
hard, and we found it much less frustrating to start again, and produce the
much shorter \texttt{Makefile.am} automake source file from scratch.

The Fortran support in autoconf is rather slim.  A large proportion of the
autoconf extensions addressed the rather inadequate support for
Fortran in the standard autoconf distribution, and added tests for
intrinsics, open specifiers, support for \texttt{\%val} 
and the like.  These modifications affected only one component of the
autoconf system, and will be offered back to the autoconf mainline.
If they are accepted, we will be running an unpatched autoconf once more.

The port to OS\,X was easier, in some ways, than we expected, partly
because the OS\,X system compiler is a modified GCC.  However Apple's
GCC installation does not include \texttt{g77}, so we needed to
install the Fink/OpenDarwin port of that.  This causes a number of
linking problems because of slight differences in the code generated
by the two GCC back-ends; this is the \texttt{restFP/saveFP} problem:
see \cite{gray04} for a summary.  The OS\,X loader architecture is
intolerant of duplicated symbols, and the linker found such symbols
which had been present for years in our code without causing problems.
As a corollary of this, the loader had problems with Fortran common
blocks.  GNU \texttt{libtool} helped with many of these problems, but
even so, while most of our code compiled perfectly happily on OS\,X,
the linking stage consumed a disproportionate amount of effort.

We couldn't automatically convert our old build system to the new one,
because, though tidily implemented, it was based on hand-adapted
template makefiles, with an idiosyncratic collection of build targets
(I never could remember the difference between \texttt{make export}
and \texttt{make export-source}), providing much scope for ad hoc
cleverness, making the conversion impossible to automate.  To the
surprise of the present author -- much given to 
delivering phillipics on the eccentricity, future expensiveness, and
general viciousness of the old system -- this didn't matter.  The
old-style makefiles had to be changed by hand into new-style
\texttt{Makefile.am} files, but here the dangerous cleverness and
eccentric targets proved less important than the presence of the
organising patterns.  Here, some structure was enough structure, and
we discovered that the conversion could be done
package by package without much thought, and that creating the
\texttt{Makefile.am} file in this way was a useful first step in
acquianting oneself with the quirks of a package last touched last century.

Starlink is a well-run project with disciplined developers, but we
were surprised by the number of individually minor hurdles we had
accumulated over a decade, despite our best efforts at vigilance, enough that working with
the code would have been daunting for a non-insider.  Our complete
build system overhaul was quite expensive, but I speculate that almost
any similar project-wide development would produce some of the same
benefits, as a by-product of scanning a large fraction of the
project's code from a new perspective.

One of our own and others' criticisms of our code base was that, over
twenty years, it had had plenty time to fall victim to `big sticky
lump' syndrome, and one of our hopes at the beginning of this effort
was that reorganising the build system would resolve this problem.
This was over-optimistic.  Although we did become aware of a few
easily removed dependencies, in part because this was the first time
that the network of dependencies was expressed explicitly, doing this
on a larger scale would have required a more formal analysis we did
not have the resources to perform, identifying refactoring targets we
had not the resources to attack.

We found dependencies more complicated than we expected, and found at
least four types in our code base, which had their effects at
different phases, and had to be handled in different ways.  We discuss
these in Sect.~\ref{s:deps}.

As a result of teasing out and articulating the sub-projects'
interdependencies, it became feasible to generate distributable
components in a much more flexible way than had been possible before.
We were able to generate and centralise package metadata, and use
this, in conjunction with the EPM package manager~\cite{epm} to
generate Debian \texttt{.deb} files, OS\,X \texttt{.mpkg} files, RPMs
and others.  If further changes are required, changes to the automake
templates will make it easy to apply them project-wide.

In parallel with this porting project, we worked through the internal
bureaucracy involved in applying a GPL licence to the code where
possible.  This turned out to be harder than we expected, largely
because of the donations, in the past, of code written in gentler
times, with bizarre licences (such as `public domain, for academic use
only'!).  We ended up conceding defeat on this, as it would clearly be
a huge effort to work through all the contributed code, identifying
first authors then consistent conditions for each module.  We have
adopted the pragmatic policy of stating that everything in the
collection with an explicit project copyright is GPL (this includes
all of the build system); all of the rest is at least licensed for
academic use; users concerned about the remainder should get in touch
with us and we'll try to work it out.

The project took a \emph{lot} longer than we expected (surprise!).  It
took around six person-months to get the initial system up and
running, and then another six to adapt the bulk of our code to the new
system, followed by another six months of low-level tinkering to
remove the wrinkles and have the nightly builds produce relocatable
and useful distribution tarballs.

\subsection{Dependencies}
\label{s:deps}

We identified no fewer than five distinct dependency types.

First, \emph{sourceset dependencies} are the components which must be
installed in order to build the complete set of sources, either for
building or for distribution.  This means those tools which are used
to build documentation or generated sources which are included in distributions.

Next, a component's \emph{build dependencies} are the dependencies
which are required in order to build it.  This most often implies
files included during compilation, but also refers to project-specific
tools -- for example to compile message files -- which must be run on
the build host rather than the host which builds the distribution.
You may not have two components which have a build dependency on each
other, since that would mean that each would have to be built before
the other, which is impossible.

Thirdly, \emph{link dependencies} are the dependencies required to
link against the libraries in a component. That means all the
libraries that this component's libraries use. These are not
necessarily build dependencies, since if you are building a library~$X$,
which uses functions in library~$Y$, the library~$Y$ does not have to be
present in order to build library~$X$, only when you subsequently want
to actually link against it.  Thus you can have two components which
have mutual link dependencies.  If you are building an application,
however, all its link dependencies will actually be build dependencies
and should be declared as such. In other words, the distinction
between build and link dependencies is important only for library
components.

\emph{Configure dependencies}
There are some components which must be installed before the configure
step, since they must be present in order for other packages to
configure against them. Components which are listed as a configure
dependency of some other component are made during the make
configure-deps stage of the bootstrapping build process.

You should try hard to avoid declaring dependencies of this type,
since they necessarily subvert the normal build order. Note that
transitivity implies that any dependencies of a configure dependency
are also configure dependencies, so that declaring a configure
dependency on sst for example (an obvious but bad plan), results in
half the software set being built before configuration. Which is
crazy. 

You can see the set of configure dependencies by looking at the configure-deps target in Makefile.dependencies.

Use dependencies
These are the dependencies required in order for the component to be
used by something else, after it has been built and installed. For
example a library which called another application as part of its
functionality would have only a use dependency on the component which
contained that application. If no use dependencies are declared, we
take the use dependencies to be the same as the link dependencies. We
expect that such use dependencies will become more important when we
start using this dependency information to build distribution
packages, or to assemble .rpm or .deb packages. You should add any use
dependencies you are aware of, even though we cannot really debug them
until a distribution system is in place. 

Test dependencies
These are required in order to run any regression tests which come
with the component. It's generally a good idea to avoid making this a
larger set than the use dependencies, but sometimes this is
unavoidable. If no test dependencies are declared, we take the test
dependencies to be the same as the use dependencies. Note that the
system does not currently make any use of this information.

The point of all this is that different dependencies are required at
different times. The set of dependencies in the master makefile is
composed of all the `sourceset' and `build' dependencies, but not
`link' or `use' dependencies, and since the core Starlink libraries
are closely interdependent, the set of `build' dependencies needs to
be kept as small as possible in order to avoid circularities (that is,
A depending on B, which depends, possibly indirectly, on A).


It is because of this intricacy that the dependencies scheme is so
complicated. In general, it is safe to declare most things to have
build dependencies, and only refine these into link or use
dependencies when things won't work otherwise.


All these relationships are transitive: if A has a build dependency on
B, and B has one on C, then A has a build dependency on C. You can
augment this by using the final `option' argument: if, in component
A's configure.ac, you say \verb|STAR_DECLARE_DEPENDENCIES(build, B, link)|,
then you declare that A has a build-time dependency on B, but that
(presumably because B is a component consisting entirely or mostly of
libraries) you need to link against B, so component A has a dependency
on all of B's link dependencies, not just its build dependencies. An
application component should generally declare such a dependency for
each of the libraries it depends on. This is (I believe) the only case
where this `option' attribute is useful, though it is legal for each
of the dependency types.


\section{Conclusions}

Though this autoconfing project consumed more resources than we
expected, it has produced worthwhile results.  The code is now in a
state where it is no longer buildable only by project initiates; this
is essential, since the project has now had its funding terminated.  A
few excess dependencies have been broken, though not as many as we had hoped.
The project toolset is now distributed in more manageable components,
even though the dependency issues mean these are still not as small as
we would like.  Finally, the codebase has been extensively preened, to the
extent that it is possibly now in a tidier state than it has been for
a decade.


\bibliographystyle{my-spe}
\bibliography{star-cvs}
%% \begin{references}

%% \reference  Cavanagh, B., Hirst, P., Jenness, T., Economou, F.,
%% Currie, M. J., Todd, S., \& Ryder, S. D. 2003, \adassxii, 237

%% \reference Currie, M.\ J.\ 2004, \adassxiii, 409
%% % paper ref P4.25

%% \reference Giaretta, D.\ L.\ et al.\ 2005, \adassxiv, \paperref{O7.3}

%% \reference Draper, P.\ W.\ et al.\ 2005, \adassxiv, \paperref{D12}

%% \reference Gray, Norman 2004, `OSX and the restFP/saveFP problem'.
%% Web: \htmladdURL{http://www\discretionary{.}{}{.}astro.gla.ac.uk/users/norman/note/2004/restFP/}
%% (cited November 2004)

%% \reference Wheeler, David A.\ 2001 (updated 2002), `More than a
%% Gigabuck: Estimating GNU/Linux's Size'.
%% Web: \htmladdURL{http://www.dwheeler.com/sloc\discretionary{/}{}{/}redhat71-v1\discretionary{/}{}{/}redhat71sloc.html}
%% (cited November 2004)

%% \end{references}



\end{document}
