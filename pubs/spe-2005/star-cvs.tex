\documentclass{speauth}

\def\RCS$#1: #2 ${\expandafter\def\csname RCS#1\endcsname{#2}}
\RCS$Revision$
\RCS$Date$

\usepackage{url,prettyref}
\usepackage[bookmarksopen,urlcolor=red]{hyperref}

\newrefformat{s}{Sect.~\ref{#1}}
\newrefformat{it}{item~\ref{#1}}

\begin{document}

% SPE-specific stuff ...
\SPE{1}{1}{00}{00}{2004}

\runningheads{N Gray}{Dragged from the stone-age [v\RCSRevision]}

\title{Dragged from the stone-age: autoconfing a big old project}

\author{Norman Gray\footnotemark}
\longaddress{Department of Physics and Astronomy,
  University of Glasgow, Glasgow G12 8QQ, UK; and\\
  Starlink Project, Rutherford Appleton Laboratory, UK}
\footnotetext{E-mail: norman@astro.gla.ac.uk}

\noreceived{}
\norevised{}
\noaccepted{}

\keywords{Starlink; CVS; autoconf}

% ... to here

\begin{abstract}
The Starlink software collection currently runs on multiple
Unix and POSIX platforms and contains around 100 separate software items,
totalling 2.5~million lines of code, in a mixture of languages.  We
recently changed the build system from a hand-maintained collection of
makefiles with hard-wired OS variants to a scheme involving
feature-discovery via GNU Autoconf.

We describe here the practical lessons we learned from this
experience, including the sometimes unexpected benefits and costs.

XXX Expand

% XXX Comment out before submission
[Version \RCSRevision]
\end{abstract}



The Starlink Software Collection \cite{draper05} is a large set of
astronomical data-reduction 
applications, written over about two decades in a variety of
languages, primarily~C, C++ and Fortran.  Until recently the source
code was managed, and the
products distributed, using a system that worked, but which was
rather ad hoc and opaque, and which was correspondingly rather
expensive to maintain.  For a variety of technical, political, and financial
reasons we made the decision to overhaul this build system, resulting
in an CVS-plus-autoconf setup which is much more efficient for the
developers, as well as more familiar to the users and and the
community which will soon have to take a larger role in its maintainance.
In \prettyref{s:background} we elaborate the 
motivations for this and the problems we faced; in
\prettyref{s:result} we briefly describe the resulting system; and
in \prettyref{s:lessons} we list some of the lessons we learned, in
the hope that these will be of use to other projects considering the
same important move.  We discuss our typology of dependencies and its
relations with other dependency systems, the management of the
repository, and the overall architecture of the build system.

We intend this article as practical advice from the front line, and
hope it will be of use to others in the same position of trying to
rationalise a large existing system.  Our system was not an ideal
starting point, as it was large, infuriatingly ad hoc in places, and
had a mass of interface commitments, at levels ranging from library
interfaces to documentation style conventions.  At the same time, it
was far from the worst, since the ad hoc features were confined within
disciplined structures; and once-irksome coding standards documents and
previous migrations meant that the code was maintainable and
portable.  All our eccentricities were in our makefiles; our Fortran
was a joy to behold.  This was specifically a build-system migration, not a
porting project, and we expected to keep code changes to a bare minimum.


\section{Background}
\label{s:background}

% Ought this section to be shorter -- folk are interested in the
% solutions, not the original problems.

The Starlink Project (\url{http://www.starlink.ac.uk}) was set up
originally in the late seventies, as a way of supplying UK astronomy
with hardware (`astronomers will never need more than six
VAXes\dots'), naturally along with the data-analysis software to go
with it, and the system management to make it all work smoothly.
Since then, the project has seen hardware and technology changes
(including multiple Digital/Compaq rebrandings), been fattened up,
slimmed down; it has discarded applications when it could, but
generally it has seen more and more of its products depended on by
some section or other of its user community.  By 2000, the project had
dropped its provision of hardware and sysadmins, renamed its legacy
codebase the `Classic Collection', and switched its focus to
developing new Java applications, in tune with more recent
developments in astronomical software.  Though it is not now actively
developed, the classic software is still depended upon by astronomers
world-wide, as well as being a valuable resource for pipelines
\cite{cavanagh03,currie04} and application engines in the VO
\cite{giaretta05}.  For this reason, and because, with further
enforced dieting, we expect the community will have to take a larger
role in its curation, we needed to make the classic software as
accessible as possible.

The biggest problem with the software was that there was an awful lot
of it!  Even with ongoing pruning, we have ended up with about 100
components, totalling around 1800\,kSLOC written by the project or
curated by it, in various languages
including Fortran, C, C++, Perl and Tcl/Tk.  We add to this another
700\,kSLOC of thirdparty code, some tweaked, and all built at the same
time.  Not included in this total is about 300\,kSLOC of Java,
built separately.  Just to put this in context, David A
Wheeler's \texttt{SLOCCount} would have us believe that that's worth
over \$100M and, according to his analysis of the RH7.1 distribution
\cite{wheeler02}, it appears that 1.8\,MSLOC is larger than anything in
that distribution except the kernel, Mozilla and XFree86.  We've been busy.

The build system we had was idiosyncratic, with code scattered amongst
the distributed developers, and a set of source and compiled build
products maintained by cut-and-paste of template makefiles.  This
system did work, and was a reasonable design for the early nineties,
but it required discipline and effort to maintain, and worked largely
because most users obtained their software through pre-built tape, network and
CD distributions, in many cases installed by system managers who were
also at that time employed by the Starlink Project (whose complaints
could therefore be bought off with beer).  As Starlink's
role changed, it became increasingly important that this software set
appear more `normal', and be maintainable (without medication) by a
broader range of developers.  One important source of complication in the
template makefiles was the need to cater for the collection's various
conventions about installation locations for libraries, headers,
documentation and other support files, plus the project's private
documentation and code-generation tools.  In order to avoid major
rewrites of the code base, it was necessary to duplicate these
conventions and tool support in the new build system.  Similar
problems would likely be faced by by projects of a similar size and age.

Portability was not expected to be a major problem (and wasn't in
fact), since the software was already being supported on three Unix
platforms; as well, some of the older applications had already been
through a port to Unix from VMS.  Also, the project has benefited from
generally good software practices over the years, so the code is
mostly clean, conservative, and well documented -- the project's
long-standing obsession with code-standards has certainly paid off,
with interest.  Known portability issues were isolated, though these
were historically handled by including platform-specific versions of
key routines, selected by the user at build-time; this limited the
target platforms to those explicitly supported by the project.

% Do we need to mention the age of the software -- crustiness?

We also need our software to be portable in time.  Astronomical data
has quite a long lifetime, and data is quite often useful a couple of
decades after it is taken, and this gives us an archival
responsibility for the software.  We can reasonably expect that people
will want to build our software in a number of years' time, even in
the absence of ongoing maintainance or traceable experts.  This
requires a system with few surprises, good documentation, and a design
sane enough to be usefully documented.

Our overall goals, therefore, were various.
\begin{enumerate}
\item \label{it:portable}The resulting source code had to be portable,
  covering as many Unix-like platforms as possible, and certainly our
  original core set of RedHat, Solaris and DUX.  This comfortably
  included MacOS\,X (though its ultra-fussy linker provided
  entertaining challenges), and with rather more effort included
  Cygwin.  As well, we aimed to avoid commitments to specific
  distribution technologies such as RPM or Debian packages.
\item The resulting system had to use only open-source tools, on the
  multiple grounds that
  (i) these are the tools our constituency would expect to use;
  (ii) our archival responsibilities mean that the build software has
  to be available, and if possible be well-known, a couple of decades
  into the future.  This pushes us firmly toward non-commercial
  software that can be embedded in the source code repository.
\item \label{it:normal}
  The system had to look as `normal' as possible, to stand any
  chance of it being maintained on an ad-hoc basis.  Documentation
  helps, of course, but the greater the proportion of the build system
  that doesn't have to be documented, or the documentation left
  unread, the greater the chance of fixes being made carefully, rather
  than as hacks.  The archival concerns mean that the software has to
  continue to look normal in the medium term: tools such as
  \texttt{make} or CVS have sufficiently large commitments to them
  (even though the latter is now dominated by Subversion for new
  projects) that they will continue to be reasonably familiar to
  developers for perhaps another couple of decades; we cannot say the
  same for any specific packaging tool, for example.  The customised
  autotools do not look normal in this sense, but their use is
  unavoidable.
%\item Related to the previous point, the system had to support
%  developers interested in only a subset of the codebase
\item The change to the build system had to involve as few as possible
  changes to the bulk of our code-base.  Our code has some test
  support, but it is not pervasive, and we could not practically
  handle a large number of bugs being introduced simply by the changed
  build system.
\end{enumerate}


\section{The Outcome}
\label{s:result}

With this project now largely complete, we have made a number of
significant improvements to the collection.

All the source code is now conveniently available in a single CVS
repository, with anonymous access
(\texttt{:pserver:anonymous@cvs.starlink.ac.uk:/cvs} with password `starlink').

The collection now uses the GNU autotools throughout.  Libtool handles the
mind-bending details of building static and shared libraries, with the
result that the collection now builds and installs the shared
libraries that were too much trouble before.  Automake generates the
makefiles, respecting all of our installation conventions and
generating support for our internal build tools.  And autoconf
generates the \texttt{./configure} scripts which test the capabilities
of the build system and adapt the makefiles and other files
appropriately.  It turned out to be impractical to use standard
autoconf and automake, and so the former was extended with new macros,
and the latter adapted with new logic.  Because we are using automake,
the amount of text we have to put into a \texttt{Makefile.am} is
substantially reduced, with very little cut-and-pasted boilerplate.

As a result of using the autotools, the entire 2.5\,MSLOC collection
now builds successfully on Linux (RHEL is our current test platform),
Solaris, Windows (using Cygwin), and Mac\,OS\,X, even though the
latter two were not explicit goals of this porting project.  At the
time of writing, we have some version-skew problems with OS\,X 10.4,
but, crucially, these will eventually be fixed once as part of the
buildsystem's maintainance.  It should
be portable to other Unix-like systems, and in particular
\textsc{posix} systems which include an X~server.

\subsection{Overall architecture}
\label{s:arch}

Building from a freshly checked-out repository, or section of the
repository, proceeds in four stages.
\begin{enumerate}
\item Bootstrap: We first build and install our patched versions of
  the autotools, and then use them to build the makefiles and
  configure scripts for the entire code-base.  This is a
  time-consuming step, but the only way of avoiding it is to include
  these generated files in the repository, which we discounted for the
  reasons rehearsed in \prettyref{s:generated}.

\item \label{it:configuredeps} Build configure dependencies: see
  \prettyref{s:deps}.

\item Configure: The entire tree is configured, using the
  autoconf-generated scripts.  The autotools are built in the
  bootstrap phase with a project-specific default installation
  location, different from the autotools' default \texttt{/usr/local},
  but that information is overridable here.  As part of this
  configuration step, each component generates an XML file summarising
  the component's metadata, such as name, version, maintainer,
  dependencies and the like; this file is the component's principal
  interface with the rest of the system.

\item Finally, the software is built and installed.
\end{enumerate}

During the final build stage, each component is in turn built and
immediately installed, in an order which respects the dependencies
described in \prettyref{s:deps}.  The order is deduced by
\texttt{make}, working on a set of dependencies between components
expressed as dependencies between component `manifest files'; these
contain component metadata, and are generated and installed as the
final step of per-component installation, by makefile code which is
generated by our modified \texttt{automake}.  This makes it possible
to build either the entire software set, or only a specific package
and its dependencies.

An alternative strategy here would have been to use a packaging tool
such as \texttt{dpkg}.  This would have advantage of a pre-existing
dependencies model, at the cost of somewhat abusing that tool's aims
(it is, after all, intended for managing ports rather than builds),
including another layer of indirection along with its associated
documentation costs, requiring to be included in our set of tracked
sources (see \prettyref{s:tracking}), and marginally violating our
`normality' constraint~\ref{it:normal} in \prettyref{s:background}.

Another potential advantage of \texttt{dpkg} would be ready-made
distributions, but since we aimed to be as promiscuous as possible in
our support for packaging technology (\prettyref{it:portable} in
\prettyref{s:background}), since we had component metadata in an
easily reprocessed form (the component XML files mentioned above), and
since our packaging requirements were not particularly intricate, we
settled in the end on using the EPM package manager~\cite{epm} to
generate Debian \texttt{.deb} files, OS\,X \texttt{.mpkg} files, RPMs
and others.  This packaging aspect is not one of our core committments
for the future, so we are not worried that we are using a
still-developing and untracked tool.

\subsection{Do not include generated code in the repository}
\label{s:generated}

In general, we feel that the disadvantages of including generated code
in the repository (which includes makefiles and configure scripts)
outweigh the advantages in all but a few cases where it is not
practically avoidable.  The FAQ in the \texttt{automake} manual
rehearses some of the arguments both for and against this practice,
but the central problem is that checking files into and out of a
repository can set file timestamps in such a way as to cause spurious
regeneration of files in contexts where the generating tools are
unavailable, or the wrong versions, or otherwise inappropriate (our
actual experience was with CVS, but we doubt there is any reasonable
model for a repository system which reliably avoids this problem).  Even
with a full understanding of the software involved, it is hard to
explain just why this happens \emph{so} often, why it causes \emph{so}
much trouble when it happens, or why working around the trouble is
quite \emph{so} very errorprone.  All we can say is that handling
those few checked-in generated files that we could not avoid caused
much more trouble and anguish than we were ever able to satisfactorily
account for.

Because we had our own versions of the autotools checked in to the
repository, and built as part of the build system, we were not subject
to the most prominent disadvantages of keeping generated files out of
the repository, and were thus able to enjoy the advantages of
robustness and predictability which flow from treating products as
products.  The only disadvantages are that our `build support' tools
(primarily the autotools) are an implicit additional dependency of all
the components in the collection, and that the bootstrapping phase
mentioned in \prettyref{s:arch} -- the phase during which these
generated files are in fact produced -- is much more protracted than
it would otherwise be.  Both of these turn out to be rather minor
disadvantages in fact.

The generated files that have nonetheless ended up checked-in are
firstly those files used in the bootstrapping itself -- which means
the makefiles and configure scripts of the build support tools, and
the component metadata files -- and secondly the generated files in
the tracked third party sources, because they are generated by tools,
or versions of tools, which we do not have to hand.

\subsection{Dependencies}
\label{s:deps}

Dependencies are a problem for any large project which is at all
componentized.  We had evaded this problem before by having a magic
shell script which built everything in a working order; apart from
being ugly, this was no longer reasonable if we were to accomodate
developers interested in only a subset of the codebase.  There are
several systems in existence for expressing and managing dependencies
between discrete software components, of which the best known are
possibly the Debian and RPM package frameworks
(see~\cite[ch~7]{debian05} and \url{http://www.rpm.org}).  Much of the
literature on `dependency analysis' is concerned to discover
fine-grained dependencies between modules or variables, as part of
reverse-engineering or redocumenting software.  The broader question of
inter-package dependencies is usually discussed in the context of
specific software tools to discover the dependencies, for example in
an early paper by Wilde and Huitt~\cite{wilde91} and in the context of
large scientific codes by Tuura and Taylor~\cite{tuura01}.

We identified no fewer than five distinct dependency types, which are
broadly consistent with the typology discussed in these sources.

First, \emph{sourceset dependencies} are the components which must be
installed in order to build the complete set of sources, either for
building or for distribution.  This means those tools which are used
to build documentation or build generated and distributed sources.
That is, these are dependencies which are necessary to bootstrap a
freshly checked-out repository, but which are not dependencies for a
distributed package.

Next, a component's \emph{build dependencies} are the dependencies
which are required in order to build it.  This most often implies
files included during compilation, but also refers to project-specific
tools -- for example to compile message files -- which must be run on
the build host rather than the host which builds the distribution.
The network of build dependencies must be non-cyclic.  These are most
closely analogous to \texttt{dpkg}'s \texttt{Build-depend} dependencies.

Thirdly, \emph{link dependencies} are the dependencies required to
link against the libraries in a component. That means all the
libraries that this component's libraries use. These are not
necessarily build dependencies, since if you are building a
library~$X$, which uses functions in library~$Y$, the library~$Y$ does
not have to be present in order to build library~$X$, only when you
subsequently want to actually link against it.  Thus there is no
problem in having cyclic link dependencies.  For an application, on
the other hand, all its link dependencies will actually be build
dependencies and should be declared as such as well.

This category is actually somewhat smaller than it might appear.  A
C~library will typically (and ideally) have a set of header files
containing C~prototypes which effectively describe the library's
interface; since these header files will be included during the
build of any software which uses the library, they must be
installed at build time, and thus the library becomes a full build
dependency.  The category was required because a large proportion of
our libraries are Fortran libraries, for which there are
(unfortunately!) no include files, so that mutually dependent
libraries can in practice be built without difficulties; the fact that
we discovered we did in fact have several such sets of cyclic
dependencies meant that we had to rely on this unfortunate property in
several places.

There is a further justification for these link dependencies, which is
that if we distribute binary packages using shared libraries, the
dependencies between them will be this set of link dependencies (thus
it is analogous to \texttt{dpkg}'s \texttt{Depend} dependencies,
without the issue of pre- and post-configuration).  This is not fully
worked out, however, since apart from a few tests, the project has not
routinely distributed binary packages.

There are some components which must be installed before the configure
step, since they must be present in order for other packages to
configure against them.  These \emph{configure dependencies} are built
prior to tree-wide configuration, in step~\ref{it:configuredeps} of
\prettyref{s:arch}.  Because this heavily subverted the build order,
we kept this set as small as possible, and it turned out to include
only a couple of third party libraries, which other components
searched for during configuration.

The final dependency category is that of \emph{use dependencies}.
These are the components which a given component requires in order to
function, but which are not link dependencies -- this rather small set
includes, for us, only a small number of cases where a library uses a
separate application to do some of its work.  These are irrelevant for
the build (this is the difference from link dependencies), but would
be essential for packaging and distribution.

%% All these relationships are transitive: if A has a build dependency on
%% B, and B has one on C, then A has a build dependency on C. You can
%% augment this by using the final `option' argument: if, in component
%% A's configure.ac, you say \verb|STAR_DECLARE_DEPENDENCIES(build, B, link)|,
%% then you declare that A has a build-time dependency on B, but that
%% (presumably because B is a component consisting entirely or mostly of
%% libraries) you need to link against B, so component A has a dependency
%% on all of B's link dependencies, not just its build dependencies. An
%% application component should generally declare such a dependency for
%% each of the libraries it depends on. This is (I believe) the only case
%% where this `option' attribute is useful, though it is legal for each
%% of the dependency types.

Debugging dependencies?  Peter's multi-build.  Hard to be sure.


\section{Lessons and Warnings}
\label{s:lessons}

The original plan was to autoconf everything with only necessary code
changes -- in principle, we wanted to touch only makefiles.  However
it was impossible to stop ourselves refactoring and tidying, sometimes
rather extensively.  This is both a warning to other projects that
they won't be able to stop developers doing this, and a benefit, in
the sense that a lot of code-hygiene tasks that have been too boring,
confusing or risky in the past, become a lot less so if you are
committed to adjusting and re-integrating everything anyway.  Doing
this made it possible to contemplate a fresh look at the way we curate
our code.  For historical reasons, including the fact that systems
like CVS were less well-known ten years ago, the master copies of code
were held by the (distributed) developers.  As a consequence of the
audit, all the code is now in one repository, we have identified and
normalised modules of generated code and the sometimes
developer-private tools used to produce them, and we rescued some code
from backup oblivion, as developers retired or left the project.  This
is a non-trivial but obviously valuable exercise.  This consolidation
overlapped with the refactoring work, since the consequent perception
that code is owned in common meant that the developers were more
willing to criticise each others' code, and with appropriate
consultation refactor or fix it.

That is, the disruption and reorganisation caused by this project was
by itself paradoxically beneficial.  Beyond the material outcomes of a
stable repository and more robust build tools, we have the intangible
but no less real benefits that the current developers have a stake in
the design of the system, along with a more communitarian attitude to
the code.

We should have bowed to the inevitable, and started patching the
autotools earlier, since delaying this meant some functionality had to
be implemented twice.  Our initial expectation was that we could
provide all of our extra boilerplate using autoconf macros (using the
now partly deprecated \texttt{aclocal} extension mechanism); while
this works for relatively simple cases, we discovered the technique
rapidly ran out of steam.  Autoconf
works by adjusting pre-existing template files, but as soon as the
boilerplate consists of extra makefile rules, so that these templates
themselves have to be significantly adjusted, the scripting involved
in supporting this seems inevitably to become extremely complicated
and obviously fragile.

Patching automake is not a trivial undertaking, but the size of our
project, the resulting increase in robustness, and the control it gave
us over the build system, made it worthwhile.  The automake code is
undeniably complicated and rather confusing in detail, and the bulk of
the program logic is in a monolithic kernel; however that kernel is
relatively simple in overall design (suck in the input file to create
a complicated internal state, modify that state, then spit out
the result), the makefile fragments which it manages are organised in
as modular a way as seems possible, and the code is well written, so
that the tool as a whole is reasonably easy to patch.  In particular,
automake is fairly amenable to cautious hacking, spotting likely
features in existing \texttt{Makefile.in} files, and tracing them back
to the code or template which generates them.

Even if we had not patched it, it is wise, in a project of this size,
to keep a repository copy of the automake and autoconf distributions
and mandate their use when bootstrapping the source tree; apart from
the increase in long-term security, there are a number of potentially
nasty version-skew problems that this practice evades.  This
admittedly adds an extra step for a developer interested in working on
CVS sources, and an extra stage to be documented and supported, but
since the bootstrapping of the CVS sources is necessarily elaborate
anyway, and since it is only the more sophisticated developers who
would work on CVS rather than distributed sources, this is probably
not too much of a disadvantage in practice.

Tracking \label{s:tracking} a third party distribution like this has
important benefits, but also important costs: you must either actively
track updates in the third party software, fixing breakage that new
versions cause, or else by removing the pressure to track the
software, lose compatibility with more stable or featureful updated
versions.  As well as the autotools, we are currently tracking Caltech
PGPlot, ESO catlib, CFITSIO, the IJG JPEG libraries, the NCAR graphics
library, Perl and Tcl/Tk.  This is a worryingly long list, and the
latter two in particular take up a large fraction of the repository.
However these distributions are generally rather quiescent (the
exception is Perl, and the project happens to have on hand personnel
with considerable Perl expertise), so that we have not in practice had
to suffer many of the disadvantages of this policy.

As a general point, we feel the case for automake (or a tool very like
it) is overwhelming.  Though
there is a rather steep learning curve, and one does naturally suspect
it of being dangerously clever, this turns out not to be a problem
\emph{as long as} you are willing to build your project the way
automake thinks it ought to be built.  Since the (GNU) coding
standards it implements are both conventional and reasonably sensible,
this is not the imposition it appears.  However it follows that
adapting a pre-existing makefile to automake can be frustrating and
hard, and we found it much less frustrating to start again, and produce the
much shorter \texttt{Makefile.am} automake source file from scratch.

The Fortran support in autoconf is rather slim.  A large proportion of the
autoconf extensions addressed the rather inadequate support for
Fortran in the standard autoconf distribution, and added tests for
intrinsics, open specifiers, support for \texttt{\%val} 
and the like.  These modifications affected only one component of the
autoconf system, and will be offered back to the autoconf mainline.
If they are accepted, we will be running an unpatched autoconf once
more.

We ended up using Fortran preprocessing quite extensively, even though
this is regarded as rather exotic in the Fortran world.  This allows
us to use useful but non-standard Fortran extensions, without closing
the door on the eccentric or conservative compilers which users might
have good reason to prefer.  Preprocessing code is non-trivial in
Fortran, as there is no standard for it; however Sun have produced an
effective standard in their full-featured \texttt{fpp} tool, available
at Netlib, \url{http://www.netlib.org/fortran/}, and as part of Sun's
compiler collection.  Several Fortran compilers can already recognise
C-style preprocesor directives, though the options for switching on
this behaviour unhelpfully vary from compiler to compiler.  In
principle, \texttt{cpp} cannot be used on Fortran, since the languages
are syntactically so different, but in practice it generally does work
in fact, as long as you avoid using \texttt{\#define} (or else risk
accidentally breaching Fortran's 72-character line limit) and ensure
the preprocessor not recognise \texttt{//} as a (C++) comment string
(it has a rather different interpretation in Fortran).  In fact we
never had to use any preprocessing constructs other than
\texttt{\#define} and \texttt{\#if}, so we didn't run into any
portability problems at all.  Standard autoconf does not include any
tests for Fortran preprocessing capability or flags, so we
incorporated Martin Wilck's autoconf extensions to provide this, and
these are part of the autoconf patches we intend to submit to the
autoconf mainline.

The port to OS\,X was easier, in some ways, than we expected, partly
because the OS\,X system compiler is a modified GCC.  However Apple's
GCC installation does not include \texttt{g77}, so we needed to
install the Fink/OpenDarwin port of that.  This causes a number of
linking problems because of slight differences in the code generated
by the two GCC back-ends; this is the \texttt{restFP/saveFP} problem:
see \cite{gray04} for a summary.  The OS\,X loader architecture is
intolerant of duplicated symbols, and the linker found such symbols
which had been present for years in our code without causing problems.
As a corollary of this, the loader had problems with Fortran common
blocks.  GNU \texttt{libtool} helped with many of these problems, but
even so, while most of our code compiled perfectly happily on OS\,X,
the linking stage consumed a disproportionate amount of effort.

We couldn't automatically convert our old build system to the new one,
because, though tidily implemented, it was based on hand-adapted
template makefiles, with an idiosyncratic collection of build targets
(I never could remember the difference between \texttt{make export}
and \texttt{make export-source}), providing much scope for ad hoc
cleverness, making the conversion impossible to automate.  To the
surprise of the present author -- much given to 
delivering phillipics on the eccentricity, future expensiveness, and
general viciousness of the old system -- this didn't matter.  The
old-style makefiles had to be changed by hand into new-style
\texttt{Makefile.am} files, but here the dangerous cleverness and
eccentric targets proved less important than the presence of the
organising patterns.  Here, some structure was enough structure, and
we discovered that the conversion could be done
package by package without much thought, and that creating the
\texttt{Makefile.am} file in this way was a useful first step in
acquianting oneself with the quirks of a package last touched last century.

One of our own and others' criticisms of our code base was that, over
twenty years, it had had plenty time to fall victim to `big sticky
lump' syndrome, and one of our hopes at the beginning of this effort
was that reorganising the build system would resolve this problem.
This was over-optimistic.  Although we did become aware of a few
easily removed dependencies, in part because this was the first time
that the network of dependencies was expressed explicitly, doing this
on a larger scale would have required a more formal analysis we did
not have the resources to perform, identifying refactoring targets we
had not the resources to attack.

Although we did not set out to resolve our more tangled dependencies
We found dependencies more complicated than we expected, and found at
least four types in our code base, which had their effects at
different phases, and had to be handled in different ways.  We discuss
these in \prettyref{s:deps}.

In parallel with this porting project, we worked through the internal
bureaucracy involved in applying a GPL licence to the code where
possible.  This turned out to be harder than we expected, largely
because of the donations, in the past, of code written in gentler
times, with bizarre licences (such as `public domain, for academic use
only'!).  We ended up conceding defeat on this, as it would clearly be
a huge effort to work through all the contributed code, identifying
first authors then consistent conditions for each module.  We have
adopted the pragmatic policy of stating that everything in the
collection with an explicit project copyright is GPL (this includes
all of the build system); all of the rest is at least licensed for
academic use; users concerned about the remainder should get in touch
with us and we'll try to work it out.

Starlink is a well-run project with disciplined developers, but we
were surprised by the number of individually minor hurdles we had
accumulated over a decade, despite our best efforts at vigilance,
enough that working with
the code would have been daunting for a non-insider.  Our complete
build system overhaul was quite expensive, but I speculate that almost
any similar project-wide development would produce some of the same
benefits, as a by-product of scanning a large fraction of the
project's code from a new perspective.

The project took a \emph{lot} longer than we expected (surprise!).  It
took around six person-months to get the initial system up and
running, and then another six to adapt the bulk of our code to the new
system, followed by another six months of low-level tinkering to
remove the wrinkles and have the nightly builds produce relocatable
and useful distribution tarballs.

\section{Conclusions}

Though this autoconfing project consumed more resources than we
expected, it has produced worthwhile results.  The code is now in a
state where it is no longer buildable only by project initiates; this
is essential, since the project has now had its funding terminated.  A
few excess dependencies have been broken, though not as many as we had hoped.
The project toolset is now distributed in more manageable components,
even though the dependency issues mean these are still not as small as
we would like.  Finally, the codebase has been extensively preened, to the
extent that it is possibly now in a tidier state than it has been for
a decade.


\bibliographystyle{my-spe}
\bibliography{star-cvs}
%% \begin{references}

%% \reference  Cavanagh, B., Hirst, P., Jenness, T., Economou, F.,
%% Currie, M. J., Todd, S., \& Ryder, S. D. 2003, \adassxii, 237

%% \reference Currie, M.\ J.\ 2004, \adassxiii, 409
%% % paper ref P4.25

%% \reference Giaretta, D.\ L.\ et al.\ 2005, \adassxiv, \paperref{O7.3}

%% \reference Draper, P.\ W.\ et al.\ 2005, \adassxiv, \paperref{D12}

%% \reference Gray, Norman 2004, `OSX and the restFP/saveFP problem'.
%% Web: \htmladdURL{http://www\discretionary{.}{}{.}astro.gla.ac.uk/users/norman/note/2004/restFP/}
%% (cited November 2004)

%% \reference Wheeler, David A.\ 2001 (updated 2002), `More than a
%% Gigabuck: Estimating GNU/Linux's Size'.
%% Web: \htmladdURL{http://www.dwheeler.com/sloc\discretionary{/}{}{/}redhat71-v1\discretionary{/}{}{/}redhat71sloc.html}
%% (cited November 2004)

%% \end{references}



\end{document}
