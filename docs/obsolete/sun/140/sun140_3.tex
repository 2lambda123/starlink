\documentstyle[11pt]{article} 
\pagestyle{myheadings}

%------------------------------------------------------------------------------
\newcommand{\stardoccategory}  {Starlink User Note}
\newcommand{\stardocinitials}  {SUN}
\newcommand{\stardocnumber}    {140.3}
\newcommand{\stardocauthors}   {H.\ Meyerdierks}
\newcommand{\stardocdate}      {2 December 1993}
\newcommand{\stardoctitle}     {SPECDRE --- Spectroscopy Data Reduction}
%------------------------------------------------------------------------------

\newcommand{\stardocname}{\stardocinitials /\stardocnumber}
\renewcommand{\_}{{\tt\char'137}}     % re-centres the underscore
\markright{\stardocname}
\setlength{\textwidth}{160mm}
\setlength{\textheight}{230mm}
\setlength{\topmargin}{-2mm}
\setlength{\oddsidemargin}{0mm}
\setlength{\evensidemargin}{0mm}
\setlength{\parindent}{0mm}
\setlength{\parskip}{\medskipamount}
\setlength{\unitlength}{1mm}

%------------------------------------------------------------------------------
% Add any \newcommand or \newenvironment commands here
%------------------------------------------------------------------------------

\begin{document}
\thispagestyle{empty}
SCIENCE \& ENGINEERING RESEARCH COUNCIL \hfill \stardocname\\
RUTHERFORD APPLETON LABORATORY\\
{\large\bf Starlink Project\\}
{\large\bf \stardoccategory\ \stardocnumber}
\begin{flushright}
\stardocauthors\\
\stardocdate
\end{flushright}
\vspace{-4mm}
\rule{\textwidth}{0.5mm}
\vspace{5mm}
\begin{center}
{\Large\bf \stardoctitle}
\end{center}
\vspace{5mm}

%------------------------------------------------------------------------------
%  Cover picture.  This looks like three plots, but counting the inset
%  and another box in the lower plot it makes 5.  When the plot is
%  generated there are actually 11 GKS 7.2 PostScript files.  The first
%  five are picdef's and the seventh is probably from Pongo's plot
%  admin.
%  The plot was made on a VAX on the pscript_ptex device, LaTeX does not
%  seem to like PostScript version 2 input as you get from GKS 7.4 on
%  Unix boxes.  Each of the (relevant) GKS 7.2 files is included by
%  LaTeX individually.  I tried to "psmerge -e" them beforehand.  But
%  LaTeX tells me it cannot find the %%BoundingBox.  It is there, but
%  perhaps beyond the end of the encapsualtion?
%  The plot was made in AGI pictures chosen with picdef to extend from
%  the top left corner 75 per cent of the page width.  This square was
%  divided into a bottom half and two top quarters.
%  How the plot was made is desribed as an example in the Graphics
%  section of SUN/140.  With the exception that for a print file arcdisp
%  had to be changed not to call pgcurs and not to change colour to
%  green.  As it happens, the numeric labels of the top view ports of
%  the fittri and arcdisp plots did not look nice.  These were edited
%  out of the EPS files.  With it went the NDF name atop the fittri
%  plot.
%  For inclusion in LaTeX the EPS files have their origin in the top
%  left of their page.  Here we make the picture also to have the origin
%  in its top left corner.  The put commands apply slight offsets to
%  improve the alignment of the plots.  This is a cover page after all.
\begin{figure}[hb]
\setlength{\unitlength}{1mm}
\begin{picture}(160,150) (0,-150)
\put(8,-3.5){\special{include sun140_cover1.ps-tex}}
\put(8,0){\special{include sun140_cover2.ps-tex}}
\put(2,0){\special{include sun140_cover3.ps-tex}}
\put(2,0){\special{include sun140_cover4.ps-tex}}
\put(2,0){\special{include sun140_cover5.ps-tex}}
\end{picture}
\end{figure}
%------------------------------------------------------------------------------

\newpage

%------------------------------------------------------------------------------
%  Add this part if you want a table of contents
\setlength{\parskip}{0mm}
\tableofcontents
\setlength{\parskip}{\medskipamount}
\markright{\stardocname}
%------------------------------------------------------------------------------

\newpage

\section{Introduction}
\label{intro}

This document contains information on version 1.0 of the Specdre package
on various levels of detail. The user will want to read Section
\ref{getstart} and consult Sections \ref{classif} and \ref{applics},
which respectively give a classified list of commands and the command
documentation in alphabetical order. But the other sections are of
interest to the user, too.  Namely Sections \ref{specfit} and
\ref{axcalib} give examples of using several applications together.

Specdre is a package for spectroscopy data reduction and analysis. It
fills the gap between Figaro and KAPPA: On one hand all its routines
conform with Starlink's concept of bad values and variances, on the
other hand they offer spectroscopy applications hitherto available only
in Figaro. Specdre can in general work on data sets with seven or less
axes. Often an application will take just a one-dimensional subset as a
spectrum.

The main application of Specdre is cube (i.e.\ image, cube,
hyper-cube) manipulation, spectral fits, axis calibration, resampling of
spectra.

Specdre uses the NDF data access library, which allows to specify
sections rather than the whole data set. Also, for the special
requirements of spectroscopy data reduction and analysis, an extension
to the Starlink Data Format is used. This stores additional information
with the data and by this means allows much enhanced communication
between Specdre applications.

Specdre is available on all Starlink-supported platforms.


\goodbreak
\section{Getting started}
\label{getstart}

The way Specdre is started and run differs slightly between VMS and Unix
platforms. At any rate the Starlink login must have been executed before
Specdre can be started. Below we will use the prompts {\tt \$, ICL>, \%}
and the comment characters {\tt !, \{, \#} to indicate the different
environments, namely VMS/DCL, ICL, and C shell. There exists a
demonstration procedure, which you may want to have a close look at.  To
execute it, you will need an Xwindows display, and on a VAX you will
have to start ICL first.  The demonstration is intended more as a test
of the installation. The script itself can give you hints about writing
your own ICL procedure or C shell script, about the syntax for command
line parameters and the problem with Unix meta characters. And it
demonstrates how the Specdre Extension is used to e.g.\ gather fit
results and pass them on to other applications.

\begin{verbatim}
   ICL> specdre                           { to define Specdre commands
   ICL> load specdre_dir:demo             { to run the demo

   % specdre                              # to define Specdre commands
   % source /star/bin/specdre/demo.csh    # to run the demo
\end{verbatim}

The transfer of Starlink Data Files between different machines is very simple.
The disks may be cross-mounted via {\tt nfs}. In this case the ``foreign'' disk
looks like a normal directory. Any {\tt .sdf} files can be read and written by
any machine. If there is no such close connection between the machines, you
will have to transfer the data with {\tt ftp}. Make sure that you transfer the
files as ``binary''. The transferred files can be used directly.

Note that this data portability is a feature of HDS files, which include
Starlink Data Files ({\tt .sdf}) and Figaro's {\tt .dst} files. Other binary
files -- especially if they contain four- or eight-byte floating-point data --
cannot be used directly on another machine architecture.

\goodbreak
\subsection{Running Specdre on a VAX}

On VMS, Specdre is run as a monolith from the interactive command
language ICL.  To invoke it you will have to initialise ADAM, start ICL,
and initialise Specdre from ICL. (You can exit with the ICL command {\tt
exit}.) There are three alternative ways to achieve this:

\begin{verbatim}
   $ adamstart
   $ icl                      $ adam
   ICL> specdre               ICL> specdre               $ adam specdre
\end{verbatim}

Once in ICL, you can initialise further packages, like KAPPA, CCDPack or
Figaro:

\begin{verbatim}
   ICL> kappa
   ICL> ccdpack
   ICL> figaro
\end{verbatim}

Any of these initialisations may write a message to the terminal that
some ``key has been redefined''. This indicates that the same command
name is used in different packages and probably for quite different
purposes. The latest initialisation overrides previous ones, and it may
be important to initialise several packages in the correct order.

\goodbreak
\subsection{Running Specdre on a Unix machine}

On a Unix machine Specdre is initialised simply by typing ``specdre''.
Note that Unix distinguishes between lower and upper case.

\begin{verbatim}
   % specdre
\end{verbatim}

This makes the commands accessible under two names each. The first is the name
of the application as in Section \ref{classif} or \ref{applics}. In case that
there is a conflict with another command of the same name, the Specdre command
can also be used with a second name:

\begin{verbatim}
   % fitgauss
   % spe_fitgauss    # This has the same effect, but is less ambiguous
\end{verbatim}

\goodbreak
\subsection{On-line help}

\begin{verbatim}
   ICL> help specdre
   % spe_help specdre
\end{verbatim}

will issue a brief description of the package. If you enter a {\tt ?} in
reply to the {\tt Topic?} prompt, you will get a list of topics
available. These are mainly the commands, but there are some with more
general information.

\begin{verbatim}
   ICL> help specdre
   Topic? classified *

   % spe_help specdre 
   Topic? classified *
\end{verbatim}

will give a classified list of all Specdre commands available. Help on any
command is obtained with

\begin{verbatim}
   ICL> help <command>    { <> always encloses something you have to replace
   % spe_help <command>
\end{verbatim}

and by inspecting the sub-topics offered.

The usual approach of the user to new software is, however, to run it
and see what happens. Thus when you are prompted for a parameter and
don't know what to enter, you can try {\tt ?}.  Often this will offer
more help than the parameter prompt.

\goodbreak
\subsection{Response to parameter prompts}

All applications will need further information, which they try to get from the
{\it parameter system}. For some parameters the application may be happy to get
some default value from the parameter system without the user knowing. Other
parameters will be prompted for. The prompt often offers a more or less
suitable default. Usually you will enter the proper value, but
there are some other answers to a prompt that make surprisingly much sense.

\begin{verbatim}
   PARNAME - Prompt text /Default/ > 
   PARNAME - Prompt text /Default/ > <Tab>       { only under ICL
   PARNAME - Prompt text /Default/ > !!
   PARNAME - Prompt text /Default/ > !
\end{verbatim}

By just hitting the {\tt Return}, the default value is used for the parameter.
{\tt <Tab>} will remove {\tt /Default/} from its previous position and put {\tt
Default} into your typing area. Thus you can edit the default value to match
your needs. {\tt !!} should cause the command to abort. {\tt !} assigns the
null value instead of the default value. The null value often will make no
sense and cause the application to abort later on. But sometimes it is a
meaningful response, a typical case being that you are asked for an output
file name but do not want an output file. The instances where a null value
makes sense are indicated as such in the parameter sub-topics of the on-line
help. For most cases where a null value makes sense it is the default value
anyway, and you will see {\tt /!/} in the prompt.

\goodbreak
\subsection{Parameters on the command line}

You need not wait for the application to prompt for a value but can set
parameter values on the command line. Actually some parameters are not prompted
for, they take default values unless you specify them on the command line.
There are also special keywords for the command line: {\tt accept, prompt,
reset}.

\begin{verbatim}
   ICL> <command> accept    { use default values rather than prompt
   ICL> <command> \         { backslash can be used instead of "accept"
   ICL> <command> prompt    { prompt for all parameters including defaulted
\end{verbatim}

Some parameters are assigned positions on the command line, but all parameters
can be specified by their name. In the following example three
parameters are specified on the command line, all others default.

\begin{verbatim}
   ICL> fitgauss in=myfile device=xw logfil=mylist accept=true
\end{verbatim}

Some parameters may be tricky to specify on the command line, namely strings
that contain spaces, vectors, and NDF sections. The syntax on the command line
is more strict in these cases. And on Unix systems it is more obscure, because
one must take precautions against the shell interpreting meta characters.

\begin{verbatim}
   ICL> <command> <stringpar>="word1 word2"
   ICL> <command> <vectorpar>=[1,2,3]
   ICL> <command> <NDFpar>=ndf(1:5,15.0~10.0)

   % <command> <stringpar>=\"word1 word2\"
   % <command> <vectorpar>=\[1,2,3\]
   % <command> <NDFpar>=ndf\(1:5,15.0~10.0\)
\end{verbatim}

If a command cannot be completed on one line, it can be split into several. The
split character -- the last character before the splitting {\tt Return} -- is a
tilde {\tt \~\ }in ICL, and a backslash $\backslash$ in Unix.

You can find real-life examples in the demonstration scripts.

\goodbreak
\subsection{Scripts and procedures}

One advantage of command line parameters is that sometimes you need not wait at
the terminal for the last prompt, which might come only after lengthy
calculations. But more importantly you can edit commands into ICL procedures or
shell scripts. This is exactly what was done for the demonstration. The demo,
however is simply a linear sequence of commands with static parameters. The ICL
language offers you a lot more in terms of flow control, modular procedures,
communication between application parameters and ICL parameters, etc. The ICL
language is documented in SG/5.

Similarly, if you are a C-shell guru you can write more sophisticated scripts
than {\tt demo.csh}. There is a complication, however. The demonstration is
executed with a {\tt source} command. This means that the script is taken as a
list of commands for the current shell, no child process is created to run the
script. Thus all current {\tt alias}es and environment variables are available
to the commands in the script. If you execute the script by using its name as
the command, then it is executed in a child process. That process is in general
ignorant of the {\tt alias}es set up in its parent. It has to do its own
Starlink login and Specdre initialisation.

\goodbreak
\subsection{Specdre's use of parameters}

The applications use the parameter system to get the necessary information
from the outside world. The source of information is not always the user's
keyboard. The specification of a parameter on the command line is slightly
different from entering the value at the prompt.

A good model to imagine the workings of the parameter system is as
follows.  The system is a pool of parameter values. On the command line
you can pass values to the parameter system (not the application). When
the application runs and needs a parameter value, it will ask the
parameter system (not the user terminal). For each parameter the system
has two sets of rules, one to arrive at a default value and one to work
out the value itself. If the value was specified on the command line,
the system will just pass that as the value to the application.
Otherwise the value is so far unknown to the parameter system and it
will construct a default value and a value according to the rules.
There are several possible sources for these two:

\begin{itemize}
\item the last used value of a global parameter (common to more than one
application),

\item the last used value (as stored on a per-application basis),

\item a dynamic default, set by the application at run-time,

\item a static default, set in the interface file,

\item response to a user prompt.
\end{itemize}

So asking the user is only one way of getting information from the parameter
system. You also see that the defaults offered -- or accepted by {\tt accept}
on the command line -- may be arrived at in a number of ways.

Some parameters used by Specdre are common to several commands. The {\tt
device} parameter is sometimes associated with the global parameter {\tt
GRAPHICS\_DEVICE}. When it is, it usually defaults.

Where {\tt in} and/or {\tt out} are NDFs, they are mostly associated with the
global {\tt DATA\_ARRAY}. The effect is that the default input is usually the
output of the previous command.

{\tt info} and {\tt dialog} are always associated with {\tt
SPECDRE\_INFO} and {\tt SPECDRE\_DIALOG}. These parameters control the
amount of information and user interaction of many applications. Once
{\tt info} is switched off all applications will become quiet until the
parameters are set true again.

{\tt varuse} is a defaulted parameter to many applications, but not associated
with a global parameter. By default it is true. Sometimes it has to be set
false in order to ignore variance information in the input data.

Other parameters like {\tt start, step, end} occur naturally in several
applications. In some instances they may be scalars, in others vectors.
Often their defaults are set by the application with knowledge of the data set
at hand.


\goodbreak
\section{Graphics}
\label{graphic}

The management of graphics output follows closely that of KAPPA. To make
full use of the graphics capabilities, you will need to use some KAPPA
commands; and you will find Pongo extremely useful to add almost
anything to your plots.  For normal use you can get along without
KAPPA or Pongo.
The actual plots are achieved through a combination of AGI, SGS,
PGPLOT, and GKS. The graphics is done with PGPLOT, but the device is
handled via AGI and SGS, and GKS is the low level package underlying
them all.  AGI stores information about the graphs produced in a data
base. This tells where on which device a plot was made and what its
world coordinates were.  Other packages use and update the same data
base so that a consistent display administration can be achieved, even
when different packages are used in turn.

Usually a command whose main task is to produce a plot,
has a parameter {\tt device} which is not
prompted for and which is associated with the global parameter {\tt
GRAPHICS\_DEVICE}. The value of this global parameter will be the name of a
graphics device, and the named device will be used for the display.
But you can always specify the {\tt device} parameter on the command line, thus
overriding the global parameter:

\begin{verbatim}
   ICL> specplot device=graphon
   ICL> specplot device=xw
   ICL> specplot device=ps_l
\end{verbatim}

This will also change the global parameter so that next time you use the same
device automatically.  (If you use KAPPA commands like {\tt display} the
change is not permanent and affects only the one invokation of that command.)

There are other Specdre commands that may or may not use a graphics device.
These will prompt for the {\tt device} parameter and offer the null value as
default. This can be accepted to avoid using a graphics device. When specifying
parameters on the command line, {\tt device=!} may not always work, but
including the {\tt accept} keyword will have the desired effect.

Some applications not only display graphics, but use a graphics display
plus mouse and keyboard to conduct a dialogue with the user.  This is
usually optional and controlled by the {\tt dialog} parameter.  {\tt
dialog} is a character parameter.  It is always allowed to be one of the
letters {\tt\{Y,N,T,F,y,n,t,f\}}.  Sometimes it may also be {\tt G} or
{\tt g} for graphics dialogue.  {\tt y,t} may or may not mean the same
as {\tt g}.

If you specify a printer or PostScript device, this may fail for the
graphics dialogue case.  But otherwise all plots can be sent to files
that you print later.  There is one important difference, you have only
one screen, but the printer has many sheets of paper.  Your plot may be
in a number of printer files and each printout starts on a new page.  If
you have done overlay plots on the screen and want the same on the
printer, then you can use as graphics device an Encapsulated PostScript
device like {\tt pscript\_ptex} or {\tt epsf\_l}.  You still get a number
of files, but they can be merged into one (without form feed) using {\tt
psmerge} (cf.\ SUN/164).  Usually the output is a complete PostScript
file ready to be printed.

As an example, here is the ICL procedure that created the plot on the
cover page.  It uses commands from KAPPA, Specdre and Pongo.

\goodbreak
\small
\begin{verbatim}
proc sun140cover dev
{
gdclear device=(dev)
{
picdef device=(dev) mode=xy current=n outline=n lbound=[0,.699] ubound=[0.75,1.449]
piclabel label=square device=(dev)
picdef device=(dev) mode=xy current=n outline=n lbound=[0,1.074] ubound=[0.375,1.449]
piclabel label=left device=(dev)
picdef device=(dev) mode=xy current=n outline=n lbound=[.375,1.074] ubound=[.75,1.449]
piclabel label=right device=(dev)
picdef device=(dev) mode=xy current=n outline=n lbound=[0,0.699] ubound=[0.75,1.074]
piclabel label=bottom device=(dev)
{
picsel bottom
specplot reset clear=n lin=n bin=y in=cover3 roman=y height=0.5 text=++++ northo=4 ~
   bottom="wavelength [\gmm]" left="Object-Dark" ~
   top="Spectrum after wavelength calibration" ~
   right="\fiSpecdre/specplot for SUN/140" ~
   world=[1.85,2.27,-1000,23000] device=(dev) accept
begplot device=(dev) overlay=y
   world given 1.85 2.27 -1000 23000
   move 2 3000
   draw 2 -200
   draw 2.06 -200
   draw 2.06 3000
   draw 2 3000
   move 2 -200
   draw 1.92 5000
   move 2.06 -200
   draw 2.08 5000
endplot
{
picdef device=(dev) mode=xy current=no outline=n lbound=[0.2,0.85] ubound=[0.45,1]
piclabel label=inset device=(dev)
picsel inset
specplot reset clear=n lin=n error=n mark=5 height=0.25 in=cover3 tick=0000 ~
   numl=0000 text=0000 device=(dev) labspc=[0,0,0,0] world=[2,2.06,-200,3000] accept
{
picsel left
fittri varuse=f dialog=n in=cover1 device=(dev) mask1=0 mask2=40 ncomp=3 cont=0 ~
   centre=[7,16,22] peak=[2000,5000,7000] fwhm=[5,5,5] ~
   cf=[0,0,0] pf=[0,0,0] wf=[0,0,0] comp=[1,2,3] accept
{
picsel right
arcdisp dialog=g in=cover2 order=2 fdb=cgs4_kr_mic device=(dev) wrange=[1.8,2.4]
{
endproc
\end{verbatim}
\normalsize

When run with {\tt sun140cover pscript\_ptex} this results in 11
Encapsulated PostScript files.  The actual plots are
in the 6th, 7th, and 9th to 11th file.  They can be merged into a
complete PostScript file and sent to the printer:

\goodbreak
\begin{verbatim}
   $ psmerge/output=merged.ps GKS_72.PS;6 GKS_72.PS;7 GKS_72.PS;9 -
        GKS_72.PS;10 GKS_72.PS;11
   $ print/queue=sys_ps/form=post merged.ps        ! site-dependent

   % psmerge gks_72.ps.6 gks_72.ps.7 gks_72.ps.9 gks_72.ps.10 \
        gks_72.ps.11 > merged.ps
   % lpr -Psys_ps merged.ps                        # site-dependent
\end{verbatim}


\goodbreak
\section{Slicing data sets}
\label{slice}

Specdre uses the NDF routines to access data in Starlink Data Files (SDF).
This allows the user to specify sections of NDFs rather than whole (i.e.\ base)
NDFs. Some applications need one-dimensional input data, but by themselves
offer no means to take a subset of a larger or multi-dimensional data set. The
desired effect can, however, be achieved by NDF. As an example, {\tt fitgauss}
will fit a spectrum only and rejects two-dimensional input. You still can use a
2-D data set by specifying a section thereof as input to {\tt fitgauss}:

\begin{verbatim}
   ICL> fitgauss in=myndf(,5)               { 5-th row of 2-D input
   ICL> fitgauss in=myndf(5,)               { 5-th column
   ICL> fitgauss in=myndf(20:30,5)          { pixels 20 to 30 of 5-th row
   ICL> fitgauss in=myndf(6540.0:,5)        { pixels beyond coordinate 6540.0
   ICL> fitgauss in=myndf(6450.0~10.0,5)    { coordinate range 6540.0 +- 5.0
\end{verbatim}

(For {\tt fitgauss} the sub-setting within the row is not necessary, because it
does its own masking of the given 1-D data.)

It should be mentioned here that the NDF fed into an application need not be at
the top level of its container file. Once your NDF got a Specdre Extension
(Section \ref{useext}) with fit results in it, you can e.g.\ plot the second
fit parameter versus the row number:

\begin{verbatim}
   ICL> specplot in=myndf.more.specdre.results(2,1,)
\end{verbatim}


\goodbreak
\section{Using the Specdre Extension}
\label{useext}

The NDF data access routines -- together will the lower-level HDS routines --
allow the construction of extensions to the data format. Extensions unknown to
an application package are propagated, and extensions known to an application
enable it to provide more functionality.

The Specdre Extension is recognised by all Specdre applications, and it
will be propagated uncritically by other packages when they get their
hands on Specdre output files. In principle this may not be the right
thing to do, but is the best that can be done. If the data never
re-enter Specdre, it doesn't matter. If they do, Specdre will notice if
something has become obviously inconsistent.

The demonstration script also shows the use of the Specdre Extension. There is
a tool {\tt editext} to look at or manipulate the Extension. {\tt resample}
will usually store some limited amount of information about the interdependence
of post-resample pixels in the Extension. If you try to {\tt fitgauss} such
resampled data, that application will pick up and use the information in the
Extension.

{\tt fitgauss} or {\tt fitcheby} will store their fit results in the
Extension. In conjunction with the NDF sections (Section \ref{slice})
you can work your way through the rows of a CCD long slit spectrum,
store each row's fit in the Extension and fill the result storage space.
The collection of all fits is again an NDF (located in the Extension)
and can be fed into other applications like {\tt specplot}, {\tt
ascout}, or indeed KAPPA's {\tt display}.

The Extension also provides a place to store a full-size array for the
wavelength or frequency etc.\ of each pixel.  This array of
spectroscopic values may be created by {\tt grow}, and is certainly used
by {\tt arcdisp} to apply individual dispersion curves for the rows of
an image.  The image may be a long slit spectrum, or the rows may be
extracted fibre spectra or echelle orders.

There is some potential for confusion here.  You may tend to count
pixels in your data starting at 1, you may use NDF pixel indices, NDF
pixel coordinates, a vector of pixel centre values, or an N-dimensional
array of spectroscopic values.

\begin{itemize}
\item {\it Pixel indices} along an axis in an NDF are counted from the
   {\it lower bound} to the {\it upper bound}.  The lower bound is also
   known as the {\it origin}.  It is not necessarily 1.  Say, if an NDF
   has bounds $1 ... N$, a section of it may have bounds $5 ... (N-6)$, or
   $-5 ... (N+6)$ etc.  If the section is made permanent in a new file, it
   will still have origin other than 1.  Usually it is these pixel
   numbers -- or the section bounds -- that are used to specify NDF
   sections in parameter values like {\tt ndf(-25:32,4:)}.

\item The centres of pixels in an NDF have {\it pixel coordinates} that
   are the pixel number minus 0.5.  So if the bounds are $5 ... 95$ then
   the pixels have centre coordinates $4.5 ... 94.5$.  The beauty of
   these coordinates is that, if you give each pixel an extent of $\pm
   0.5$ on either side, then the sequence of pixels $1 .... N$ cover the
   coordinate range $[0;N]$.  These are only default coordinates, but
   they are worth mentioning because they are similar to the pixel
   numbers, and because they are not equal to the numbers.

\item Each axis of the NDF may have an explicit vector with the
   coordinates of the pixel {\it centres}.  These can run non-linear,
   backwards, or in loops.  You may find that some software cannot cope
   with the weirder of these options.  These {\it centres} can be used
   to specify NDF sections.  You simply give a floating point bound
   instead of an integer bound.  Note that this works only because each
   axis has a {\it vector} of pixel centres as long as the NDF axis, not
   an N-dimensional {\it array} of pixel centres.

\item For Specdre it is useful to have {\it in addition} an
   N-dimensional array where for example a wavelength calibration can be
   stored for each row of an image or a cube individually.  For this
   purpose there may exist an array of {\it spectroscopic values} in the
   Specdre Extension to the main NDF.  That array is an NDF with the
   same bounds as the main NDF.  Where the main NDF stores the count
   rate, brightness etc.\ of a pixel, the spectroscopic values' NDF
   stores the wavelength, frequency, radial velocity etc.\ for that
   pixel.  This information is recognised only by Specdre.  You cannot
   expect KAPPA's {\tt linplot} to use it for axis labelling, Specdre's
   {\tt specplot} does use it of course.  You also cannot use this array
   to specify an NDF section in a parameter value.

\item There is no rule as to what happens to the {\it centres} of the
   spectroscopic axis when N-D {\it spectroscopic values} are created or
   modified.  Both arrays may lead rather independent lives.
\end{itemize}


\goodbreak
\section{Cube manipulation}
\label{cubeman}

A spectrum may be thought of as a one-dimensional data set.  But
spectroscopists are also aware of the two-dimensional space of sky
position and might use time as an axis in data sets.  So the data
handling aspects of spectroscopy are more demanding than image
processing -- Specdre applications may encounter data with any
dimensionality.  Two-dimensional detectors are often used to take
spectra and where observations are not very time-consuming
three-dimensional data sets may be quite common.

Specdre can work on data with any dimensionality.  The limit is 7-D due
to the HDS data access routines.  In practice the limit may be 6-D since
one structure in the Specdre Extension has one dimension more than the
main data array.

However, Specdre is about spectra.  An N-D cube is taken as a set of
spectra, each spectrum is a row or a column in the cube.  A {\it row}
extends along the {\it first} axis of the cube, while a {\it column}
extends along {\it any} axis of the cube.  In any cube Specdre assumes
that there is exactly one {\it spectroscopic axis}, by default this is
the first axis.  The Specdre Extension specifies which axis is the
spectroscopic one.

Specdre's handling of N-D data falls into three categories.

\begin{itemize}
\item Some applications work on one spectrum at a time.  They will
   insist on 1-D input, but you can specify a column to work on.  Often
   the column must extend along the spectroscopic axis.  These
   applications can be used successively on several or all columns of a
   cube.  Insofar as they produce output will they work ``in situ'' and
   modify the input file.

\item Other applications take on a whole cube at once and work on each
   row in turn.  Usually spectra have to be in rows and the
   spectroscopic axis must be the first axis.  These applications may
   also refuse to work on only a section of the input.

\item Then there are applications that don't deal with spectra at all
   but are tools to manipulate cubes.  It is important that Specdre has
   such a set of tools, since the Specdre Extension may have a number of
   cubes in it as well.  The main and extension cubes must be
   manipulated in a consistent manner, or the Extension becomes invalid.

\end{itemize}

{\tt subset} is very similar to KAPPA's {\tt ndfcopy} or to taking an
``on the fly'' section as input to an application.  The differences are
that {\tt subset} also takes subsets of NDFs in the Specdre
Extension consistent with the subset of the main NDF,
and that it removes ``degenerate'' axes.  Consider the command

\begin{verbatim}
   ICL> subset image(5,1:10) row
\end{verbatim}

When {\tt subset} gets the input it is still an image of size 1 by
10.  But in the output the degenerate axis has been removed so that it
is genuinely 1-D.

Where {\tt subset} may remove axes, {\tt grow} deliberately adds new
axes -- degenerate or genuine ones.  So we could reverse the command
above with

\begin{verbatim}
   ICL> grow row expand=[1,0] stapix=[1,0] endpix=[1,0] size=[1,0] ~
           new=y out=image
\end{verbatim}

The zeros as second vector elements just show that the second axis of
{\tt image} matches the axis in {\tt row}.  {\tt expand} shows which
of the output axes are and are not in the input.  Normally those new
axes will of course not be degenerate, so {\tt size} might be {\tt
[5,0]}.  In that case {\tt row} could be copied into any of the
output columns, into one of them or repeatedly into a whole range of
columns.  The main idea of {\tt grow} is that you assemble rows into
images, images into cubes etc.  So when {\tt new=n} then you will
copy into an existing file.  The following puts two spectra and one
image into a common cube.  Whatever part of the cube is not copied to,
remains filled with bad values.

\begin{verbatim}
   ICL> grow row5_1 expand=[0,1,1] stapix=[0,5,1] endpix=[0,5,1] ~
           size=[0,5,3] new=y out=cube_x_by_5_by_3
   ICL> grow plane2 expand=[0,0,1] stapix=[0,0,2] endpix=[0,0,2] ~
           new=n out=cube_x_by_5_by_3
   ICL> grow row3_to_5_3 expand=[0,1,1] stapix=[0,3,3] endpix=[0,5,3] ~
           new=n out=cube_x_by_5_by_3
\end{verbatim}

%   +-----------+
%   |     3 3 3 |
%   | 2 2 2 2 2 |
%   |         1 |
%   +-----------+

If {\tt grow} creates and expands new axes, {\tt xtract} collapses
existing axes.  This is done by averaging the pixels along each
collapsed axis.  Note that this is an average and not a sum, which
makes a big difference for the use of input variances and the meaning
of output variances.  Basically an average assumes that all values
entering the mean are equal and scatter at random.

{\tt grow} copies input into output of higher dimensionality, the
common dimensions must match.  {\tt fillcube} is different.  It
copies input into output of the same dimensionality.  Dimensions need
not match, the copy is positioned in output by matching {\it centre}
coordinates.  Indeed the copy may not be contiguous in output.  The
output is an existing file, so you can fill it successively from
different input files.  This is mosaicing in N-D.

{\tt resample}, too, plays a r\^ole in cube manipulation, since it
can homogenise and linearise the coordinates along the spectroscopic
axis.  When used in {\tt mode=Cube} it resamples each row of a cube
individually.  Afterwards all rows have the same linear coordinate
grid as expressed by the new vector of {\it centres} for the
spectroscopic axis.  Any spectroscopic values in the Specdre
Extension are thus obsolete and removed.  Sometimes it is necessary
for other applications that grids are linear or that there is no
array of spectroscopic values in the Specdre Extension.


\goodbreak
\section{Spectral fits}
\label{specfit}

Specdre has four applications to fit analytical functions to spectral
features. Two are line fit routines for Gauss and triangular profiles.
These can fit up to six components at a time. The lines can be blended
and the line parameters can be free, fixed, or tied to the corresponding
(free) parameters of another component. A similar routine fits up to six
diluted Planck curves.  Finally, a polynomial fit can be performed. The
fit parameters in {\tt fitcheby} are not the common coefficients of a
straightforward polynomial, but those of a series of Chebyshev
polynomials. The order can be up to 7.

The fit routines can run with a (non-graphic) dialogue or not, they can
display graphically data and fit at different stages of the fitting
process (masking, guessing, fit residuals).

All these fit routines work on one-dimensional data only. But you can
pass an NDF section that is (part of) a row or column in an image or
cube. For the fit only data inside the union of up to six masking
intervals are used. The fit results go first of all to the standard
output (the terminal), but can also be recorded in (appended to) an
ASCII file. In addition fit results will always be stored in a results'
NDF in the Specdre Extension. Those results can be used to generate a
model data set. You could then subtract that from the original data.

Here is a complex example how you might proceed with a long slit
spectrum, it won't work like that since parameters are missing. Also the
telluric correction may not be the correct way of doing things. The
example only shows how you can juggle with Specdre/KAPPA applications.

\begin{verbatim}
   ICL> fitcheby image(,1) mask1=[1,3,5] mask2=[2,4,6] comp=1             { 1)
   ICL> fitcheby image(,2) mask1=[1,3,5] mask2=[2,4,6] comp=1
        ...
   ICL> fitcheby image(,9) mask1=[1,3,5] mask2=[2,4,6] comp=1
   ICL> evalfit  image conti comp=1                                       { 2)
   ICL> div      image conti normal                                       { 3)
   ICL> fitgauss normal(,1) mask1=[2,4] mask2=[3,5] ncomp=3 comp=[2,3,4]  { 4)
   ICL> fitgauss normal(,2) mask1=[2,4] mask2=[3,5] ncomp=3 comp=[2,3,4]
        ...
   ICL> fitgauss normal(,9) mask1=[2,4] mask2=[3,5] ncomp=3 comp=[2,3,4]
   ICL> evalfit  normal tellur1 comp=3                                    { 5)
   ICL> sub      normal tellur1 stellar1
   ICL> cadd     tellur1 1.0 tellur2                                      { 6)
   ICL> div      normal tellur2 stellar2
\end{verbatim}

\begin{itemize}
\item[1)] For each row of the image use the abscissa ranges
   $[1;2]\cup[3;4]\cup[5;6]$ to fit a polynomial continuum. The two
   intervals excluded here probably contain spectral lines. For each row
   the parameters describing the fit are stored as the first component
   in the results' NDF in the Specdre Extension of {\tt image}. You
   would probably run the fit routine with dialogue and graphics the
   first time round to play with the mask intervals.

\item[2)] For the whole image use the stored result to generate a
   corresponding data set representing the fitted continuum.

\item[3)] Use a KAPPA command to divide the original image by the
   continuum.

\item[4)] For each row of the normalised image use the intervening
   abscissa ranges and fit three Gauss profiles to them. These are stored
   as components 2, 3 and 4 in the Extension. Again you would use
   dialogue to play with masks and parameter guesses in the first use of
   {\tt fitgauss}.  For subsequent image rows you would try without
   dialogue and just recall the command line to edit the row number.

\item[5)] The second line fitted, the third component stored, is not
   stellar but terrestrial, subtract it with the KAPPA command {\tt sub}.

\item[6)] Use KAPPA's {\tt cadd} to turn the simple telluric lines into
   proper telluric spectra with continuum at 1.0. Then divide the
   normalised spectra by the telluric spectra.

\end{itemize}


\goodbreak
\section{Arc spectrum axis calibration}
\label{axcalib}

There are four applications to do a wavelength calibration, well a
calibration in spectroscopic values anyway.  You can use frequencies or
photon energies if you feel like it.  You can also use nm, $\mu$m or
\AA\ as you please.

At the heart of this axis calibration is an algorithm written by Dave
Mills of UCL (1992) to automatically identify features in an arc
spectrum.  For this to work, you must have a data base of arc feature
identifications rather than just a simple line list.  You can use {\tt
arcgendb} to convert a line list -- as distributed with Figaro -- into a
``feature data base''.  Unfortunately the data base takes some time to
build, but it is also rather big, 10 to 100 times bigger than the simple
list.  So there may be a point in taking only the relevant wavelength
range from the 3500-lines Th-Ar list and convert it into a feature data
base.

With such a data base at your disposal, you still cannot run the
auto-identifier.  This is because the calibration procedure as performed
by Figaro's {\tt arc} is broken up into three steps:

\begin{itemize}
\item In the first step you go through the arc spectrum and locate
   features, that is you find out where in pixel coordinates the
   observed line features are.  For this first step you normally use
   {\tt arclocat}.  Usually this first step will be done in two passes.
   In the first pass you run {\tt arclocat dialog=f} so that it tries on
   its own to find un-blended narrow emission features in the arc
   spectrum.  In a second pass you run {\tt arclocat dialog=g} and use
   the graphic dialogue to improve the set of located features.  You may
   add features not found before, or delete features that are blended
   or known to be absent from the feature data base.  In general
   you should locate as many features as possible, you can always leave
   some un-identified.

   Instead of {\tt arclocat} you can also use {\tt fitgauss} or {\tt
   fittri}, in case that the simplified line fit in {\tt arclocat} is
   not good enough.  {\tt arclocat} has modes {\tt Gauss} and {\tt
   triangle}, too, but it fits only one line at a time.

\item Only in the second step do you auto-identify the features, that is
   the Mills algorithm will try to establish for some of the located
   features what their identification might be in terms of wavelength,
   frequency or whatever you use in the feature data base.  This second
   step is performed by {\tt arcident}.  The result must be regarded
   with some scepticism, since there is a small chance that it finds a
   grossly wrong solution or makes a slightly unfavourable selection of
   features to identify.  All this can be corrected in the third step.

\item The third step again may or may not be in a graphics dialogue.
   {\tt arcdisp dialog=g} will display a plot of wavelength, frequency
   etc.\ versus pixel coordinate.  It does not show the spectrum.
   Instead vertical lines indicate unidentified (but located) features,
   horizontal lines indicate all identifications from the feature data
   base, and crosses indicate identified features.
   Finally the would-be dispersion curve is displayed.  You can
   now add or remove identifications with mouse and keyboard.

   The improvement of feature identification is one goal of {\tt
   arcdisp}.  The other is to do a polynomial fit to the
   pixel-wavelength relation and to convert pixel coordinates into
   wavelengths using that fit.  This latter goal can also be achieved
   without the graphics dialogue with {\tt arcdisp dialog=n}.  This does
   not replace the {\it pixel coordinates} of the main NDF, but creates
   a new array of {\it spectroscopic values} in the Specdre Extension.

\end{itemize}

{\tt arclocat, arcident, arcdisp} in general work not on a spectrum, but
on an image or cube that has spectra in its rows (rows, not columns).
{\tt arclocat dialog=g} allows you to switch from one row to another as
you please, {\tt arclocat dialog=n} will scan through all rows in
sequence.  {\tt arcident} will do an independent auto-identification on
each spectrum, so it is suitable for a collapsed echellogram where
successive extracted orders are in the rows of an image.

{\tt arcdisp dialog=g} will work on each row in sequence, the user can
work on one row and proceed to the next in her own time.  She can quit
at any time; this will leave the file without spectroscopic values, but
any improved feature identifications are kept.  If the user steps
through all rows, then the spectroscopic values will be kept as well.
(You can have spectroscopic values in the Specdre Extension either for
all rows or none at all!).  You may want to set the label and unit for
the spectroscopic values after {\tt arcdisp} with KAPPA's {\tt setlabel,
setunits}.

So far you have only succeeded in producing an array of calibrated
spectroscopic values in the Specdre Extension of the {\it arc spectrum}.
You will want to copy that array into the celestial observation you are
actually interested in.  This can be done with {\tt ndfcopy}, provided
the sky spectrum does have a Specdre Extension and does not have a {\tt
SPECVALS} component in it.  That status can be achieved by two {\tt
editext} commands.  Finally you may want to resample each spectrum (row
in a cube) so that all use the same linear grid of spectroscopic values.
To that end you use {\tt resample mode=cube}.  Here is the whole
procedure.  (The commands are not complete, parameters are missing.  Also
there may be no Th-Ar line lists available for photon energies in the
MeV range.)

\begin{verbatim}
   ICL> arcgendb thar.arc thar_arc
   ICL> arclocat dialog=n arc1
   ICL> arclocat dialog=g arc1
   ICL> arcident arc1 arc2 thar_arc
   ICL> arcdisp arc2
   ICL> setlabel arc2.more.specdre.specvals "particle energy"
   ICL> setunits arc2.more.specdre.specvals "log10(10**9*eV)"
   ICL> editext "create" sky1
   ICL> editext "delete specvals" sky1
   ICL> ndfcopy arc2.more.specdre.specvals sky1.more.specdre.specvals
   ICL> resample cube sky1 sky2
\end{verbatim}


\goodbreak
\section{Hickups}
\label{hickup}

This section deals with some commonly encountered problems. When using ICL or
application packages from ICL, you may hit limits of computing resources. Since
each package is kept hibernating in its own subprocess, you cannot run as many
packages simultaneously as you like. You can get a list of ``tasks'' running
and you can stop tasks to get some breathing space again:

\begin{verbatim}
   ICL> tasks                       { list of tasks running
   ICL> kill specdre_dir:specdre    { kill Specdre package
   ICL> kill kappa_dir:kappa        { kill KAPPA package
\end{verbatim}

This does not de-initialise the packages. When you issue another Specdre
command, ICL will (try to) re-load Specdre. If you find that the limits imposed
on you are too tight, you might persuade your site manager to modify your
quotas (the computing resource limits).

If you hit disk quota problems on a VAX, it may help to redefine the logical
names {\tt HDS\_SCRATCH} or {\tt SYS\$SCRATCH}. By default these point to the
working directory and the login directory respectively. The definition must be
done in the job table, since ICL runs a number of processes and an ordinary
{\tt define} affects only the process where it is issued.

\begin{verbatim}
   $ define/job hds_scratch <your_scratch_space>        ! Before starting ICL
   $ define/job sys$scratch <your_scratch_space>        !   or
   ICL> $define/job hds_scratch <your_scratch_space>    { After starting ICL
   ICL> $define/job sys$scratch <your_scratch_space>
\end{verbatim}

Often unnoticed, some files are kept in {\tt ADAM\_USER} and {\tt
AGI\_USER}. On a VAX these are logical names that point by default to
the directory {\tt [.ADAM]} below {\tt SYS\$LOGIN}. On a Unix machine
they are environment variables, and by default {\tt \$HOME/adam} and
{\tt \$HOME} are used respectively.

{\tt ADAM\_USER} contains the file {\tt GLOBAL.sdf} to hold global parameters
and similar files to hold application parameters. When run from ICL, {\tt
ADAM\_USER:specdre.sdf} is used, but on Unix each application creates a file
{\tt \$ADAM\_USER/<applic>.sdf}. In addition error report files may be created
in this directory. Except for {\tt GLOBAL.sdf} you may want to occasionally
delete all files in {\tt ADAM\_USER}.

{\tt AGI\_USER} contains the AGI data bases {\tt agi\_<hostname>.sdf} --
separate files for each node or host. These files may grow to
considerable size and often are not relevant over extended periods of
time.  You might just delete them occasionally.

If you want to use Figaro or another package using DSA data access
routines on Starlink Data Files (files with extension {\tt .sdf}), you
must choose the NDF data format, at least as the second or first
alternative:

\begin{verbatim}
   $ define/job figaro_formats "ndf"        ! Use NDF only
   $ define/job figaro_formats "ndf,dst"    ! Use preferably NDF
   $ define/job figaro_formats "dst,ndf"    ! Use NDF as second choice
\end{verbatim}


\goodbreak
\section{Related software}
\label{related}

Specdre is first of all related to Figaro.  Portable Figaro 3.1 is
described in SUN/86.  The current version on VAXs is version 3.0 and
described in a Miscellaneous User Document (MUD) by Keith Shortridge.
The user can apply Figaro and Specdre on the same data sets. What's
more, Specdre is basically inspired by the spectroscopy applications
available in Figaro. For that matter it is hoped that Specdre offers
improved functionality. But Specdre is not as complete a package as
Figaro.

On the other hand Specdre is installed and invoked much in the same way as
KAPPA. It can be run from ICL so that all the advantages of ICL -- like editing
ICL procedures, having to load the package executable only once and keeping
several packages loaded in separate sub-processes -- can benefit the use of
Specdre. ICL is described in SG/5.

Another advantage under ICL is that through AGI Specdre, KAPPA and Pongo
can share information about what has been plotted.  Pongo is a versatile
plotting programme to do anything beyond displaying an image or a
spectrum from binary data files.  It is documented in SUN/137.

Users should also note that part of the reduction traditionally done with
Figaro may be available in CCDPack, described in SUN/139. The combination of
KAPPA, CCDPack and Specdre has the advantage that all are well integrated.

The following list includes also other spectroscopy software items.

\begin{itemize}

\item KAPPA (SUN/95) is a kernel of applications for image processing,
   data visualisation with flexible control of the location and size of
   the pictures, and manipulation of NDF components.

\item CCDPACK (SUN/139) has been specifically produced to perform the
   initial processing of CCD data.

\item PONGO (SUN/137) is a set of applications for interactively
   plotting data. These applications are written to behave in a similar
   way to MONGO.

\item CONVERT (SUN/55) is a data-format-conversion package.

\item FIGARO (SUN/86) is a set of general astronomical data reduction
   programmes originally developed at Caltech and now supported by AAO
   and by Starlink. Figaro optionally supports Starlink Data Files.

\item SPECX (SUN/17) is a millimetre wave spectral reduction package.
   This is commonly used for spectra obtained e.g.\ with the James Clerk
   Maxwell Telescope. It uses its own data format. A format conversion
   is currently not available.

\item DIPSO (SUN/50) is a ``friendly spectrum analysis program''.  Under
   VMS it uses its own data format, a conversion to and from Starlink
   Data Files is available in the Convert package. Under Unix Dipso will
   use NDF format.

\item APIG (SUN/103) is a package to perform analysis of the
   interstellar absorption lines detected in the spectra of Galactic and
   extragalactic sources.

\item NDPROGS (SUN/19) is a spectral line data cube manipulation
   program. It uses Figaro data access routines.

\item TWODSPEC (SUN/16) contains some additions to Figaro. It uses
   Figaro data access routines, with extensions to the basic DST data
   structure.

\item CGS4DR (SUN/27) is used for the first stages of the reduction of
   data from UKIRT's Cooled Grating Spectrometer No.\ 4. It uses Figaro
   data access routines, with extensions to the basic DST data
   structure.

\item TAUCAL (SUN/147) is used for the first stages of the reduction of
   data from RGO's TAURUS Fabry-P\'erot interferometer. It uses Figaro
   data access routines.

\item SAM (SUN/149) is designed for extracting spectra from CCD data
   which have been processed by e.g.\ CCDPACK.

\item HRTS (SUN/138): NRL High Resolution Telescope and Spectrograph
   Package.

\end{itemize}


\goodbreak
\section{Spectroscopy with Starlink Data Files}
\label{spandsdf}

Starlink's Extensible N-Dimensional Data Format (NDF) is described in SGP/38,
but SUN/33 may be a more useful description of the data format and the NDF
routines used to access the data. NDF files are Starlink Data Files and can be
recognised by their file name extension {\tt .sdf}. This data format is used by
other Starlink packages like KAPPA and CCDPack, and it is one of the formats
supported by Figaro (version 3.0) and DSA-based packages.

Spectroscopy is basically a one-dimensional type of problem. But the inclusion
of one or two spatial axes and possibly a time axis make dealing with
multi-dimensional data sets necessary. Specdre applications will sometimes
not depend on the input data being one-dimensional. Where they do, the user can
specify a one-dimensional NDF section rather than a base NDF.

For spectroscopy the requirements regarding the (spectroscopic) axis of the
data sets are more specific -- but also more demanding -- than provided by the
NDF specification. Therefore on one hand, Specdre implements a reduced set of
axis and data labels and units, which can be generated and interpreted
according to the task of an application. On the other hand, Specdre uses its
own extension to the NDF format to store information specific to spectroscopy.


\begin{center}
\begin{tabular}{|l|l|l|}
\hline \hline
intensity           &Jy/sr                &$I_{\nu}$\\
\hline
intensity           &W/(m$**$2$*$eV$*$sr) &$I_E=I_{\nu}/h$\\
\hline
lambda intensity    &W/(m$**$3$*$sr)      &$I_{\lambda}=(c/\lambda^2)I_{\nu}$\\
                    &W/(m$**$2$*$micron$*$sr) & \\
                    &W/(m$**$2$*$Ang$*$sr) & \\
\hline
flux density        &Jy                   &$S_{\nu}=\int{d\Omega I_{\nu}}$\\
\hline
flux density        &W/(m$**$2$*$eV)      &$S_E=S_{\nu}/h$\\
\hline
lambda flux density &W/(m$**$3)           &$S_{\lambda}=(c/\lambda^2)S_{\nu}$\\
                    &W/(m$**$2$*$micron)  & \\
                    &W/(m$**$2$*$Ang)     & \\
\hline
AB magnitude        &                     &$I_{\nu}/{\rm mJy}=-0.4AB+6.56$\\
\hline
brightness temperature &K                 &$T_B=I_{\nu}c^2/(2k\nu^2)$\\
\hline
counts              &                     & \\
\hline
count rate          &1/s                  & \\
\hline \hline
frequency      &Hz                  &$\nu$\\
\hline
particle energy&eV                  &$E=h\nu$\\
\hline
wavelength     &m, micron, Angstrom&$\lambda=c/(n\nu)$\\
\hline
radial velocity&m/s, km/s           &$v=c(\nu_0^2-\nu^2)/(\nu_0^2+\nu^2)$\\
\hline
redshift       &                    &$z=(\nu_0-\nu)/\nu$\\ \hline \hline
\end{tabular}
\end{center}

\subsection{Units and labels}

Both the label and unit strings are usually of length 64. All unit strings may
have ``{\tt10**$n$*}'' in front of the strings permitted by the table.
($n$ is a one- or two-digit integer with an optional sign.) If the data or axis
data are logarithmic, this is indicated by inserting the string ``log10('' or
``ln('' for base 10 and base $e$ respectively at the beginning of the unit and
to add a ``)'' at the end. The label string remains unchanged.

Specdre supports only a limited set of data units. Their conversion is often
not just a scaling with a constant, but with a function of wavelength or
frequency. It seems worthwhile to use the label and unit character strings
supported by NDF (SGP/38, tables 8 and 9), and to set up rules for their
syntax. The table gives in the first column the supported label
strings. The second column lists possible unit strings, and the third column
gives a definition of the quantity. The use of magnitudes should be avoided.
The table gives the definition of the monochromatic (or $AB$) magnitude (Oke \&
Gunn 1983). $AB$ is equal to the $V$ magnitude for an object with a flat
spectrum.

The table also lists supported axis label and unit strings and the conversion
from frequency. Note that for most conversions an arbitrary constant for the
refractive index or rest-frame frequency is necessary. The relations for $v$
and $z$ use the accurate longitudinal Doppler effect.


\subsection{Axis values in spectroscopy}

The values along the axis could be just any numbers, but often they have a
special meaning to the spectroscopist. Frequency and radial velocity are not
equivalent sets of information, neither are frequency and wavelength. The
standard spectroscopic quantity should really
be the particle energy, but we will
actually use frequency as the standard quantity; it is related to particle
energy by one universal constant. All other permitted spectroscopic quantities
are derived from frequency.

It is instructive to consider the fate of a photon from emission to detection.
The first complication is introduced by the Doppler effect. The emitter will
move in the observer's frame of reference at some speed $V=\beta c$ in some
arbitrary direction. The emitter's rest frame $(t',x',y')$ and the observer's
rest frame $(t,x,y)$ are related through a Lorentz transformation. The
emitter's motion is along the $x'$-axis while the photon moves at an angle
$\theta$ with that axis and heads toward the observer. In the emitter's rest
frame the photon has the same frequency as observed in a laboratory
$\nu_{lab}$, the photon's oscillation period is $\Delta t'=1/\nu_{lab}$. In the
observer's rest frame that time interval is seen as $\Delta
t=\gamma/\nu_{lab}$. But the second oscillation begins when the emitter has
moved by $\Delta x=\gamma V/\nu_{lab}$ thus altering the arrival time at the
observer. Assuming that this move is short compared to the distance between
emitter and observer, the arrival delay of the second event is
$-\gamma\beta\cos(\theta)/\nu_{lab}$ and the observed frequency is
$$\nu_{obs}=\nu_{lab}{\sqrt{1-\beta^2}\over 1-\beta\cos(\theta)}$$
By introducing the velocity components parallel and perpendicular to the
photon's motion this can be separated into the longitudinal and the transversal
Doppler effect
$$\nu_{obs}^2=\nu_{lab}^2
   \left[{1+\beta_{\parallel}\over 1-\beta_{\parallel}}
         -\left({\beta_{\perp}\over 1-\beta_{\parallel}}\right)^2\right]$$
For non-relativistic velocities the transversal part can be neglected. If we
introduce the radial velocity of the source with respect to the observer
$v=-c\beta_{\parallel}$, then
$$\nu_{obs}=\nu_{lab}\sqrt{{c-v}\over{c+v}}$$

Due to the Doppler effect the observed frequency (or particle energy) depends
on the reference frame chosen. Usually this is not the actual rest frame of the
telescope but a heliocentric frame or the frame of the local standard of rest.
Information about the rest frame cannot be stored in the standard NDF structure
but can be included in an extension (c.f.\ Section \ref{extension}).

The observer may prefer to use wavelength rather than frequency. This is not
quite equivalent because the observed wavelength depends on the refractive
index of the medium in which the photon's wavelength is determined. A constant
refractive index can again be stored in an extension to the NDF. However, the
refractive index is a function of wavelength.
Between 0.2 $\mu$m and 0.7 $\mu$m the refractive index of air
is related to the vacuum wavelength $\lambda$ by (Lang 1980):
$$n = 1 + 6432.8\ 10^{-8}
        + {{2949810}\over{146\ 10^8 - (\lambda/\mu m)^{-2}}}
        + {{25540}\over{41\ 10^8 - (\lambda/\mu m)^{-2}}}$$
Often the problem with the refractive index is avoided due to the way in which
wavelength calibration is achieved.

The observer may choose the laboratory frequency of one observed transition and
may want to use radial velocities or redshifts instead of observed frequencies.
In fact any reference frequency can be chosen to define a relation between the
frequency axis and the velocity or redshift axis. The reference frequency can
again be stored in an extension to the NDF.


\goodbreak
\section{The Specdre Extension}
\label{extension}

As we have seen, the spectroscopic axis information is not quite complete with
only label and units in the axis structure. Another requirement most useful in
spectroscopy would be to store continuum and line fits along with the data. The
concept of extensions to an NDF provides a good means to store such
information. The design of Specdre's own extension is outlined here.

The Specdre Extension is the structure {\tt <myndf>.MORE.SPECDRE} and is
of type {\tt SPECDRE\_EXT}.
{\tt <myndf>} is the creation name of the main NDF, the one with the data, axes
etc. {\tt <myndf>.MORE} contains all the extensions to this main NDF. The
Specdre Extension may {\it contain} other NDFs, thus we have to distinguish
between the ``main NDF'' and the ``Extension NDFs''.
The items of the Extension are described below. All
top-level items are optional, but each is either complete or absent.

\begin{itemize}

\item{\tt .SPECAXIS} is an \_INTEGER scalar which defaults to 1. Its value is
the number of the spectroscopic axis. This is the axis along which the
one-dimensional ``spectroscopic subsets'' extend. It must be greater than or
equal to 1 and less than or equal to the number of axes in {\tt <myndf>}. A
change of $specaxis$ may render other components invalid as regards their
values or shapes.

\item{\tt .RESTFRAME} is a \_CHAR$*$32 scalar which defaults to ``unknown''.
Its value describes the rest frame used to express the observed frequency,
wavelength, radial velocity, or redshift.

\item{\tt .INDEXREFR} is a \_REAL or \_DOUBLE scalar which defaults to 1. Its
value is the index of refraction needed to convert frequency into wavelength
and vice versa.

\item{\tt .FREQREF} is a \_REAL or \_DOUBLE scalar which defaults to the bad
value. Its value is the reference frequency needed to convert between radial
velocity and redshift on the one hand and frequency on the other hand. The
value is evaluated together with {\tt .FREQUNIT}:
$$\nu_0=freqref*10^{frequnit}*{\rm Hz}$$

\item{\tt .FREQUNIT} is an \_INTEGER scalar which defaults to 0. Its value is
the common logarithm of the unit used for {\tt .FREQREF} divided by Hertz.
Note that this is neither a \_CHAR nor a \_REAL or \_DOUBLE.

\item{\tt .SPECVALS} is an NDF structure with data, label and units
components. {\tt .SPECVALS.\-DATA\_\-ARRAY} is a \_REAL or \_DOUBLE
array which has the same shape as {\tt <myndf>}.
It defaults to a multiple copy of the centre array of the
spectroscopic axis {\tt <myndf>.AXIS\-($specaxis$)\-.DATA\_\-ARRAY}.
This structure contains spectroscopic axis values (either
wavelength, or frequency, or velocity, or redshift) for each pixel of the data
cube. Labels and units must be stored with this data structure. Their default
values are copies from the spectroscopic axis
{\tt <myndf>.AXIS($specaxis$).LABEL, .UNITS} in
the main NDF or ``unknown'' if the axis has no label or unit.
A modification of spectroscopic values may render parts of {\tt
.RESULTS} invalid, but no rules are formulated in this respect.
This structure must not contain bad values.

\item{\tt .SPECWIDS} is an NDF structure with only a data component. {\tt
.SPECWIDS.DATA\_\-ARRAY} is a \_REAL or \_DOUBLE array which has the same shape
as {\tt <myndf>}. It defaults to (i) a derivative from {\tt .SPECVALS}
in the way prescribed by SUN/33 (Chapter 16), or (ii) to a multiple copy
of the width array 
of the spectroscopic axis. Like {\tt .SPECVALS} contains the multidimensional
array of spectroscopic values for the pixel centres, this array contains the
spectroscopic widths for the pixels. Labels and units are not to be stored with
this data structure. This structure is always
considered together with {\tt .SPECVALS}. It must not contain bad values.

\item{\tt .COVRS} is an NDF-type structure with only a data component. {\tt
.COVRS.DATA\_ARRAY} is a \_REAL or \_DOUBLE array which has the same shape as
{\tt <myndf>}. {\tt .COVRS} defaults to non-existence. The meaning is as
follows: 
For some reason pixels belonging to the same spectroscopic subset may be
interrelated. While no complete covariance matrix is stored, this structure
holds the sum over the rows of the covariance matrix. For multi-dimensional
data note that this holds information only about the interrelation {\it within
any one} spectroscopic subset, not between different such subsets. That means
we know only about interrelation along the spectroscopic axis (within a
spectrum) but not perpendicular to that axis (between spectra).

\item{\tt .RESULTS} is an NDF-type structure with data and variance components
and a number of extensions in the {\tt .MORE} component. All these extensions
are HDS vectors. They have either one element per component or one element per
parameter. The shape of the {\tt .RESULTS} structure is defined by (i) the
shape of {\tt <myndf>}, (ii) the total number of parameters $tnpar\geq 1$,
(iii) the number of components $ncomp\geq 1$.
Each component has allocated a certain number of
parameters $npara(comp)\geq 0$. The total number of parameters must be
$$tnpar\geq\sum_{comp=1}^{ncomp}npara(comp)$$
while the parameter index for any component runs from
$$ para1(comp)=\sum_{i=1}^{comp-1}npara(i)+1
   \mbox{\phantom{mmm}to\phantom{mmm}}
   para2(comp)=\sum_{i=1}^{comp}npara(i)$$

Components are additive, i.e.\ the combined
result is the sum of all those components
that can be evaluated.

Note that the concept of a component is different from that of a transition or
a kinematic component: A component is a line feature you can discern in the
spectrum. Any component can in general be assigned a transition and a radial
velocity. So you may have several components belonging to the same transition
and several components of similar velocity belonging to different transitions.
You may at any time decide that a discernible component has been misidentified
and just change its identification. The concept of a component is even more
general, in that it can be the continuum, in which case there is in general no
laboratory frequency for identification.

There is, however, a restriction for data sets with more than one spectrum. Any
component may differ from spectrum to spectrum only by the fitted values. The
mathematical type and identification of components is common to all spectra.

\begin{itemize}
\item{\tt .RESULTS.DATA\_ARRAY} and {\tt .RESULTS.VARIANCE} are \_REAL
or \_DOUBLE array structures which default to bad values. They have one axis
more than {\tt <myndf>}: The spectroscopic axis is skipped; instead new first and
second axes are inserted. The first axis counts the fit parameters up to the
maximum ($tnpar$). The second axis is of length 1 and may be used in future.
All further axes are of the same length as the
corresponding non-spectroscopic axes in {\tt <myndf>}.

\item{\tt .RESULTS.MORE.LINENAME} is a \_CHAR$*$32 vector which defaults to
``unidentified component''. There is one element for each component. Its value
is a spectroscopist's description of the component, like ``[OIII] 5007 v-comp
\#1'', ``12CO J=1-0'', ``nebulium'', ``5500 K black body''. It is essential
that the strings are of length 32.

\item{\tt .RESULTS.MORE.LABFREQ} is a \_REAL or \_DOUBLE vector which
defaults to bad values. There is one element for each component. The value is
the laboratory frequency of the transition. The unit used it the one stored in
{\tt .FREQUNIT}. The laboratory frequency is the frequency as observed in the
emitter's rest frame. The meaning of this frequency is similar to that of {\tt
.FREQREF} in that the laboratory frequency of a transition is a useful value
for the reference frequency of the velocity or redshift axis. The difference is
that each component fitted may or may not have its own laboratory frequency.
{\tt .FREQREF} will usually be a copy of one of the elements of {\tt
.RESULTS.MORE.LABFREQ}.

\item{\tt .RESULTS.MORE.COMPTYPE} is a \_CHAR$*$32 vector which defaults to
``unknown function''. There is one element for each component. Its value is a
mathematician's description of the component, like ``Gauss'', ``triangle'',
``Lorentz-Gauss'', ``Voigt'', ``polynomial'', ``Chebyshev series'', ``sine''.
It is essential that the strings are of length 32.

\item{\tt .RESULTS.MORE.NPARA} is an \_INTEGER vector defaulting to
{\tt INT($tnpar/ncomp$)}. There is one element for each component. Its value is
the number of parameters stored for that component. When more components are
added to an existing {\tt .RESULTS} structure, then the new components are
allocated by default {\tt INT($(tnpar-tnpar_{old})/(ncomp-ncomp_{old})$)}
parameters. The numbers of parameters must be greater
than or equal to zero.

\item{\tt .RESULTS.MORE.MASKL} and {\tt .RESULTS.MORE.MASKR} are \_REAL or
\_DOUBLE vectors which default to bad values; both are of the same type. There
is one element in either vector for each component. A component $comp$ is
evaluated according to type and parameters in the range of spectroscopic
values between $maskl(comp)$ and $maskr(comp)$. The component is assumed to be
zero outside this interval. Bad values indicate $\mp\infty$.

\item{\tt .RESULTS.MORE.PARATYPE} is a \_CHAR$*$32 vector which defaults to
``unknown parameter''. There is one element for each parameter. Its value is a
mathematician's description of the parameter. A Gauss profile might be
specified by parameters ``centre'', ``peak'', ``sigma width'', ``integral''. It
is essential that the strings are of length 32.
\end{itemize}

It should be noted that there exist no rules about how to store certain
components, like ``A Gauss profile must be called Gauss and have parameters
such and such''. What is stored should be described by the strings so that a
human reader knows what it is all about. This does not prevent pairs of
applications from storing and retrieving components if they use the strings
in a consistent way. The documentation of any application that writes ore reads
results should specify what strings are written or recognised.
\end{itemize}

Specdre contains a number of subroutines for easy and consistent access to
the Extension. Programmers are advised to contact the author if they want to
make use of the Specdre Extension in their programmes.

\goodbreak
\subsection{An example Specdre Extension}

As an example, a sensible Specdre Extension has been added to a
three-dimensional NDF, which in fact is a data cube of HI 21 cm spectra taken
on a grid of galactic longitudes and latitudes. Here is how {\tt trace} sees
the file before the Extension is added:
\small
\begin{verbatim}
LVCHI  <NDF>
   TITLE          <_CHAR*12>      'KAPPA - Cadd'
   DATA_ARRAY(32,37,150)  <_REAL>   -0.1216488,0.1160488,4.3972015E-02,
                                    ... 0.4012871,-7.1809769E-02,0.4626274,*
   AXIS(3)        <AXIS>          {array of structures}
   Contents of AXIS(1)
      DATA_ARRAY(32)  <_REAL>        130,130.1667,130.3333,130.5,130.6667,
                                     ... 134.6668,134.8335,135.0002,135.1668
   Contents of AXIS(2)
      DATA_ARRAY(37)  <_REAL>        17,17.16667,17.33333,17.5,17.66666,
                                     ... 22.49998,22.66665,22.83331,22.99998
   Contents of AXIS(3)
      DATA_ARRAY(150)  <_REAL>       -163.855,-162.565,-161.275,-159.985,
                                     ... 24.48488,25.77489,27.06489,28.35489
\end{verbatim}
\normalsize
\goodbreak
Now almost all components that can exist in a Specdre Extension are set, mostly
to their default values. The only component missing is the sum of rows of a
covariance matrix. This is because that structure usually must not exist: Other
structures can be assigned ``harmless'' values, but the sheer existence of {\tt
.COVRS} makes a difference. The Extension was actually made with the {\tt
editext} command, which can also list a summary of the Extension:
\goodbreak
\small
\begin{verbatim}
List of Specdre Extension (v. 0.7)

Name of NDF:         DISK$REVAXL_SYS:[HME.DATA]LVCHI
Spectroscopic axis:  3
Reference frame:     local standard of rest
Refractive index:    1.0000000
Reference frequency: 1420.405751786000 [10**9 Hz]
Spectroscopic values exist and are velocity [km/s].
Spectroscopic widths do exist.
Covariance row sums  do not exist.
Result structure provides for 4 parameters in 1 component.

#  line name               lab freq.          type  npara  mask from     to
1  HI (21cm) LV component  1420.405751786000  Gauss     4  -.1701412E+39 -.1701412E+39

# parameter type
1 centre
2 peak
3 FWHM
4 integral

End of list.
\end{verbatim}
\goodbreak
\normalsize
{\tt trace}ing the NDF now yields:
\small
\begin{verbatim}
LVCHI  <NDF>
   TITLE          <_CHAR*17>      'SPECDRE - Editext'
   DATA_ARRAY(32,37,150)  <_REAL>   -0.1216488,0.1160488,4.3972015E-02,
                                    ... 0.4012871,-7.1809769E-02,0.4626274,*
   AXIS(3)        <AXIS>          {array of structures}
   Contents of AXIS(1)
      DATA_ARRAY(32)  <_REAL>        130,130.1667,130.3333,130.5,130.6667,
                                     ... 134.6668,134.8335,135.0002,135.1668
   Contents of AXIS(2)
      DATA_ARRAY(37)  <_REAL>        17,17.16667,17.33333,17.5,17.66666,
                                     ... 22.49998,22.66665,22.83331,22.99998
   Contents of AXIS(3)
      DATA_ARRAY(150)  <_REAL>       -163.855,-162.565,-161.275,-159.985,
                                     ... 24.48488,25.77489,27.06489,28.35489
   MORE           <EXT>           {structure}
      SPECDRE        <SPECDRE_EXT>   {structure}
         SPECAXIS       <_INTEGER>      3                                   (1)
         RESTFRAME      <_CHAR*32>      'local standard of rest'            (2)
         INDEXREFR      <_REAL>         1                                   (3)
         FREQREF        <_DOUBLE>       1420.405751786                      (4)
         FREQUNIT       <_INTEGER>      9                                   (4)
         SPECVALS       <NDF>           {structure}
            DATA_ARRAY     <ARRAY>         {structure}                      (5)
               DATA(32,37,150)  <_REAL>       -163.855,-163.855,-163.855,
                                              ... 28.35489,28.35489,28.35489
               ORIGIN(3)      <_INTEGER>      1,1,1
 
            LABEL          <_CHAR*64>      'velocity'                       (6)
            UNITS          <_CHAR*64>      'km/s'                           (6)
         SPECWIDS       <NDF>           {structure}
            DATA_ARRAY     <ARRAY>         {structure}                      (7)
               DATA(32,37,150)  <_REAL>       1.289993,1.289993,1.289993,
                                              ... 1.290001,1.290001,1.290001
               ORIGIN(3)      <_INTEGER>      1,1,1
         RESULTS        <NDF>           {structure}                         (8)
            DATA_ARRAY     <ARRAY>         {structure}                      (9)
               DATA(4,1,32,37)  <_REAL>       *,*,*,*,*,*,*,*,*,*,*,*,*,*,
                                              *,*,*,*,*,*,*,*,*,*,*,*,*,*
               ORIGIN(4)      <_INTEGER>      1,1,1,1
            VARIANCE       <ARRAY>         {structure}                     (10)
               DATA(4,1,32,37)  <_REAL>       *,*,*,*,*,*,*,*,*,*,*,*,*,*,
                                              *,*,*,*,*,*,*,*,*,*,*,*,*,*
               ORIGIN(4)      <_INTEGER>      1,1,1,1
            MORE           <EXT>           {structure}                     (11)
               LINENAME(1)    <_CHAR*32>      'HI (21cm) LV component'
               LABFREQ(1)     <_DOUBLE>       1420.405751786
               COMPTYPE(1)    <_CHAR*32>      'Gauss'
               NPARA(1)       <_INTEGER>      4
               MASKL(1)       <_REAL>         *
               MASKR(1)       <_REAL>         *
               PARATYPE(4)    <_CHAR*32>      'centre','peak','FWHM','inte...'
\end{verbatim}
\normalsize
\goodbreak
\begin{enumerate}
\item The third axis of the main NDF is declared the spectroscopic axis. In
fact this is the velocity axis of the data.

\item The telescope's local oscillator was controlled such that the
spectrometer recorded frequency in the reference frame of the local standard of
rest. Alternatively, someone could have re-calibrated the spectroscopic axis at
some stage to refer to the LSR.

\item The refractive index is set to 1, which amounts to ignoring refraction
for the purpose of wavelength calibration. Probably no one will ever use
wavelengths for these data anyway. If this structure were missing, a value of 1
would be assumed whenever necessary.

\item The reference frequency is encoded in two numbers. Taken together they
say that the velocity axis refers to a laboratory frequency of 1420.405751786
Mc/s (or MHz).

\item Here is a data cube of the same shape as the main data. It stores for
each sky position $(l,b)$ the velocity axis. Thus for each pixel in the
three-dimensional data cube, there is a separate velocity value stored for the
pixel centre position. In this case it is a waste of disk space since all
positions have the same velocity grid. But a slit spectrum recorded with a CCD
may have a shift of wavelength scales from one row to the next.

\item The cube with the spectroscopic pixel centres should have its own label
and unit. As the data reduction and analysis progresses, these may change,
while the main NDF's {\tt .AXIS(3)} structure will in general remain untouched.

\item Here is a data cube of the same shape as that with the pixel centres,
only that it stores the pixel widths. Each of these -- centres and widths --
may or may not exist independently. Though in general the width is only useful
if there is also an array of centres.

\item Here is a rather complex NDF structure. It is used to store and retrieve
fit results. The results can be different from one sky position to the next,
but there is only one set of parameters per position. The spectroscopic axis --
of length 150 -- is scrapped, the two positional axes -- lengths 32 and 37
respectively -- are retained. Inserted are the first and second axes, here of
lengths 4 and 1 respectively. Don't wonder about the second axis, it is always
of length 1.

Usually the result structure will be manipulated by applications as they find
it necessary to store data in it. But most of the structure can be manipulated
explicitly with the command {\tt editext}.

\item This should contain the values of the parameters, by default they take
the bad value, which is represented here by asterisks.

\item This should contain the variances of the parameters as a measure of their
uncertainties. Again, by default these values are bad.

\item The extensions to the result NDF are seven vectors. Most have one element
per spectral component -- in the example 1. The {\tt .PARATYPE} vector has four
elements, one for each parameter. The sole component provided for in the result
NDF is the ``LV component'' of the 21 cm line. Its laboratory frequency is
repeated here, the value is independent of the reference frequence above, but
the same unit ($10^9$ Hz) is used. We obviously expect that the spectral line
has the shape of a Gauss curve and we want to store four parameters describing
that curve. {\tt .PARATYPE} indicates the meaning of all parameters. No mask is
enabled to limit the velocity range where the Gauss curve applies, i.e. {\tt
.MASKL} and {\tt .MASKR} have the bad value.

We might want to store also the results for a parabolic baseline fit. Then we
would add a second spectral component with three parameters. The vectors that
are now of length 1 would become of length 2, {\tt .PARATYPE} would become of
length 7. The additional second vector elements would be ``baseline'', bad
value, ``polynomial of 2nd order'', 3, bad value, bad value. The fifth to
seventh element of {\tt .PARATYPE} could be ``coeff. 0'', ``coeff. 1'',
``coeff. 2''.

\end{enumerate}


%\goodbreak
%\section{Changes since previous versions}
%\label{changes}
%\input changes


\goodbreak
\section{References}
\label{refer}

\begin{itemize}

\item SG/5: Bailey, J.A., Chipperfield, A.J., 1991, ICL -- The
   interactive command language for ADAM -- User's guide, Starlink Guide
   5

\item SGP/38: Currie, M.J., Wallace, P.T., Warren-Smith, R.F., 1989,
   Starlink standard data structures, Starlink General Paper 38

\item SUN/16: Wilkins, T.N., Axon, D.J., 1991, TWODSPEC -- Some
   additions to Figaro, Starlink User Note 16

\item SUN/17: Lightfoot, J.F., Prestage, R.M., 1992, SPECX -- A
   millimetre wave spectral reduction package, Starlink User Note 17

\item SUN/19: Gold, J., Fuller, N., Lewis, J., Benn, C., 1992, NDPROGS
   -- n-d image manipulation programs, Starlink User Note 19

\item SUN/27: Daly, P.N., Beard, S.M., 1992, CGS4DR -- v1.6--0 User's
   guide, Starlink User Note 27

\item SUN/33: Warren-Smith, R.F., 1991, NDF -- A subroutine library for
   accessing NDF data structures, Starlink User Note 33

\item SUN/50: Howarth, I.D., Murray, J., 1991, DIPSO -- A friendly
   spectrum analysis program, Starlink User Note 50

\item SUN/55: Currie, M.J., 1992, CONVERT -- A format-conversion
   package, Starlink User Note 55

\item SUN/86: Meyerdierks, H., 1992, FIGARO -- A general data reduction
   system, Starlink User Note 86

\item SUN/95: Currie, M.J., 1992, KAPPA -- Kernel application package,
   Starlink User Note 95

\item SUN/103: Davenhall, A.C., Pettini, M., 1990, APIG -- Absorption
   profiles in the interstellar gas

\item SUN/137: Harrison, P., Rees, P., 1993, PONGO -- A set of
   applications for interactive data plotting, Starlink User Note 137

\item SUN/138: McSherry, M., 1991, HRTS -- NRL High Resolution Telescope
   and Spectrograph package, Starlink User Note 138

\item SUN/139: Draper, P.W., 1992, CCDPACK -- CCD data reduction package,
   Starlink User Note 139

\item SUN/147: Bly, M.J., 1992, TAUCAL --- TAURUS data reduction,
   Starlink User Note 147

\item SUN/149: Lewis, J.R., 1992, SAM -- A spectral extraction package,
   Starlink User Note 149

\item SUN/164: Terrett, D.L., 1993, PSMERGE -- Encapsulated PostScript
   handling utility, Starlink User Note 164

\item Lang, K.R., 1980, Astrophysical formulae, Springer, Heidelberg,
   Berlin, New York

\item Mills, D., 1992, Automatic ARC wavelength calibration, in P.J.
   Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data Analysis
   Workshop, Garching, 13 - 14 May 1992, ESO Conference and Workshop
   Proceedings No. 41, Garching bei M\"unchen, 1992

\item Oke, J.B., Gunn, J.E., 1983, Secondary standard stars for absolute
   spectrophotometry, ApJ, 266, 713

\item Shortridge, K., 1991, FIGARO -- General data reduction and
   analysis, version 3.0, Users guide, Starlink, Miscellaneous User
   Document

\end{itemize}

%----------------------------------- Appendices -------------------------------
%     \linebreak, \newpage, \goodbreak, and \vspace{-15pt} (or 12pt) have been
%     inserted, which may be wrong or in the wrong place when the layout is
%     changed.
%------------------------------------------------------------------------------


\appendix

\newpage
\section{Classified list of commands}
\label{classif}

%  Classified list of commands
%  ===========================

{\bf Input/Output}
\begin{verbatim}
  ASCIN      Read a 1-D or N-D data set from an ASCII table.
  ASCOUT     Write an NDF to an ASCII table.
\end{verbatim}

{\bf Display}
\begin{verbatim}
  SPECPLOT   Plot a spectrum.
\end{verbatim}

{\bf Structure manipulation}
\begin{verbatim}
  EDITEXT    Edit the Specdre Extension.
\end{verbatim}

{\bf Data manipulation}
\begin{verbatim}
  GOODVAR    Replace negative, zero and bad variance or error values.
\end{verbatim}

%{\bf Data arithmetics}
%\begin{verbatim}
%\end{verbatim}

{\bf Statistics}
\begin{verbatim}
  CORREL     Correlate two or three data sets.
\end{verbatim}

{\bf Reshaping}
\begin{verbatim}
  GROW       Copy from an N-dim. cube into part of an (N+M)-dimensional one.
  SUBSET     Take a subset of a data set.
  XTRACT     Average an N-dimensional cube into an (N-M)-dimensional one.
\end{verbatim}

{\bf Convolution, resampling, merging}
\begin{verbatim}
  FILLCUBE   Copy one NDF into part of another.
  RESAMPLE   Resample and average several spectra.
\end{verbatim}

{\bf Data calibration}   % including Spiketra
\begin{verbatim}
  BBODY      Calculate a black body spectrum.
\end{verbatim}

{\bf Axis calibration}
\begin{verbatim}
  ARCDISP    Fit polynomial dispersion curve.
  ARCGENDB   Convert list of laboratory values to feature data base.
  ARCIDENT   Auto-identify located features.
  ARCLOCAT   Locate line features in a set of spectra.
\end{verbatim}

{\bf Fitting}
\begin{verbatim}
  EVALFIT    Evaluate fit results.
  FITBB      Fit diluted Planck curves to a spectrum.
  FITCHEBY   Fit a series of Chebyshev polynomials to a spectrum.
  FITGAUSS   Fit Gauss profiles to a spectrum.
  FITTRI     Fit triangular profiles to a spectrum.
\end{verbatim}

{\bf Miscellaneous}
\begin{verbatim}
  SPE_HELP   Provide Specdre on-line help.
\end{verbatim}
% ----------------------------------------------------------------------------


\newpage
\section{Specdre commands in detail}
\label{applics}

\small
%+
%  Name:
%     LAYOUT.TEX

%  Purpose:
%     Define Latex commands for laying out documentation produced by PROLAT.

%  Language:
%     Latex

%  Type of Module:
%     Data file for use by the PROLAT application.

%  Description:
%     This file defines Latex commands which allow routine documentation
%     produced by the SST application PROLAT to be processed by Latex. The
%     contents of this file should be included in the source presented to
%     Latex in front of any output from PROLAT. By default, this is done
%     automatically by PROLAT.

%  Notes:
%     The definitions in this file should be included explicitly in any file
%     which requires them. The \include directive should not be used, as it
%     may not then be possible to process the resulting document with Latex
%     at a later date if changes to this definitions file become necessary.

%  Authors:
%     RFWS: R.F. Warren-Smith (STARLINK)

%  History:
%     10-SEP-1990 (RFWS):
%        Original version.
%     10-SEP-1990 (RFWS):
%        Added the implementation status section.
%     12-SEP-1990 (RFWS):
%        Added support for the usage section and adjusted various spacings.
%     10-DEC-1991 (RFWS):
%        Refer to font files in lower case for UNIX compatibility.
%     {enter_further_changes_here}

%  Bugs:
%     {note_any_bugs_here}

%-

%  Define length variables.
\newlength{\sstbannerlength}
\newlength{\sstcaptionlength}

%  Define a \tt font of the required size.
\font\ssttt=cmtt10 scaled 1095

%  Define a command to produce a routine header, including its name,
%  a purpose description and the rest of the routine's documentation.
\newcommand{\sstroutine}[3]{
   \goodbreak
   \rule{\textwidth}{0.5mm}
   \vspace{-7ex}
   \newline
   \settowidth{\sstbannerlength}{{\Large {\bf #1}}}
   \setlength{\sstcaptionlength}{\textwidth}
   \addtolength{\sstbannerlength}{0.5em}
   \addtolength{\sstcaptionlength}{-2.0\sstbannerlength}
   \addtolength{\sstcaptionlength}{-4.45pt}
   \parbox[t]{\sstbannerlength}{\flushleft{\Large {\bf #1}}}
   \parbox[t]{\sstcaptionlength}{\center{\Large #2}}
   \parbox[t]{\sstbannerlength}{\flushright{\Large {\bf #1}}}
   \begin{description}
      #3
   \end{description}
}

%  Format the description section.
\newcommand{\sstdescription}[1]{\item[Description:] #1}

%  Format the usage section.
\newcommand{\sstusage}[1]{\item[Usage:] \mbox{} \\[1.3ex] {\ssttt #1}}

%  Format the invocation section.
\newcommand{\sstinvocation}[1]{\item[Invocation:]\hspace{0.4em}{\tt #1}}

%  Format the arguments section.
\newcommand{\sstarguments}[1]{
   \item[Arguments:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Format the returned value section (for a function).
\newcommand{\sstreturnedvalue}[1]{
   \item[Returned Value:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Format the parameters section (for an application).
\newcommand{\sstparameters}[1]{
   \item[Parameters:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Format the examples section.
\newcommand{\sstexamples}[1]{
   \item[Examples:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Define the format of a subsection in a normal section.
\newcommand{\sstsubsection}[1]{\item[{#1}] \mbox{} \\}

%  Define the format of a subsection in the examples section.
\newcommand{\sstexamplesubsection}[1]{\item[{\ssttt #1}] \mbox{} \\}

%  Format the notes section.
\newcommand{\sstnotes}[1]{\item[Notes:] \mbox{} \\[1.3ex] #1}

%  Provide a general-purpose format for additional (DIY) sections.
\newcommand{\sstdiytopic}[2]{\item[{\hspace{-0.35em}#1\hspace{-0.35em}:}] \mbox{} \\[1.3ex] #2}

%  Format the implementation status section.
\newcommand{\sstimplementationstatus}[1]{
   \item[{Implementation Status:}] \mbox{} \\[1.3ex] #1}

%  Format the bugs section.
\newcommand{\sstbugs}[1]{\item[Bugs:] #1}

%  Format a list of items while in paragraph mode.
\newcommand{\sstitemlist}[1]{
  \mbox{} \\
  \vspace{-3.5ex}
  \begin{itemize}
     #1
  \end{itemize}
}

%  Define the format of an item.
\newcommand{\sstitem}{\item}

%  End of LAYOUT.TEX layout definitions.
%.
\goodbreak
\sstroutine{
   ARCDISP
}{
   Fit polynomial dispersion curve
}{
   \sstdescription{
      This routine fits a polynomial dispersion curve to a list of
      identified arc features and transforms the NDF pixel coordinates
      to spectroscopic values. Optionally you can use a graphical
      dialogue to improve on the previous feature identification, until
      you like the appearance of the dispersion curve.

      The input data must be a base NDF. They can be a single spectrum
      or a set of spectra. Examples for the latter are a long slit
      spectrum, a set of extracted fibre spectra, or a collapsed
      echellogram (a set of extracted orders from an echelle
      spectrograph). It is necessary that the spectroscopic axis be the
      first axis in the data set. It does not matter how many further
      axes there are, the data will be treated as a linear set of rows
      with each row a spectrum.

      The actual input is the results structure in the Specdre
      Extension. This must be a set of components of type `arc
      feature'. Each must have two parameters `centre' and `laboratory
      value'. These must be corresponding locations one expressed in
      NDF pixel coordinates, the other in spectroscopic values
      (wavelength, frequency etc.). The centres must be strictly
      monotonically increasing, their variances must be available.
      Laboratory values may be bad values to signify unidentified
      features.

      In the graphical dialogue the results structure may be updated.
      The locations remain unchanged; all located features form a fixed
      list of potentially identified features. Identifications may be
      added, deleted or modified. The user has to work on each row in
      turn (unless Quit is chosen). When the user switches from one row
      to the next, the dispersion curve for the finished row is applied
      and its spectroscopic values in the Specdre Extension are set.
      When the last row is finished, the application exits; the output
      of this routine is (i) an updated list of identifications in the
      results structure of the Specdre Extension and (ii) an array of
      spectroscopic values according to the dispersion curves for each
      row, also in the Specdre Extension. At any point the user can
      quit. In this case the array of spectroscopic values is
      discarded, but the updated identifications are retained. If run
      without dialogue, this routine simply performs the polynomial fit
      of the dispersion curve for each row in turn and works out the
      array of spectroscopic values. The list of identifications is
      input only and remains unchanged. If for any row the fit cannot
      be performed, then the spectroscopic values calculated so far are
      discarded and the routine aborts.

      There must not yet be any spectroscopic value information: There
      must be no array of spectroscopic values or widths in the Specdre
      Extension. The pixel centre array for the spectroscopic axis
      (i.e.\ the first axis) must be NDF pixel coordinates (usually 0.5,
      1.5, ...).

      This routine works on each row (spectrum) in turn. It fits a
      polynomial to the existing identifications. In the optional
      graphical dialogue two plots are displayed and updated as
      necessary. The lower panel is a plot of laboratory values
      (wavelength, frequency etc.) versus pixel coordinate shows
      \sstitemlist{

         \sstitem
         all possible identifications from the feature data base as
            horizontal lines,

         \sstitem
         all unidentified located features as vertical lines,

         \sstitem
         all identified located features as diagonal crosses,

         \sstitem
         the dispersion curve.

      }
      In the upper panel, a linear function is subtracted so that it
      displays the higher-order components of the dispersion curve.
      Crosses indicate the identified located features. Since the scale
      of this upper panel is bigger, it can be used to spot outlying
      feature identifications. In the dialogue you can
      \sstitemlist{

         \sstitem
         R - Switch to next row, accepting the current fit for this row

         \sstitem
         X - X-zoom 2x on cursor

         \sstitem
         Y - Y-zoom 2x on cursor

         \sstitem
         W - Unzoom to show whole row

         \sstitem
         N - Pan by 75\% of current plot range

         \sstitem
         A - Add ID for location nearest to cursor (from FDB)

         \sstitem
         S - Set ID for location nearest to cursor (from cursor y pos.)

         \sstitem
         D - Delete ID for feature nearest to cursor

         \sstitem
         Q - Quit (preserves updated IDs, discards applied fits for all
             rows)

         \sstitem
         ? - Help

      }
      Whenever the list of identifications is changed, the dispersion
      curve is fitted again and re-displayed. If there are too few
      identifications for the order chosen, then the dialogue will
      display the maximum order possible. But such an under-order fit
      cannot be accepted, the R option will result in an error.

      The Q option will always result in an error report, formally the
      routine aborts. After all, it does not achieve the main goal of
      applying individual dispersion curves to all rows.

      On one hand the output of this routine may be an updated list of
      identifications, which could in principle be used in a future run
      of this routine. On the other hand this routine will always
      result in an array of spectroscopic values. The existence of
      these spectroscopic values prevents using this routine again.
      Before using this routine again on the same input NDF you have to
      delete the SPECVALS component in the Specdre Extension.

      In order to facilitate repeated use of this routine on the same
      data, it always uses the Specdre Extension to store spectroscopic
      values, even if the data are one-dimensional and the first axis
      centre array would suffice to hold that information. This leaves
      the first axis centre array at NDF pixel coordinates, as
      necessary for re-use of this routine.
   }
   \sstusage{
      arcdisp in order
   }
   \sstparameters{
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If this is `Y', `T' or `G', then the graphical dialogue is
         entered before the polynomial dispersion curve for any row is
         accepted and applied. If this is `N' or `F' then the dialogue
         is not entered and separate dispersion curves are applied to
         all rows. The string is case-insensitive. [`G']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The spectrum or set of spectra in which emission features are
         to be located. This must be a base NDF, the spectroscopic axis
         must be the first axis. No spectroscopic values or widths must
         exist in the Specdre Extension. The pixel centres along the
         first axis must be NDF pixel coordinates. Update access is
         necessary, the results structure in the Specdre Extension may
         be modified, an array of spectroscopic values will be created
         in the Specdre Extensions.
      }
      \sstsubsection{
         ORDER = \_INTEGER (Read)
      }{
         The polynomial order of dispersion curves. This cannot be changed
         during the graphical dialogue. Neither can it differ between
         rows.  [2]
      }
      \sstsubsection{
         FDB = NDF (Read)
      }{
         The feature data base. Only the simple list of values FTR\_WAVE is
         used and only in graphics dialogue. It serves to find the
         identification for an as yet unidentified -- but located
         feature.
      }
      \sstsubsection{
         DEVICE = GRAPHICS (Read)
      }{
         The graphics device to be used. This is unused if DIALOG is
         false.
      }
      \sstsubsection{
         WRANGE( 2 ) = \_REAL (Read)
      }{
         In graphical dialogue this parameter is used repeatedly to get
         a range of laboratory values. This is used for plotting as well
         as for finding identifications in the feature data base.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
}
\goodbreak
\sstroutine{
   ARCGENDB
}{
   Convert list of laboratory values to feature data base
}{
   \sstdescription{
      This routine converts an arc line list -- i.e.\ an ASCII list of
      laboratory wavelengths or frequencies of known features in an arc
      spectrum -- into a feature data base. That can be used for
      automatic identification of features in an observed arc spectrum.

      Since generating the feature data base may take some time, you may
      want to do it once for any line lists you often use, and keep the
      feature data bases. On the other hand, the feature data bases may
      be rather big.

      This routine reads a list of laboratory values (wavelengths or
      frequencies). The list must be an unformatted ASCII file. From the
      beginning of each line one value is read. If this fails, the line
      is ignored. Comment lines can be inserted by prefixing them with
      ``$*$'', ``!'' or ``\#''. The value can be followed by any comment, but can
      be preceded only by blanks. The list must be strictly
      monotonically increasing.

      The list should to some degree match an expected observation. Its
      spectral extent should be wider than that of an expected
      observation. But it should not contain a significant number of
      features that are usually not detected. This is because the
      automatic identification algorithm uses relative distances between
      neighbouring features. If most neighbours in the list of
      laboratory values are not detected in the actual arc observation,
      then the algorithm may fail to find a solution or may return the
      wrong solution.

      The given list is converted to a feature data base according to
      Mills (1992). The data base contains information about the
      distances between neighbours of features. The scope of the feature
      data base is the number of neighbours about which information is
      stored. The feature data base is stored in an extension to a dummy
      NDF. The NDF itself has only the obligatory data array. The data
      array is one-dimensional with 1 pixel. All the actual information
      is in an extension with the name ``ECHELLE'' and of type
      ``ECH\_FTRDB''. Its HDS components are:
      \sstitemlist{

         \sstitem
         FTR\_WAVE(NLINES)           $<$\_REAL$>$

         \sstitem
         FTR\_DB(10,10,NLINES)       $<$\_REAL$>$

         \sstitem
         FTR\_LEFT(10,10,NLINES)     $<$\_BYTE$>$

         \sstitem
         FTR\_RIGHT(10,10,NLINES)    $<$\_BYTE$>$

         \sstitem
         WAVE\_INDEX(10,10,NLINES)   $<$\_UWORD$>$

         \sstitem
         QUICK\_INDEX(5000)          $<$\_INTEGER$>$

         \sstitem
         QENTRIES(5000)             $<$\_REAL$>$

      }
      NLINES is the number of features listed in the input file. The
      scope (=10) controls about how many neighbours information is
      stored in the data base. The index size is fixed to 5000, which
      seems sufficient for NLINES = 3500. The size of the FDB is

         $$(804 * {\rm NLINES} + 40000) {\rm bytes}$$

      plus a small overhead for the HDS structure and the nominal NDF.
      So it is 10 to 100 times bigger than the original ASCII list. The
      point about the FDB is the reduced computing time when
      auto-identifying features in an observed arc spectrum.
   }
   \sstusage{
      arcgendb in fdb
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If true, informational messages will be issued.
      }
      \sstsubsection{
         IN = FILENAME (Read)
      }{
         The name of the input ASCII list of wavelengths or frequencies.
         The list must be strictly monotonically increasing.
      }
      \sstsubsection{
         FDB = NDF (Read)
      }{
         The name of the output file to hold the feature data base.
         This is formally an NDF.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         arcgendb \$FIGARO\_PROG\_S/thar.arc thar\_arc
      }{
         This will convert the Th-Ar list from the Figaro release into a
         ``feature data base'' by the name of ``thar\_arc.sdf''.
      }
   }
   \sstdiytopic{
      References
   }{
      Mills, D., 1992, Automatic ARC wavelength calibration, in P.J.
      Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data Analysis
      Workshop, Garching, 13 - 14 May 1992, ESO Conference and Workshop
      Proceedings No. 41, Garching bei M\"unchen, 1992
   }
}
\goodbreak
\sstroutine{
   ARCIDENT
}{
   Auto-identify located features
}{
   \sstdescription{
      This routine identifies located features in a set of spectra.
      Auto-identification is done from scratch (without prior
      identification of any features) with the algorithm by Mills
      (1992).

      The input data must be a base NDF. They can be a single spectrum
      or a set of spectra. Examples for the latter are a long slit
      spectrum, a set of extracted fibre spectra, or a collapsed
      echellogram (a set of extracted orders from an echelle
      spectrograph). It is necessary that the spectroscopic axis be the
      first axis in the data set. It does not matter how many further
      axes there are, the data will be treated as a linear set of rows
      with each row a spectrum.

      The features for which an identification should be attempted must
      have been located. That is, they must be components of type
      `Gauss', `triangle', `Gauss feature' or `triangle feature' in the
      results structure of the Specdre Extension. Each of these
      components must have at least a `centre' and `peak' parameter. The
      centres (feature locations) must be a strictly monotonically
      increasing list. Their variances must be available. The locations
      (centre parameters) must be in terms of NDF pixel coordinates. The
      peaks must be positive. They are used as a measure of the
      importance of a feature.

      The coverage in spectroscopic values of all spectra (rows) should
      either be similar (long slit or fibres) or roughly adjacent
      (echellogram). There must not yet be any spectroscopic value
      information: There must be no array of spectroscopic values or
      widths in the Specdre Extension. The pixel centre array for the
      spectroscopic axis (i.e.\ the first axis) must be NDF pixel
      coordinates (usually 0.5, 1.5, ...). The data must be arranged
      such that spectroscopic values increase left to right. In the case
      of rows with adjacent coverage spectroscopic values must also
      increase with row number. In a collapsed echellogram this usually
      means that for wavelength calibration the order number must
      decrease with increasing row number. If this is not the case then
      it is still possible to work on a collapsed echellogram: You can
      set ECHELLE false and thus use the full WRANGE for each row, but
      you must adjust DRANGE to be a more reasonable guess of the
      dispersion.

      Identification is done by comparison with a feature data base
      according to Mills (1992). The feature data base should to some
      degree match the observation. Its spectral extent should be wider
      than that of the observation. But it should not contain a
      significant number of features that are not located. This is
      because the automatic identification algorithm uses relative
      distances between neighbouring features. If most neighbours in the
      list of laboratory values are not detected in the actual arc
      observation, then the algorithm may fail to find a solution or may
      return the wrong solution.

      This routine works on each row (spectrum) in turn. It establishes
      information about relative distances between neighbouring located
      features and compares this with a feature data base. This serves
      to identify at least a specified number of features. An
      auto-identification should always be checked in the process of
      fitting a polynomial dispersion curve. All located features are
      retained by this routine, so that further identifications can be
      added or some identifications can be cancelled.

      The result of this routine is a list of feature identifications.
      All located features are retained, though some will have not been
      identified. The locations and identifications (pixel coordinates
      and laboratory values) are stored in the results structure of the
      Specdre Extension of the input data. This replaces the
      pre-existing results extension. The locations are strictly
      monotonically increasing, as are in all probability the
      identifications.

      The new results structure provides for as many component as the
      old one had components of any recognised type. Each component has
      on output the type `arc feature'. It has two parameters `centre'
      and `laboratory value'. Located but unidentified features will
      have bad values as laboratory values. The variances of laboratory
      values are set to zero.

      Mills' (1992) algorithm performs only an initial line
      identification. It is important to verify the returned values by
      fitting a wavelength or frequency scale (e.g.\ polynomial or spline
      fit), and to reject any out-liers. The algorithm should be given
      the positions of all conceivable features in the spectra. It does
      not use the fainter ones unless it is unable to identify using
      only the brightest, but you will get more robust behaviour if you
      always provide all possible candidate lines for potential
      identification. The algorithm should not be fed severely blended
      line positions as the chance of incorrect identifications will be
      significantly higher (this is the exception to the rule above).

      The speed of the algorithm varies approximately linearly with
      wavelength/frequency range and also with dispersion range so the
      better constraints you provide the faster it will run. The
      algorithm takes your constraints as hard limits and it is usually
      more robust to accept a slightly longer runtime by relaxing the
      ranges a little.

      If the algorithm runs and keeps looping increasing its set of
      neighbours, then the most likely causes are as follows:
      \sstitemlist{

         \sstitem
         wavelength/frequency scale does not increase with increasing x
            (set the CHKRVS parameter true and try again).

         \sstitem
         WRANGE or DRANGE are too small (increase them both by
            a factor of 2 and try again).
      }
   }
   \sstusage{
      arcident in out fdb wrange=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [YES]
      }
      \sstsubsection{
         ECHELLE = \_LOGICAL (Read)
      }{
         If false, the given WRANGE is used for each row, assuming the
         rows are similar spectra (long slit or fibre). If true, a
         collapsed echellogram is assumed. In that case each row is an
         extracted order with different wavelength/frequency range. This
         routine will divide the given WRANGE into as many sub-ranges as
         there are rows (orders) in the given input. [NO]
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The spectrum or set of spectra in which located features are to
         be identified. This must be a base NDF, the spectroscopic axis
         must be the first axis. No spectroscopic values or widths must
         exist in the Specdre Extension. The pixel centres along the
         first axis must be NDF pixel coordinates. The input NDF must
         have a results structure in its Specdre Extension, and the
         results must contain a number of line components with strictly
         monotonically increasing position (centre).
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output NDF is a copy of the input, except that the results
         structure holds feature identifications rather than locations
         (`peak' parameters will have been replaced with `laboratory
         value' parameters).
      }
      \sstsubsection{
         FDB = NDF (Read)
      }{
         The feature data base. The actual data base is a set of
         primitive arrays in an extension to this NDF called ECHELLE.
         A feature data base can be generated from a list of wavelengths
         or frequencies with ARCGENDB.
      }
      \sstsubsection{
         WRANGE( 2 ) = \_REAL (Read)
      }{
         The approximate range of wavelengths or frequencies. The
         narrower this range the faster is the identification algorithm.
         But if in doubt give a wider range.
      }
      \sstsubsection{
         DRANGE( 2 ) = \_REAL (Read)
      }{
         The range into which the dispersion in pixels per wavelength or
         per frequency falls. The narrower this range the faster is the
         identification algorithm. But if in doubt give a wider range.
      }
      \sstsubsection{
         STRENGTH = \_REAL (Read)
      }{
         This specifies the maximum ratio between the strength of
         features that are to be used initially for identification. If
         the strongest feature has peak 1000, then the weakest
         feature used initially has peak greater than 1000/STRENGTH.
         [50.0]
      }
      \sstsubsection{
         THRESH = \_REAL (Read)
      }{
         This specifies the maximum difference between the ratios of
         neighbour distances as observed and as found in the feature
         data base. The difference is evaluated as
            ABS(1 $-$ ABS(obs/ref)) $<$? THRESH.
         Values much larger than 0.1 are likely to generate a lot of
         coincidence matches; values less than 0.01 may well miss `good'
         matches in less-than-ideal data. You may need to relax this
         parameter if your arc spectra are very distorted (non-linear
         dispersion). [0.03]
      }
      \sstsubsection{
         MAXLOC = \_INTEGER (Read)
      }{
         This specifies the maximum number of features to be used when
         generating ratios for initial identification. In general, a
         good solution can be found using only the strongest 8 to 16
         features. The program slowly increases the number of features
         it uses until an adequate solution if found. However, there may
         be a large numbers of weak features present which are not in
         the reference database. This parameter allows the setting of an
         absolute maximum on the number of features (per row) which
         are to be considered. If less than MAXLOC features are located
         in a given row, then the number of identified features is used
         instead for that row. [30]
      }
      \sstsubsection{
         MINIDS = \_INTEGER (Read)
      }{
         The minimum number of features that must be identified for the
         algorithm to be successful. If fewer than MINIDS features are
         located in a given row, then a smaller number is used instead
         for that row. [9]
      }
      \sstsubsection{
         NEIGHB( 2 ) = \_INTEGER (Read)
      }{
         NEIGHB(1) specifies the starting number of neighbouring
         features (on each side) to examine when generating ratios for
         matching. (These are neighbours in the observed spectra, not in
         the feature data base.) Increasing this will lead to
         exponential increases in CPU time, so it should be used with
         caution when all else fails. The default value is 3. Higher
         values are tried automatically by the program if no solution
         can be found. The number of neighbours considered is increased
         until it reaches the maximum of NEIGHB(2), when the program
         gives up. [3,6]
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.
   }
   \sstdiytopic{
      References
   }{
      Mills, D., 1992, Automatic ARC wavelength calibration, in P.J.
      Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data
      Analysis Workshop, Garching, 13 - 14 May 1992, ESO Conference and
      Workshop Proceedings No. 41, Garching bei M\"unchen, 1992
   }
}
\goodbreak
\sstroutine{
   ARCLOCAT
}{
   Locate line features in a set of spectra
}{
   \sstdescription{
      This routine locates narrow features in a set of spectra. Features
      can be located from scratch automatically. In a different mode,
      feature locations can be added or deleted in a graphical dialogue.
      The feature location and peak are determined by a Gauss or
      triangle line fit.

      The input data must be a base NDF. They can be a single spectrum
      or a set of spectra. Examples for the latter are a long slit
      spectrum, a set of extracted fibre spectra, or a collapsed
      echellogram (a set of extracted orders from an echelle
      spectrograph). It is necessary that the spectroscopic axis be the
      first axis in the data set. It does not matter how many further
      axes there are, the data will be treated as a set of rows with
      each row a spectrum.

      The coverage in spectroscopic values of all spectra (rows) should
      either be similar (long slit or fibres) or roughly adjacent
      (echellogram). There must not yet be any spectroscopic value
      information: There must be no array of spectroscopic values or
      widths in the Specdre Extension. The pixel centre array for the
      spectroscopic axis (i.e.\ the first axis) must be NDF pixel
      coordinates (usually 0.5, 1.5, ...). The data must be arranged
      such that spectroscopic values increase left to right. In the case
      of rows with adjacent coverage spectroscopic values must also
      increase with row number. In a collapsed echellogram this usually
      means that for wavelength calibration the order number must
      decrease with increasing row number.

      In automatic mode this routine works on each row (spectrum) in
      turn. It scans through the spectrum and looks for pixels that
      exceed the local background level by at least the given threshold
      value. When such a pixel is spotted, a single-component line fit
      is tried no the local profile. The local profile is centred on the
      pixel suspected to be part of an emission feature. It includes 1.5
      times the guessed FWHM on either side and a further 5 baseline
      pixels on either side. A local linear baseline is subtracted prior
      to the line fit. In order for the feature to be entered into the
      list of located features, the fit must succeed, the fitted peak
      must exceed the threshold, and the fitted peak must exceed the
      absolute difference of background levels between the left and
      right.

      When run with graphics dialogue this routine works on any choice
      of rows. It uses a pre-existing list of located features to which
      can be added or from which features can be deleted. Graphics
      dialogue can also be used to just check the locations. The graph
      displays the spectrum currently worked on in bin-style. The current
      list of located features is indicated by dashed vertical lines.
      The options in the graphical dialogue are:
      \sstitemlist{

         \sstitem
         R - Choose different row to work on

         \sstitem
         X - X-zoom 2x on cursor

         \sstitem
         Y - Y-zoom 2x on cursor

         \sstitem
         W - Unzoom to show whole row

         \sstitem
         N - Pan left/right by 75\% of current x range

         \sstitem
         A - Add the feature under cursor to list (subject to line fit)

         \sstitem
         S - Add the cursor position as feature to list

         \sstitem
         D - Delete the feature nearest cursor from list

         \sstitem
         Q - Quit, preserving the updated list of located features

         \sstitem
         ? - Help

      }
      The difference between the A and S options is that A tries a line
      fit to the local profile around the cursor, while S accepts the
      cursor x position as exact centre and the cursor y position as
      exact peak of a new feature; (the variance of the centre is set
      to 0.25, the variance of the peak to the bad value).

      The result of this routine is a list of Gauss or triangle
      features. Their locations in NDF pixel coordinates and their peak
      values are stored in the results structure of the Specdre
      Extension of the input data. If run in automatic mode, this
      routine will replace any previously existing results structure. If
      run with graphics dialogue, this routine will try to work with a
      pre-existing list of located features. But if the pre-existing
      results structure does not conform to the required format, then a
      new results structure is created.

      The list of located features (for each row) is always sorted such
      that the locations are strictly monotonically increasing.

      The results structure provides for a certain number of components.
      These have component type `Gauss feature' or `triangle feature'.
      Each component has two parameters `centre' and `peak'. The number
      of components is determined when the results structure is created,
      it is derived from the approximate width of features and the
      number of pixels in each spectrum.
   }
   \sstusage{
      arclocat in fwhm thresh
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If true, messages about the progress of auto-locating features
         are issued. [YES]
      }
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If this is `Y', `T' or `G', then no auto-locating takes place
         and the graphics dialogue is entered. If this is `N' or `F'
         then the dialogue is not entered and auto-locating is done
         instead. The string is case-insensitive.  [`G']
      }
      \sstsubsection{
         MODE = \_CHAR (Read)
      }{
         This can be `Gauss' or `triangle' and chooses the line profile
         to be fitted. This string is case-insensitive and can be
         abbreviated to one character. [`Gauss']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The spectrum or set of spectra in which emission features are
         to be located. This must be a base NDF, the spectroscopic axis
         must be the first axis. No spectroscopic values or widths must
         exist in the Specdre Extension. The pixel centres along the
         first axis must be NDF pixel coordinates. Update access is
         necessary, the results structure in the Specdre Extension will
         be modified, possibly re-created.
      }
      \sstsubsection{
         FWHM = \_REAL (Read)
      }{
         The guessed full width at half maximum of the features to be
         located. This is used to estimate the maximum number of
         features that might be located, to locate baseline ranges next
         to suspected features, and as a guess for the line fit.
      }
      \sstsubsection{
         THRESH = \_REAL (Read)
      }{
         The threshold. While scanning a pixel must exceed this
         threshold to initiate a line fit. The fitted peak also must
         exceed the threshold in order that the feature location be
         accepted. This parameter is significant only for automatic
         location of features.
      }
      \sstsubsection{
         DEVICE = GRAPHICS (Read)
      }{
         The graphics device to be used. This is unused if DIALOG is
         false.
      }
      \sstsubsection{
         ROWNUM = \_INTEGER (Read)
      }{
         In graphics dialogue this parameter is used to switch to a
         different row (spectrum).
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         arclocat in 4.\ 20.\ mode=triangle dialog=f
      }{
         This will scan through (all rows of) the NDF called ``in''. It
         looks out for features of 4 pixels full width at half maximum
         and with a peak value of at least 20 above the local
         background. The features are fitted as triangles. The search is
         automatic. Thus a new results structure in the input NDF's
         Specdre Extension is created with the locations (centres) and
         peaks of located features.
      }
      \sstexamplesubsection{
         arclocat in 4.\ mode=Gauss dialog=g rownum=5
      }{
         This will use the graphic dialogue. Starting with the fifth row
         the user can use the mouse cursor to choose features that are
         to be deleted from or added to the list of located features.
         This can be used to improve on an automatic run, or when no
         features have been located so far. If you try to add a feature
         to the list, a Gauss fit is tried in the vicinity of the
         cursor-selected position.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
}
\goodbreak
\sstroutine{
   ASCIN
}{
   Read a 1-D or N-D data set from an ASCII table
}{
   \sstdescription{
      This routine reads axis values, pixel widths, data values, and
      data errors from an ASCII table into an NDF data structure.
      Most of these items are optional, mandatory are only
      axis values for each axis and data values. Pixel widths can be
      read only in the one-dimensional case.

      The user specifies in which columns the different items are to be
      found. A range of line numbers to be used can be specified.
      Comment lines may be interspersed in this line range, if they are
      marked by an exclamation mark in the first or second character.
      All columns leftward of the rightmost used column must be
      numeric, non-numeric data may follow in further columns.
      Up to 132 characters are read from table lines. Numbers are read
      as \_REAL.

      If the result is one-dimensional, the axis values will be taken
      literally to define a grid, which in general may be non-linear and
      non-monotonic. If the result is multi-dimensional, the routine
      will guess from the table a grid that is linear in all directions.
      The parameter system is consulted to confirm or modify the
      suggested grid.

      The data value read from a line will be stored into exactly one
      output pixel, if and only if the table coordinates match that
      pixel's coordinate to within a specified fraction of the pixel
      step. Pixels for which no data are in the table are assigned the
      bad value. Table data equal to a specified ``alternative bad value''
      are replaced by the bad value before insertion into the data set.
      Where more than one table line corresponds to the same pixel, the
      pixel is assigned the last value from the table. That is, later
      specifications of the same pixel override previous ones.
   }
   \sstusage{
      ascin in lines colaxes=?\ coldata=?\ [start=?\ step=?\ end=?]\ out=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. This parameter is of significance only
         if the output is multi-dimensional. [YES]
      }
      \sstsubsection{
         TOL = \_REAL (Read)
      }{
         The tolerated fraction of the pixel size by which the table
         coordinates may deviate from the pixel coordinates. For a line
         read from the ASCII table, if any one of the axis values
         deviates by more than TOL times the pixel step, then the
         information from the table is disregarded. This parameter is of
         no significance, if the output is one-dimensional, since in
         that case the axis values found will define the exact
         (non-linear) grid. [0.2]
      }
      \sstsubsection{
         BAD = \_REAL (Read)
      }{
         The alternative bad value, i.e.\ the bad value used in the
         table. Any data or error value found in the table that is equal
         to BAD, is replaced by the bad value before insertion into the
         output. [$-$999999.]
      }
      \sstsubsection{
         IN = FILENAME (Read)
      }{
         The file containing the ASCII table.
      }
      \sstsubsection{
         LINES( 2 ) = \_INTEGER (Read)
      }{
         The line numbers of the first and last lines to be used from
         the table file. [1,9999]
      }
      \sstsubsection{
         COLAXES( 7 ) = \_INTEGER (Read)
      }{
         The column numbers where the axis values are to be found. All
         axes must be specified, i.e.\ at least one. The number of
         leading non-zero elements defines the number of axes in the
         output. [1,2]
      }
      \sstsubsection{
         COLWIDTH = \_INTEGER (Read)
      }{
         The column numbers where the pixel width values are to be
         found. This parameter is of significance only if the output is
         one-dimensional. Enter a 0 if no width information is
         available. [0]
      }
      \sstsubsection{
         COLDATA( 2 ) = \_INTEGER (Read)
      }{
         The column numbers where the data values (first element) and
         their associated error values (second element) are to be
         found. If no error information is available, enter 0 as second
         element. [3,0]
      }
      \sstsubsection{
         START( 7 ) = \_REAL (Read)
      }{
         The coordinates of the first pixel. This parameter is of
         no significance, if the output is one-dimensional, since in
         that case the axis values found will define the exact
         (non-linear) grid.
      }
      \sstsubsection{
         STEP( 7 ) = \_REAL (Read)
      }{
         The coordinate increments per pixel. This parameter is of
         no significance, if the output is one-dimensional, since in
         that case the axis values found will define the exact
         (non-linear) grid.
      }
      \sstsubsection{
         END( 7 ) = \_REAL (Read)
      }{
         The coordinates of the last pixel. This parameter is of
         no significance, if the output is one-dimensional, since in
         that case the axis values found will define the exact
         (non-linear) grid.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The NDF where to store the data.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         \parbox[t]{143mm}{ascin in [1,9999]\ colaxes=[1,2]\ coldata=[3,4]
         start=[0,0]\ end=[2.5,5]\ \\ step=[0.1,1]\ out=out}
      }{
         This will read the data from the ASCII file IN, using line
         numbers 1 to 9999 (or till end of file if there are less lines
         in IN). The 1st axis data are taken from the first column, the
         2nd axis data from the second column. The image data are taken
         from the 3rd column and their errors from the 4th column. The
         routine tries to store the table data into a grid with the 1st
         axis running from 0 to 2.5 in steps of 0.1 (26 pixels) and the
         2nd axis running from 0 to 5 in steps of 1 (6 pixels). If a
         coordinate pair from columns 1\&2 matches any pixel centre well
         enough, the data from columns 4\&5 are entered into the
         corresponding element of the data and errors array. The data
         file is OUT.
      }
      \sstexamplesubsection{
         ascin in out [25,39]\ colaxes=5 coldata=[3,0]
      }{
         Here the output is one-dimensional and without errors array
         (thus the zero in COLDATA). Only lines 25 to 39 from IN are
         used. The axis data are from the 5th column and the spectrum
         data from the 3rd column. (Note that columns 1, 2 and 4 must
         contain numeric data.) The axis grid need not be specified. The
         axis values from the table will be taken literally to form a
         grid that is in general non-linear and non-monotonic.
      }
   }
   \sstimplementationstatus{
      It is not possible to read axis values from the table in double
      precision or create a double precision axis array.
   }
}
\goodbreak
\sstroutine{
   ASCOUT
}{
   Write an NDF to an ASCII table
}{
   \sstdescription{
      This routine takes an NDF (section) and writes it to an ASCII
      table. The first part of the output file is a header giving
      textual information and a head for the table. These lines start
      with a blank carriage return control character followed by an
      exclamation mark as the first printed character. The table itself
      has to the left all the axis values and optionally the pixel
      widths, and to the right the data value and its error if known.
      The spectroscopic axis is written with higher precision (12
      significant digits instead of 7) if its storage type is \_DOUBLE.
      The total number of table columns can be 8 at most. All pixel
      widths are written if and only if requested, regardless of whether
      there is explicit information in the input file. Each width
      occupies the column to the right of the corresponding centre
      value.
   }
   \sstusage{
      ascout in out
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [YES]
      }
      \sstsubsection{
         WIDTH = \_LOGICAL (Read)
      }{
         True if pixel widths are to be written, too. [NO]
      }
      \sstsubsection{
         BAD = \_REAL (Read)
      }{
         The alternative bad value. Where the data or variance array has
         bad values, BAD is written to the ASCII table.
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF.
      }
      \sstsubsection{
         OUT = FILENAME (Read)
      }{
         The ASCII output file.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         ascout in(1.5:2.5) out
      }{
         This expects a 1-D data set in IN and will write to the ASCII
         file OUT the information for axis values between 1.5 and 2.5.
         Should IN be more than 1-D, the first hyper-row would be used.
      }
      \sstexamplesubsection{
         ascout in(1.5:2.5,10:15) out
      }{
         This will accept a 2-D data set in IN and write to OUT the
         information for 1st axis coordinate values between 1.5 and 2.5
         and for 2nd axis pixel number between 10 and 15. Note that
         integers in the section specification are interpreted as pixel
         numbers.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.
   }
}
\goodbreak
\sstroutine{
   BBODY
}{
   Calculate a black body spectrum
}{
   \sstdescription{
      This routine calculates for a given (vacuum) wavelength or
      frequency axis the intensity of a black body at given temperature.
      The intensity is the energy per unit time, per unit area, per unit
      solid angle, and per unit frequency (and for all polarisations):
      $$B_{\nu}={{2h\nu^3}\over{c^2}}
         {{1}\over{\exp{\left({{h\nu}\over{kT}}\right)}-1}}$$
      where $c$ is the velocity of light, and $h$ and $k$ are the
      Planck and Boltzmann constants.
   }
   \sstusage{
      bbody temp in=?\ xstart=?\ xstep=?\ xend=?\ xlabel=?\ xunit=?\ out=?
   }
   \sstparameters{
      \sstsubsection{
         LOGAR = LOGICAL (Read)
      }{
         True if the common logarithm of intensity is to be written
         rather than the intensity itself. [NO]
      }
      \sstsubsection{
         TEMP = REAL (Read)
      }{
         The black body temperature in Kelvin.
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The file holding axis data to be used. Enter the null value (!)
         to read axis data parameters from keyboard.
      }
      \sstsubsection{
         XSTART = REAL (Read)
      }{
         The spectroscopic value (pixel centre) for the first output
         pixel.
      }
      \sstsubsection{
         XSTEP = REAL (Read)
      }{
         The spectroscopic step (pixel distance) for the output pixels.
      }
      \sstsubsection{
         XEND = REAL (Read)
      }{
         The spectroscopic value (pixel centre) for the last output
         pixel.
      }
      \sstsubsection{
         XLABEL = CHARACTER (Read)
      }{
         The label for the spectroscopic axis. Allowed values are
         ``wavelength'' and ``frequency''. [wavelength]
      }
      \sstsubsection{
         XUNIT = CHARACTER (Read)
      }{
         The unit for the spectroscopic axis.
         If the label is ``wavelength'' then the unit can basically be ``m''
         for metre, ``micron'' for micrometre, or ``Angstrom'' for
         Angstr\"om. If the label is ``frequency'' then the unit must be
         basically ``Hz'' for Hertz.
         Any of these units may be preceded by a power of ten, so it
         could be ``10$*$$*$1$*$Angstrom'' if you want to use nanometre as unit,
         or ``10$*$$*$$-$9$*$m'' to the same effect. The power must be an
         integer.
         You can achieve a logarithmic axis by specifying something like
         ``log10(10$*$$*$$-$3$*$micron)''. In this example the axis values will be
         the common logarithms of the wavelength in nanometres.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output file.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         bbody 5500 in=in out=out
      }{
         This calculates the black-body spectrum for 5500 K. The
         spectrum is written to file OUT. The routine tries to find all
         necessary information for the 1st (and only) axis in OUT from
         the spectroscopic axis of the file IN. Since LOGAR is left at
         its default value of FALSE, the data are intensity in Jy/sr.
      }
      \sstexamplesubsection{
         \parbox[t]{143mm}{bbody 2.7 logar=true in=!\ xstart=0 xstep=0.05 
           xend=6 xlabel=wavelength \\ xunit=log(micron) out=out}
      }{
         This calculates the black-body spectrum for 2.7 K. The spectrum
         is written to OUT. No input file is specified. The axis
         contains the logarithms of wavelengths in micron, which run
         from 0 (1 micron) to 6 (1 metre). Since LOGAR=TRUE, the data
         are the logarithms of intensity in Jy/sr.
      }
      \sstexamplesubsection{
         \parbox[t]{143mm}{bbody 1e6 logar=true in=!\ xstart=-1 xstep=0.05 
         xend=2 xlabel=frequency \\ xunit=log10(10$*$$*$15$*$Hz) out=out}
      }{
         This calculates the black-body spectrum for 1 million K. This
         time the axis is logarithms of frequency, the units used are
         $10^{15}$ Hz. The frequency range covered is from $10^{14}$ Hz to
         $10^{17}$ Hz.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.
   }
   \sstdiytopic{
      References
   }{
      Lang, K.R., 1980, Astrophysical Formulae, Springer, Heidelberg,
      Berlin, New York, p. 21
   }
}
\goodbreak
\sstroutine{
   CORREL
}{
   Correlate two or three data sets
}{
   \sstdescription{
      This routine correlates two or three data sets. Either pair is
      subjected to a linear fit and the third data set is subjected to a
      two-parameter linear fit (i.e.\ regarded as a linear function of
      the first and second data sets). Each data set may be an NDF
      section. All must have the same dimensions.
   }
   \sstusage{
      correl inlist out logfil
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, input variances are ignored. [YES]
      }
      \sstsubsection{
         INLIST = LITERAL (Read)
      }{
         The group of input NDFs. Two or three NDFs must be specified.
         A complicated INLIST could look something like

         M\_51(25:35,$-$23.0,$-$24.0),M101,NGC1$*$,NGC201\%,$\wedge$LISTFILE.LIS

         This example NDF group specification consists of
         \sstitemlist{

            \sstitem
            one identified NDF from which a subset is to be taken,

            \sstitem
            one identified NDF,

            \sstitem
            two wild-carded NDF specifications, and

            \sstitem
            an indirection to an ASCII file containing more NDF group
               specifications. That file may have comment lines and in-line
               comments, which are recognised as beginning with a hash (\#).
               (Berry, 1992a, 1992b).
         }
      }
      \sstsubsection{
         OUT = FILENAME (Read)
      }{
         The ASCII output file where the data points are written into a
         table. A new file will be opened. No file will be opened, if
         ``!'' is entered.
         The table in OUT is without any information else than the
         values from the 1st, 2nd, 3rd data array and errors from the
         1st, 2nd, 3rd variance array in that order. [!]
      }
      \sstsubsection{
         LOGFIL = FILENAME (Read)
      }{
         The ASCII log file where fit results are written to. This will
         be opened for append, if such a file exists.
      }
   }
}
\goodbreak
\sstroutine{
   EDITEXT
}{
   Edit the Specdre Extension
}{
   \sstdescription{
      This routine allows the user to modify the Specdre Extension. See
      the topic ``Requests'' for details. Users should also consult the
      description of the Specdre Extension in SUN/140.
   }
   \sstusage{
      editext request in
   }
   \sstparameters{
      \sstsubsection{
         REQUEST = \_CHAR (Read)
      }{
         The action required. This consists of blank separated words.
         The following is a brief reminder of the syntax and permissible
         requests. For the full details refer to the ``Requests'' topic.
         \sstitemlist{

            \sstitem
            LIST

            \sstitem
            CREATE

            \sstitem
            CREATE RESULTS type1 type2 type3 int1 int2

            \sstitem
            DELETE

            \sstitem
            DELETE struc

            \sstitem
            SET ndf-struct

            \sstitem
            SET SPECVALS.comp value

            \sstitem
            SET scalar value

            \sstitem
            SET vector element value

            \sstitem
            TYPE scalar type

            \sstitem
            TYPE ndf-struct type

            \sstitem
            TYPE RESULTS type1 type2 type3

            \sstitem
            SHAPE RESULTS int1 int2
         }
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The NDF the Specdre Extension of which is to be modified. The
         modification is done in situ, i.e.\ there is no separate output
         NDF. In most modes, the routine requires update access. Only in
         list mode is read access sufficient.
      }
      \sstsubsection{
         LOGFIL = FILENAME (Read)
      }{
         The filename for the ASCII output file in list mode. If this
         file exists, it is opened for append access. A null value for
         this parameter will signal that no file is to be used. The
         output will then be directed to the standard output device (the
         user's screen).
         [!]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         editext list in accept
      }{
         This will look for the Specdre Extension to the main NDF called IN
         and list the Extension's contents to the default output device
         (usually the user's screen). Some character strings that may be
         up to 32 characters long are truncated to 16 characters in
         order to fit on the screen.
      }
      \sstexamplesubsection{
         editext list in logfil=out
      }{
         This will look for the Specdre Extension to the main NDF called IN
         and list the Extension's contents to the ASCII file OUT.DAT.
         This happens without string truncation.
      }
      \sstexamplesubsection{
         editext delete in
      }{
         This will look for the Specdre Extension to the main NDF called IN
         and delete the Extension.
      }
      \sstexamplesubsection{
         editext "set restframe heliocentric" in
      }{
         This will access the main NDF called IN, find or create its Specdre
         Extension, find or create the RESTFRAME structure in the
         Extension, and put the string ``heliocentric'' into the RESTFRAME
         structure.
      }
      \sstexamplesubsection{
         editext "set frequnit 6"
      }{
         This will access the main NDF called IN, find or create its Specdre
         Extension, find or create the FREQUNIT structure in the
         Extension, and put the value 6 into the FREQUNIT structure.
         This is to mean that reference and laboratory frequencies will
         be expressed in MHz (10$*$$*$6 Hz).
      }
      \sstexamplesubsection{
         editext "set labfreq 5 1420" in
      }{
         This will access the main NDF called IN, find its Specdre Extension
         and find the RESULTS structure in the Extension (which is an
         NDF). If this is successful the routine will find the LABFREQ
         extension of the result NDF and set its fifth element to 1420.
         This is the laboratory frequency of the fifth spectral
         component. In conjunction with a FREQUNIT of 6, this is
         (very roughly) the frequency of the 21 cm ground state
         hyperfine transition of neutral atomic hydrogen.
      }
      \sstexamplesubsection{
         editext "set npara 5 3" in
      }{
         This will access the main NDF called IN, find its Specdre Extension
         and find the RESULTS structure in the Extension (which is an
         NDF). If this is successful the routine will find the NPARA
         extension of the result NDF and set its fifth element to 3.
         This is to mean that the fifth spectral component is allocated
         space for three parameters in the result NDF. Changing this
         number may require to increase the total number of parameters
         which in turn affects the shape of the result NDF and of the
         PARATYPE extension to the result NDF. Changing NPARA(5) also
         makes it necessary to shift information in the result NDF's
         data and variance structures as well as in the PARATYPE
         extension to the result NDF. All this is handled consistently by
         this routine.
      }
      \sstexamplesubsection{
         editext "shape results 6 20" in
      }{
         This will access the main NDF called IN, find or create its Specdre
         Extension, find or create the RESULTS structure in the
         Extension, and shape it to provide for six spectral components
         and a total of 20 parameters. If results existed before, it
         will be expanded or contracted ``at the end''. That is, existing
         components 1 to 6 and parameter 1 to 20 would be retained.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
   \sstdiytopic{
      Requests
   }{
      The request or action required consists of blank-separated
      words. The first word is a verb specifying the kind of action.
      The verb can be LIST, CREATE, DELETE, SET, TYPE or SHAPE. The
      verb is case-insensitive. The length of the request is
      restricted to 130 characters.

      There may or may not follow a second word specifying the
      structure affected. This can be any of the scalar structures in
      the Specdre Extension, i.e.\ SPECAXIS, RESTFRAME, INDEXREFR,
      FREQREF, FREQUNIT. It can also be any of the NDF-type structures
      in the Specdre Extension, i.e.\ SPECVALS, SPECWIDS, COVRS, RESULTS.
      Finally it can be any structure which is an extension to the
      (NDF-)structure RESULTS. These latter structures are all HDS
      vectors, their names are LINENAME, LABFREQ, COMPTYPE, NPARA,
      MASKL, MASKR, PARATYPE. The structure specification is
      case-insensitive.

      Further words contain parameter values, usually one word per
      parameter. But if the last parameter is a string, it may
      consist of several words. No quotes are necessary.

      There is only one LIST request, namely the sole word LIST. This
      will cause the complete Specdre Extension -- except the contents of
      NDF arrays -- to be listed to the log file or to the screen.

      There are two possible CREATE requests.
      \sstitemlist{

         \sstitem
         ``CREATE'' on its own will create an empty Specdre Extension,
            or fail if a Specdre Extension already exists.

         \sstitem
         ``CREATE RESULTS type1 type2 type3 int1 int2'' needs five
            parameters. Three parameters are case-insensitive HDS data
            types. These are either \_DOUBLE or assumed to be \_REAL. The
            result structure is an NDF-type structure and the different
            type specifications apply to (i) the data and variance
            structures of the NDF, (ii) the laboratory frequency
            extension to the result NDF, (iii) the left and right mask
            extensions to the result NDF. All extensions to the result
            NDF are HDS vectors. Some of these have one element for
            each spectral component, their created length is specified
            by the fourth (last but one) request parameter, i.e.\ the
            sixth word. This word must convert to an integer greater
            than zero. Other HDS vectors in the extension to the result
            NDF have one element for each result parameter, their
            created length is specified by the fifth (last) request
            parameter, i.e.\ the seventh word. This word must convert to
            an integer greater than zero. ``CREATE RESULTS'' fails if the
            result NDF already exists.

      }
      ``DELETE'' on its own will delete the whole Specdre Extension.
      ``DELETE struc'' will delete the specified structure. This can be
      any of the NDF-type structures SPECVALS, SPECWIDS, COVRS, RESULTS.
      Deleting a structure does not include deleting the whole
      Extension, even if it becomes empty.

      All SET request will create the Specdre Extension, even if the
      request is not recognised as a valid one.

      ``SET ndf-struct'', where the second word specifies an NDF-type
      structure, will set the values of the specified structure to
      bad values. This does not work for COVRS, since it defaults to
      non-existence. The structure is created if it does not already
      exist. For SPECVALS and SPECWIDS only the NDF's data structure
      is affected. For RESULTS the NDF's data and variance structures
      are set to bad values, but all the vectors in the result NDF's
      extension remain unchanged.
      \sstitemlist{

         \sstitem
         ``SET SPECVALS'' will set the values in the data array of
            spectroscopic values to the default values. These are copies
            of the spectroscopic axis centres in the main NDF.

         \sstitem
         ``SET SPECWIDS'' will set the values in the data array of
            spectroscopic widths to the default values. These are copies
            of the spectroscopic axis widths in the main NDF.

         \sstitem
         ``SET RESULTS'' will set the values in the data and variance
            arrays of the result NDF to bad values.

      }
      ``SET SPECVALS.comp value'' can be used to set the label and unit
      components of the spectroscopic values' NDF.
      \sstitemlist{

         \sstitem
         ``SET SPECVALS.LABEL label'' will set the value of the label
            of the spectroscopic values' NDF.

         \sstitem
         ``SET SPECVALS.UNITS unit'' will set the value of the unit of
            the spectroscopic values' NDF.

      }
      ``SET scalar value'' will convert the third word to a value and
      put it in the scalar structure specified by the second word.
      \sstitemlist{

         \sstitem
         ``SET SPECAXIS int'' will try to convert the third word into
            an integer. It must be between 1 and the number of axes in
            the NDF to which this Specdre Extension is an extension. If
            the value is actually changed, then this command will also
            delete the NDF-type structures SPECVALS, COVRS and RESULTS.
            This is because the contents of those structures depends on
            the choice of spectroscopic axis and become invalid when the
            value is changed. This command will also create the Specdre
            Extension and spectroscopic axis structure if they do not
            yet exist.

         \sstitem
         ``SET RESTFRAME more words'' will put the third and following
            words (case-sensitive) into the reference frame structure.
            This command will also create the Specdre Extension and
            reference frame structure if they do not yet exist.

         \sstitem
         ``SET INDEXREFR value'' will try to convert the third word
            into a real or double value, depending on the current type
            of the refractive index structure. This command will also
            create the Specdre Extension and refractive index structure
            if they do not yet exist.

         \sstitem
         ``SET FREQREF value'' will try to convert the third word
            into a real or double value, depending on the current type
            of the reference frequency structure. This command will also
            create the Specdre Extension and reference frequency
            structure if they do not yet exist.

         \sstitem
         ``SET FREQUNIT int'' will try to convert the third word into
            an integer. This command will also create the Specdre
            Extension and frequency unit structure if they do not yet
            exist.

      }
      ``SET vector element value'' will change the value of the
      specified element in the specified vector. The vector must be
      one of the extensions of the result NDF. The result NDF must
      exist beforehand, which implies the existence of the vector.
      The vector must also be long enough to contain the element
      specified and the element number must be integer and greater
      than zero. There are two kinds of vectors, those indexed by
      spectral component and those indexed by result parameter.
      \sstitemlist{

         \sstitem
         ``SET LINENAME comp more words'' will put the forth and
            following words (case-sensitive) into the comp-th element
            of the line name structure.

         \sstitem
         ``SET LABFREQ comp value'' will try to convert the fourth
            word into a real or double value, depending on the current
            type of the laboratory frequency structure. It will then put
            the value into the comp-th element of the laboratory
            frequency structure.

         \sstitem
         ``SET COMPTYPE comp more words'' will put the forth and
            following words (case-sensitive) into the comp-th element
            of the component type structure.

         \sstitem
         ``SET NPARA comp npara'' will try to convert the fourth word
            into an integer greater than or equal to zero. This is the
            new number of parameters allocated to the comp-th component.
            Changing this value will affect several parts of the result
            structure both in their shapes and values. If the comp-th
            spectral component is allocated more parameters than before,
            then it may be necessary to provide for a higher total
            number of parameters, which implies increasing the size of
            .MORE.SPECDRE.RESULTS.DATA\_ARRAY and VARIANCE and of
            .MORE.SPECDRE.RESULTS.MORE.PARATYPE. At any rate, the
            information about spectral components with indices higher
            than comp must be relocated within those arrays.

         \sstitem
         ``SET MASKL comp value'' and ``SET MASKR comp value'' will try
            to convert the fourth word into a real or double value,
            depending on the current type of the mask structures. It
            will then put the value into the comp-th element of the
            relevant mask structure.

         \sstitem
         ``SET PARATYPE para more words'' will put the forth and
            following words (case-sensitive) into the para-th element
            of the parameter type structure.

      }
      A TYPE request can be applied to \_REAL or \_DOUBLE structures,
      and of these to scalars and NDF-type structures. Changing the
      type(s) of the result NDF needs specification of three separate
      types.
      \sstitemlist{

         \sstitem
         ``TYPE scalar type'' can be applied to INDEXREFR and FREQREF.
            The type specification is case-insensitive. If it is not
            \_DOUBLE, then \_REAL is assumed.

         \sstitem
         ``TYPE ndf-struct type'', will change the type of the
            specified NDF. The type specification is case-insensitive. It
            must be \_DOUBLE or is assumed to be \_REAL. This command can
            be applied to SPECVALS, SPECWIDS and COVRS. SPECVALS and
            SPECWIDS will be created if necessary, COVRS will not be
            created.

         \sstitem
         ``TYPE RESULTS type1 type2 type3'' will change the types of
            (i) the NDF's data and variance, (ii) the NDF's laboratory
            frequency extension, (iii) the NDF's mask extensions. the
            parameters are case-insensitive. They must be \_DOUBLE or are
            assumed to be \_REAL. This command includes creation of the
            result structure if necessary.

      }
      ``SHAPE RESULTS int1 int2'' will change the shape of the result
      structure. The two command parameters must convert to integers
      greater than zero. The first is the number of spectral
      components to be provided for, the second is the total number
      of parameters. If the result structure does not exist, then it
      is created. If it exists, then existing values are retained
      unless they were stored outside the new bounds.
   }
}
\goodbreak
\sstroutine{
   EVALFIT
}{
   Evaluate fit results
}{
   \sstdescription{
      This routine turns components in the result structure of the
      Specdre Extension into a fake data set representing those results.
      Such a data set is necessary to perform arithmetic operations
      between the result (otherwise expressed only as a set of
      parameters) and the original data.

      The routine takes as input a base NDF (a section is not
      acceptable). The output is a copy of the input, except for the
      main NDF data and variance components. These are re-calculated from
      certain components in the result structure of the Specdre
      Extension. Thus the output contains the fit results both in the
      result structure and in the main NDF. The main NDF can then be
      compared pixel by pixel with the original data.

      If the input main NDF has a variance component, the output
      variances will be set to zero.

      This routine recognises result components created by FITCHEBY,
      FITGAUSS, or FITTRI. Unrecognised components are ignored, i.e.\ not
      added into the data. A warning to that effect is given.
      If a component in any particular position has bad values as
      parameters, then that component is ignored on that position. No
      warning to this effect is given.

      A component is accepted as 7th order series of Chebyshev
      polynomials if the component type is `Chebyshev series' and it has
      11 parameters. These are assumed to be order, xmin, xmax, coeff0
      ... coeff7.

      A component is accepted as Gauss or triangle if the component type
      is `Gauss' or `triangle' and it has 4 parameters. The first three
      are assumed to be centre, peak, FWHM.

      The string comparison to check the component type is
      case-insensitive.
   }
   \sstusage{
      evalfit in out comp=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, this routine will issue only error messages and no
         informational message. [YES]
      }
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If `T', the routine can evaluate several sets of components.
         After a set of components has been evaluated, the user will be
         asked whether she wants to specify another set. [`T']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. This must be a base NDF. If you need only a
         section of an NDF, you use SUBSET first to create the section
         permanently.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output NDF.
      }
      \sstsubsection{
         COMP = \_INTEGER (Read)
      }{
         The numbers of up to 6 components to be added into the output
         data component. If you are not sure which component is which,
         you should inspect the result structure of the data first with
         EDITEXT.
      }
      \sstsubsection{
         REPLY = \_LOGICAL (Read)
      }{
         Set true to work on another set of components. This parameter
         is relevant only if DIALOG is true.
         [NO]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         evalfit in out comp=[2,5,1,2] accept
      }{
         This will take the input NDF IN and create an equally shaped
         NDF called OUT. The specified components stored in IN's (and
         OUT's) Specdre Extension are evaluated and added up to make up
         the main data in OUT. Note that component no. 2 is added twice.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.
   }
}
\goodbreak
\sstroutine{
   FILLCUBE
}{
   Copy one NDF into part of another
}{
   \sstdescription{
      This routine copies data, variance etc. from one NDF into another
      existing NDF. By successive calls the output NDF can be filled
      with data from a number of input NDFs. The target area in the
      output is identified by matching axis data (not pixel indices).
      Data are copied from input to output only if the input data value
      is not bad, apart from that existing data in the output are
      overwritten.

      This application is more akin to ASCIN than to GROW. The main
      differences to ASCIN are that FILLCUBE updates an existing output
      and that its input is an NDF rather than an ASCII table.
      Its main advantage over GROW is that input and output may
      (actually must) have the same dimensionality, but any dimensions
      or axis data can differ. Also it is not necessary that target
      pixels form a contiguous subset in the output: The input pixels
      could match, say, every second or third output pixel.
      The disadvantages are that results and spectroscopic values in the
      Specdre Extension are not handled, and that the coordinates along
      each axis in input and output must be linear.

      For each input pixel, FILLCUBE looks for the output pixel that is
      nearest in the space of axis data coordinates. Data are copied
      only if the output pixel is hit close to its centre. However, if
      an axis is degenerate (has only one pixel) in both input and
      output, then the coordinates are assumed to match.

      No indication is given as to how many input pixels did not match
      any output pixel.
   }
   \sstusage{
      fillcube in out
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         True if informational messages are to be issued.
      }
      \sstsubsection{
         TOL = \_REAL (Read)
      }{
         The tolerated fraction of the pixel size by which the input
         coordinates may deviate from the output coordinates. If any one
         of the axis values deviates more than TOL times the coordinate
         step, then the input data are ignored and the output data left
         unchanged. [0.2]
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output NDF. This must already exist, update access is
         required.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7, although
      it is largely ignored.

      This routine works in situ on an existing output file.

      Spectroscopic values must not exist in the Extension of either
      the input or the output NDF: A unique coordinate axis is
      required for all axes, including the spectroscopic one, in
      order to locate the target pixels by matching coordinates
      between input and output. If this is inconvenient, GROW may be
      a more suitable application for your purpose.

      Spectroscopic widths must not exist in the Extension of the
      output NDF and are ignored in the input NDF: This information
      is likely to be present only when spectroscopic values are
      present as well.

      Covariance row sums must not exist in the Extension of the
      output NDF: The validity of this information is difficult to
      assess when only parts of spectra might be copied from one cube
      to another, and when these parts are contiguous in the input
      but might not be in the output. Input covariance row sums are
      ignored.

      The results in the input Extension are ignored, and results
      must not exist in the output Extension.
   }
}
\goodbreak
\sstroutine{
   FITBB
}{
   Fit diluted Planck curves to a spectrum
}{
   \sstdescription{
      This routine fits up to six diluted Planck curves to a
      one-dimensional data set. This can be specified as an NDF section.
      The data set must extend along the spectroscopic axis. The fit is
      done on a double logarithmic representation of the data. The axis
      data must be the common logarithm of frequency in Hertz. The data
      themselves must be the common logarithm of intensity or flux
      density in arbitrary units.

      A diluted Plank component is defined as
      $$10^{f_j} = 10^{\Theta_j}
           \left({{\nu}\over{\rm Hz}}\right)^{\alpha_j}
           {{2h\nu^3}\over{c^2}}
           {{1}\over{\exp{\left({{h\nu}\over{kT_j}}\right)}-1}}$$
      This assumes that the optical depth is small and the emissivity is
      proportional to the frequency to the power of alpha. $10^\Theta$ is
      the hypothetical optical depth at frequency 1 Hz.

      If the optical depth is large, a single simple Planck function
      should be fitted, i.e.\ only one component with $\alpha = 0$. In this
      case $10^\Theta$ is the conversion factor from the Planck function in
      Jy/sr to the (linear) data values. If for example the data are the
      common logarithm of the calibrated flux density of a source in Jy,
      then Theta is the logarithm of the solid angle (in sr) subtended
      by the source.

      The fit is performed in double logarithmic representation, i.e.\
      the fitted function is

      $$f = \lg{\left[\sum_j 10^{f_j}\right]}$$

      The choice of $\Theta$, $\alpha$ and lg($T$) as fit parameters is
      intuitive, but makes the fit routine ill-behaved. Very often $\alpha$
      cannot be fitted at all and must be fixed. $\Theta$ and $\alpha$ usually
      anti-correlate completely. Even with fixed $\alpha$ do $\Theta$ and lg($T$)
      anti-correlate strongly.

      Furthermore, $\Theta$ is difficult to guess. From any initial guess
      of $\Theta$ one can improve by using $\Theta$ plus the average
      deviation of the data from the guessed spectrum.

      After accessing the data and the (optional) plot device, the data
      will be subjected to a mask that consists of up to six abscissa
      intervals. These may or may not overlap and need not lie within
      the range of existing data. The masking will remove data which are
      bad, have bad variance or have zero variance. The masking will
      also provide weights for the fit. If the given data have no
      variances attached, or if the variances are to be ignored, all
      weights will be equal.

      After the data have been masked, guessed values for the fit are
      required. These are
      \sstitemlist{

         \sstitem
         the number of components to be fitted,

         \sstitem
         the components' guessed scaling constants $\Theta$,

         \sstitem
         emissivity exponents $\alpha$ and

         \sstitem
         common logarithms of colour temperatures in Kelvin. Finally,

         \sstitem
         fit flags for each of the parameters are needed.

      }
      The fit flags specify whether any parameter is fixed, fitted, or
      kept at a constant offset to another fitted parameter.

      The masked data and parameter guesses are then fed into the fit
      routine. Single or multiple fits are made. Fit parameters may be
      free, fixed, or tied to the corresponding parameter of another
      component fitted at the same time. They are tied by fixing the
      offset, Up to six components can be fitted simultaneously.

      The fit is done by minimising chi-squared (or rms if variances are
      unavailable or are chosen to be ignored). The covariances between
      fit parameters -- and among these the uncertainties of parameters --
      are estimated from the curvature of psi-squared. psi-squared is
      usually the same as chi-squared. If, however, the given data are
      not independent measurements, a slightly modified function
      psi-squared should be used, because the curvature of chi-squared
      gives an overoptimistic estimate of the fit parameter uncertainty.
      In that function the variances of the given measurements are
      substituted by the sums over each row of the covariance matrix of
      the given data. If the data have been resampled with a Specdre
      routine, that routine will have stored the necessary additional
      information in the Specdre Extension, and this routine will
      automatically use that information to assess the fit parameter
      uncertainties. A full account of the psi-squared function is given
      in Meyerdierks, 1992a/b. But note that these covariance row sums
      are ignored if the main variance is ignored or unavailable.

      If the fit is successful, then the result is reported to
      the standard output device and plotted on the graphics device. The
      final plot viewport is saved in the AGI data base and can be used
      by further applications.

      The result is stored in the Specdre Extension of the input NDF.
      Optionally, the complete description (input NDF name, mask used,
      result, etc.) is written (appended) to an ASCII log file.

      Optionally, the application can interact with the user. In that
      case, a plot is provided before masking, before guessing and
      before fitting. After masking, guessing and fitting, a screen
      report and a plot are provided and the user can improve the
      parameters. Finally, the result can be accepted or rejected, that
      is, the user can decide whether to store the result in the Specdre
      Extension or not.

      The screen plot consists of two viewports. The lower one shows the
      data values (full-drawn bin-style) overlaid with the guess or fit
      (dashed line-style). The upper box shows the residuals (cross
      marks) and error bars. The axis scales are arranged such that
      all masked data can be displayed. The upper box displays a
      zero-line for reference, which also indicates the mask.

      The Extension provides space to store fit results for each
      non-spectroscopic coordinate. Say, if you have a 2-D image each
      row being a spectrum, then you can store results for each row. The
      whole set of results can be filled successively by fitting one row
      at a time and always using the same component number to store the
      results for that row. (See also the example.)

      The components fitted by this routine are specified as follows:
      The line names and laboratory frequencies are the default values
      and are not checked against any existing information in the
      input's Specdre Extension. The component types are `Planck'. The
      numbers of parameters allocated to each component are 3, the
      three guessed and fitted parameters. The parameter types are in
      order of appearance: `Theta', `alpha', `lg(T)'.
   }
   \sstusage{
      \parbox[t]{143mm}{fitbb in device=?\ mask1=?\ mask2=?\ ncomp=?\ 
      theta=?\ alpha=?\ lgtemp=? \\ sf=?\ af=?\ tf=?\ comp=?\ logfil=?}
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, this routine will issue only error messages and no
         informational message. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, input variances are ignored. [YES]
      }
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If `T', the routine offers in general more options for
         interaction. The mask or guess can be improved after
         inspections of a plot. Also, the routine can resolve
         uncertainties about where to store results by consulting the
         user. [`T']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. This must be a one-dimensional (section of an)
         NDF. You can specify e.g.\ an image column as IN(5,) or part of
         an image row as IN(2.2:3.3,10). Update access is necessary to
         store the fit result in the NDF's Specdre Extension.
      }
      \sstsubsection{
         REPAIR = \_LOGICAL (Read)
      }{
         If DIALOG is true, REPAIR can be set true in order to change
         the spectroscopic number axis in the Specdre Extension. [NO]
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the plot device. Enter the null value (!) to
         disable plotting. [!]
      }
      \sstsubsection{
         MASK1( 6 ) = \_REAL (Read)
      }{
         Lower bounds of mask intervals. The mask is the part(s) of the
         spectrum that is (are) fitted and plotted. The mask is put
         together from up to six intervals:
         \begin{eqnarray*}
            mask&=&[MASK1(1);MASK2(1)]\cup[MASK1(2);MASK2(2)]\\
                &\cup& ... \cup[MASK1(MSKUSE);MASK2(MSKUSE)]
         \end{eqnarray*}
         The elements of the MASK parameters are not checked for
         monotony. Thus intervals may be empty or overlapping. The
         number of intervals to be used is derived from the number of
         lower/upper bounds entered. Either MASK1 or MASK2 should be
         entered with not more numbers than mask intervals required.
      }
      \sstsubsection{
         MASK2( 6 ) = \_REAL (Read)
      }{
         Upper bounds of mask intervals. See MASK1.
      }
      \sstsubsection{
         NCOMP = \_INTEGER (Read)
      }{
         The number of Planck curves to be fitted. Must be between 1
         and 6. [1]
      }
      \sstsubsection{
         THETA( 6 ) = \_REAL (Read)
      }{
         Guess scaling constant for each diluted Planck component.
      }
      \sstsubsection{
         ALPHA( 6 ) = \_REAL (Read)
      }{
         Guess emissivity exponent for each diluted Planck component.
      }
      \sstsubsection{
         LGTEMP( 6 ) = \_REAL (Read)
      }{
         Guess common logarithm of colour temperature in Kelvin for
         each diluted Planck component.
      }
      \sstsubsection{
         SF( 6 ) = \_INTEGER (Read)
      }{
         For each component I, a value SF(I)=0 indicates that
         THETA(I) holds a guess which is free to be fitted.
         A positive value SF(I)=I indicates that THETA(I) is fixed.
         A positive value SF(I)=J$<$I indicates that THETA(I) has to
         keep a fixed offset from THETA(J).
      }
      \sstsubsection{
         AF( 6 ) = \_INTEGER (Read)
      }{
         For each component I, a value AF(I)=0 indicates that
         ALPHA(I) holds a guess which is free to be fitted.
         A positive value AF(I)=I indicates that ALPHA(I) is fixed.
         A positive value AF(I)=J$<$I indicates that ALPHA(I) has to
         keep a fixed offset to ALPHA(J).
      }
      \sstsubsection{
         TF( 6 ) = \_INTEGER (Read)
      }{
         For each component I, a value TF(I)=0 indicates that
         LGTEMP(I) holds a guess which is free to be fitted.
         A positive value TF(I)=I indicates that LGTEMP(I) is fixed.
         A positive value TF(I)=J$<$I indicates that LGTEMP(I) has to
         keep a fixed ratio to LGTEMP(J).
      }
      \sstsubsection{
         REMASK = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the mask.
         [NO]
      }
      \sstsubsection{
         REGUESS = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the guess and
         fit. [NO]
      }
      \sstsubsection{
         FITGOOD = \_LOGICAL (Read)
      }{
         Reply YES to store the result in the Specdre Extension. [YES]
      }
      \sstsubsection{
         COMP = \_INTEGER (Read)
      }{
         The results are stored in the Specdre Extension of the data.
         This parameter specifies which existing components are being
         fitted. You should give NCOMP values, which should all be
         different and which should be between zero and the number of
         components that are currently stored in the Extension. Give a
         zero for a hitherto unknown component. If a COMP element is
         given as zero or if it specifies a component unfit to store the
         results of this routine, then a new component will be created
         in the result storage structure. In any case this routine will
         report which components were actually used and it will deposit
         the updated values in the parameter system. [1,2,3,4,5,6]
      }
      \sstsubsection{
         LOGFIL = FILENAME (Read)
      }{
         The file name of the log file. Enter the null value (!) to
         disable logging. The log file is opened for append. [!]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         \parbox[t]{143mm}{fitbb in device=xw mask1=10.5 mask2=14.5
            ncomp=1 theta=0.5 alpha=0 \\ lgtemp=3.5 sf=0 af=1 tf=0
            comp=1 logfil=planck}
      }{
         This fits a Planck curve to the range of frequencies between
         about 30 GHz and 3E14 Hz. The temperature is guessed to be
         3000 K. The fit result is reported to the text file PLANCK and
         stored as component number 1 in the input file's Specdre
         Extension.
         Since DIALOG is not turned off, the user will be prompted for
         improvements of the mask and guess, and will be asked whether
         the final fit result is to be accepted (stored in the Extension
         and written to PLANCK.DAT).
         The XWINDOWS graphics device will display the spectrum before
         masking, guessing, and fitting. Independent of the DIALOG
         switch, a plot is produced after fitting.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
   \sstdiytopic{
      References
   }{
      Meyerdierks, H., 1992a, Covariance in resampling and model fitting,
      Starlink, Spectroscopy Special Interest Group

      Meyerdierks, H., 1992b, Fitting resampled spectra, in P.J.
      Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data Analysis
      Workshop, Garching, 13 - 14 May 1992, ESO Conference and Workshop
      Proceedings No. 41, Garching bei M\"unchen, 1992
   }
}
\goodbreak
\sstroutine{
   FITCHEBY
}{
   Fit a series of Chebyshev polynomials to a spectrum
}{
   \sstdescription{
      This routine fits a series of Chebyshev polynomials to a
      one-dimensional data set. This can be specified as an NDF section.
      The data set must extend along the spectroscopic axis. The
      abscissa values must be monotonic (increasing or decreasing).

      After accessing the data and the (optional) plot device, the data
      will be subjected to a mask that consists of up to six abscissa
      intervals. These may or may not overlap and need not lie within
      the range of existing data. The masking will remove data which are
      bad, have bad variance or have zero variance. The masking will
      also provide weights for the fit. If the given data have no
      variances attached, or if the variances are to be ignored, all
      weights will be equal.

      The masked data are then fed into the fit routine. The highest
      polynomial order possible is 7. The fit weights data points
      according to their errors. The coefficients reported are those of
      a finite series in Chebyshev polynomials (first kind).
      Let $(x,y)$ be the measurements with $x_{min}$ and $x_{max}$ the extreme
      $x$ values, $y(x)$ be the polynomial of order $n$ fitting the
      measurements, $c_i$ $(i = 1, ..., n+1)$ be the fitted coefficients.
      Then $y(x)$ can be calculated as
         $$y(x) = 0.5 c_1 T_0(x') + c_2 T_1(x') + c_3 T_2(x')
                                           + ... + c_{n+1} T_n(x')$$
         $$x' = [(x - x_{min}) - (x_{max} - x)] / [x_{max} - x_{min}]$$
         $$T_i(x') := \cos( i \arccos(x') )$$
      $T_i$ are defined only for $x'$ in the interval $[-1;+1]$,
      corresponding
      to $x$ in the interval $[x_{min};x_{max}]$. Thus these two numbers are
      regarded as fit parameters in addition to the coefficients $c_i$.

      If the fit is successful, then the result is reported to the
      screen and plotted on the graphics device. The final plot viewport
      is saved in the AGI data base and can be used by further
      applications.

      The result is stored in the Specdre Extension of the input NDF.
      Optionally, the complete description (input NDF name, mask used,
      result, etc.) is written (appended) to an ASCII log file.

      Optionally, the application can interact with the user. In that
      case, a plot is provided before masking and before specifying the
      polynomial order. After masking and fitting, a screen report and a
      plot (optional) are provided and the user can improve the
      parameters. Finally, the result can be accepted or rejected, that
      is the user can decide whether to store the result in the Specdre
      Extension or not.

      The screen plot consists of two viewports. The lower one shows the
      data values (full-drawn bin-style) overlaid with the fit (dashed
      line-style). The upper box shows the residuals (cross marks)
      and error bars. The axis scales are arranged such that
      all masked data can be displayed. The upper box displays a
      zero-line for reference, which also indicates the mask.

      The Extension provides space to store fit results for each
      non-spectroscopic coordinate. Say, if you have a 2-D image each
      row being a spectrum, then you can store results for each row. The
      whole set of results can be filled successively by fitting one row
      at a time and always using the same component number to store the
      results for that row. (See also the example.)

      The component fitted by this routine is specified as follows: The
      line name and laboratory frequency are the default values and are
      not checked against any existing information in the input's
      Specdre Extension. The component type is `Chebyshev series'. The
      number of parameters allocated to the component is 11. The
      parameter types are in order of appearance: `order', `xmin',
      `xmax', `coeff0', ... `coeff7'. Note that $x_{min}$ and $x_{max}$ are
      essential to evaluate the Chebyshev series. Unused coefficients are
      stored as zero.
   }
   \sstusage{
      fitcheby in device=?\ mask1=?\ mask2=?\ order=?\ comp=?\ logfil=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, input variances are ignored. [YES]
      }
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If `T', the routine offers in general more options for
         interaction. The mask or guess can be improved after
         inspections of a plot. Also, the routine can resolve
         uncertainties about where to store results by consulting the
         user. [`T']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. This must be a one-dimensional (section of an)
         NDF. You can specify e.g.\ an image column as IN(5,) or part of
         an image row as IN(2.2:3.3,10). Update access is necessary to
         store the fit result in the NDF's Specdre Extension.
      }
      \sstsubsection{
         REPAIR = \_LOGICAL (Read)
      }{
         If DIALOG is true, REPAIR can be set true in order to change
         the spectroscopic number axis in the Specdre Extension. [NO]
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the plot device. Enter the null value (!) to
         disable plotting. [!]
      }
      \sstsubsection{
         MASK1( 6 ) = \_REAL (Read)
      }{
         Lower bounds of mask intervals. The mask is the part(s) of the
         spectrum that is (are) fitted and plotted. The mask is put
         together from up to six intervals:
         \begin{eqnarray*}
            mask&=&[MASK1(1);MASK2(1)]\cup[MASK1(2);MASK2(2)]\\
                &\cup& ... \cup[MASK1(MSKUSE);MASK2(MSKUSE)]
         \end{eqnarray*}
         The elements of the MASK parameters are not checked for
         monotony. Thus intervals may be empty or overlapping. The
         number of intervals to be used is derived from the number of
         lower/upper bounds entered. Either MASK1 or MASK2 should be
         entered with not more numbers than mask intervals required.
      }
      \sstsubsection{
         MASK2( 6 ) = \_REAL (Read)
      }{
         Upper bounds of mask intervals. See MASK1.
      }
      \sstsubsection{
         ORDER = \_INTEGER (Read)
      }{
         The polynomial order of the fit. Must be between 0 and 7. [1]
      }
      \sstsubsection{
         REMASK = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the mask.
         [NO]
      }
      \sstsubsection{
         REGUESS = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the guess and
         fit. [NO]
      }
      \sstsubsection{
         FITGOOD = \_LOGICAL (Read)
      }{
         Reply YES to store the result in the Specdre Extension. [YES]
      }
      \sstsubsection{
         COMP = \_INTEGER (Read and Write)
      }{
         The results are stored in the Specdre Extension of the data.
         This parameter specifies which existing component is being
         fitted. It should be between zero and the number of components
         that are currently stored in the Extension. Give zero for a
         hitherto unknown component. If COMP is given as zero or if it
         specifies a component unfit to store the results of this
         routine, then a new component will be created in the result
         storage structure. In any case this routine will report which
         component was actually used and it will deposit the updated
         value in the parameter system. [1]
      }
      \sstsubsection{
         LOGFIL = FILENAME (Read)
      }{
         The file name of the log file. Enter the null value (!) to
         disable logging. The log file is opened for append. [!]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitcheby in device=!\ mask1=2.2 mask2=3.3 order=3 comp=1 logfil=!
      }{
         IN is a 1-D NDF. A 3rd order fit is made to the abscissa range
         between 2.2 and 3.3. The result is stored in component number 1
         of the result structure in the Specdre Extension of IN. The
         plot device and ASCII log file are de-selected.
      }
      \sstexamplesubsection{
         \parbox[t]{143mm}{fitcheby in(,15) device=xw mask1=[2.0,2.3,3.4]\ 
         mask2=[2.1,3.2,4.0] \\ order=2 comp=0 logfil=myfil}
      }{
         Here IN is 2-D and the 15th row is selected as the 1-D input
         for the fit. The mask consists of three intervals
         [2.0;2.1] $\cup$ [2.3;3.2] $\cup$ [3.4,4.0]. The fit is a parabola. Space
         for a new component is created for storage in the Specdre
         Extension. The plot device is xwindows.
      }
      \sstexamplesubsection{
         \parbox[t]{143mm}{fitcheby in(,20) device=xw
         mask1=[2.0,2.3,3.4]\ mask2=[2.1,3.2,4.0] \\ order=4 comp=2
         logfil=myfil}
      }{
         In a follow-up from the previous example, now the 20th row is
         fitted with 4th order. If in the previous run the routine told
         us that it had used component number 2, then COMP=2 is what we
         want to use to store a similar fit for a different row.
         The first time round, the description of component 2 was
         created, saying that it is a Chebyshev series with order of 7
         or less etc. And the fit result for the 15th row was stored in
         an array that has space for all rows in the input file.
         So the second time round, FITCHEBY checks whether component 2
         is suitable, whether it is a Chebyshev series with maximum
         order 7. It then stores the new result for the 20th row in the
         place reserved for this row.
         Gradually all rows can be fitted and their results stored in
         the Extension. Possibly this could be automated by writing a
         looping ICL procedure or shell script.
         In the end the corresponding results for all rows are stored in
         one data structure, and could for example be converted into a
         plot of the n-th parameter value versus row number.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
}
\goodbreak
\sstroutine{
   FITGAUSS
}{
   Fit Gauss profiles to a spectrum
}{
   \sstdescription{
      This routine fits up to six Gauss profiles at a time to a
      one-dimensional data set. This can be specified as an NDF section.
      The data set must extend along the spectroscopic axis.

      After accessing the data and the (optional) plot device, the data
      will be subjected to a mask that consists of up to six abscissa
      intervals. These may or may not overlap and need not lie within
      the range of existing data. The masking will remove data which are
      bad, have bad variance or have zero variance. The masking will
      also provide weights for the fit. If the given data have no
      variances attached, or if the variances are to be ignored, all
      weights will be equal.

      After the data have been masked, guessed values for the fit are
      required. These are
      \sstitemlist{

         \sstitem
         the number of components to be fitted,

         \sstitem
         the value of any underlying constant continuum (this must be an
            a-priori known constant),

         \sstitem
         the components' guessed centre positions,

         \sstitem
         peak heights and

         \sstitem
         full widths at half maxima. Finally,

         \sstitem
         fit flags for each of the Gauss parameters are needed.

      }
      The fit flags specify whether any parameter is fixed, fitted, or
      kept at a constant ratio or offset to another fitted parameter.

      The masked data and parameter guesses are then fed into the fit
      routine. Single or multiple Gauss fits are made to line features.
      Gauss fit parameters may be free, fixed, or tied to the
      corresponding parameter of another Gauss component fitted at the
      same time. Peak and width are tied by fixing the ratios, the
      centre is tied by fixing the offset. Up to six Gauss components
      can be fitted simultaneously.

      The fit is done by minimising chi-squared (or rms if variances are
      unavailable or are chosen to be ignored). The covariances between
      fit parameters -- and among these the uncertainties of parameters --
      are estimated from the curvature of psi-squared. psi-squared is
      usually the same as chi-squared. If, however, the given data are
      not independent measurements, a slightly modified function
      psi-squared should be used, because the curvature of chi-squared
      gives an overoptimistic estimate of the fit parameter uncertainty.
      In that function the variances of the given measurements are
      substituted by the sums over each row of the covariance matrix of
      the given data. If the data have been resampled with a Specdre
      routine, that routine will have stored the necessary additional
      information in the Specdre Extension, and this routine will
      automatically use that information to assess the fit parameter
      uncertainties. A full account of the psi-squared function is given
      in Meyerdierks, 1992a/b. But note that these covariance row sums
      are ignored if the main variance is ignored or unavailable.

      If the fit is successful, then the result is reported to
      the standard output device and plotted on the graphics device. The
      final plot viewport is saved in the AGI data base and can be used
      by further applications.

      The result is stored in the Specdre Extension of the input NDF.
      Optionally, the complete description (input NDF name, mask used,
      result, etc.) is written (appended) to an ASCII log file.

      Optionally, the application can interact with the user. In that
      case, a plot is provided before masking, before guessing and
      before fitting. After masking, guessing and fitting, a screen
      report and a plot are provided and the user can improve the
      parameters. Finally, the result can be accepted or rejected, that
      is, the user can decide whether to store the result in the Specdre
      Extension or not.

      The screen plot consists of two viewports. The lower one shows the
      data values (full-drawn bin-style) overlaid with the guess or fit
      (dashed line-style). The upper box shows the residuals (cross
      marks) and error bars. The axis scales are arranged such that
      all masked data can be displayed. The upper box displays a
      zero-line for reference, which also indicates the mask.

      The Extension provides space to store fit results for each
      non-spectroscopic coordinate. Say, if you have a 2-D image each
      row being a spectrum, then you can store results for each row. The
      whole set of results can be filled successively by fitting one row
      at a time and always using the same component number to store the
      results for that row. (See also the example.)

      The components fitted by this routine are specified as follows:
      The line names and laboratory frequencies are the default values
      and are not checked against any existing information in the
      input's Specdre Extension. The component types are `Gauss'. The
      numbers of parameters allocated to each component are 4, the
      three guessed and fitted parameters and the line integral. The
      parameter types are in order of appearance: `centre', `peak',
      `FWHM', `integral'.
   }
   \sstusage{
      \parbox[t]{143mm}{fitgauss in device=?\ mask1=?\ mask2=?\ ncomp=?\ 
      cont=?\ centre=?\ peak=? \\ fwhm=?\ cf=?\ pf=?\ wf=?\ comp=?\ logfil=?}
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, this routine will issue only error messages and no
         informational message. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, input variances are ignored. [YES]
      }
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If `T', the routine offers in general more options for
         interaction. The mask or guess can be improved after
         inspections of a plot. Also, the routine can resolve
         uncertainties about where to store results by consulting the
         user. [`T']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. This must be a one-dimensional (section of an)
         NDF. You can specify e.g.\ an image column as IN(5,) or part of
         an image row as IN(2.2:3.3,10). Update access is necessary to
         store the fit result in the NDF's Specdre Extension.
      }
      \sstsubsection{
         REPAIR = \_LOGICAL (Read)
      }{
         If DIALOG is true, REPAIR can be set true in order to change
         the spectroscopic number axis in the Specdre Extension. [NO]
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the plot device. Enter the null value (!) to
         disable plotting. [!]
      }
      \sstsubsection{
         MASK1( 6 ) = \_REAL (Read)
      }{
         Lower bounds of mask intervals. The mask is the part(s) of the
         spectrum that is (are) fitted and plotted. The mask is put
         together from up to six intervals:
         \begin{eqnarray*}
            mask&=&[MASK1(1);MASK2(1)]\cup[MASK1(2);MASK2(2)]\\
                &\cup& ... \cup[MASK1(MSKUSE);MASK2(MSKUSE)]
         \end{eqnarray*}
         The elements of the MASK parameters are not checked for
         monotony. Thus intervals may be empty or overlapping. The
         number of intervals to be used is derived from the number of
         lower/upper bounds entered. Either MASK1 or MASK2 should be
         entered with not more numbers than mask intervals required.
      }
      \sstsubsection{
         MASK2( 6 ) = \_REAL (Read)
      }{
         Upper bounds of mask intervals. See MASK1.
      }
      \sstsubsection{
         NCOMP = \_INTEGER (Read)
      }{
         The number of Gauss profiles to be fitted. Must be between 1
         and 6. [1]
      }
      \sstsubsection{
         CONT = \_REAL (Read)
      }{
         This value indicates the level of the continuum. Any constant
         value for CONT is acceptable. [0]
      }
      \sstsubsection{
         CENTRE( 6 ) = \_REAL (Read)
      }{
         Guess centre position for each Gauss component.
      }
      \sstsubsection{
         PEAK( 6 ) = \_REAL (Read)
      }{
         Guess peak height for each Gauss component.
      }
      \sstsubsection{
         FWHM( 6 ) = \_REAL (Read)
      }{
         Guess full width at half maximum for each Gauss component.
      }
      \sstsubsection{
         CF( 6 ) = \_INTEGER (Read)
      }{
         For each Gauss component I, a value CF(I)=0 indicates that
         CENTRE(I) holds a guess which is free to be fitted.
         A positive value CF(I)=I indicates that CENTRE(I) is fixed.
         A positive value CF(I)=J$<$I indicates that CENTRE(I) has to
         keep a fixed offset from CENTRE(J).
      }
      \sstsubsection{
         PF( 6 ) = \_INTEGER (Read)
      }{
         For each Gauss component I, a value PF(I)=0 indicates that
         PEAK(I) holds a guess which is free to be fitted.
         A positive value PF(I)=I indicates that PEAK(I) is fixed.
         A positive value PF(I)=J$<$I indicates that PEAK(I) has to
         keep a fixed ratio to PEAK(J).
      }
      \sstsubsection{
         WF( 6 ) = \_INTEGER (Read)
      }{
         For each Gauss component I, a value WF(I)=0 indicates that
         FWHM(I) holds a guess which is free to be fitted.
         A positive value WF(I)=I indicates that FWHM(I) is fixed.
         A positive value WF(I)=J$<$I indicates that FWHM(I) has to
         keep a fixed ratio to FWHM(J).
      }
      \sstsubsection{
         REMASK = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the mask.
         [NO]
      }
      \sstsubsection{
         REGUESS = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the guess and
         fit. [NO]
      }
      \sstsubsection{
         FITGOOD = \_LOGICAL (Read)
      }{
         Reply YES to store the result in the Specdre Extension. [YES]
      }
      \sstsubsection{
         COMP = \_INTEGER (Read)
      }{
         The results are stored in the Specdre Extension of the data.
         This parameter specifies which existing components are being
         fitted. You should give NCOMP values, which should all be
         different and which should be between zero and the number of
         components that are currently stored in the Extension. Give a
         zero for a hitherto unknown component. If a COMP element is
         given as zero or if it specifies a component unfit to store the
         results of this routine, then a new component will be created
         in the result storage structure. In any case this routine will
         report which components were actually used and it will deposit
         the updated values in the parameter system. [1,2,3,4,5,6]
      }
      \sstsubsection{
         LOGFIL = FILENAME (Read)
      }{
         The file name of the log file. Enter the null value (!) to
         disable logging. The log file is opened for append. [!]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         \parbox[t]{143mm}{fitgauss in device=xw mask1=-1.5 mask2=2.5
         ncomp=1 cont=1.0 centre=0.5 \\ peak=-0.5 fwhm=1.5 cf=0 pf=0 wf=0
         comp=1 logfil=line}
      }{
         This fits a single Gauss profile to the x range [$-$1.5,2.5]. The
         continuum is assumed to be constant at 1.0. The Gauss is
         guessed to be centred at 0.5 with width 1.5. It is guessed to
         be an absorption line with an amplitude of $-$0.5.
         All Gauss parameters are free to be fitted. The fit result is
         reported to the text file LINE.DAT and stored as component
         number 1 in the input file's Specdre Extension.
         Since DIALOG is not turned off, the user will be prompted for
         improvements of the mask and guess, and will be asked whether
         the final fit result is to be accepted (stored in the Extension
         and written to LINE.DAT).
         The XWINDOWS graphics device will display the spectrum before
         masking, guessing, and fitting. Independent of the DIALOG
         switch, a plot is produced after fitting.
      }
      \sstexamplesubsection{
         \parbox[t]{143mm}{fitgauss in(,5) device=!\ mask1=-1.5 mask2=2.5
          ncomp=1 cont=0.0 \\ centre=0.5 peak=13.0 1.5 cf=0 pf=0 wf=1
         comp=0 logfil=!\ dialog=false}
      }{
         This fits a single Gauss profile to the x range [$-$1.5,2.5] of
         the 5th row in the 2-D image IN. The baseline is assumed to be
         constant at 0.0. The Gauss is guessed to be centred at 0.5 with
         width 1.5. It is guessed to be an emission line with an
         amplitude of 13. Centre position and peak height are free to be
         fitted, but the width is fixed to 1.5. User interaction
         (DIALOG) and plotting (DEVICE) are de-selected. There is also no
         log file where to the results are written. If INFO were also
         switched off, no report whatsoever would be made. However, the
         results are stored as a new component (COMP=0) in the Specdre
         Extension of the input file.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
   \sstdiytopic{
      References
   }{
      Meyerdierks, H., 1992a, Covariance in resampling and model fitting,
      Starlink, Spectroscopy Special Interest Group

      Meyerdierks, H., 1992b, Fitting resampled spectra, in P.J.
      Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data Analysis
      Workshop, Garching, 13 - 14 May 1992, ESO Conference and Workshop
      Proceedings No. 41, Garching bei M\"unchen, 1992
   }
}
\goodbreak
\sstroutine{
   FITTRI
}{
   Fit triangular profiles to a spectrum
}{
   \sstdescription{
      This routine fits up to six triangular profiles at a time to a
      one-dimensional data set. This can be specified as an NDF section.
      The data set must extend along the spectroscopic axis.

      After accessing the data and the (optional) plot device, the data
      will be subjected to a mask that consists of up to six abscissa
      intervals. These may or may not overlap and need not lie within
      the range of existing data. The masking will remove data which are
      bad, have bad variance or have zero variance. The masking will
      also provide weights for the fit. If the given data have no
      variances attached, or if the variances are to be ignored, all
      weights will be equal.

      After the data have been masked, guessed values for the fit are
      required. These are
      \sstitemlist{

         \sstitem
         the number of components to be fitted,

         \sstitem
         the value of any underlying constant continuum (this must be an
            a-priori known constant),

         \sstitem
         the components' guessed centre positions,

         \sstitem
         peak heights and

         \sstitem
         full widths at half maxima. Finally,

         \sstitem
         fit flags for each of the triangle parameters are needed.

      }
      The fit flags specify whether any parameter is fixed, fitted, or
      kept at a constant ratio or offset to another fitted parameter.

      The masked data and parameter guesses are then fed into the fit
      routine. Single or multiple triangle fits are made to line
      features. Triangle fit parameters may be free, fixed, or tied to
      the corresponding parameter of another triangle component fitted
      at the same time. Peak and width are tied by fixing the ratios,
      the centre is tied by fixing the offset. Up to six triangle
      components can be fitted simultaneously.

      The fit is done by minimising chi-squared (or rms if variances are
      unavailable or are chosen to be ignored). The covariances between
      fit parameters - and among these the uncertainties of parameters -
      are estimated from the curvature of psi-squared. psi-squared is
      usually the same as chi-squared. If, however, the given data are
      not independent measurements, a slightly modified function
      psi-squared should be used, because the curvature of chi-squared
      gives an overoptimistic estimate of the fit parameter uncertainty.
      In that function the variances of the given measurements are
      substituted by the sums over each row of the covariance matrix of
      the given data. If the data have been resampled with a Specdre
      routine, that routine will have stored the necessary additional
      information in the Specdre Extension, and this routine will
      automatically use that information to assess the fit parameter
      uncertainties. A full account of the psi-squared function is given
      in Meyerdierks, 1992a/b. But note that these covariance row sums
      are ignored if the main variance is ignored or unavailable.

      If the fit is successful, then the result is reported to
      the standard output device and plotted on the graphics device. The
      final plot viewport is saved in the AGI data base and can be used
      by further applications.

      The result is stored in the Specdre Extension of the input NDF.
      Optionally, the complete description (input NDF name, mask used,
      result, etc.) is written (appended) to an ASCII log file.

      Optionally, the application can interact with the user. In that
      case, a plot is provided before masking, before guessing and
      before fitting. After masking, guessing and fitting, a screen
      report and a plot are provided and the user can improve the
      parameters. Finally, the result can be accepted or rejected, that
      is, the user can decide whether to store the result in the Specdre
      Extension or not.

      The screen plot consists of two viewports. The lower one shows the
      data values (full-drawn bin-style) overlaid with the guess or fit
      (dashed line-style). The upper box shows the residuals (cross
      marks) and error bars. The axis scales are arranged such that
      all masked data can be displayed. The upper box displays a
      zero-line for reference, which also indicates the mask.

      The Extension provides space to store fit results for each
      non-spectroscopic coordinate. Say, if you have a 2-D image each
      row being a spectrum, then you can store results for each row. The
      whole set of results can be filled successively by fitting one row
      at a time and always using the same component number to store the
      results for that row. (See also the example.)

      The components fitted by this routine are specified as follows:
      The line names and laboratory frequencies are the default values
      and are not checked against any existing information in the
      input's Specdre Extension. The component types are `triangle'. The
      numbers of parameters allocated to each component are 4, the
      three guessed and fitted parameters and the line integral. The
      parameter types are in order of appearance: `centre', `peak',
      `FWHM', `integral'.
   }
   \sstusage{
      \parbox[t]{143mm}{fittri in device=?\ mask1=?\ mask2=?\ ncomp=?\ cont=?\ 
         centre=?\ peak=? \\ fwhm=?\ cf=?\ pf=?\ wf=?\ comp=?\ logfil=?}
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, this routine will issue only error messages and no
         informational message. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, input variances are ignored. [YES]
      }
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If `T', the routine offers in general more options for
         interaction. The mask or guess can be improved after
         inspections of a plot. Also, the routine can resolve
         uncertainties about where to store results by consulting the
         user. [`T']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. This must be a one-dimensional (section of an)
         NDF. You can specify e.g.\ an image column as IN(5,) or part of
         an image row as IN(2.2:3.3,10). Update access is necessary to
         store the fit result in the NDF's Specdre Extension.
      }
      \sstsubsection{
         REPAIR = \_LOGICAL (Read)
      }{
         If DIALOG is true, REPAIR can be set true in order to change
         the spectroscopic number axis in the Specdre Extension. [NO]
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the plot device. Enter the null value (!) to
         disable plotting. [!]
      }
      \sstsubsection{
         MASK1( 6 ) = \_REAL (Read)
      }{
         Lower bounds of mask intervals. The mask is the part(s) of the
         spectrum that is (are) fitted and plotted. The mask is put
         together from up to six intervals:
         \begin{eqnarray*}
            mask&=&[MASK1(1);MASK2(1)]\cup[MASK1(2);MASK2(2)]\\
                &\cup& ... \cup[MASK1(MSKUSE);MASK2(MSKUSE)]
         \end{eqnarray*}
         The elements of the MASK parameters are not checked for
         monotony. Thus intervals may be empty or overlapping. The
         number of intervals to be used is derived from the number of
         lower/upper bounds entered. Either MASK1 or MASK2 should be
         entered with not more numbers than mask intervals required.
      }
      \sstsubsection{
         MASK2( 6 ) = \_REAL (Read)
      }{
         Upper bounds of mask intervals. See MASK1.
      }
      \sstsubsection{
         NCOMP = \_INTEGER (Read)
      }{
         The number of triangle profiles to be fitted. Must be between 1
         and 6. [1]
      }
      \sstsubsection{
         CONT = \_REAL (Read)
      }{
         This value indicates the level of the continuum. Any constant
         value for CONT is acceptable. [0]
      }
      \sstsubsection{
         CENTRE( 6 ) = \_REAL (Read)
      }{
         Guess centre position for each triangle component.
      }
      \sstsubsection{
         PEAK( 6 ) = \_REAL (Read)
      }{
         Guess peak height for each triangle component.
      }
      \sstsubsection{
         FWHM( 6 ) = \_REAL (Read)
      }{
         Guess full width at half maximum for each triangle component.
      }
      \sstsubsection{
         CF( 6 ) = \_INTEGER (Read)
      }{
         For each triangle component I, a value CF(I)=0 indicates that
         CENTRE(I) holds a guess which is free to be fitted.
         A positive value CF(I)=I indicates that CENTRE(I) is fixed.
         A positive value CF(I)=J$<$I indicates that CENTRE(I) has to
         keep a fixed offset from CENTRE(J).
      }
      \sstsubsection{
         PF( 6 ) = \_INTEGER (Read)
      }{
         For each triangle component I, a value PF(I)=0 indicates that
         PEAK(I) holds a guess which is free to be fitted.
         A positive value PF(I)=I indicates that PEAK(I) is fixed.
         A positive value PF(I)=J$<$I indicates that PEAK(I) has to
         keep a fixed ratio to PEAK(J).
      }
      \sstsubsection{
         WF( 6 ) = \_INTEGER (Read)
      }{
         For each triangle component I, a value WF(I)=0 indicates that
         FWHM(I) holds a guess which is free to be fitted.
         A positive value WF(I)=I indicates that FWHM(I) is fixed.
         A positive value WF(I)=J$<$I indicates that FWHM(I) has to
         keep a fixed ratio to FWHM(J).
      }
      \sstsubsection{
         REMASK = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the mask.
         [NO]
      }
      \sstsubsection{
         REGUESS = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the guess and
         fit. [NO]
      }
      \sstsubsection{
         FITGOOD = \_LOGICAL (Read)
      }{
         Reply YES to store the result in the Specdre Extension. [YES]
      }
      \sstsubsection{
         COMP = \_INTEGER (Read)
      }{
         The results are stored in the Specdre Extension of the data.
         This parameter specifies which existing components are being
         fitted. You should give NCOMP values, which should all be
         different and which should be between zero and the number of
         components that are currently stored in the Extension. Give a
         zero for a hitherto unknown component. If a COMP element is
         given as zero or if it specifies a component unfit to store the
         results of this routine, then a new component will be created
         in the result storage structure. In any case this routine will
         report which components were actually used and it will deposit
         the updated values in the parameter system. [1,2,3,4,5,6]
      }
      \sstsubsection{
         LOGFIL = FILENAME (Read)
      }{
         The file name of the log file. Enter the null value (!) to
         disable logging. The log file is opened for append. [!]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         \parbox[t]{143mm}{fittri in device=xw mask1=-1.5 mask2=2.5
         ncomp=1 cont=1.0 centre=0.5 \\ peak=-0.5 fwhm=1.5 cf=0 pf=0 wf=0
         comp=1 logfil=line}
      }{
         This fits a single triangular profile to the x range
         [$-$1.5,2.5]. The continuum is assumed to be constant at 1.0. The
         triangle is guessed to be centred at 0.5 with width 1.5. It is
         guessed to be an absorption line with an amplitude of $-$0.5.
         All triangle parameters are free to be fitted. The fit result
         is reported to the text file LINE and stored as component
         number 1 in the input file's Specdre Extension.
         Since DIALOG is not turned off, the user will be prompted for
         improvements of the mask and guess, and will be asked whether
         the final fit result is to be accepted (stored in the Extension
         and written to LINE.DAT).
         The XWINDOWS graphics device will display the spectrum before
         masking, guessing, and fitting. Independent of the DIALOG
         switch, a plot is produced after fitting.
      }
      \sstexamplesubsection{
         \parbox[t]{143mm}{fittri in(,5) device=! mask1=-1.5 mask2=2.5
            ncomp=1 cont=0.0 \\ centre=0.5 peak=13.0 1.5 cf=0 pf=0 wf=1
            comp=0 logfil=! dialog=false}
      }{
         This fits a single triangular profile to the x range [$-$1.5,2.5]
         of the 5th row in the 2-D image IN. The baseline is assumed to
         be constant at 0.0. The triangle is guessed to be centred at
         0.5 with width 1.5. It is guessed to be an emission line with
         an amplitude of 13. Centre position and peak height are free to
         be fitted, but the width is fixed to 1.5. User interaction
         (DIALOG) and plotting (DEVICE) are de-selected. There is also no
         log file where to the results are written. If INFO were also
         switched off, no report whatsoever would be made. However, the
         results are stored as a new component (COMP=0) in the Specdre
         Extension of the input file.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
   \sstdiytopic{
      References
   }{
      Meyerdierks, H., 1992a, Covariance in resampling and model fitting,
      Starlink, Spectroscopy Special Interest Group

      Meyerdierks, H., 1992b, Fitting resampled spectra, in P.J.
      Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data Analysis
      Workshop, Garching, 13 - 14 May 1992, ESO Conference and Workshop
      Proceedings No. 41, Garching bei M\"unchen, 1992
   }
}
\goodbreak
\sstroutine{
   GOODVAR
}{
   Replace negative, zero and bad variance values
}{
   \sstdescription{
      This routine checks the variance component of an NDF for values that
      are bad, negative, or zero and replaces them by values specified
      by the user. The specified value can be the null value (``!'') which
      is translated into the bad value.
   }
   \sstusage{
      goodvar in out bad neg zero
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output NDF.
      }
      \sstsubsection{
         BAD = \_REAL (Read)
      }{
         The value which replaces bad values. Enter an exclamation mark
         to keep bad values.
         Bad values in VARIANCE or ERRORS are not allowed by Figaro. If
         DSA has to convert these arrays while mapping them, floating
         overflows or square roots of negative numbers may occur, and
         the application is liable to crash. [!]
      }
      \sstsubsection{
         NEG = \_REAL (Read)
      }{
         The value which replaces negative values. Enter an exclamation
         mark to replace negative values with the bad value. Negative
         errors or variances are nonsense. Negative variances often will
         cause an application to crash because it takes the square root
         to calculate the error. [!]
      }
      \sstsubsection{
         ZERO = \_REAL (Read)
      }{
         The value which replaces zeros. Enter an exclamation mark
         to replace zeros with the bad value.
         Errors of zero sometimes are reasonable or necessary for error
         propagation. In other instances they cause problems, because
         statistical weights often are the reciprocal of the variance.
         [!]
      }
   }
}
\goodbreak
\sstroutine{
   GROW
}{
   Copy an N-dimensional cube into part of an (N$+$M)-dimensional one
}{
   \sstdescription{
      This routine increases the number of axes of a data set by
      duplicating pixels along some axes while retaining other axes.
      A simple and common example is turning a single row into a set of
      identical rows or a set of identical columns. This routine copies
      an N-dimensional cube into (part of) an (N$+$M)-dimensional one. The
      input cube is in general copied several times into the output, but
      need not fill the output cube. If the output file is new, its size
      has to be given. If it is an existing file, it cannot be reshaped,
      the axes of input and output have to be consistent.
   }
   \sstusage{
      grow in expand stapix endpix size=?\ out=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [YES]
      }
      \sstsubsection{
         NEW = \_LOGICAL (Read)
      }{
         True if a new output file is to be created. [NO]
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         Input NDF.
      }
      \sstsubsection{
         EXPAND( 7 ) = \_INTEGER (Read)
      }{
         For each axis in OUT a 0 indicates that this is an axis with a
         correspondent in IN. A 1 indicates that it is an new (or expanded
         axis without correspondent in IN.
      }
      \sstsubsection{
         STAPIX( 7 ) = \_INTEGER (Read)
      }{
         There is an EXPAND vector parameter that indicates which axes in
         OUT are new or have a corresponding axis in IN. Here, for each
         axis in OUT the value indicates where the multiple copy of input
         should start. Only the values for new axes in OUT are relevant,
         but a value for each axis in OUT must be supplied. The number of
         STAPIX elements given must match the number of axes in OUT.
      }
      \sstsubsection{
         ENDPIX( 7 ) = \_INTEGER (Read)
      }{
         There is an EXPAND vector parameter that indicates which axes in
         OUT are new or have a corresponding axis in IN. Here, for each
         axis in OUT the value indicates where the multiple copy of input
         should end. Only the values for new axes in OUT are relevant,
         but a value for each axis in OUT must be supplied. The number of
         ENDPIX elements given must match the number of axes in OUT.
      }
      \sstsubsection{
         SIZE( 7 ) = \_INTEGER (Read)
      }{
         For each axis in OUT a 0 indicates that the axis is to be taken
         from IN, an integer greater than 1 indicates that the axis is
         a new one and that the SIZE value is to be the length of that
         axis. The number of SIZE elements given must match the number
         of axes in OUT. The number of zeros given must be the number of
         axes in IN.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         Output NDF, containing the expanded data set.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         grow spectrum [0,1]\ [0,1]\ [0,5]\ size=[0,5]\ out=image new=t info=f
      }{
         Grows a spectrum into an image of 5 identical rows. It forces
         the creation of a new output file even if IMAGE exists.
         Informational messages are suppressed.
      }
      \sstexamplesubsection{
         grow spectrum [1,0]\ [2,0]\ [4,0]\ out=image
      }{
         Grows a spectrum into an image of 3 identical columns. Column 1
         and columns beyond 4 in IMAGE remain unchanged. Since NEW is
         not specified, IMAGE must already exist. Its second axis must
         match the first axis of SPECTRUM, and its first axis must be
         at least 4 pixels long.
      }
      \sstexamplesubsection{
         grow spectrum [0,1,1]\ [0,1,1]\ [0,2,4]\ out=cube size=[0,4,8]\ new=t
      }{
         Grow the spectrum into a cube with the spectral axis the 1st
         cube axis.
      }
      \sstexamplesubsection{
         grow spectrum [1,0,1]\ [1,0,1]\ [2,0,4]\ out=cube size=[4,0,8]\ new=t
      }{
         Grow the spectrum into a cube with the spectral axis the 2nd
         cube axis.
      }
      \sstexamplesubsection{
         grow spectrum [1,0,1]\ [1,1,0]\ [2,4,0]\ out=cube size=[4,8,0]\ new=t
      }{
         Grow the spectrum into a cube with the spectral axis the 3rd
         cube axis.
      }
      \sstexamplesubsection{
         grow image [0,0,1]\ [0,0,1]\ [0,0,5]\ out=cube size=[0,0,5]\ new=t
      }{
         Grow an image into a cube, using the image as an xy-plane.
      }
      \sstexamplesubsection{
         grow image [0,1,0]\ [0,1,0]\ [0,5,0]\ out=cube size=[0,5,0]\ new=t
      }{
         Grow an image into a cube, using the image as an xt-plane.
      }
      \sstexamplesubsection{
         grow image [1,0,0]\ [1,0,0]\ [5,0,0]\ out=cube size=[5,0,0]\ new=t
      }{
         Grow an image into a cube, using the image as a yt-plane.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7. This
      routine does not propagate any other extensions even when a new
      output file is created.

      This routine may work in situ on an existing output file.

      When IN is given as a subset of lower actual dimensionality
      than its base NDF, the dimensionality will formally be the same
      as that of the base NDF with interspersed dimensions (axis
      lengths) of 1. If this is inconvenient, use the application
      SUBSET to create the subset in advance and without degenerate
      axes.
   }
}
\goodbreak
\sstroutine{
   RESAMPLE
}{
   Resample and average several spectra
}{
   \sstdescription{
      Depending on the operation mode this routine either
      \sstitemlist{

         \sstitem
         takes a list of one-dimensional NDFs as input, resamples them
            to a common linear grid of axis values, and averages them into
            a single one-dimensional NDF, or

         \sstitem
         takes a single N-dimensional NDF as input and resamples each
            row into a new row of a similar output NDF; resampling is
            along the first axis, all further axes are retained.

      }
      The resampling creates an interdependence between pixels of the
      output NDF. Only limited information on this interdependence is
      stored in the output and ignored by most applications. Data input
      to this routine is assumed to have no such interdependence.

      The resampling algorithm is reintegration (Meyerdierks, 1992a)
      and it is applied to each input NDF separately. Any resampled data
      value is a weighted sum of the input values. The weights are the
      normalised overlaps between the output and input pixels. The
      resampled spectra are averaged pixel by pixel.

      If input variances are to be ignored it is assumed that the
      variance is a global constant, i.e.\ equal in all pixels of all
      input NDFs. The resampling may still result in different weights
      for different pixels. In the averaging process the global input
      variance is calculated and reported. The output variance will be
      derived on a pixel-by-pixel basis from the data scatter in the
      averaging process.

      In any input NDF, this routine recognises axis centres (pixel
      positions), pixel widths, data values, and data variances. (This
      routine also recognises the Specdre Extension and will use it
      where relevant.) Any other information is propagated from the
      first input NDF.

      Labels and units are checked for consistency, but only a warning
      is given. In interpreting the data all labels and units are
      assumed to be the same as in the first input NDF.

      All input NDFs must have a variance component (unless VARUSE is
      set false). NDFs without variances are ignored. A warning to that
      effect is issued. If VARUSE is set false, input NDFs may or may
      not have variances, such information will be ignored at any rate.

      The output NDF is based primarily on the first input NDF. There
      will be no pixel widths, since the pixel positions are linear and
      the pixels contiguous. The pixel positions, data values, and
      data variances will be affected by the resampling process. The
      output Specdre Extension will be based on the first input NDF or
      will be created.

      The vector of row sums of the covariance matrix (Meyerdierks,
      1992a/b) will be created in the Specdre Extension. This is an NDF
      structure with only a data component of the same shape as the main
      data array. If such a structure is found in one of the input
      NDFs, a warning is issued but such information is ignored.
   }
   \sstusage{
      resample mode inlist out start step end
   }
   \sstparameters{
      \sstsubsection{
         MODE = \_CHAR (Read)
      }{
         The operating mode. This can be abbreviated to one character,
         is case-insensitive and must be one of the following:
         \sstitemlist{

            \sstitem
            `SPECTRA': Average several 1-D input NDFs into a single
               1-D output NDF. Resample before averaging.

            \sstitem
            `CUBE': Accept only one - but N-D - input NDF. Resample each
               row (1-D subset extending along first axis) separately.
            Note that a single spectrum could be handled by both modes; it
            is more effective to treat it as a 1-D cube than as an N=1
            average.
            [`Cube']
         }
      }
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, informational and warning messages are suppressed.
         [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If true, input NDFs without variance information are skipped.
         If false, variance information in the input is ignored.
         [YES]
      }
      \sstsubsection{
         INLIST = LITERAL (Read)
      }{
         The group of input NDFs. In a complicated case this could be
         something like

         M\_51(25:35,$-$23.0,$-$24.0),M101,NGC1$*$,NGC201\%,$\wedge$LISTFILE.LIS

         This NDF group specification consists of
         \sstitemlist{

            \sstitem
            one identified NDF from which a subset is to be taken,

            \sstitem
            one identified NDF,

            \sstitem
            two wild-carded NDF specifications, and

            \sstitem
            an indirection to an ASCII file containing more NDF group
               specifications. That file may have comment lines and in-line
               comments, which are recognised as beginning with a hash (\#).
               (Berry, 1992a, 1992b).
         }
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output NDF.
      }
      \sstsubsection{
         START = \_DOUBLE (Read)
      }{
         The first pixel position in the output NDF. The prompt value is
         derived from the first valid input NDF.
      }
      \sstsubsection{
         STEP = \_DOUBLE (Read)
      }{
         The pixel position increment in the output NDF. The prompt
         value is derived as the average increment in the first valid
         input NDF.
      }
      \sstsubsection{
         END = \_DOUBLE (Read)
      }{
         The last pixel position in the output NDF. The prompt value is
         derived from the first valid input NDF.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         resample spectra inlist out 3.5 0.0254902 10.0
      }{
         The names of input NDFs are read from an ASCII list file called
         INLIST. The result will be stored in OUT which has 256
         pixels covering the coordinates from 3.5 to 10.0
      }
      \sstexamplesubsection{
         resample spectra inlist out varuse=false accept
      }{
         The names of input NDFs are read from an ASCII list file called
         INLIST. The input NDFs either have no variance, or their
         variance information is to be ignored. The output will be in
         OUT. The start and end pixel positions for OUT are the same as
         in the first input NDF. OUT also has the same number of pixels.
         The pixel spacings are constant in OUT while they may not be in
         the input NDF.
      }
      \sstexamplesubsection{
         resample cube inlist out 3.5 0.0254902 10.0
      }{
         INLIST contains only one NDF probably with more than one
         dimension. OUT will have the same dimensions except the first,
         which is the resampled one.
      }
   }
   \sstnotes{
      The axis normalisation flag is ignored.

      This routine recognises the Specdre Extension v. 0.7.
   }
   \sstdiytopic{
      Pitfalls
   }{
      This routine uses pixel widths. If there is no width array in the
      input NDF, the widths default as described in SUN/33. This may
      have undesired effects on resampling spectra that cover several
      non-adjacent coordinate ranges and where the missing ranges are
      not covered by bad pixels. Such spectra have highly non-linear
      pixel positions and the default pixel widths will not be as
      desired. To illustrate this consider the following spectrum with
      four pixels, the intended extents of the pixels and the defaulted
      extents:

{\ssttt
\parbox{146mm}{
\hspace*{\fill} \\ 
\phantom{~~~~~~~~~}x~~~~~~~x~~~~~~~~~~~~~~~~~~~~~~~~~x~~~~~~~x \\
\hspace*{\fill} \\ 
\phantom{~~~~~}|1111111|2222222|~~~~~~~~~~~~~~~~~|3333333|4444444| \\
\hspace*{\fill} \\ 
\phantom{~~~~~}|1111111|~~~~~~~~~~~~|333333333333333333333333333333333| \\ 
|222222222222222222222222222222222|~~~~~~~~~~~~|4444444|
}}

\vspace{5mm}

      Since this routine uses the overlap between input and output
      pixels as weights for resampling, non-bad pixels next to such a
      gap in data will affect too many output pixels with too much
      weight.
      Users should be aware that spectra as illustrated here are
      somewhat pathologic and that they should be given an explicit
      width array.

      The routine accesses one input NDF at a time and needs not hold
      all input NDFs at the same time. However, The routine needs
      temporary workspace. If KMAX is the number of pixels in an input
      NDF and LMAX the number of output pixels, the routine needs
      \sstitemlist{

         \sstitem
         one vector of length LMAX,

         \sstitem
         one matrix of size KMAX by LMAX,

         \sstitem
         two matrices of size LMAX by LMAX.

      }
      These workspaces are usually of type \_REAL. All (!) are of type
      \_DOUBLE if the first valid input NDF has type \_DOUBLE for either
      of the following:
      \sstitemlist{

         \sstitem
         pixel position,

         \sstitem
         pixel width,

         \sstitem
         data value,

         \sstitem
         data variance.

      }
      In addition one integer vector of length LMAX is needed.

      There is an oddity about this routine if only one input NDF is
      used and its variance array is used and some or all variance
      values are zero. In this case the output will formally still be an
      average of input NDFs using 1/variance as weights. Data with zero
      variance cannot be weighted and are regarded as bad. If this is a
      problem, users can set VARUSE false to ignore all the input
      variances. (Note that zero variances always cause that pixel to be
      ignored by this routine. But where it really calculates an average
      of two or more spectra, this is considered proper procedure.)
   }
   \sstdiytopic{
      Timing
   }{
      The time used by this routine is about proportional to the number
      of input NDFs. It is proportional to the square of the number of
      output pixels. Timing can be optimized, if the input NDFs cover
      about the same coordinate range as the output NDF rather than
      include a lot of data irrelevant for the output.
   }
   \sstdiytopic{
      References
   }{
      Berry, D.S., 1992a, IRG: NDF Groups access subroutine package,
      IRAS90 Document 5.7 (ID/5.7)

      Berry, D.S., 1992b, IRH: List access subroutine package, IRAS90
      Document 9.7 (ID/9.7)

      Meyerdierks, H., 1992a, Covariance in resampling and model fitting,
      Starlink, Spectroscopy Special Interest Group

      Meyerdierks, H., 1992b, Fitting resampled spectra, in P.J.
      Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data Analysis
      Workshop, Garching, 13 - 14 May 1992, ESO Conference and Workshop
      Proceedings No. 41, Garching bei M\"unchen, 1992
   }
}
\goodbreak
\sstroutine{
   SPECPLOT
}{
   Plot a spectrum
}{
   \sstdescription{
      This routine plots a spectrum (or any one-dimensional NDF section)
      in the current (AGI) picture of the graphics device.

      The plot can basically be an overlay over the most recent data
      picture inside the current picture, or a new plot inside the
      current picture. (The current picture after SPECPLOT is the
      same as before.)

      The screen contents of the current picture can be erased or not.

      The plot location and size is governed by the outer and the
      inner box. The inner box is the area where data are plotted,
      the outer box contains the inner box and the plot labels.

      In the overlay case the inner box and its world coordinates are
      identified with the most recent data picture inside the current
      picture. No labelling is done in the overlay case, so the outer
      box has no meaning in this case.

      In the case of a new plot, the outer box will be identified
      with the current picture, although the plot labels are allowed
      to extend beyond this area. Depending on the choice of
      labelling, a sensible location for the inner box is offered.
      After the inner box is specified, its world coordinates are
      enquired. The prompt values correspond to the extreme values
      found in the data. The location and world coordinates of the inner
      box are saved as a data picture in the AGI data base.

      The labelling consists of axes, axis ticks, numeric labels at
      the major ticks, and text labels. The axes are counted from
      bottom clockwise. Each axis can be drawn or not. Each
      drawn axis can have ticks or not. Each axis can have numeric
      labels or not. The left and right axes can have either
      horizontal (orthogonal) or vertical (parallel) numeric labels.
      Each axis can have a text label or not.

      The kind of labelling is controlled by several 4-character
      strings. Each character is the switch for axis 1, 2, 3, 4
      respectively. ``0'' turns an option off for that axis, ``$+$'' turns
      it on. For the ticks and for numeric labels of axes 2 and 4,
      ``$-$'' is also allowed. It yields inward ticks and vertical
      numeric labels.

      The data can be plotted as a set of markers, as a line-style
      polygon connecting the data points, or as a bin-style polygon.
      In addition error bars or pixel width bars can be plotted. Each
      of the options can be selected independent of the others, i.e.\
      several (or all) options can be selected at the same time. If
      no variance information is available, error bars are de-selected
      automatically. Bad data are omitted from the plot. If error
      bars are selected, bad variances cause the corresponding data
      also to be omitted.

      The attributes of the plot can be selected. These are
      \sstitemlist{

         \sstitem
         colour

         \sstitem
         line thickness

         \sstitem
         character height (equivalent to marker size)

         \sstitem
         simple or roman font

         \sstitem
         dash pattern for polygon connections

      }
      Most parameters default to the last used value.
   }
   \sstusage{
      specplot in overlay=?\ bottom=?\ left=?\ top=?\ right=?\ 
         labspc=?\ world=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [TRUE]
      }
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         If true, the part of the graphics device corresponding to the
         current (AGI) picture is erased before the plot is drawn.
         [TRUE]
      }
      \sstsubsection{
         OVERLAY = \_LOGICAL (Read)
      }{
         If true, the plot will be an overlay on the most recent (AGI)
         data picture within the current (AGI) picture.
         If false, the plot will be user-defined, but the inner box is
         restricted to the current (AGI) picture.
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF.
      }
      \sstsubsection{
         LIN = \_LOGICAL (Read)
      }{
         If true, the data points will be connected by a line-style
         polygon. [TRUE]
      }
      \sstsubsection{
         BIN = \_LOGICAL (Read)
      }{
         If true, the data points will be connected by a bin-style (or
         histogram-style) polygon. [FALSE]
      }
      \sstsubsection{
         MARK = \_INTEGER (Read)
      }{
         This parameter determines the kind of marker to be drawn at
         each data point [0]:
         \sstitemlist{

            \sstitem
            0: No markers drawn,

            \sstitem
            1: Diagonal cross,

            \sstitem
            2: Asterisk,

            \sstitem
            3: Open circle,

            \sstitem
            4: Open square,

            \sstitem
            5: Filled circle,

            \sstitem
            6: Filled square.
         }
      }
      \sstsubsection{
         ERROR = \_LOGICAL (Read)
      }{
         If true and variance information available, error bars will be
         drawn. [FALSE]
      }
      \sstsubsection{
         WIDTH = \_LOGICAL (Read)
      }{
         If true, the pixel width will be indicated by horizontal bars.
         [FALSE]
      }
      \sstsubsection{
         ROMAN = \_LOGICAL (Read)
      }{
         If true, PGPLOT's roman font is used for drawing text. If
         false, the normal (single-stroke) font is used. [FALSE]
      }
      \sstsubsection{
         HEIGHT = \_REAL (Read)
      }{
         The height of the characters. This also affects the size of the
         markers. Markers are about half the size of characters. The
         height is measured in units of PGPLOT default text heights,
         which is approximately 1/40 of the height of the (AGI) base
         picture (i.e.\ 1/40 the height of the workstation window, screen
         or paper). [1.]
      }
      \sstsubsection{
         COLOUR = \_INTEGER (Read)
      }{
         The PGPLOT colour index to be used for the plot. This can be
         formally between 0 and 255, but not all devices support all
         colours. The default colour representation is:
         \sstitemlist{

            \sstitem
            0: Background,           

            \sstitem
            1: Foreground (default),

            \sstitem
            2: Red,                  

            \sstitem
            3: Green,

            \sstitem
            4: Blue,                 

            \sstitem
            5: Cyan,

            \sstitem
            6: Magenta,              

            \sstitem
            7: Yellow,

            \sstitem
            8: Orange,               

            \sstitem
            9: Green/Yellow,

            \sstitem
            10: Green/Cyan,           

            \sstitem
            11: Blue/Cyan,

            \sstitem
            12: Blue/Magenta,         

            \sstitem
            13: Red/Magenta,

            \sstitem
            14: Dark grey,            

            \sstitem
            15: Light grey.
         }
      }
      \sstsubsection{
         THICK = \_INTEGER (Read)
      }{
         The PGPLOT line thickness. Can be between 1 and 21. [1]
      }
      \sstsubsection{
         DASH = \_INTEGER (Read)
      }{
         The PGPLOT dash pattern [1]:
         \sstitemlist{

            \sstitem
            1: Full line,

            \sstitem
            2: Long dash,

            \sstitem
            3: Dash-dot-dash-dot,

            \sstitem
            4: Dotted,

            \sstitem
            5: Dash-dot-dot-dot.
         }
      }
      \sstsubsection{
         AXES = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of either of
         the four box sides. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be
         ``0'' or ``$+$''. E.g.\ `00$+$$+$' would switch off the bottom and left
         axes and switch on the top and right axes. [`$+$$+$$+$$+$']
      }
      \sstsubsection{
         TICK = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of ticks along
         either axis. Ticks are drawn only if the corresponding axis is
         also drawn. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be
         ``0'', ``$+$'' or ``$-$''. E.g.\ `00$+$$-$' would switch off the bottom and
         left ticks and switch on the top and right ticks. The top axis
         would have ticks outward, the right axis would have ticks
         inward. [`$-$$-$$-$$-$']
      }
      \sstsubsection{
         NUML = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of numeric
         labels along either axis. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be
         ``0'' or ``$+$''; the second and fourth switch can also be ``$-$''. E.g.\
         `0$+$0$-$' would switch off the bottom and top labels and switch
         on the left and right labels. The left axis would have labels
         horizontal (orthogonal), the right axis would have labels
         vertical (parallel). [`$+$$+$00']
      }
      \sstsubsection{
         TEXT = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of text labels
         along either axis. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be
         ``0'' or ``$+$''. E.g.\ `0$+$$+$0' would switch off the bottom and right
         labels and switch on the left and top labels. [`$+$$+$$+$0']
      }
      \sstsubsection{
         NORTHO = \_REAL (Read)
      }{
         If orthogonal numeric labels have been selected, you must
         specify how much space there must be between the
         axis and the text label, i.e.\ how long the longest numeric
         label along the left or right axis will be. The unit is character
         heights. [1]
      }
      \sstsubsection{
         MAJOR( 2 ) = \_REAL (Read)
      }{
         The distance in world coordinates between major tick marks. The
         first element is for the horizontal direction, the second for
         the vertical direction. This is also the distance along the
         axis between numeric labels. Values of 0 cause PGPLOT to choose
         the major tick interval automatically. [0.,0.]
      }
      \sstsubsection{
         MINOR( 2 ) = \_INTEGER (Read)
      }{
         The number of minor tick intervals per major tick interval. The
         first element is for the horizontal direction, the second for
         the vertical direction. Values of 0 for MINOR or MAJOR cause
         PGPLOT to choose the minor tick interval automatically. [0,0]
      }
      \sstsubsection{
         BOTTOM = \_CHAR (Read)
      }{
         The text label for the first axis. Within the string, you can
         use the following escape sequences:
         \vspace{-12pt}
         \sstitemlist{

            \sstitem
            $\backslash$fn Normal (single stroke) font,

            \sstitem
            $\backslash$fr Roman font,

            \sstitem
            $\backslash$fi Italic font,

            \sstitem
            $\backslash$fs Script font,

            \sstitem
            $\backslash$u  Superscript (use only paired with $\backslash$d),

            \sstitem
            $\backslash$d  Subscript (use only paired with $\backslash$u),

            \sstitem
            $\backslash$b  Backspace,

            \sstitem
            $\backslash$$\backslash$  Backslash,

            \sstitem
            $\backslash$A  Danish umlaut (\AA ngstr\"om),

            \sstitem
            $\backslash$g  Any greek letter.
         }
      }
      \sstsubsection{
         LEFT = \_CHAR (Read)
      }{
         The text label for the second axis. See also BOTTOM.
      }
      \sstsubsection{
         TOP = \_CHAR (Read)
      }{
         The text label for the third axis. See also BOTTOM.
      }
      \sstsubsection{
         RIGHT = \_CHAR (Read)
      }{
         The text label for the fourth axis. See also BOTTOM.
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device for the plot.
      }
      \sstsubsection{
         LABSPC( 4 ) = \_REAL (Read)
      }{
         The space between outer box (AGI current picture) and inner box
         measured in units of character heights. The four numbers are
         for the bottom, left, top, right labelling space in that order.
         The dynamic default offered is based on the space requirements
         for the axis labelling, and can in general be accepted.
      }
      \sstsubsection{
         WORLD( 4 ) = \_REAL (Read)
      }{
         The world coordinates that the left, right, bottom and top ends
         of the inner box should correspond to.
         The dynamic default is based on the coordinates of the first
         and last pixel of the selected subset and on the extreme data
         values of the selected subset. Reverse axes can be achieved by
         giving WORLD(1) $>$ WORLD(2) and/or WORLD(3) $>$ WORLD(4).
      }
   }
\goodbreak
   \sstexamples{
      \sstexamplesubsection{
         specplot spectrum accept
      }{
         This is the simplest way to plot a 1-D data set in its full
         length.
      }
      \sstexamplesubsection{
         specplot imagerow(-100.:50.,15.) accept
      }{
         This will take a 2-D data set IMAGEROW and plot part of the
         row specified by the second coordinate being 15. The part of
         the row plotted corresponds to the first coordinate being
         between $-$100 and $+$50. Note that the decimal point forces use of
         axis data. Omitting the period would force use of pixel
         numbers.
      }
      \sstexamplesubsection{
         specplot imagecol(15.,-100.:50.) accept
      }{
         This will take a 2-D data set IMAGEROW and plot part of the
         column specified by the first coordinate being 15. The part of
         the row plotted corresponds to the second coordinate being
         between $-$100 and $+$50. Note that the decimal point forces use of
         axis data. Omitting the period would force use of pixel
         numbers.
      }
      \sstexamplesubsection{
         specplot spectrum lin=false bin=true accept
      }{
         Replace direct connections between data points by bin-style
         connections.
      }
      \sstexamplesubsection{
         specplot spectrum mark=1 accept
      }{
         Mark each data point by a diagonal cross.
      }
      \sstexamplesubsection{
         specplot spectrum error=true width=true accept
      }{
         Draw an error bar and a pixel width bar for each data point.
      }
      \sstexamplesubsection{
         specplot spectrum roman=true height=1.5 colour=3 accept
      }{
         Draw text with the roman font, draw text and makers 1.5 times
         their normal size, and plot the whole thing in green colour.
      }
      \sstexamplesubsection{
         specplot spectrum bottom=Weekday left="Traffic noise [dBA]" accept
      }{
         Specify text labels on the command line instead of constructing
         them from the file's axis and data info.
      }
      \sstexamplesubsection{
         specplot spectrum overlay=true clear=false accept
      }{
         The position and scale of the plot are determined by the
         previous plot (which might have been produced by a different
         application).
      }
      \sstexamplesubsection{
         specplot spectrum world=[0.,1.,-1.,1.]\ accept
      }{
         Use plot limits different from the extreme data values.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine recognises and uses coordinate transformations in
      AGI pictures.
   }
}
\goodbreak
\sstroutine{
   SUBSET
}{
   Take a subset of a data set
}{
   \sstdescription{
      Takes a rectangular subset of a data set. The given data set and
      the resulting subset may have up to seven axes. Axes that become
      degenerate by subsetting -- i.e.\ along which only one pixel is
      chosen -- are deleted from the subset. Thus the subset may have
      smaller dimensionality than the original.
   }
   \sstusage{
      subset in out
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input file.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output file.
      }
   }
\goodbreak
   \sstexamples{
      \sstexamplesubsection{
         subset in(1.5:2.5,10:12) out
      }{
         This takes the data from IN and writes the subset to OUT. The
         subset is specified as having 1st axis coordinates between 1.5
         and 2.5 and 2nd axis pixel numbers between 10 and 12.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.
   }
}
\goodbreak
\sstroutine{
   XTRACT
}{
   Average an N-dimensional cube into an (N-M)-dimensional one
}{
   \sstdescription{
      This routine reduces the number of axes of a data set by averaging
      pixels along some axes while retaining other axes. A simple and
      common example is averaging all or a certain range of rows (or
      columns) of an image to create a single row, e.g.\ an averaged
      spectrum from a 2-D slit spectrum. Input pixels with bad or zero
      variance are treated as bad, i.e.\ disregarded in the averaging
      (unless NOVAR is true).
   }
   \sstusage{
      xtract in colaps out
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, data variance in the input is ignored and output
         variance is calculated from the scatter of averaged values.
         If true, data variance in the input is used to weight mean
         values and to calculate output variance. [YES]
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         Input file.
      }
      \sstsubsection{
         COLAPS( 7 ) = \_INTEGER (Read)
      }{
         For each axis in IN a 0 indicates that the axis is to be
         retained in OUT, a 1 indicates that along that axis pixels
         from IN are to be averaged.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         Output file, containing the extracted data set.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         xtract cube(-30.:30.,1.5:2.5,10:20) [0,0,1]\ xyplane
      }{
         This first takes a subset from the 3-D data cube extending
         from $-$30 to $+$30, 1.5 to 2.5, 10 to 20 along the 1st, 2nd, 3rd
         axes respectively. (Coordinates are used along the 1st and 2nd
         axes, pixel indices along the 3rd.) From that sub-cube all the
         x-y-planes are averaged to create a 2-D image.
         (E.g.\ this averages the channel maps between 10 and 20 into an
         integrated map.)
      }
      \sstexamplesubsection{
         xtract cube(-30.:30.,1.5:2.5,10:20) [1,1,0]\ spectrum
      }{
         This averages each x-y-plane into a single point of the output
         row. The subset used is the same as above. (E.g.\ this averages
         the cube of channel maps into a mean spectrum.)
      }
      \sstexamplesubsection{
         xtract image(-30.:30.,1.5:2.5) [0,1]\ spectrum info=no varuse=no
      }{
         This averages all rows between 1.5 and 2.5 into a spectrum. The
         spectrum extends from -30 to $+$30. Informational messages are
         suppressed, and data variances in the image are ignored. The
         variances in the spectrum are calculated from the row-to-row
         scatter in each column.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7. However, no
      extraction is performed on NDFs in the input Specdre Extension. If
      the spectroscopic axis is retained, then the scalar components in
      the Extension are propagated. If the spectroscopic axis is
      collapsed, the Extension is not propagated at all.
   }
}
\goodbreak
\sstroutine{
   SPE\_HELP
}{
   Provide Specdre on-line help
}{
   \sstdescription{
      This routine interfaces the portable help library for the Specdre
      package with a terminal. The ADAM parameter TOPIC is used only for
      the initial entry into the help library. The user can then
      navigate through the library with the following responses to the
      prompt:
      \sstitemlist{

         \sstitem
         A blank response gets you one level up in the topic hierarchy.

         \sstitem
         A question mark (?) re-displays the current topic.

         \sstitem
         An end-of-file character exits spe\_help. Note that this is
            Ctrl-z under VMS but usually Ctrl-d under Unix.

         \sstitem
         Any normal text specifies (sub-) topics to look for.

         \sstitem
         Each blank-separated word stands for one topic in the
            hierarchy. E.g.\ three blank-separated words go down three
            levels in the hierarchy.

         \sstitem
         Each underscore-separated word stands for an
            underscore-separated word in a single topic

         \sstitem
         Words (whether separated by blanks or underscores) that are not
            unique topics or include wild card characters are expanded and
            help is given on all matching topics. Wild card characters are
            \% for a single character and $*$ for any number of characters
            including none. In the word expansion A\_G\_N would match
            active\_galactic\_nuclei, which is one topic. The same is true
            for A$*$\_G$*$\_N$*$ or active or active$*$.

      }
      When the help text to be printed is longer than the terminal page,
      then the user is asked to press the Return key before proceeding
      with output. At this point, too, can an end-of-file character be
      given to exit spe\_help immediately.
   }
   \sstusage{
      spe\_help [topic]
   }
   \sstparameters{
      \sstsubsection{
         PAGE = \_INTEGER (Read)
      }{
         The number of lines that are a screen-full of information on
         the terminal. This is used so that SPE\_HELP knows when to wait
         for the reader to hit the return key. To turn paging off set
         this parameter zero. If this is not given, then the routine will
         try to find out about the terminal on its own.
      }
      \sstsubsection{
         WIDTH = \_INTEGER (Read)
      }{
         The number of columns on the screen. If this is not given, then
         the routine will try to find out about the terminal on its own.
      }
      \sstsubsection{
         LIBRARY = \_CHAR (Read)
      }{
         The full file name of the library to be enquired. If this is
         not given, then the translation of the environment variable
         SPECDRE\_HELP will be used.
      }
      \sstsubsection{
         TOPIC = \_CHAR (Read)
      }{
         A initial topic to be looked for in the library. If this is not
         given, the top level of the library will be presented.
      }
   }
   \sstnotes{
      This routine is available only under Unix from a Unix shell.
   }
}
\end{document}
\normalsize

\end{document}
