\documentstyle[11pt]{article}
\pagestyle{myheadings}

% -----------------------------------------------------------------------------
% ? Document identification
\newcommand{\stardoccategory}  {Starlink User Note}
\newcommand{\stardocinitials}  {SUN}
\newcommand{\stardocsource}    {sun140.4}
\newcommand{\stardocnumber}    {140.4}
\newcommand{\stardocauthors}   {H.\,Meyerdierks}
\newcommand{\stardocdate}      {4 December 1995}
\newcommand{\stardoctitle}     {SPECDRE --- Spectroscopy Data Reduction}
\newcommand{\stardocversion}   {Version 1.1}
\newcommand{\stardocmanual}    {User's Guide}
% ? End of document identification
% -----------------------------------------------------------------------------

\newcommand{\stardocname}{\stardocinitials /\stardocnumber}
\markright{\stardocname}
\setlength{\textwidth}{160mm}
\setlength{\textheight}{230mm}
\setlength{\topmargin}{-2mm}
\setlength{\oddsidemargin}{0mm}
\setlength{\evensidemargin}{0mm}
\setlength{\parindent}{0mm}
\setlength{\parskip}{\medskipamount}
\setlength{\unitlength}{1mm}

% -----------------------------------------------------------------------------
%  Hypertext definitions.
%  ======================
%  These are used by the LaTeX2HTML translator in conjunction with star2html.

%  Comment.sty: version 2.0, 19 June 1992
%  Selectively in/exclude pieces of text.
%
%  Author
%    Victor Eijkhout                                      <eijkhout@cs.utk.edu>
%    Department of Computer Science
%    University Tennessee at Knoxville
%    104 Ayres Hall
%    Knoxville, TN 37996
%    USA

%  Do not remove the %\begin{rawtex} and %\end{rawtex} lines (used by 
%  star2html to signify raw TeX that latex2html cannot process).
%\begin{rawtex}
\makeatletter
\def\makeinnocent#1{\catcode`#1=12 }
\def\csarg#1#2{\expandafter#1\csname#2\endcsname}

\def\ThrowAwayComment#1{\begingroup
    \def\CurrentComment{#1}%
    \let\do\makeinnocent \dospecials
    \makeinnocent\^^L% and whatever other special cases
    \endlinechar`\^^M \catcode`\^^M=12 \xComment}
{\catcode`\^^M=12 \endlinechar=-1 %
 \gdef\xComment#1^^M{\def\test{#1}
      \csarg\ifx{PlainEnd\CurrentComment Test}\test
          \let\html@next\endgroup
      \else \csarg\ifx{LaLaEnd\CurrentComment Test}\test
            \edef\html@next{\endgroup\noexpand\end{\CurrentComment}}
      \else \let\html@next\xComment
      \fi \fi \html@next}
}
\makeatother

\def\includecomment
 #1{\expandafter\def\csname#1\endcsname{}%
    \expandafter\def\csname end#1\endcsname{}}
\def\excludecomment
 #1{\expandafter\def\csname#1\endcsname{\ThrowAwayComment{#1}}%
    {\escapechar=-1\relax
     \csarg\xdef{PlainEnd#1Test}{\string\\end#1}%
     \csarg\xdef{LaLaEnd#1Test}{\string\\end\string\{#1\string\}}%
    }}

%  Define environments that ignore their contents.
\excludecomment{comment}
\excludecomment{rawhtml}
\excludecomment{htmlonly}
%\end{rawtex}

%  Hypertext commands etc. This is a condensed version of the html.sty
%  file supplied with LaTeX2HTML by: Nikos Drakos <nikos@cbl.leeds.ac.uk> &
%  Jelle van Zeijl <jvzeijl@isou17.estec.esa.nl>. The LaTeX2HTML documentation
%  should be consulted about all commands (and the environments defined above)
%  except \xref and \xlabel which are Starlink specific.

\newcommand{\htmladdnormallinkfoot}[2]{#1\footnote{#2}}
\newcommand{\htmladdnormallink}[2]{#1}
\newcommand{\htmladdimg}[1]{}
\newenvironment{latexonly}{}{}
\newcommand{\hyperref}[4]{#2\ref{#4}#3}
\newcommand{\htmlref}[2]{#1}
\newcommand{\htmlimage}[1]{}
\newcommand{\htmladdtonavigation}[1]{}

%  Starlink cross-references and labels.
\newcommand{\xref}[3]{#1}
\newcommand{\xlabel}[1]{}

%  LaTeX2HTML symbol.
\newcommand{\latextohtml}{{\bf LaTeX}{2}{\tt{HTML}}}

%  Define command to re-centre underscore for Latex and leave as normal
%  for HTML (severe problems with \_ in tabbing environments and \_\_
%  generally otherwise).
\newcommand{\latex}[1]{#1}
\newcommand{\setunderscore}{\renewcommand{\_}{{\tt\symbol{95}}}}
\latex{\setunderscore}

%  Redefine the \tableofcontents command. This procrastination is necessary 
%  to stop the automatic creation of a second table of contents page
%  by latex2html.
\newcommand{\latexonlytoc}[0]{\tableofcontents}

% -----------------------------------------------------------------------------
%  Debugging.
%  =========
%  Remove % on the following to debug links in the HTML version using Latex.

% \newcommand{\hotlink}[2]{\fbox{\begin{tabular}[t]{@{}c@{}}#1\\\hline{\footnotesize #2}\end{tabular}}}
% \renewcommand{\htmladdnormallinkfoot}[2]{\hotlink{#1}{#2}}
% \renewcommand{\htmladdnormallink}[2]{\hotlink{#1}{#2}}
% \renewcommand{\hyperref}[4]{\hotlink{#1}{\S\ref{#4}}}
% \renewcommand{\htmlref}[2]{\hotlink{#1}{\S\ref{#2}}}
% \renewcommand{\xref}[3]{\hotlink{#1}{#2 -- #3}}
% -----------------------------------------------------------------------------
% ? Document specific \newcommand or \newenvironment commands.
% ? End of document specific commands
% -----------------------------------------------------------------------------
%  Title Page.
%  ===========
\renewcommand{\thepage}{\roman{page}}
\begin{document}
\thispagestyle{empty}

%  Latex document header.
%  ======================
\begin{latexonly}
   CCLRC / {\sc Rutherford Appleton Laboratory} \hfill {\bf \stardocname}\\
   {\large Particle Physics \& Astronomy Research Council}\\
   {\large Starlink Project\\}
   {\large \stardoccategory\ \stardocnumber}
   \begin{flushright}
   \stardocauthors\\
   \stardocdate
   \end{flushright}
   \vspace{-4mm}
   \rule{\textwidth}{0.5mm}
   \vspace{5mm}
   \begin{center}
   {\Huge\bf  \stardoctitle \\ [2.5ex]}
   {\LARGE\bf \stardocversion \\ [4ex]}
   {\Huge\bf  \stardocmanual}
   \end{center}
   \vspace{5mm}

% ? Heading for abstract if used.
%  \vspace{10mm}
%  \begin{center}
%     {\Large\bf Abstract}
%  \end{center}
% ? End of heading for abstract.
\end{latexonly}

%  HTML documentation header.
%  ==========================
\begin{htmlonly}
   \xlabel{}
   \begin{rawhtml} <H1> \end{rawhtml}
      \stardoctitle\\
      \stardocversion\\
      \stardocmanual
   \begin{rawhtml} </H1> \end{rawhtml}

% ? Add picture here if required.
% ? End of picture

   \begin{rawhtml} <P> <I> \end{rawhtml}
   \stardoccategory \stardocnumber \\
   \stardocauthors \\
   \stardocdate
   \begin{rawhtml} </I> </P> <H3> \end{rawhtml}
      \htmladdnormallink{CCLRC}{http://www.cclrc.ac.uk} /
      \htmladdnormallink{Rutherford Appleton Laboratory}
                        {http://www.cclrc.ac.uk/ral} \\
      \htmladdnormallink{Particle Physics \& Astronomy Research Council}
                        {http://www.pparc.ac.uk} \\
   \begin{rawhtml} </H3> <H2> \end{rawhtml}
      \htmladdnormallink{Starlink Project}{http://www.starlink.ac.uk/}
   \begin{rawhtml} </H2> \end{rawhtml}
   \htmladdnormallink{\htmladdimg{source.gif} Retrieve hardcopy}
      {http://www.starlink.ac.uk/cgi-bin/hcserver?\stardocsource}\\

%  HTML document table of contents. 
%  ================================
%  Add table of contents header and a navigation button to return to this 
%  point in the document (this should always go before the abstract \section). 
  \label{stardoccontents}
  \begin{rawhtml} 
    <HR>
    <H2>Contents</H2>
  \end{rawhtml}
  \renewcommand{\latexonlytoc}[0]{}
  \htmladdtonavigation{\htmlref{\htmladdimg{contents_motif.gif}}
        {stardoccontents}}

% ? New section for abstract if used.
% \section{\xlabel{abstract}Abstract}
% ? End of new section for abstract
\end{htmlonly}

% -----------------------------------------------------------------------------
% ? Document Abstract. (if used)
%   ==================
% ? End of document abstract
% -----------------------------------------------------------------------------
% ? Latex document Table of Contents (if used).
%  ===========================================
 \newpage
 \begin{latexonly}
   \setlength{\parskip}{0mm}
   \latexonlytoc
   \setlength{\parskip}{\medskipamount}
   \markright{\stardocname}
 \end{latexonly}
% ? End of Latex document table of contents
% -----------------------------------------------------------------------------
\newpage
\renewcommand{\thepage}{\arabic{page}}
\setcounter{page}{1}

% -----------------------------------------------------------------------------

\section{\label{intro}\xlabel{intro}Introduction}

\begin{latexonly}
   This document contains information on version 1.1 of the Specdre
   package (for SPEctroscopy Data Reduction) on various levels of
   detail. The user will want to read
Section~\ref{getstart}
   and use
Sections~\ref{classif}
   and
\ref{applics}
   for reference, which respectively give a classified list of commands
   and the command documentation in alphabetical order. But the other
   sections are of interest to the user, too. Namely
Sections~\ref{specfit}
   and
\ref{axcalib}
   give examples of using several applications together.
\end{latexonly}
\begin{htmlonly}
   This document contains information on version 1.1 of the Specdre
   package (for SPEctroscopy Data Reduction) on various levels of
   detail. The user will want to read the section on
\htmlref{how to get started}{getstart}
   and use the
\htmlref{classified list of commands}{classif}
   and the
\htmlref{detailed documentation of the applications}{applics}
   for reference. But the other sections are of interest to the user,
   too. Namely the sections on
\htmlref{spectral fits}{specfit}
   and
\htmlref{arc calibration}{axcalib}
   give examples of using several applications together.
\end{htmlonly}

   Specdre is a package for spectroscopy data reduction and analysis.
   Some of the general features of the package are:

\begin{itemize}
\item {\bf Hyper-cubes:} The Specdre data set is in general a hyper-cube
   where each row or hyper-column is a spectrum. Even where a single
   spectrum is required as input, this can be an appropriate section of
   the hyper-cube cut out ``on the fly'' as the application accesses the
   data.
\item {\bf Coherent storage of fit results:} The results of line or
   continuum fits are stored along with the data. In the case where a
   hyper-cube is a coherent set of spectra, fit results will also be
   stored coherently. For example, in a three-dimensional data set the
   two-dimensional map of line integrals is immediately available to
   display routines.
\item {\bf Bad values and variance:} Bad values (or quality information
   are recognised and ignored or propagated, as appropriate. If present,
   variance information is propagated or used in the processing,
   e.g.\ for statistical weights. It can optionally be ignored. Where
   covariance is created (namely re-sampling), an approximate measure of
   this is stored along with the data. Other applications (namely fit
   routines) will use the ordinary variance or the measure of
   covariance, as appropriate.
\end{itemize}

   The topics addressed by the applications are mainly:

\begin{itemize}
\item {\bf ASCII I/O:} The data and errors of hyper-cubes can be written
   to or read from printable/editable tables. Bad values are converted
   between the two formats. Single spectra can be read even if the axis
   data are not linear or monotonic.
\item {\bf Graphics:} Display applications allow full control of
   the plot, including font, colour, line styles, error bars, etc.
   Overlay on previous plots according to their ``world coordinates'' is
   possible. This includes overlays on grey/colour/line plots made by
\xref{KAPPA,}{sun95}{}
\xref{Pongo,}{sun137}{}
\xref{Figaro,}{sun86}{}
   etc.
\item {\bf Cube manipulation:} You can extract averaged hyper-planes
   from hyper-cubes, assemble hyper-cubes from hyper-planes, or fill in
   a hyper-cube from several given hyper-cubes.
\item {\bf Arc line axis (wavelength) calibration:} While full user
   interaction via graphics is granted, automatic arc line
   identification is also possible.
\item {\bf Re-sampling:} The application for re-sampling can either
   res-ample all spectra in a hyper-cube, or re-sample and average into
   one spectrum any number of input spectra. It allows information about
   the covariance between pixels to be carried through to a line fit
   routine.
\item {\bf Spectral fits:} You can fit polynomials, blended Gauss or
   triangle profiles. Fit results are stored along with the data and can
   be turned into fake data sets for later subtraction, division, etc.
\end{itemize}

   Specdre uses the
\xref{NDF data access library,}{sun33}{}
   which allows to
\htmlref{specify sections}{slice}
   rather than the whole data set. Also, for the special requirements of
   spectroscopy data reduction and analysis, an
\htmlref{extension to the NDF format}{useext}
   is used. This stores additional information with the data and by this
   means allows much enhanced communication between Specdre applications.

   Specdre is available on all Starlink-supported platforms.

% -----------------------------------------------------------------------------

\section{\label{getstart}\xlabel{getstart}Getting started}

   Before you can run Specdre, your shell startup script and your shell
   login script should have {\tt source}'d the corresponding Starlink
   scripts. You can run Specdre directly from a C or Tc shell, but the
   preferred way is to run it from Starlink's command interpreter
\xref{ICL.}{sg5}{}
   Amongst other things that saves you masking all kinds of
   meta-characters (such as {\tt "[]\{\}()'}) that the Unix shell would
   interpret and remove from the command line, but are needed in the
   parameter syntax.

   There is a demonstration procedure, which you may want to have a
   close look at. To execute it, you will need an X windows display. The
   demonstration is intended more as a test of the installation, but the
   script itself can give you hints about writing your own ICL
   procedure and about the syntax for command line parameters. It also
   demonstrates how the Specdre Extension is used to e.g.\ gather fit
   results and pass them on to other applications.

   To start ICL, then Specdre, and to run the demo type:

\begin{verbatim}
   % icl
   ICL> specdre
   ICL> load $SPECDRE_DIR:demo
\end{verbatim}

   Once in ICL, you can initialise further packages, like
\xref{KAPPA}{sun95}{}
   or
\xref{Figaro:}{sun86}{}

\begin{verbatim}
   ICL> kappa
   ICL> figaro
\end{verbatim}

   Any of these initialisations may write a message to the terminal that
   some ``key has been redefined''. This indicates that the same command
   name is used in different packages and probably for quite different
   purposes. The latest initialisation overrides previous ones, and it
   may be important to initialise several packages in the correct order.

% -----------------------------------------------------------------------------

\subsection{On-line help}

\begin{verbatim}
   ICL> help specdre
\end{verbatim}

   will issue a brief description of the package. If you enter a {\tt ?}
   in reply to the {\tt Topic?} prompt, you will get a list of topics
   available. These are mainly the commands, but there are some with
   more general information.

\begin{verbatim}
   ICL> help specdre
   Topic? classified *
\end{verbatim}

   will give a classified list of all Specdre commands available. Help
   on any command is obtained with

\begin{verbatim}
   ICL> help <command>
\end{verbatim}

   and by inspecting the sub-topics offered.

   From the Unix shell you cannot use {\tt help}, but {\tt spe\_help}
   provides very much the same service.

   The usual approach of the user to new software is, however, to run it
   and see what happens. Thus when you are prompted for a parameter and
   don't know what to enter, you can try {\tt ?}. Often this will offer
   more help than the parameter prompt.

% -----------------------------------------------------------------------------

\subsection{Response to parameter prompts}

   All applications will need further information, which they try to get
   from the {\it parameter system}. For some parameters the application
   may be happy to get some default value from the parameter system
   without the user knowing. Other parameters will be prompted for. The
   prompt often offers a more or less suitable default. Usually you will
   enter the proper value, but there are some other answers to a prompt
   that make surprisingly much sense.

\begin{verbatim}
   PARNAME - Prompt text /Default/ > 
   PARNAME - Prompt text /Default/ > <Tab>
   PARNAME - Prompt text /Default/ > \
   PARNAME - Prompt text /Default/ > !!
   PARNAME - Prompt text /Default/ > !
\end{verbatim}

   By just hitting {\tt Return}, the default value is used for the
   parameter. {\tt <Tab>} will copy {\tt Default} into your typing area
   for editing. When you edit the value you can use all four cursor
   keys, {\tt Up} and {\tt Down} move through the history of responses
   to any prompt within ICL, be it commands or parameters. As you may
   guess, the {\tt <Tab>} and cursor keys do not work in this way from
   the Unix shell.

   Responding with a \verb+\+ tells the parameter system that from now
   on it should accept defaults and not prompt again.
   {\tt !!} should cause the command to abort. {\tt !} assigns the
   so-called null value instead of the default value. The null value
   often will make no sense and cause the application to abort later
   on. But sometimes it is a meaningful response, a typical case being
   that you are asked for an output file name but do not want an output
   file. The instances where a null value makes sense are indicated as
   such in the parameter sub-topics of the on-line help. For most cases
   where a null value makes sense it is the default value anyway, and
   you will see {\tt /!/} in the prompt.

% -----------------------------------------------------------------------------

\subsection{Parameters on the command line}

   You need not wait for the application to prompt for a value but can
   set parameter values on the command line. Actually some parameters
   are not prompted for, they take default values unless you specify
   them on the command line. There are also some special keywords for
   the command line: {\tt accept}, {\tt prompt}, {\tt reset}.

\begin{verbatim}
   ICL> <command> accept
   ICL> <command> \
   ICL> <command> prompt
\end{verbatim}

   Some parameters are assigned positions on the command line, but all
   parameters can be specified by their name. In the following example
   three parameters are specified on the command line, all others
   default.

\begin{verbatim}
   ICL> fitgauss in=myfile device=xw logfil=mylist accept
\end{verbatim}

   Some parameters may be tricky to specify on the command line, namely
   strings that contain spaces, vectors, and NDF sections. The syntax on
   the command line is more strict in these cases. And from a Unix shell
   it is more obscure, because one must take precautions against the
   shell interpreting meta-characters.

\begin{verbatim}
   ICL> <command> <stringpar>="word1 word2"
   ICL> <command> <vectorpar>=[1,2,3]
   ICL> <command> <NDFpar>=ndf(1:5,15.0~10.0)

   % <command> <stringpar>=\"word1 word2\"
   % <command> <vectorpar>=\[1,2,3\]
   % <command> <NDFpar>=ndf\(1:5,15.0~10.0\)
\end{verbatim}

   If a command cannot be completed on one line, it can be split into
   several. The split character -- the last character before the
   splitting {\tt Return} -- is a tilde {\tt\~{}} in ICL.

   You can find real-life examples in the demonstration script.

% -----------------------------------------------------------------------------

\subsection{Scripts and procedures}

   One advantage of command line parameters is that sometimes you need
   not wait at the terminal for the last prompt, which might come only
   after lengthy calculations. But more importantly you can edit
   commands into ICL procedures. This is exactly what was done for the
   demonstration. The demo, however, is simply a linear sequence of
   commands with static parameters. The
\xref{ICL language}{sg5}{}
   offers you a lot more in terms of flow control, modular procedures,
   communication between application parameters and ICL variables,
   etc. The ICL language is documented in SG/5.

% -----------------------------------------------------------------------------

\subsection{Specdre's use of parameters}

   The applications use the parameter system to get the necessary
   information from the outside world. The source of information is not
   always the user's keyboard. The specification of a parameter on the
   command line is slightly different from entering the value at the
   prompt.

   A good model to imagine the workings of the parameter system is as
   follows.  The system is a pool of parameter values. On the command
   line you can pass values to the parameter system (not the
   application). When the application runs and needs a parameter value,
   it will ask the parameter system (not the user terminal). For each
   parameter the system has two sets of rules, one to arrive at a
   default value and one to work out the value itself. If the value was
   specified on the command line, the system will just pass that as the
   value to the application. Otherwise the value is so far unknown to
   the parameter system and it will construct a default value and a
   value according to the rules. There are several possible sources for
   these two:

\begin{itemize}
\item the last used value of a global parameter (common to more than one
   application),
\item the last used value (as stored on a per-application basis),
\item a dynamic default, set by the application at run-time,
\item a static default, set in the interface file,
\item response to a user prompt.
\end{itemize}

   So asking the user is only one way of getting information from the
   parameter system. You also see that the defaults offered -- or
   accepted by {\tt accept} on the command line -- may be arrived at in
   a number of ways.

   Some parameters used by Specdre are common to several commands. The
   {\tt device} parameter is sometimes associated with the global
   parameter {\tt GRAPHICS\_DEVICE}. When it is, it usually
   defaults. And these parameters are really global, in the sense that
   other packages may use and change them, too.

   Where {\tt in} and/or {\tt out} are NDFs, they are mostly associated
   with the global {\tt DATA\_ARRAY}. The effect is that the default
   input is usually the output of the previous command.

   {\tt info} and {\tt dialog} are always associated with {\tt
   SPECDRE\_INFO} and {\tt SPECDRE\_DIALOG}. These parameters control
   the amount of information and user interaction of many applications.
   Once {\tt info} is switched off all applications will become quiet
   until the parameters are set true again.

   {\tt varuse} is a defaulted parameter to many applications, but not
   associated with a global parameter. By default it is true. Sometimes
   it has to be set false in order to ignore variance information in the
   input data.

   Other parameters like {\tt start, step, end} occur naturally in
   several applications. In some instances they may be scalars, in
   others vectors. Often their defaults are set by the application with
   knowledge of the data set at hand.

% -----------------------------------------------------------------------------

\section{\label{graphic}\xlabel{graphic}Graphics}

   The management of graphics output follows closely that of
\xref{KAPPA.}{sun95}{}
   To make full use of the graphics capabilities, you will need to use
   some KAPPA commands; and you will find
\xref{Pongo}{sun137}{}
   extremely useful to add almost anything to your plots.  For normal
   use you can get along without KAPPA or Pongo. The actual plots are
   achieved through a combination of AGI, SGS, PGPLOT, and GKS.
   The graphics is done with
\xref{PGPLOT,}{sun15}{}
   but the device is handled via
\xref{AGI}{sun48}{}
   and
\xref{SGS,}{sun85}{}
   and
\xref{GKS}{sun83}{}
   is the low level package underlying them all.  AGI stores information
   about the graphs produced in a data base. This tells where on which
   device a plot was made and what its world coordinates were.  Other
   packages use and update the same data base so that a consistent
   display administration can be achieved, even when different packages
   are used in turn. The overlay options in Specdre applications use
   this information and allow you to plot with Specdre in the right
   place on to of -- say an image displayed in grey or colour with
   KAPPA.

   Usually a command whose main task is to produce a plot, has a
   parameter {\tt device} which is not prompted for and which is
   associated with the global parameter {\tt GRAPHICS\_DEVICE}. The
   value of this global parameter will be the name of a graphics device,
   and the named device will be used for the display. But you can always
   specify the {\tt device} parameter on the command line, thus
   overriding the global parameter:

\begin{verbatim}
   ICL> specplot device=graphon
   ICL> specplot device=xw
   ICL> specplot device=ps_l
\end{verbatim}

   This will also change the global parameter so that next time you use
   the same device automatically.

   There are other Specdre commands that may or may not use a graphics
   device. These will prompt for the {\tt device} parameter and offer
   the null value as default. This can be accepted to avoid using a
   graphics device. When specifying parameters on the command line, {\tt
   device=!} may not always work, but including the {\tt accept} keyword
   will have the desired effect.

   Some applications not only display graphics, but use a graphics
   display plus mouse and keyboard to conduct a dialogue with the user.
   This is usually optional and controlled by the {\tt dialog}
   parameter.  {\tt dialog} is a character parameter.  It is always
   allowed to be one of the letters {\tt\{Y,N,T,F,y,n,t,f\}}.  Sometimes
   it may also be {\tt G} or {\tt g} for graphics dialogue.  {\tt y,t}
   may or may not mean the same as {\tt g}.

   If you specify a printer or PostScript device, this may fail for the
   graphics dialogue case.  But otherwise all plots can be sent to files
   that you print later.  There is one important difference, you have
   only one screen, but the printer has many sheets of paper.  Your plot
   may be in a number of printer files and each printout starts on a new
   page.  If you have done overlay plots on the screen and want the same
   on the printer, then you can use as graphics device an Encapsulated
   PostScript device like {\tt epsf\_l}.  You still get a number 
   of files, but they can be merged into one (without form feed) using
{\tt\xref{psmerge}{sun164}{}}
   (cf.\ SUN/164).  Usually the output is a complete PostScript file
   ready to be printed.

% -----------------------------------------------------------------------------

\section{\label{slice}\xlabel{slice}Slicing data sets}

   Specdre uses the
\xref{NDF routines to access data}{sun33}{}
   in Starlink Data Files (SDF). This allows the user to specify
   sections of NDFs rather than whole (i.e.\ base) NDFs. Some
   applications need one-dimensional input data, but by themselves offer
   no means to take a subset of a larger or multi-dimensional data
   set. The desired effect can, however, be achieved by NDF. As an
   example, {\tt fitgauss} will fit a spectrum only and rejects
   two-dimensional input. You still can use a 2-D data set by specifying
   a section thereof as input to {\tt fitgauss}:

\begin{verbatim}
   ICL> fitgauss in=myndf(,5)               { 5-th row of 2-D input
   ICL> fitgauss in=myndf(5,)               { 5-th column
   ICL> fitgauss in=myndf(20:30,5)          { pixels 20 to 30 of 5-th row
   ICL> fitgauss in=myndf(6540.0:,5)        { pixels beyond coordinate 6540.0
   ICL> fitgauss in=myndf(6450.0~10.0,5)    { coordinate range 6540.0 +- 5.0
\end{verbatim}

   (For {\tt fitgauss} the sub-setting within the row is not necessary,
   because it does its own masking of the given 1-D data.)

\begin{latexonly}
   It should be mentioned here that the NDF fed into an application need
   not be at the top level of its container file. Once your NDF got a
   Specdre Extension
(Section~\ref{useext})
   with fit results in it, you can e.g.\ plot the second fit parameter
   versus the row number:
\end{latexonly}
\begin{htmlonly}
   It should be mentioned here that the NDF fed into an application need
   not be at the top level of its container file. Once your NDF got a
\htmlref{Specdre Extension}{useext}
   with fit results in it, you can e.g.\ plot the second fit parameter
   versus the row number:
\end{htmlonly}

\begin{verbatim}
   ICL> specplot in=myndf.more.specdre.results(2,1,)
\end{verbatim}

% -----------------------------------------------------------------------------

\section{\label{useext}\xlabel{useext}Using the Specdre Extension}

   The
\xref{NDF data access routines}{sun33}{}
   -- together will the lower-level
\xref{HDS routines}{sun92}{}
   -- allow the construction of extensions to the data format.
   Extensions unknown to an application package are propagated, and
   extensions known to an application enable it to provide more
   functionality.

   The
\htmlref{Specdre Extension}{extension}
   is recognised by all Specdre applications, and it will be propagated
   uncritically by other packages when they get their hands on Specdre
   output files. In principle this may not be the right thing to do, but
   is the best that can be done. If the data never re-enter Specdre, it
   doesn't matter. If they do, Specdre will notice if something has
   become obviously inconsistent.

   The demonstration script shows the use of the Specdre Extension.
   There is a tool
{\tt\htmlref{editext}{EDITEXT}} to look at or manipulate the
   Extension. {\tt resample} will usually store some limited amount of
   information about the interdependence of post-resample pixels in the
   Extension. If you try to {\tt fitgauss} such re-sampled data, that
   application will pick up and use the information in the Extension.

\begin{latexonly}
{\tt\htmlref{fitgauss}{FITGAUSS}}
   or
{\tt\htmlref{fitpoly}{FITPOLY}}
   will store their fit results in the Extension. In conjunction with
   the NDF sections
(Section~\ref{slice}) 
   you can work your way through the rows of a CCD long-slit spectrum,
   store each row's fit in the Extension and fill the result storage
   space. The collection of all fits is again an NDF (located in the
   Extension) and can be fed into other applications like {\tt
   specplot}, {\tt ascout}, or indeed KAPPA's
{\tt\xref{display}{sun95}{DISPLAY}}.
\end{latexonly}
\begin{htmlonly}
{\tt\htmlref{fitgauss}{FITGAUSS}}
   or
{\tt\htmlref{fitpoly}{FITPOLY}}
   will store their fit results in the Extension. In conjunction with
   the
\htmlref{NDF sections}{slice}
   you can work your way through the rows of a CCD long-slit spectrum,
   store each row's fit in the Extension and fill the result storage
   space. The collection of all fits is again an NDF (located in the
   Extension) and can be fed into other applications like {\tt
   specplot}, {\tt ascout}, or indeed KAPPA's
{\tt\xref{display}{sun95}{DISPLAY}}.
\end{htmlonly}

   The Extension also provides a place to store a full-size array for
   the wavelength or frequency etc.\ of each pixel.  This array of
   spectroscopic values may be created by {\tt grow}, and is certainly
   used by {\tt arcdisp} to apply individual dispersion curves for the
   rows of an image. The image may be a long-slit spectrum, or the rows
   may be extracted fibre spectra or echelle orders.

   There is some potential for confusion here.  You may tend to count
   pixels in your data starting at 1, you may use NDF pixel indices, NDF
   pixel coordinates, a vector of pixel centre values, or an
   N-dimensional array of spectroscopic values.

\begin{itemize}
\item {\it Pixel indices} along an axis in an NDF are counted from the
   {\it lower bound} to the {\it upper bound}.  The lower bound is also
   known as the {\it origin}.  It is not necessarily 1.  Say, if an NDF
   has bounds 1 ... N, a section of it may have bounds 5 ... (N-6), or
   -5 ... (N+6) etc.  If the section is made permanent in a new file, it
   will still have origin other than 1.  Usually it is these pixel
   numbers -- or the section bounds -- that are used to specify NDF
   sections in parameter values like {\tt ndf(-25:32,4:)}.

\item The centres of pixels in an NDF have {\it pixel coordinates} that
   are the pixel number minus 0.5.  So if the bounds are 5 ... 95 then
   the pixels have centre coordinates 4.5 ... 94.5.  The beauty of
   these coordinates is that, if you give each pixel an extent of
   0.5 on either side, then the sequence of pixels 1 .... N cover the
   coordinate range [0;N].  These are only default coordinates, but
   they are worth mentioning because they are similar to the pixel
   indices, and because they are not equal to the pixel indices.

\item Each axis of the NDF may have an explicit vector with the
   coordinates of the pixel {\it centres}.  These can run non-linear,
   backwards, or in loops.  You may find that some software cannot cope
   with the weirder of these options.  These {\it centres} can be used
   to specify NDF sections.  You simply give a floating point bound
   instead of an integer bound.  Note that this works only because each
   axis has a {\it vector} of pixel centres as long as the NDF axis, not
   an N-dimensional {\it array} of pixel centres.

\item For Specdre it is useful to have {\it in addition} an
   N-dimensional array where for example a wavelength calibration can be
   stored for each row of an image or a cube individually.  For this
   purpose there may exist an array of {\it spectroscopic values} in the
   Specdre Extension to the main NDF.  That array is an NDF with the
   same bounds as the main NDF.  Where the main NDF stores the count
   rate, brightness etc.\ of a pixel, the spectroscopic values' NDF
   stores the wavelength, frequency, radial velocity etc.\ for that
   pixel.  This information is recognised only by Specdre.  You cannot
   expect Kappa's {\tt linplot} to use it for axis labelling, Specdre's
   {\tt specplot} does use it of course.  You also cannot use this array
   to specify an NDF section in a parameter value.

\item There is no rule as to what happens to the {\it centres} of the
   spectroscopic axis when N-D {\it spectroscopic values} are created or
   modified.  Both arrays may lead rather independent lives.
\end{itemize}

% -----------------------------------------------------------------------------

\section{\label{cubeman}\xlabel{cubeman}Cube manipulation}

   A spectrum may be thought of as a one-dimensional data set.  But
   spectroscopists are also aware of the two-dimensional space of sky
   position and might use time as an axis in data sets.  So the data
   handling aspects of spectroscopy are more demanding than image
   processing -- Specdre applications may encounter data with any
   dimensionality.  Two-dimensional detectors are often used to take
   spectra and where observations are not very time-consuming
   three-dimensional data sets may be quite common.

   Specdre can work on data with any dimensionality.  The limit is 7-D
   due to the HDS data access routines.  In practice the limit may be
   6-D since one structure in the Specdre Extension has one dimension
   more than the main data array.

   However, Specdre is about spectra.  An N-D cube is taken as a set of
   spectra, each spectrum is a row or a column in the cube.  A {\it row}
   extends along the {\it first} axis of the cube, while a {\it column}
   extends along {\it any} axis of the cube.  In any cube Specdre
   assumes that there is exactly one {\it spectroscopic axis}, by
   default this is the first axis.  The Specdre Extension specifies
   which axis is the spectroscopic one.

   Specdre's handling of N-D data falls into three categories.

\begin{itemize}
\item Some applications work on one spectrum at a time.  They will
   insist on 1-D input, but you can specify a column to work on.  Often
   the column must extend along the spectroscopic axis.  These
   applications can be used successively on several or all columns of a
   cube.  Insofar as they produce output will they work ``in situ'' and
   modify the input file.

\item Other applications take on a whole cube at once and work on each
   row in turn.  Usually spectra have to be in rows and the
   spectroscopic axis must be the first axis.  These applications may
   also refuse to work on only a section of the input.

\item Then there are applications that don't deal with spectra at all
   but are tools to manipulate cubes.  It is important that Specdre has
   such a set of tools, since the Specdre Extension may have a number of
   cubes in it as well.  The main and extension cubes must be
   manipulated in a consistent manner, or the Extension becomes invalid.
   The fact that the cube manipulation routines handle only version 0.7
   of the Specdre Extension and not version 1.1, means that the COORD
   component of the Extension is lost when these routines go about their
   business.
\end{itemize}

{\tt\htmlref{subset}{SUBSET}}
   is very similar to KAPPA's
{\tt\xref{ndfcopy}{sun95}{NDFCOPY}}
   or to taking an ``on the fly'' section as input to an application.
   The differences are that {\tt subset} also takes subsets of NDFs in
   the Specdre Extension (v.\,0.7) consistent with the subset of the
   main NDF, and that it removes ``degenerate'' axes.  Consider the
   command

\begin{verbatim}
   ICL> subset image(5,1:10) row
\end{verbatim}

   When {\tt subset} gets the input it is still an image of size 1 by
   10.  But in the output the degenerate axis has been removed so that
   it is also officially 1-D.

   Where {\tt subset} may remove axes,
{\tt\htmlref{grow}{GROW}}
   deliberately adds new axes -- degenerate or genuine ones.  So we
   could reverse the command above with

\begin{verbatim}
   ICL> grow row expand=[1,0] stapix=[1,0] endpix=[1,0] size=[1,0] ~
           new=y out=image
\end{verbatim}

   The zeros as second vector elements just show that the second axis of
   {\tt image} matches the axis in {\tt row}.  {\tt expand} shows which
   of the output axes are and are not in the input.  Normally those new
   axes will of course not be degenerate, so {\tt size} might be {\tt
   [5,0]}.  In that case {\tt row} could be copied into any of the
   output columns, into one of them or repeatedly into a whole range of
   columns.  The main idea of {\tt grow} is that you assemble rows into
   images, images into cubes etc.  So when {\tt new=n} then you will
   copy into an existing file.  The following puts two spectra and one
   image into a common cube.  Whatever part of the cube is not copied
   to, remains filled with bad values.

\begin{verbatim}
   ICL> grow row5_1 expand=[0,1,1] stapix=[0,5,1] endpix=[0,5,1] ~
           size=[0,5,3] new=y out=cube_x_by_5_by_3
   ICL> grow plane2 expand=[0,0,1] stapix=[0,0,2] endpix=[0,0,2] ~
           new=n out=cube_x_by_5_by_3
   ICL> grow row3_to_5_3 expand=[0,1,1] stapix=[0,3,3] endpix=[0,5,3] ~
           new=n out=cube_x_by_5_by_3
\end{verbatim}

%   +-----------+
%   |     3 3 3 |
%   | 2 2 2 2 2 |
%   |         1 |
%   +-----------+

   If {\tt grow} creates and expands new axes,
{\tt\htmlref{xtract}{XTRACT}}
   collapses existing axes.  This is done by averaging the pixels along
   each collapsed axis.  Note that this is an average and not a sum,
   which makes a big difference for the use of input variances and the
   meaning of output variances.  Basically an average assumes that all
   values entering the mean are equal and scatter at random.

   {\tt grow} copies input into output of higher dimensionality, the
   common dimensions must match.
{\tt\htmlref{fillcube}{FILLCUBE}}
   is different.  It copies input into output of the same
   dimensionality.  Dimensions need not match, the copy is positioned in
   output by matching {\it centre} coordinates.  Indeed the copy may not
   be contiguous in output.  The output is an existing file, so you can
   fill it successively from different input files.  This is mosaicing
   in N-D.

{\tt\htmlref{resample}{RESAMPLE}},
   too, plays a r\^ole in cube manipulation, since it can homogenise and
   linearise the coordinates along the spectroscopic axis.  When used in
   {\tt mode=Cube} it re-samples each row of a cube individually.
   Afterwards all rows have the same linear coordinate grid as expressed
   by the new vector of {\it centres} for the spectroscopic axis.  Any
   spectroscopic values in the Specdre Extension are thus obsolete and
   removed.  Sometimes it is necessary for other applications that grids
   are linear or that there is no array of spectroscopic values in the
   Specdre Extension.

% -----------------------------------------------------------------------------

\section{\label{specfit}\xlabel{specfit}Spectral fits}

   Specdre has a number of applications to fit analytical functions to
   spectral features. Two are line fit routines for Gauss and triangular
   profiles. These can fit up to six components at a time. The lines can
   be blended and the line parameters can be free, fixed, or tied to the
   corresponding (free) parameters of another component. A similar
   routine fits up to six diluted Planck curves.  Finally, a polynomial
   fit can be performed, the order can be up to 7.

   The fit routines can run with a (non-graphic) dialogue or not, they
   can display data and fit graphically at different stages of the
   fitting process (masking, guessing, fit residuals).

   These fit routines work on one-dimensional data only. But you can
   pass an NDF section that is (part of) a row or column in an image or
   cube. For the fit only data inside the union of up to six masking
   intervals are used. The fit results go first of all to the standard
   output (the terminal), but can also be recorded in (appended to) an
   ASCII file. In addition fit results will always be stored in a
   results' NDF in the Specdre Extension. Those results can be used to
   generate a model data set. You could then subtract that from the
   original data.

   Here is a complex example how you might proceed with a long-slit
   spectrum, it won't work like that since parameters are missing. Also,
   the telluric correction may not be the correct way of doing
   things. The example only shows how you can juggle with
Specdre/\xref{KAPPA}{sun95}{}
   applications.

\begin{verbatim}
   ICL> fitpoly  image(,1) mask1=[1,3,5] mask2=[2,4,6] comp=1             { 1)
   ICL> fitpoly  image(,2) mask1=[1,3,5] mask2=[2,4,6] comp=1
        ...
   ICL> fitpoly  image(,9) mask1=[1,3,5] mask2=[2,4,6] comp=1
   ICL> evalfit  image conti comp=1                                       { 2)
   ICL> div      image conti normal                                       { 3)
   ICL> fitgauss normal(,1) mask1=[2,4] mask2=[3,5] ncomp=3 comp=[2,3,4]  { 4)
   ICL> fitgauss normal(,2) mask1=[2,4] mask2=[3,5] ncomp=3 comp=[2,3,4]
        ...
   ICL> fitgauss normal(,9) mask1=[2,4] mask2=[3,5] ncomp=3 comp=[2,3,4]
   ICL> evalfit  normal tellur1 comp=3                                    { 5)
   ICL> sub      normal tellur1 stellar1
   ICL> cadd     tellur1 1.0 tellur2                                      { 6)
   ICL> div      normal tellur2 stellar2
\end{verbatim}

\begin{itemize}
\item[1)] For each row of the image use the abscissa ranges
   [1;2], [3;4], and [5;6] to
\htmlref{fit a polynomial}{FITPOLY}
   continuum. The two intervals excluded here probably contain spectral
   lines. For each row the parameters describing the fit are stored as
   the first component in the results' NDF in the Specdre Extension of
   {\tt image}. You would probably run the fit routine with dialogue and
   graphics the first time round to play with the mask intervals.

\item[2)] For the whole image use the stored result to
\htmlref{generate a corresponding data set}{EVALFIT}
   representing the fitted continuum.

\item[3)] Use a
\xref{KAPPA}{sun95}{DIV}
   command to divide the original image by the continuum.

\item[4)] For each row of the normalised image use the intervening
   abscissa ranges and
\htmlref{fit three Gauss profiles}{FITGAUSS}
   to them. These are stored as components 2, 3 and 4 in the Extension.
   Again you would use dialogue to play with masks and parameter guesses
   in the first use of {\tt fitgauss}.  For subsequent image rows you
   would try without dialogue and just recall the command line to edit
   the row number.

\item[5)] The second line fitted, the third component stored, is not
   stellar but terrestrial, subtract it with the KAPPA command
{\tt\xref{sub}{sun95}{SUB}}.

\item[6)] Use KAPPA's
{\tt\xref{cadd}{sun95}{CADD}}
   to turn the simple telluric lines into proper telluric spectra with
   continuum at 1.0. Then divide the normalised spectra by the telluric
   spectra.
\end{itemize}

% -----------------------------------------------------------------------------

\section{\label{axcalib}\xlabel{axcalib}Arc spectrum axis calibration}

   There are four applications to do a wavelength calibration, well a
   calibration in spectroscopic values anyway.  You can use frequencies
   or photon energies if you feel like it.  You can also use nanometre,
   micrometre or \AA ngstr\"om, as you please.

   At the heart of this axis calibration is an algorithm written by
\htmlref{Mills (1992)}{refer}
   to automatically identify features in an arc spectrum.  For this to
   work, you must have a data base of arc feature identifications rather
   than just a simple line list.  You can use
{\tt\htmlref{arcgendb}{ARCGENDB}}
   to convert a
\xref{line list (as distributed with Figaro)}{sun86}{arc_documentation}
   into a ``feature data base''.  Unfortunately the data base takes some
   time to build, but it is also rather big, 10 to 100 times bigger than
   the simple list.  So there may be a point in taking only the relevant
   wavelength range from long line lists like Th-Ar and convert it into a
   feature data base.

   With such a data base at your disposal, you still cannot run the
   auto-identifier.  This is because the calibration procedure as
   performed by Figaro's
{\tt\xref{arc}{sun86}{ARC}}
   is broken up into three steps:

\begin{itemize}
\item In the first step you go through the arc spectrum and locate
   features, that is you find out where in pixel coordinates the
   observed line features are.  For this first step you normally use
{\tt\htmlref{arclocat}{ARCLOCAT}}.
   Usually this first step will be done in two passes. In the first pass
   you run {\tt arclocat dialog=f} so that it tries on its own to find
   un-blended narrow emission features in the arc spectrum.  In a second
   pass you run {\tt arclocat dialog=g} and use the graphic dialogue to
   improve the set of located features.  You may add features not found
   before, or delete features that are blended or known to be absent
   from the feature data base.  In general you should locate as many
   features as possible, you can always leave some un-identified.

   Instead of {\tt arclocat} you can also use
{\tt\htmlref{fitgauss}{FITGAUSS}}
   or
{\tt\htmlref{fittri}{FITTRI}},
   in case that the simplified line fit in {\tt arclocat} is
   not good enough.  {\tt arclocat} has modes {\tt Gauss} and {\tt
   triangle}, too, but it fits only one line at a time.

\item Only in the second step do you auto-identify the features, that is
   the Mills algorithm will try to establish for some of the located
   features what their identification might be in terms of wavelength,
   frequency or whatever you use in the feature data base.  This second
   step is performed by
{\tt\htmlref{arcident}{ARCIDENT}}.
   The result must be regarded with some scepticism, since there is a
   small chance that it finds a grossly wrong solution or makes a
   slightly unfavourable selection of features to identify.  All this
   can be corrected in the third step.

\item The third step again may or may not be in a graphics dialogue.
{\tt\htmlref{arcdisp}{ARCDISP} dialog=g} will display a plot of
   wavelength, frequency etc.\ versus pixel coordinate.  It does not
   show the spectrum. Instead vertical lines indicate unidentified (but
   located) features, horizontal lines indicate all possible
   identifications from the feature data base, and crosses indicate
   identified features. Finally the would-be dispersion curve is
   displayed.  You can now add or remove identifications with mouse and
   keyboard by clicking on the intersection of the feature location
   (vertial line) and potential identification (horizontal line).

   The improvement of feature identification is one goal of {\tt
   arcdisp}.  The other is to do a polynomial fit to the
   pixel-wavelength relation and to convert pixel coordinates into
   wavelengths using that fit.  This latter goal can also be achieved
   without the graphics dialogue with {\tt arcdisp dialog=n}.  This does
   not replace the {\it pixel coordinates} of the main NDF, but creates
   a new array of {\it spectroscopic values} in the Specdre Extension.
\end{itemize}

   {\tt arclocat}, {\tt arcident}, {\tt arcdisp} in general work not on
   a spectrum, but on an image or cube that has spectra in its rows
   (rows, not columns). {\tt arclocat dialog=g} allows you to switch
   from one row to another as you please, {\tt arclocat dialog=n} will
   scan through all rows in sequence.  {\tt arcident} will do an
   independent auto-identification on each spectrum, so it is suitable
   for a collapsed echellogram where successive extracted orders are in
   the rows of an image.

   {\tt arcdisp dialog=g} will work on each row in sequence, the user
   can work on one row and proceed to the next in her own time.  She can
   quit at any time; this will leave the file without spectroscopic
   values, but any improved feature identifications are kept.  If the
   user steps through all rows, then the spectroscopic values will be
   kept as well. (You can have spectroscopic values in the Specdre
   Extension either for all rows or none at all!).  You may want to set
   the label and unit for the spectroscopic values after {\tt arcdisp}
   with KAPPA's
{\tt\xref{setlabel}{sun95}{SETLABEL}}
   and
{\tt\xref{setunits}{sun95}{SETUNITS}}.

   So far you have only succeeded in producing an array of calibrated
   spectroscopic values in the Specdre Extension of the {\it arc
   spectrum}. You will want to copy that array into the celestial
   observation you are actually interested in.  This can be done with
{\tt\xref{ndfcopy}{sun95}{NDFCOPY}},
   provided the sky spectrum does have a Specdre Extension and does not
   have a {\tt SPECVALS} component in it.  That status can be achieved
   by two
{\tt\htmlref{editext}{EDITEXT}}
   commands.  Finally you may want to re-sample each spectrum (row in a
   cube) so that all use the same linear grid of spectroscopic
   values. To that end you use
{\tt\htmlref{resample}{RESAMPLE} mode=cube}.
   Here is the whole procedure.  (The commands are not complete,
   parameters are missing.  Also there may be no Th-Ar line lists
   available for photon energies in the MeV range.)

\begin{verbatim}
   ICL> arcgendb thar.arc thar_arc
   ICL> arclocat dialog=n arc1
   ICL> arclocat dialog=g arc1
   ICL> arcident arc1 arc2 thar_arc
   ICL> arcdisp arc2
   ICL> setlabel arc2.more.specdre.specvals "particle energy"
   ICL> setunits arc2.more.specdre.specvals "log10(10**9*eV)"
   ICL> editext "create" sky1
   ICL> editext "delete specvals" sky1
   ICL> ndfcopy arc2.more.specdre.specvals sky1.more.specdre.specvals
   ICL> resample cube sky1 sky2
\end{verbatim}

% -----------------------------------------------------------------------------

\section{\label{extension}\xlabel{extension}The Specdre Extension}

   The spectroscopic axis information is not quite complete with only
   label and units in the axis structure. For a frequency scale you have
   to know which reference frame it refers to, for wavelengths you might
   also be worried about the refractive index of the medium in which the
   wavelength is measured. And for radial velocity scales you need to
   know the rest frequency of the spectral line. Another thing most
   useful in spectroscopy would be to store continuum and line fits
   along with the data. The concept of
\xref{extensions to an NDF}{sun33}{extensions}
   provides a
   good means to store such information. The design of Specdre's own
   extension is outlined here.

   The Specdre Extension is the structure {\tt <myndf>.MORE.SPECDRE} and
   is of type {\tt SPECDRE\_EXT}. {\tt <myndf>} is the creation name of
   the main NDF, the one with the data, axes etc. {\tt <myndf>.MORE}
   contains all the extensions to this main NDF. The Specdre Extension
   may {\it contain} other NDFs, thus we have to distinguish between the
   ``main NDF'' and the ``Extension NDFs''.

   Specdre contains a number of subroutines for easy and consistent
   access to the Extension. Programmers are advised to contact the
   author if they want to make use of the Specdre Extension in their
   programmes.

   All Specdre applications support version 0.7 of the Specdre
   Extension, which was introduced at version 0.7 of the software. Some
   applications support an advanced version 1.1 of the Extension, as
   introduced in version 1.1 of the software.

% -----------------------------------------------------------------------------

\subsection{Specdre Extension v.\,0.7}

   The items of the Extension as introduced with Specdre version 0.7 are
   described below. All top-level items are optional, but each is either
   complete or absent.

\begin{itemize}

\item{\tt .SPECAXIS} is an \_INTEGER scalar which defaults to 1. Its
   value is the number of the spectroscopic axis. This is the axis along
   which the one-dimensional ``spectroscopic subsets'' extend. It must
   be greater than or equal to 1 and less than or equal to the number of
   axes in {\tt <myndf>}. A change of {\it specaxis} may render other
   components invalid as regards their values or shapes.

\item{\tt .RESTFRAME} is a \_CHAR*32 scalar which defaults to
   ``unknown''.  Its value describes the rest frame used to express the
   observed frequency, wavelength, radial velocity, or redshift.

\item{\tt .INDEXREFR} is a \_REAL or \_DOUBLE scalar which defaults to
   1. Its value is the index of refraction needed to convert frequency
   into wavelength and vice versa.

\item{\tt .FREQREF} is a \_REAL or \_DOUBLE scalar which defaults to the
   bad value. Its value is the reference frequency needed to convert
   between radial velocity and redshift on the one hand and frequency on
   the other hand. The value is evaluated together with {\tt .FREQUNIT}:

   \[\nu_0=freqref*10^{frequnit}*{\rm Hz}\]

\item{\tt .FREQUNIT} is an \_INTEGER scalar which defaults to 0. Its
   value is the common logarithm of the unit used for {\tt .FREQREF}
   divided by Hertz. Note that this is neither a \_CHAR nor a \_REAL or
   \_DOUBLE.

\item{\tt .SPECVALS} is an NDF structure with data, label and units
   components. {\tt .SPECVALS.\-DATA\_\-ARRAY} is a \_REAL or \_DOUBLE
   array which has the same shape as {\tt <myndf>}. It defaults to a
   multiple copy of the centre array of the spectroscopic axis {\tt
   <myndf>.AXIS\-({\it specaxis})\-.DATA\_\-ARRAY}. This structure
   contains spectroscopic axis values (either wavelength, or frequency,
   or velocity, or redshift) for each pixel of the data cube. Labels and
   units must be stored with this data structure. Their default values
   are copies from the spectroscopic axis {\tt <myndf>.AXIS({\it
   specaxis}).LABEL, .UNITS} in the main NDF or ``unknown'' if the axis
   has no label or unit. A modification of spectroscopic values may
   render parts of {\tt .RESULTS} invalid, but no rules are formulated
   in this respect. This structure must not contain bad values.

\item{\tt .SPECWIDS} is an NDF structure with only a data component.
   {\tt .SPECWIDS.DATA\_\-ARRAY} is a \_REAL or \_DOUBLE array which has
   the same shape as {\tt <myndf>}. It defaults to (i) a derivative from
   {\tt .SPECVALS} in the way prescribed by
\xref{SUN/33}{sun33}{axis_components},
   or (ii) to a multiple copy of the width array of the spectroscopic
   axis. Like {\tt .SPECVALS} contains the multidimensional array of
   spectroscopic values for the pixel centres, this array contains the
   spectroscopic widths for the pixels. Labels and units are not to be
   stored with this data structure. This structure is always considered
   together with {\tt .SPECVALS}. It must not contain bad values.

\item{\tt .COVRS} is an NDF-type structure with only a data component.
   {\tt .COVRS.DATA\_ARRAY} is a \_REAL or \_DOUBLE array which has the
   same shape as {\tt <myndf>}. {\tt .COVRS} defaults to
   non-existence. The meaning is as follows: For some reason pixels
   belonging to the same spectroscopic subset may be interrelated. While
   no complete covariance matrix is stored, this structure holds the sum
   over the rows of the covariance matrix (cf.\ Meyerdierks, 1992). For
   multi-dimensional data note that this holds information only about
   the interrelation {\it within any one} spectroscopic subset, not
   between different such subsets. That means we know only about
   interrelation along the spectroscopic axis (within a spectrum) but
   not perpendicular to that axis (between spectra). 

\item{\tt .RESULTS} is an NDF-type structure with data and variance
   components and a number of extensions in the {\tt .MORE}
   component. All these extensions are HDS vectors. They have either one
   element per component or one element per parameter. The shape of the
   {\tt .RESULTS} structure is defined by (i) the shape of {\tt
   <myndf>}, (ii) the total number of parameters {\it tnpar {\tt>} 0},
   (iii) the number of components {\it ncomp {\tt>} 0}. Each component
   has allocated a certain number of parameters {\it npara(comp) {\tt
   >=} 0}. The total number of parameters must be

   \[tnpar\geq\sum_{comp=1}^{ncomp}npara(comp)\]

   while the parameter index for any component runs from

   \[para1(comp)=\sum_{i=1}^{comp-1}npara(i)+1
   \mbox{\phantom{mmm}to\phantom{mmm}}
   para2(comp)=\sum_{i=1}^{comp}npara(i)\]

   Components are additive, i.e.\ the combined result is the sum of all
   those components that can be evaluated.

   Note that the concept of a component is different from that of a
   transition or a kinematic component: A component is a line feature
   you can discern in the spectrum. Any component can in general be
   assigned a transition and a radial velocity. So you may have several
   components belonging to the same transition and several components of
   similar velocity belonging to different transitions. You may at any
   time decide that a discernible component has been misidentified and
   just change its identification. The concept of a component is even
   more general, in that it can be the continuum, in which case there is
   in general no laboratory frequency for identification.

   There is, however, a restriction for data sets with more than one
   spectrum. Any component may differ from spectrum to spectrum only by
   the fitted values. The mathematical type and identification of
   components is common to all spectra.

\end{itemize}

% -----------------------------------------------------------------------------

\subsection{The results structure in the Specdre Extension v.\,0.7}

\begin{itemize}

\item{\tt .RESULTS.DATA\_ARRAY} and {\tt .RESULTS.VARIANCE} are \_REAL
   or \_DOUBLE array structures which default to bad values. They have
   one axis more than {\tt <myndf>}: The spectroscopic axis is skipped;
   instead new first and second axes are inserted. The first axis counts
   the fit parameters up to the maximum ({\it tnpar}). The second axis
   is of length 1 and may be used in future. All further axes are of the
   same length as the corresponding non-spectroscopic axes in {\tt
   <myndf>}.

\item{\tt .RESULTS.MORE.LINENAME} is a \_CHAR*32 vector which defaults
   to ``unidentified component''. There is one element for each
   component. Its value is a spectroscopist's description of the
   component, like ``[OIII] 5007 v-comp \#1'', ``12CO J=1-0'',
   ``nebulium'', ``5500 K black body''. It is essential that the strings
   are of length 32.

\item{\tt .RESULTS.MORE.LABFREQ} is a \_REAL or \_DOUBLE vector which
   defaults to bad values. There is one element for each component. The
   value is the laboratory frequency of the transition. The unit used it
   the one stored in {\tt .FREQUNIT}. The laboratory frequency is the
   frequency as observed in the emitter's rest frame. The meaning of
   this frequency is similar to that of {\tt .FREQREF} in that the
   laboratory frequency of a transition is a useful value for the
   reference frequency of the velocity or redshift axis. The difference
   is that each component fitted may or may not have its own laboratory
   frequency. {\tt .FREQREF} will usually be a copy of one of the
   elements of {\tt .RESULTS.MORE.LABFREQ}.

\item{\tt .RESULTS.MORE.COMPTYPE} is a \_CHAR*32 vector which defaults
   to ``unknown function''. There is one element for each component. Its
   value is a mathematician's description of the component, like
   ``Gauss'', ``triangle'', ``Lorentz-Gauss'', ``Voigt'',
   ``polynomial'', ``Chebyshev series'', ``sine''. It is essential that
   the strings are of length 32.

\item{\tt .RESULTS.MORE.NPARA} is an \_INTEGER vector defaulting to {\tt
   INT({\it tnpar/ncomp})}. There is one element for each component. Its
   value is the number of parameters stored for that component. When
   more components are added to an existing {\tt .RESULTS} structure,
   then the new components are allocated by default {\tt INT}{\it(( tnpar
   - tnpar\_old) / (ncomp - ncomp\_old))} parameters. The numbers of
   parameters must be greater than or equal to zero.

\item{\tt .RESULTS.MORE.MASKL} and {\tt .RESULTS.MORE.MASKR} are \_REAL
   or \_DOUBLE vectors which default to bad values; both are of the same
   type. There is one element in either vector for each component. A
   component {\it comp} is evaluated according to type and parameters in
   the range of spectroscopic values between {\it maskl(comp)} and {\it
   maskr(comp)}. The component is assumed to be zero outside this
   interval. Bad values indicate that the range is not restricted.

\item{\tt .RESULTS.MORE.PARATYPE} is a \_CHAR*32 vector which defaults
   to ``unknown parameter''. There is one element for each
   parameter. Its value is a mathematician's description of the
   parameter. A Gauss profile might be specified by parameters
   ``centre'', ``peak'', ``sigma width'', ``integral''. It is essential
   that the strings are of length 32.
\end{itemize}

   It should be noted that there exist no rules about how to store
   certain components, like ``A Gauss profile must be called Gauss and
   have parameters such and such''. What is stored should be described
   by the strings so that a human reader knows what it is all about.
   This does not prevent pairs of applications from storing and
   retrieving components if they use the strings in a consistent
   way. The documentation of any application that writes ore reads
   results should specify what strings are written or recognised.

% -----------------------------------------------------------------------------

\subsection{Specdre Extension v.\,1.1}

   The items of the Extension as added with Specdre version 1.1 are
   described below. All top-level items are optional, but each is either
   complete or absent.

\begin{itemize}

\item{\tt .COORD1} and {\tt .COORD2} are NDF structures each with data,
   label and units components. {\tt .COORD{\it i}.DATA\_ARRAY} are \_REAL
   or \_DOUBLE. Both have the same shape, which is similar to that of
   {\tt <myndf>}. The only difference is that in {\tt .COORD{\it i}} the
   spectroscopic axis is degenerate (has only one pixel). {\tt .COORD1}
   and {\tt .COORD2} either exist both or neither of them must exist.
   The data values default to a multiple copy of the pixel centres of
   the main array along the first and second non-spectroscopic axes. For
   example, if {\tt .SPECAXIS} is 2, then {\tt .COORD1.DATA\_ARRAY} is a
   multiple copy of {\tt <myndf>.AXIS(1).DATA\_ARRAY} and {\tt
   .COORD2.DATA\_ARRAY} is a multiple copy of {\tt
   <myndf>.AXIS(3).DATA\_ARRAY}. In the same example, if {\tt <myndf>}
   has shape {\it nx} by {\it ny} by {\it nz}, then both {\tt .COORD{\it
   i}} have shape {\it nx} by 1 by {\it nz}. {\tt .COORD{\it i}} store
   for each spectrum a two-dimensional position. This position could be
   used by a plot routine to position spectra according to {\tt
   COORD{\it i}} on the plot. The values may or may not be sky
   positions. They could be in any coordinate system and using any
   units. Labels and units must be stored with both NDF
   structures. Their default values are copies from the relevant axes of
   {\tt <myndf>}, or ``unknown'' if the relevant axis has no {\tt
   .LABEL} or {\tt .UNITS}. The data components must not contain bad
   values.

\end{itemize}

   One difference between {\tt .SPECVALS/.SPECWIDS} and {\tt .COORD{\it
   i}} is important to note. {\tt .SPECVALS} and {\tt .SPECWIDS} are to
   be used (within Specdre) as replacement of the centres and widths in
   {\tt <myndf>.AXIS({\it specaxis})}. But {\tt .COORD{\it i}} are {\em
   not\/} intended as replacements for the corresponding axis
   information, these structures are used only in special circumstances.
   In practice that means the following:

\begin{itemize}

\item If an application requires axis information for any axis, then
   {\tt .SPECVALS/SPECWIDS} must override {\tt <myndf>.AXIS({\it
   specaxis})}, but {\tt .COORD{\it i}} are ignored completely.

\item If an application requires information about the more general
   position information as provided by {\tt .COORD{\it i}}, then these
   structures are looked for. Only in their absence are the relevant
   axes in {\tt <myndf>} used to generate the same information.

\end{itemize}

% -----------------------------------------------------------------------------

\subsection{An example Specdre Extension}

   As an example, a sensible Specdre Extension has been added to a
   three-dimensional NDF, which in fact is a data cube of HI 21 cm
   spectra taken on a grid of galactic longitudes and latitudes. Here is
   how
{\tt\xref{hdstrace}{sun102}{}}
   sees the file before the Extension is added:

\small
\begin{verbatim}
LVCHI  <NDF>
   TITLE          <_CHAR*12>      'KAPPA - Cadd'
   DATA_ARRAY(32,37,150)  <_REAL>   -0.1216488,0.1160488,4.3972015E-02,
                                    ... 0.4012871,-7.1809769E-02,0.4626274,*
   AXIS(3)        <AXIS>          {array of structures}
   Contents of AXIS(1)
      DATA_ARRAY(32)  <_REAL>        130,130.1667,130.3333,130.5,130.6667,
                                     ... 134.6668,134.8335,135.0002,135.1668
   Contents of AXIS(2)
      DATA_ARRAY(37)  <_REAL>        17,17.16667,17.33333,17.5,17.66666,
                                     ... 22.49998,22.66665,22.83331,22.99998
   Contents of AXIS(3)
      DATA_ARRAY(150)  <_REAL>       -163.855,-162.565,-161.275,-159.985,
                                     ... 24.48488,25.77489,27.06489,28.35489
\end{verbatim}
\normalsize

   Now almost all components that can exist in a Specdre Extension are
   set, mostly to their default values. The only component missing is
   the sum of rows of a covariance matrix. This is because that
   structure usually must not exist: Other structures can be assigned
   ``harmless'' values, but the sheer existence of {\tt .COVRS} makes a
   difference. The Extension was actually made with the {\tt editext}
   command, which can also list a summary of the Extension:

\small
\begin{verbatim}
List of Specdre Extension (v. 1.1)

Name of NDF:         /home/hme/lvchi
Spectroscopic axis:  3
Reference frame:     local standard of rest
Refractive index:    1.0000000
Reference frequency: 1420.405751786000 [10**9 Hz]
Spectroscopic values exist and are velocity [km/s].
Spectroscopic widths do exist.
First coordinates    exist and are galactic longitude [degree]
Second coordinates   exist and are galactic latitude [degree]
Covariance row sums  do not exist.
Result structure provides for 4 parameters in 1 component.

#  line name               lab freq.          type  npara  mask from     to
1  HI (21cm) LV component  1420.405751786000  Gauss     4  -.1701412E+39 -.1701412E+39

# parameter type
1 centre
2 peak
3 FWHM
4 integral

End of list.
\end{verbatim}
\normalsize

{\tt hdstrace}ing the NDF now yields:

\small
\begin{verbatim}
LVCHI  <NDF>
   TITLE          <_CHAR*12>      'KAPPA - Cadd'
   DATA_ARRAY(32,37,150)  <_REAL>   -0.1216488,0.1160488,4.3972015E-02,
                                    ... 0.4012871,-7.1809769E-02,0.4626274,*
   AXIS(3)        <AXIS>          {array of structures}
   Contents of AXIS(1)
      DATA_ARRAY(32)  <_REAL>        130,130.1667,130.3333,130.5,130.6667,
                                     ... 134.6668,134.8335,135.0002,135.1668
   Contents of AXIS(2)
      DATA_ARRAY(37)  <_REAL>        17,17.16667,17.33333,17.5,17.66666,
                                     ... 22.49998,22.66665,22.83331,22.99998
   Contents of AXIS(3)
      DATA_ARRAY(150)  <_REAL>       -163.855,-162.565,-161.275,-159.985,
                                     ... 24.48488,25.77489,27.06489,28.35489
   MORE           <EXT>           {structure}
      SPECDRE        <SPECDRE_EXT>   {structure}
         SPECAXIS       <_INTEGER>      3                                   (1)
         RESTFRAME      <_CHAR*32>      'local standard of rest'            (2)
         INDEXREFR      <_REAL>         1                                   (3)
         FREQREF        <_DOUBLE>       1420.405751786                      (4)
         FREQUNIT       <_INTEGER>      9                                   (4)
         SPECVALS       <NDF>           {structure}
            DATA_ARRAY     <ARRAY>         {structure}                      (5)
               DATA(32,37,150)  <_REAL>       -163.855,-163.855,-163.855,
                                              ... 28.35489,28.35489,28.35489
               ORIGIN(3)      <_INTEGER>      1,1,1
 
            LABEL          <_CHAR*64>      'velocity'                       (6)
            UNITS          <_CHAR*64>      'km/s'                           (6)
         SPECWIDS       <NDF>           {structure}
            DATA_ARRAY     <ARRAY>         {structure}                      (7)
               DATA(32,37,150)  <_REAL>       1.289993,1.289993,1.289993,
                                              ... 1.290001,1.290001,1.290001
               ORIGIN(3)      <_INTEGER>      1,1,1
         COORD1         <NDF>           {structure}                         (8)
            DATA_ARRAY     <ARRAY>         {structure}
               DATA(32,37,1)  <_REAL>         130,130.1667,130.3333,130.5,
                                              ... 134.8335,135.0002,135.1668
               ORIGIN(3)      <_INTEGER>      1,1,1
            LABEL          <_CHAR*64>      'galactic longitude'
            UNITS          <_CHAR*64>      'degree'
         COORD2         <NDF>           {structure}                         (8)
            DATA_ARRAY     <ARRAY>         {structure}
               DATA(32,37,1)  <_REAL>         17,17,17,17,17,17,17,17,17,17,
                                              ... 22.99998,22.99998,22.99998
               ORIGIN(3)      <_INTEGER>      1,1,1
            LABEL          <_CHAR*64>      'galactic longitude'
            UNITS          <_CHAR*64>      'degree'
         RESULTS        <NDF>           {structure}                         (9)
            DATA_ARRAY     <ARRAY>         {structure}                     (10)
               DATA(4,1,32,37)  <_REAL>       *,*,*,*,*,*,*,*,*,*,*,*,*,*,
                                              *,*,*,*,*,*,*,*,*,*,*,*,*,*
               ORIGIN(4)      <_INTEGER>      1,1,1,1
            VARIANCE       <ARRAY>         {structure}                     (11)
               DATA(4,1,32,37)  <_REAL>       *,*,*,*,*,*,*,*,*,*,*,*,*,*,
                                              *,*,*,*,*,*,*,*,*,*,*,*,*,*
               ORIGIN(4)      <_INTEGER>      1,1,1,1
            MORE           <EXT>           {structure}                     (12)
               LINENAME(1)    <_CHAR*32>      'HI (21cm) LV component'
               LABFREQ(1)     <_DOUBLE>       1420.405751786
               COMPTYPE(1)    <_CHAR*32>      'Gauss'
               NPARA(1)       <_INTEGER>      4
               MASKL(1)       <_REAL>         *
               MASKR(1)       <_REAL>         *
               PARATYPE(4)    <_CHAR*32>      'centre','peak','FWHM','inte...'
\end{verbatim}
\normalsize

\begin{enumerate}
\item The third axis of the main NDF is declared the spectroscopic axis.
   In fact this is the velocity axis of the data.

\item The telescope's local oscillator was controlled such that the
   spectrometer recorded frequency in the reference frame of the local
   standard of rest. Alternatively, someone could have re-calibrated the
   spectroscopic axis at some stage to refer to the LSR.

\item The refractive index is set to 1, which amounts to ignoring
   refraction for the purpose of wavelength calibration. Probably no one
   will ever use wavelengths for these data anyway. If this structure
   were missing, a value of 1 would be assumed whenever necessary.

\item The reference frequency is encoded in two numbers. Taken together
   they say that the velocity axis refers to a laboratory frequency of
   1420.405751786 MHz.

\item Here is a data cube of the same shape as the main data. It stores
   the velocity axis for each sky position {\it(l,b)}. Thus for each
   pixel in the three-dimensional data cube, there is a separate
   velocity value stored for the pixel centre position. In this case it
   is a waste of disk space since all positions have the same velocity
   grid. But a slit spectrum recorded with a CCD may have a shift of
   wavelength scales from one row to the next.

\item The cube with the spectroscopic pixel centres should have its own
   label and unit. As the data reduction and analysis progresses, these
   may change, while the main NDF's {\tt .AXIS(3)} structure will in
   general remain untouched.

\item Here is a data cube of the same shape as that with the pixel
   centres, only that it stores the pixel widths. Each of these --
   centres and widths -- may or may not exist independently. Though in
   general the width is only useful if there is also an array of
   centres.

\item These two structures look similar to the cube of spectroscopic
   pixel centres. But these ``cubes'' do not extend along the
   spectroscopic axis. Each contains one number per spectrum. The two
   structures combine to give each spectrum a two-dimensional position,
   be that on the sky, on plotting paper, or both. In this example the
   first spectrum is at (130,17), the second at (130.167,17) and so
   on. The positions thus coincide with those according to the first and
   second axes of the main NDF. In this case it is a waste of disk
   space. But in general these two structures allow for non-orthogonal
   and even non-linear projections from NDF axis centre space to {\tt
   COORD1/COORD2} space. A two-dimensional main NDF might actually be an
   arbitrary sequence of spectra, and still these two structures could
   help to sort each spectrum into its place on the sky.

\item Here is a rather complex NDF structure. It is used to store and
   retrieve fit results. The results can be different from one sky
   position to the next, but there is only one set of parameters per
   position. The spectroscopic axis -- of length 150 -- is scrapped, the
   two positional axes -- lengths 32 and 37 respectively -- are
   retained. Inserted are the first and second axes, here of lengths 4
   and 1 respectively. Don't wonder about the second axis, it is always
   of length 1.

   Usually the result structure will be manipulated by applications as
   they find it necessary to store data in it. But most of the structure
   can be manipulated explicitly with the command
{\tt\htmlref{editext}{EDITEXT}}.

\item This should contain the values of the parameters, by default they
   take the bad value, which is represented here by asterisks.

\item This should contain the variances of the parameters as a measure
   of their uncertainties. Again, by default these values are bad.

\item The extensions to the result NDF are seven vectors. Most have one
   element per spectral component -- in the example 1. The {\tt
   .PARATYPE} vector has four elements, one for each parameter. The sole
   component provided for in the result NDF is the ``LV component'' of
   the 21 cm line. Its laboratory frequency is repeated here, the value
   is independent of the reference frequency above, but the same unit
   (MHz) is used. We obviously expect that the spectral line has the
   shape of a Gauss curve and we want to store four parameters
   describing that curve. {\tt .PARATYPE} indicates the meaning of all
   parameters. No mask is enabled to limit the velocity range where the
   Gauss curve applies, i.e. {\tt .MASKL} and {\tt .MASKR} have the bad
   value.

   We might want to store also the results for a parabolic baseline
   fit. Then we would add a second spectral component with three
   parameters. The vectors that are now of length 1 would become of
   length 2, {\tt .PARATYPE} would become of length 7. The additional
   second vector elements would be ``baseline'', bad value, ``polynomial
   of 2nd order'', 3, bad value, bad value. The fifth to seventh element
   of {\tt .PARATYPE} could be ``coeff. 0'', ``coeff. 1'', ``coeff. 2''.

\end{enumerate}

% -----------------------------------------------------------------------------

\appendix
\newpage
\section{\label{refer}\xlabel{refer}References}

\begin{itemize}
\item Bailey J.A., Chipperfield A.J., 1991, ICL -- The interactive
   command language for ADAM, Starlink Guide 5
\item Currie M.J., 1992, KAPPA -- Kernel application package, Starlink
   User Note 95
\item Currie M.J., 1994, HDSTRACE -- Listing HDS data files, Starlink
   User Note 102
\item Eaton N., 1995, AGI -- Application Graphics Interface -- A
   subroutine library for accessing the graphics database, Starlink User
   Note 48
\item Harrison P., Rees P., Draper P., 1994, PONGO -- A set of
   applications for interactive data plotting, Starlink User Note 137
\item Meyerdierks H., 1992, Fitting resampled spectra, in
   P.J.\ Grosb\o l, R.C.E.\ de Ruijsscher (eds), 4th ESO/ST-ECF Data
   Analysis Workshop, Garching, 13 -- 14 May 1992, ESO Conference and
   Workshop Proceedings No. 41, Garching bei M\"unchen
\item Meyerdierks H., 1995, FIGARO -- A general data reduction system,
   Starlink User Note 86
\item Mills D., 1992, Automatic ARC wavelength calibration, in
   P.J.\ Grosb\o l, R.C.E.\ de Ruijsscher (eds), 4th ESO/ST-ECF Data
   Analysis Workshop, Garching, 13 -- 14 May 1992, ESO Conference and
   Workshop Proceedings No. 41, Garching bei M\"unchen
\item Shortridge K., 1991, FIGARO -- General data reduction and
   analysis, Starlink Miscellaneous User Document
\item Terrett D.L., 1991, GKS -- Graphical Kernel System, Starlink
   User Note 83
\item Terrett D.L., 1993, PSMERGE -- Encapsulated PostScript
   handling utility, Starlink User Note 164
\item Terrett D.L., 1995, PGPLOT -- Graphics subroutine library,
   Starlink User Note 15
\item Wallace P.T., Terrett D.L., 1995, SGS -- Simple Graphics System,
   Starlink User Note 85
\item Warren-Smith R.F., 1995, NDF -- Routines for accessing the
   Extensible N-Dimensional Data Format, Starlink User Note 33
\item Warren-Smith R.F., 1995, HDS -- Hierarchical Data System, Starlink
   User Note 92
\end{itemize}

% -----------------------------------------------------------------------------

\newpage
\section{\label{classif}\xlabel{classif}Classified list of commands}

{\bf Input/output}

\htmlref{ASCIN}{ASCIN} -- Read a 1-D or N-D data set from an ASCII table.\\
\htmlref{ASCOUT}{ASCOUT} -- Write an NDF to an ASCII table.

{\bf Display}

\htmlref{MOVIE}{MOVIE} -- Browse through slices of a cube.\\
\htmlref{SPECCONT}{SPECCONT} -- Contour a two-dimensional cut.\\
\htmlref{SPECGRID}{SPECGRID} -- Plot spectra on position grid.\\
\htmlref{SPECPLOT}{SPECPLOT} -- Plot a spectrum.

{\bf Statistics, fitting}

\htmlref{CORREL}{CORREL} -- Correlate two or three data sets.\\
\htmlref{EVALFIT}{EVALFIT} -- Evaluate fit results.\\
\htmlref{FITBB}{FITBB} -- Fit diluted Planck curves to a spectrum.\\
\htmlref{FITGAUSS}{FITGAUSS} -- Fit Gauss profiles to a spectrum.\\
\htmlref{FITPOLY}{FITPOLY} -- Fit a polynomial to a spectrum.\\
\htmlref{FITTRI}{FITTRI} -- Fit triangular profiles to a spectrum.\\
\htmlref{MOMENTS}{MOMENTS} -- Calculate moments of spectra in a cube.

{\bf Axis calibration}

\htmlref{ARCDISP}{ARCDISP} -- Fit polynomial dispersion curve.\\
\htmlref{ARCGENDB}{ARCGENDB} -- Convert list of laboratory values to feature data base.\\
\htmlref{ARCIDENT}{ARCIDENT} -- Auto-identify located features.\\
\htmlref{ARCLOCAT}{ARCLOCAT} -- Locate line features in a set of spectra.

{\bf Data calibration}

\htmlref{BBODY}{BBODY} -- Calculate a black body spectrum.

{\bf Convolution, re-sampling, merging}

\htmlref{FILLCUBE}{FILLCUBE} -- Copy one NDF into part of another.\\
\htmlref{RESAMPLE}{RESAMPLE} -- Re-sample and average several spectra.

{\bf Reshaping}

\htmlref{GROW}{GROW} -- Copy an N-dimensional cube into part of an (N+M)-dimensional one.\\
\htmlref{SUBSET}{SUBSET} -- Take a subset of a data set.\\
\htmlref{XTRACT}{XTRACT} -- Average an N-dimensional cube into an (N-M)-dimensional one.

{\bf Miscellaneous}

\htmlref{EDITEXT}{EDITEXT} -- Edit the Specdre Extension.\\
\htmlref{GOODVAR}{GOODVAR} -- Replace negative, zero and bad variance values.\\
\htmlref{SPE\_HELP}{SPE\_HELP} -- Browse through a Starlink help library.

% -----------------------------------------------------------------------------

\newpage
\section{\label{applics}\xlabel{applics}Specdre commands in detail}
\small

%+
%  Name:
%     SST.TEX

%  Purpose:
%     Define LaTeX commands for laying out Starlink routine descriptions.

%  Language:
%     LaTeX

%  Type of Module:
%     LaTeX data file.

%  Description:
%     This file defines LaTeX commands which allow routine documentation
%     produced by the SST application PROLAT to be processed by LaTeX and
%     by LaTeX2html. The contents of this file should be included in the
%     source prior to any statements that make of the sst commnds.

%  Notes:
%     The commands defined in the style file html.sty provided with LaTeX2html 
%     are used. These should either be made available by using the appropriate
%     sun.tex (with hypertext extensions) or by putting the file html.sty 
%     on your TEXINPUTS path (and including the name as part of the  
%     documentstyle declaration).

%  Authors:
%     RFWS: R.F. Warren-Smith (STARLINK)
%     PDRAPER: P.W. Draper (Starlink - Durham University)

%  History:
%     10-SEP-1990 (RFWS):
%        Original version.
%     10-SEP-1990 (RFWS):
%        Added the implementation status section.
%     12-SEP-1990 (RFWS):
%        Added support for the usage section and adjusted various spacings.
%     8-DEC-1994 (PDRAPER):
%        Added support for simplified formatting using LaTeX2html.
%     {enter_further_changes_here}

%  Bugs:
%     {note_any_bugs_here}

%-

%  Define length variables.
\newlength{\sstbannerlength}
\newlength{\sstcaptionlength}
\newlength{\sstexampleslength}
\newlength{\sstexampleswidth}

%  Define a \tt font of the required size.
\newfont{\ssttt}{cmtt10 scaled 1095}

%  Define a command to produce a routine header, including its name,
%  a purpose description and the rest of the routine's documentation.
\newcommand{\sstroutine}[3]{
   \goodbreak
   \rule{\textwidth}{0.5mm}
   \vspace{-7ex}
   \newline
   \settowidth{\sstbannerlength}{{\Large {\bf #1}}}
   \setlength{\sstcaptionlength}{\textwidth}
   \setlength{\sstexampleslength}{\textwidth}
   \addtolength{\sstbannerlength}{0.5em}
   \addtolength{\sstcaptionlength}{-2.0\sstbannerlength}
   \addtolength{\sstcaptionlength}{-5.0pt}
   \settowidth{\sstexampleswidth}{{\bf Examples:}}
   \addtolength{\sstexampleslength}{-\sstexampleswidth}
   \parbox[t]{\sstbannerlength}{\flushleft{\Large {\bf #1}}}
   \parbox[t]{\sstcaptionlength}{\center{\Large #2}}
   \parbox[t]{\sstbannerlength}{\flushright{\Large {\bf #1}}}
   \begin{description}
      #3
   \end{description}
}

%  Format the description section.
\newcommand{\sstdescription}[1]{\item[Description:] #1}

%  Format the usage section.
\newcommand{\sstusage}[1]{\item[Usage:] \mbox{} \\[1.3ex] {\ssttt #1}}


%  Format the invocation section.
\newcommand{\sstinvocation}[1]{\item[Invocation:]\hspace{0.4em}{\tt #1}}

%  Format the arguments section.
\newcommand{\sstarguments}[1]{
   \item[Arguments:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Format the returned value section (for a function).
\newcommand{\sstreturnedvalue}[1]{
   \item[Returned Value:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Format the parameters section (for an application).
\newcommand{\sstparameters}[1]{
   \item[Parameters:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Format the examples section.
\newcommand{\sstexamples}[1]{
   \item[Examples:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Define the format of a subsection in a normal section.
\newcommand{\sstsubsection}[1]{ \item[{#1}] \mbox{} \\}

%  Define the format of a subsection in the examples section.
\newcommand{\sstexamplesubsection}[2]{\sloppy
\item[\parbox{\sstexampleslength}{\ssttt #1}] \mbox{} \\ #2 }

%  Format the notes section.
\newcommand{\sstnotes}[1]{\item[Notes:] \mbox{} \\[1.3ex] #1}

%  Provide a general-purpose format for additional (DIY) sections.
\newcommand{\sstdiytopic}[2]{\item[{\hspace{-0.35em}#1\hspace{-0.35em}:}] \mbox{} \\[1.3ex] #2}

%  Format the implementation status section.
\newcommand{\sstimplementationstatus}[1]{
   \item[{Implementation Status:}] \mbox{} \\[1.3ex] #1}

%  Format the bugs section.
\newcommand{\sstbugs}[1]{\item[Bugs:] #1}

%  Format a list of items while in paragraph mode.
\newcommand{\sstitemlist}[1]{
  \mbox{} \\
  \vspace{-3.5ex}
  \begin{itemize}
     #1
  \end{itemize}
}

%  Define the format of an item.
\newcommand{\sstitem}{\item}

%% Now define html equivalents of those already set. These are used by
%  latex2html and are defined in the html.sty files.


\begin{htmlonly}

%  Re-define \ssttt.
   \newcommand{\ssttt}{\tt}

%  sstroutine. Note that the positioning of the \xlabel and \label commands
%  are designed to work with any version of latex2html. Ideally these would
%  both be in the \subsection{} command rather than after it, but this
%  doesn't work at present. We need \label for internal cross-references
%  with \htmlref.
   \renewcommand{\sstroutine}[3]{
%      \subsection{#1 - #2}
%      \xlabel{#1}
%      \label{#1}
      \subsection{#1 - #2\xlabel{#1}\label{#1}}
      \begin{description}
         #3
      \end{description}
   }

%  sstdescription
   \renewcommand{\sstdescription}[1]{\item[Description:]
      \begin{description}
         #1
      \end{description}
   }

%  sstusage
   \renewcommand{\sstusage}[1]{\item[Usage:]
      \begin{description}
         {\ssttt #1}
      \end{description}
   }

%  sstinvocation
   \renewcommand{\sstinvocation}[1]{\item[Invocation:]
      \begin{description}
         {\ssttt #1}
      \end{description}
   }

%  sstarguments
   \renewcommand{\sstarguments}[1]{
      \item[Arguments:]
      \begin{description}
         #1
      \end{description}
   }

%  sstreturnedvalue
   \renewcommand{\sstreturnedvalue}[1]{
      \item[Returned Value:]
      \begin{description}
         #1
      \end{description}
   }

%  sstparameters
   \renewcommand{\sstparameters}[1]{
      \item[Parameters:]
      \begin{description}
         #1
      \end{description}
   }

%  sstexamples
   \renewcommand{\sstexamples}[1]{
      \item[Examples:]
      \begin{description}
         #1
      \end{description}
   }

%  sstsubsection
   \renewcommand{\sstsubsection}[1]{\item[{#1}]}

%  sstexamplesubsection
   \renewcommand{\sstexamplesubsection}[2]{\item[{\ssttt #1}] \\ #2}

%  sstnotes
   \renewcommand{\sstnotes}[1]{\item[Notes:]
      \begin{description}
         #1
      \end{description}
   }

%  sstdiytopic
   \renewcommand{\sstdiytopic}[2]{\item[{#1}]
      \begin{description}
         #2
      \end{description}
   }

%  sstimplementationstatus
   \renewcommand{\sstimplementationstatus}[1]{\item[Implementation Status:] 
      \begin{description}
         #1
      \end{description}
   }

%  sstitemlist
   \newcommand{\sstitemlist}[1]{
      \begin{itemize}
         #1
      \end{itemize}
   }
\end{htmlonly}

%  End of "sst.tex" layout definitions.
%.
% @(#)sst.tex   1.2   95/03/06 11:29:07   95/03/06 11:36:00

\sstroutine{
   ARCDISP
}{
   Fit polynomial dispersion curve
}{
   \sstdescription{
      This routine fits a polynomial dispersion curve to a list of
      identified arc features and transforms the NDF pixel coordinates
      to spectroscopic values. Optionally you can use a graphical
      dialogue to improve on the previous feature identification, until
      you like the appearance of the dispersion curve.

      The input data must be a base NDF. They can be a single spectrum
      or a set of spectra. Examples for the latter are a long slit
      spectrum, a set of extracted fibre spectra, or a collapsed
      echellogram (a set of extracted orders from an echelle
      spectrograph). It is necessary that the spectroscopic axis be the
      first axis in the data set. It does not matter how many further
      axes there are, the data will be treated as a linear set of rows
      with each row a spectrum.

      The actual input is the results structure in the Specdre
      Extension. This must be a set of components of type `arc
      feature'. Each must have two parameters `centre' and `laboratory
      value'. These must be corresponding locations one expressed in
      NDF pixel coordinates, the other in spectroscopic values
      (wavelength, frequency etc.). The centres must be strictly
      monotonically increasing, their variances must be available.
      Laboratory values may be bad values to signify unidentified
      features.

      In the graphical dialogue the results structure may be updated.
      The locations remain unchanged; all located features form a fixed
      list of potentially identified features. Identifications may be
      added, deleted or modified. The user has to work on each row in
      turn (unless Quit is chosen). When the user switches from one row
      to the next, the dispersion curve for the finished row is applied
      and its spectroscopic values in the Specdre Extension are set.
      When the last row is finished, the application exits; the output
      of this routine is (i) an updated list of identifications in the
      results structure of the Specdre Extension and (ii) an array of
      spectroscopic values according to the dispersion curves for each
      row, also in the Specdre Extension. At any point the user can
      quit. In this case the array of spectroscopic values is
      discarded, but the updated identifications are retained. If run
      without dialogue, this routine simply performs the polynomial fit
      of the dispersion curve for each row in turn and works out the
      array of spectroscopic values. The list of identifications is
      input only and remains unchanged. If for any row the fit cannot
      be performed, then the spectroscopic values calculated so far are
      discarded and the routine aborts.

      There must not yet be any spectroscopic value information: There
      must be no array of spectroscopic values or widths in the Specdre
      Extension. The pixel centre array for the spectroscopic axis
      (i.e.\ the first axis) must be NDF pixel coordinates (usually 0.5,
      1.5, ...).

      This routine works on each row (spectrum) in turn. It fits a
      polynomial to the existing identifications. In the optional
      graphical dialogue two plots are displayed and updated as
      necessary. The lower panel is a plot of laboratory values
      (wavelength, frequency etc.) versus pixel coordinate shows
      \sstitemlist{
         \sstitem
         all possible identifications from the feature data base as
            horizontal lines,
         \sstitem
         all unidentified located features as vertical lines,
         \sstitem
         all identified located features as diagonal crosses,
         \sstitem
         the dispersion curve.
      }
      In the upper panel, a linear function is subtracted so that it
      displays the higher-order components of the dispersion curve.
      Crosses indicate the identified located features. Since the scale
      of this upper panel is bigger, it can be used to spot outlying
      feature identifications. In the dialogue you can
      \sstitemlist{
         \sstitem
         R - Switch to next row, accepting the current fit for this row
         \sstitem
         X - X-zoom 2x on cursor
         \sstitem
         Y - Y-zoom 2x on cursor
         \sstitem
         W - Unzoom to show whole row
         \sstitem
         N - Pan by 75\% of current plot range
         \sstitem
         A - Add ID for location nearest to cursor (from FDB)
         \sstitem
         S - Set ID for location nearest to cursor (from cursor y pos.)
         \sstitem
         D - Delete ID for feature nearest to cursor
         \sstitem
         Q - Quit (preserves updated IDs, discards applied fits for all
             rows)
         \sstitem
         ? - Help
      }
      Whenever the list of identifications is changed, the dispersion
      curve is fitted again and re-displayed. If there are too few
      identifications for the order chosen, then the dialogue will
      display the maximum order possible. But such an under-order fit
      cannot be accepted, the R option will result in an error.

      The Q option will always result in an error report, formally the
      routine aborts. After all, it does not achieve the main goal of
      applying individual dispersion curves to all rows.

      On one hand the output of this routine may be an updated list of
      identifications, which could in principle be used in a future run
      of this routine. On the other hand this routine will always
      result in an array of spectroscopic values. The existence of
      these spectroscopic values prevents using this routine again.
      Before using this routine again on the same input NDF you have to
      delete the SPECVALS component in the Specdre Extension.

      In order to facilitate repeated use of this routine on the same
      data, it always uses the Specdre Extension to store spectroscopic
      values, even if the data are one-dimensional and the first axis
      centre array would suffice to hold that information. This leaves
      the first axis centre array at NDF pixel coordinates, as
      necessary for re-use of this routine.
   }
   \sstusage{
      arcdisp in order
   }
   \sstparameters{
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If this is `Y', `T' or `G', then the graphical dialogue is
         entered before the polynomial dispersion curve for any row is
         accepted and applied. If this is `N' or `F' then the dialogue
         is not entered and separate dispersion curves are applied to
         all rows. The string is case-insensitive. [`G']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The spectrum or set of spectra in which emission features are
         to be located. This must be a base NDF, the spectroscopic axis
         must be the first axis. No spectroscopic values or widths must
         exist in the Specdre Extension. The pixel centres along the
         first axis must be NDF pixel coordinates. Update access is
         necessary, the results structure in the Specdre Extension may
         be modified, an array of spectroscopic values will be created
         in the Specdre Extensions.
      }
      \sstsubsection{
         ORDER = \_INTEGER (Read)
      }{
         The polynomial order of dispersion curves. This cannot be changed
         during the graphical dialogue. Neither can it differ between
         rows.  [2]
      }
      \sstsubsection{
         FDB = NDF (Read)
      }{
         The feature data base. Only the simple list of values FTR\_WAVE is
         used and only in graphics dialogue. It serves to find the
         identification for an as yet unidentified -- but located
         feature.
      }
      \sstsubsection{
         DEVICE = GRAPHICS (Read)
      }{
         The graphics device to be used. This is unused if DIALOG is
         false.
      }
      \sstsubsection{
         WRANGE( 2 ) = \_REAL (Read)
      }{
         In graphical dialogue this parameter is used repeatedly to get
         a range of laboratory values. This is used for plotting as well
         as for finding identifications in the feature data base.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
}

\sstroutine{
   ARCGENDB
}{
   Convert list of laboratory values to feature data base
}{
   \sstdescription{
      This routine converts an arc line list -- i.e.\ an ASCII list of
      laboratory wavelengths or frequencies of known features in an arc
      spectrum -- into a feature data base. That can be used for
      automatic identification of features in an observed arc spectrum.

      Since generating the feature data base may take some time, you may
      want to do it once for any line lists you often use, and keep the
      feature data bases. On the other hand, the feature data bases may
      be rather big.

      This routine reads a list of laboratory values (wavelengths or
      frequencies). The list must be an unformatted ASCII file. From the
      beginning of each line one value is read. If this fails, the line
      is ignored. Comment lines can be inserted by prefixing them with
      ``*'', ``!'' or ``{\tt\#}''. The value can be followed
      by any comment, but can
      be preceded only by blanks. The list must be strictly
      monotonically increasing.

      The list should to some degree match an expected observation. Its
      spectral extent should be wider than that of an expected
      observation. But it should not contain a significant number of
      features that are usually not detected. This is because the
      automatic identification algorithm uses relative distances between
      neighbouring features. If most neighbours in the list of
      laboratory values are not detected in the actual arc observation,
      then the algorithm may fail to find a solution or may return the
      wrong solution.

      The given list is converted to a feature data base according to
      Mills (1992). The data base contains information about the
      distances between neighbours of features. The scope of the feature
      data base is the number of neighbours about which information is
      stored. The feature data base is stored in an extension to a dummy
      NDF. The NDF itself has only the obligatory data array. The data
      array is one-dimensional with 1 pixel. All the actual information
      is in an extension with the name ``ECHELLE'' and of type
      ``ECH\_FTRDB''. Its HDS components are:
      \sstitemlist{
         \sstitem
         FTR\_WAVE(NLINES)           {\tt<}\_REAL{\tt>}
         \sstitem
         FTR\_DB(10,10,NLINES)       {\tt<}\_REAL{\tt>}
         \sstitem
         FTR\_LEFT(10,10,NLINES)     {\tt<}\_BYTE{\tt>}
         \sstitem
         FTR\_RIGHT(10,10,NLINES)    {\tt<}\_BYTE{\tt>}
         \sstitem
         WAVE\_INDEX(10,10,NLINES)   {\tt<}\_UWORD{\tt>}
         \sstitem
         QUICK\_INDEX(5000)          {\tt<}\_INTEGER{\tt>}
         \sstitem
         QENTRIES(5000)             {\tt<}\_REAL{\tt>}
      }
      NLINES is the number of features listed in the input file. The
      scope (=10) controls about how many neighbours information is
      stored in the data base. The index size is fixed to 5000, which
      seems sufficient for NLINES = 3500. The size of the FDB is

         (804 * NLINES + 40000) bytes

      plus a small overhead for the HDS structure and the nominal NDF.
      So it is 10 to 100 times bigger than the original ASCII list. The
      point about the FDB is the reduced computing time when
      auto-identifying features in an observed arc spectrum.
   }
   \sstusage{
      arcgendb in fdb
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If true, informational messages will be issued.
      }
      \sstsubsection{
         IN = FILENAME (Read)
      }{
         The name of the input ASCII list of wavelengths or frequencies.
         The list must be strictly monotonically increasing.
      }
      \sstsubsection{
         FDB = NDF (Read)
      }{
         The name of the output file to hold the feature data base.
         This is formally an NDF.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         arcgendb \$FIGARO\_PROG\_S/thar.arc thar\_arc
      }{
         This will convert the Th-Ar list from the Figaro release into a
         ``feature data base'' by the name of ``thar\_arc.sdf''.
      }
   }
   \sstdiytopic{
      References
   }{
      Mills, D., 1992, Automatic ARC wavelength calibration, in P.J.
      Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data Analysis
      Workshop, Garching, 13 -- 14 May 1992, ESO Conference and Workshop
      Proceedings No. 41, Garching bei M\"unchen, 1992
   }
}

\sstroutine{
   ARCIDENT
}{
   Auto-identify located features
}{
   \sstdescription{
      This routine identifies located features in a set of spectra.
      Auto-identification is done from scratch (without prior
      identification of any features) with the algorithm by Mills
      (1992).

      The input data must be a base NDF. They can be a single spectrum
      or a set of spectra. Examples for the latter are a long slit
      spectrum, a set of extracted fibre spectra, or a collapsed
      echellogram (a set of extracted orders from an echelle
      spectrograph). It is necessary that the spectroscopic axis be the
      first axis in the data set. It does not matter how many further
      axes there are, the data will be treated as a linear set of rows
      with each row a spectrum.

      The features for which an identification should be attempted must
      have been located. That is, they must be components of type
      `Gauss', `triangle', `Gauss feature' or `triangle feature' in the
      results structure of the Specdre Extension. Each of these
      components must have at least a `centre' and `peak' parameter. The
      centres (feature locations) must be a strictly monotonically
      increasing list. Their variances must be available. The locations
      (centre parameters) must be in terms of NDF pixel coordinates. The
      peaks must be positive. They are used as a measure of the
      importance of a feature.

      The coverage in spectroscopic values of all spectra (rows) should
      either be similar (long slit or fibres) or roughly adjacent
      (echellogram). There must not yet be any spectroscopic value
      information: There must be no array of spectroscopic values or
      widths in the Specdre Extension. The pixel centre array for the
      spectroscopic axis (i.e.\ the first axis) must be NDF pixel
      coordinates (usually 0.5, 1.5, ...). The data must be arranged
      such that spectroscopic values increase left to right. In the case
      of rows with adjacent coverage spectroscopic values must also
      increase with row number. In a collapsed echellogram this usually
      means that for wavelength calibration the order number must
      decrease with increasing row number. If this is not the case then
      it is still possible to work on a collapsed echellogram: You can
      set ECHELLE false and thus use the full WRANGE for each row, but
      you must adjust DRANGE to be a more reasonable guess of the
      dispersion.

      Identification is done by comparison with a feature data base
      according to Mills (1992). The feature data base should to some
      degree match the observation. Its spectral extent should be wider
      than that of the observation. But it should not contain a
      significant number of features that are not located. This is
      because the automatic identification algorithm uses relative
      distances between neighbouring features. If most neighbours in the
      list of laboratory values are not detected in the actual arc
      observation, then the algorithm may fail to find a solution or may
      return the wrong solution.

      This routine works on each row (spectrum) in turn. It establishes
      information about relative distances between neighbouring located
      features and compares this with a feature data base. This serves
      to identify at least a specified number of features. An
      auto-identification should always be checked in the process of
      fitting a polynomial dispersion curve. All located features are
      retained by this routine, so that further identifications can be
      added or some identifications can be cancelled.

      The result of this routine is a list of feature identifications.
      All located features are retained, though some will have not been
      identified. The locations and identifications (pixel coordinates
      and laboratory values) are stored in the results structure of the
      Specdre Extension of the input data. This replaces the
      pre-existing results extension. The locations are strictly
      monotonically increasing, as are in all probability the
      identifications.

      The new results structure provides for as many component as the
      old one had components of any recognised type. Each component has
      on output the type `arc feature'. It has two parameters `centre'
      and `laboratory value'. Located but unidentified features will
      have bad values as laboratory values. The variances of laboratory
      values are set to zero.

      Mills' (1992) algorithm performs only an initial line
      identification. It is important to verify the returned values by
      fitting a wavelength or frequency scale (e.g.\ polynomial or spline
      fit), and to reject any out-liers. The algorithm should be given
      the positions of all conceivable features in the spectra. It does
      not use the fainter ones unless it is unable to identify using
      only the brightest, but you will get more robust behaviour if you
      always provide all possible candidate lines for potential
      identification. The algorithm should not be fed severely blended
      line positions as the chance of incorrect identifications will be
      significantly higher (this is the exception to the rule above).

      The speed of the algorithm varies approximately linearly with
      wavelength/frequency range and also with dispersion range so the
      better constraints you provide the faster it will run. The
      algorithm takes your constraints as hard limits and it is usually
      more robust to accept a slightly longer runtime by relaxing the
      ranges a little.

      If the algorithm runs and keeps looping increasing its set of
      neighbours, then the most likely causes are as follows:
      \sstitemlist{
         \sstitem
         wavelength/frequency scale does not increase with increasing x
            (set the CHKRVS parameter true and try again).
         \sstitem
         WRANGE or DRANGE are too small (increase them both by
            a factor of 2 and try again).
      }
   }
   \sstusage{
      arcident in out fdb wrange=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [YES]
      }
      \sstsubsection{
         ECHELLE = \_LOGICAL (Read)
      }{
         If false, the given WRANGE is used for each row, assuming the
         rows are similar spectra (long slit or fibre). If true, a
         collapsed echellogram is assumed. In that case each row is an
         extracted order with different wavelength/frequency range. This
         routine will divide the given WRANGE into as many sub-ranges as
         there are rows (orders) in the given input. [NO]
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The spectrum or set of spectra in which located features are to
         be identified. This must be a base NDF, the spectroscopic axis
         must be the first axis. No spectroscopic values or widths must
         exist in the Specdre Extension. The pixel centres along the
         first axis must be NDF pixel coordinates. The input NDF must
         have a results structure in its Specdre Extension, and the
         results must contain a number of line components with strictly
         monotonically increasing position (centre).
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output NDF is a copy of the input, except that the results
         structure holds feature identifications rather than locations
         (`peak' parameters will have been replaced with `laboratory
         value' parameters).
      }
      \sstsubsection{
         FDB = NDF (Read)
      }{
         The feature data base. The actual data base is a set of
         primitive arrays in an extension to this NDF called ECHELLE.
         A feature data base can be generated from a list of wavelengths
         or frequencies with ARCGENDB.
      }
      \sstsubsection{
         WRANGE( 2 ) = \_REAL (Read)
      }{
         The approximate range of wavelengths or frequencies. The
         narrower this range the faster is the identification algorithm.
         But if in doubt give a wider range.
      }
      \sstsubsection{
         DRANGE( 2 ) = \_REAL (Read)
      }{
         The range into which the dispersion in pixels per wavelength or
         per frequency falls. The narrower this range the faster is the
         identification algorithm. But if in doubt give a wider range.
      }
      \sstsubsection{
         STRENGTH = \_REAL (Read)
      }{
         This specifies the maximum ratio between the strength of
         features that are to be used initially for identification. If
         the strongest feature has peak 1000, then the weakest
         feature used initially has peak greater than 1000/STRENGTH.
         [50.0]
      }
      \sstsubsection{
         THRESH = \_REAL (Read)
      }{
         This specifies the maximum difference between the ratios of
         neighbour distances as observed and as found in the feature
         data base. The difference is evaluated as
            ABS(1 {\tt-} ABS(obs/ref)) {\tt<}? THRESH.
         Values much larger than 0.1 are likely to generate a lot of
         coincidence matches; values less than 0.01 may well miss `good'
         matches in less-than-ideal data. You may need to relax this
         parameter if your arc spectra are very distorted (non-linear
         dispersion). [0.03]
      }
      \sstsubsection{
         MAXLOC = \_INTEGER (Read)
      }{
         This specifies the maximum number of features to be used when
         generating ratios for initial identification. In general, a
         good solution can be found using only the strongest 8 to 16
         features. The program slowly increases the number of features
         it uses until an adequate solution if found. However, there may
         be a large numbers of weak features present which are not in
         the reference database. This parameter allows the setting of an
         absolute maximum on the number of features (per row) which
         are to be considered. If less than MAXLOC features are located
         in a given row, then the number of identified features is used
         instead for that row. [30]
      }
      \sstsubsection{
         MINIDS = \_INTEGER (Read)
      }{
         The minimum number of features that must be identified for the
         algorithm to be successful. If fewer than MINIDS features are
         located in a given row, then a smaller number is used instead
         for that row. [9]
      }
      \sstsubsection{
         NEIGHB( 2 ) = \_INTEGER (Read)
      }{
         NEIGHB(1) specifies the starting number of neighbouring
         features (on each side) to examine when generating ratios for
         matching. (These are neighbours in the observed spectra, not in
         the feature data base.) Increasing this will lead to
         exponential increases in CPU time, so it should be used with
         caution when all else fails. The default value is 3. Higher
         values are tried automatically by the program if no solution
         can be found. The number of neighbours considered is increased
         until it reaches the maximum of NEIGHB(2), when the program
         gives up. [3,6]
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.
   }
   \sstdiytopic{
      References
   }{
      Mills, D., 1992, Automatic ARC wavelength calibration, in P.J.
      Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data
      Analysis Workshop, Garching, 13 -- 14 May 1992, ESO Conference and
      Workshop Proceedings No. 41, Garching bei M\"unchen, 1992
   }
}

\sstroutine{
   ARCLOCAT
}{
   Locate line features in a set of spectra
}{
   \sstdescription{
      This routine locates narrow features in a set of spectra. Features
      can be located from scratch automatically. In a different mode,
      feature locations can be added or deleted in a graphical dialogue.
      The feature location and peak are determined by a Gauss or
      triangle line fit.

      The input data must be a base NDF. They can be a single spectrum
      or a set of spectra. Examples for the latter are a long slit
      spectrum, a set of extracted fibre spectra, or a collapsed
      echellogram (a set of extracted orders from an echelle
      spectrograph). It is necessary that the spectroscopic axis be the
      first axis in the data set. It does not matter how many further
      axes there are, the data will be treated as a set of rows with
      each row a spectrum.

      The coverage in spectroscopic values of all spectra (rows) should
      either be similar (long slit or fibres) or roughly adjacent
      (echellogram). There must not yet be any spectroscopic value
      information: There must be no array of spectroscopic values or
      widths in the Specdre Extension. The pixel centre array for the
      spectroscopic axis (i.e.\ the first axis) must be NDF pixel
      coordinates (usually 0.5, 1.5, ...). The data must be arranged
      such that spectroscopic values increase left to right. In the case
      of rows with adjacent coverage spectroscopic values must also
      increase with row number. In a collapsed echellogram this usually
      means that for wavelength calibration the order number must
      decrease with increasing row number.

      In automatic mode this routine works on each row (spectrum) in
      turn. It scans through the spectrum and looks for pixels that
      exceed the local background level by at least the given threshold
      value. When such a pixel is spotted, a single-component line fit
      is tried no the local profile. The local profile is centred on the
      pixel suspected to be part of an emission feature. It includes 1.5
      times the guessed FWHM on either side and a further 5 baseline
      pixels on either side. A local linear baseline is subtracted prior
      to the line fit. In order for the feature to be entered into the
      list of located features, the fit must succeed, the fitted peak
      must exceed the threshold, and the fitted peak must exceed the
      absolute difference of background levels between the left and
      right.

      When run with graphics dialogue this routine works on any choice
      of rows. It uses a pre-existing list of located features to which
      can be added or from which features can be deleted. Graphics
      dialogue can also be used to just check the locations. The graph
      displays the spectrum currently worked on in bin-style. The current
      list of located features is indicated by dashed vertical lines.
      The options in the graphical dialogue are:
      \sstitemlist{
         \sstitem
         R - Choose different row to work on
         \sstitem
         X - X-zoom 2x on cursor
         \sstitem
         Y - Y-zoom 2x on cursor
         \sstitem
         W - Unzoom to show whole row
         \sstitem
         N - Pan left/right by 75\% of current x range
         \sstitem
         A - Add the feature under cursor to list (subject to line fit)
         \sstitem
         S - Add the cursor position as feature to list
         \sstitem
         D - Delete the feature nearest cursor from list
         \sstitem
         Q - Quit, preserving the updated list of located features
         \sstitem
         ? - Help
      }
      The difference between the A and S options is that A tries a line
      fit to the local profile around the cursor, while S accepts the
      cursor x position as exact centre and the cursor y position as
      exact peak of a new feature; (the variance of the centre is set
      to 0.25, the variance of the peak to the bad value).

      The result of this routine is a list of Gauss or triangle
      features. Their locations in NDF pixel coordinates and their peak
      values are stored in the results structure of the Specdre
      Extension of the input data. If run in automatic mode, this
      routine will replace any previously existing results structure. If
      run with graphics dialogue, this routine will try to work with a
      pre-existing list of located features. But if the pre-existing
      results structure does not conform to the required format, then a
      new results structure is created.

      The list of located features (for each row) is always sorted such
      that the locations are strictly monotonically increasing.

      The results structure provides for a certain number of components.
      These have component type `Gauss feature' or `triangle feature'.
      Each component has two parameters `centre' and `peak'. The number
      of components is determined when the results structure is created,
      it is derived from the approximate width of features and the
      number of pixels in each spectrum.
   }
   \sstusage{
      arclocat in fwhm thresh
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If true, messages about the progress of auto-locating features
         are issued. [YES]
      }
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If this is `Y', `T' or `G', then no auto-locating takes place
         and the graphics dialogue is entered. If this is `N' or `F'
         then the dialogue is not entered and auto-locating is done
         instead. The string is case-insensitive.  [`G']
      }
      \sstsubsection{
         MODE = \_CHAR (Read)
      }{
         This can be `Gauss' or `triangle' and chooses the line profile
         to be fitted. This string is case-insensitive and can be
         abbreviated to one character. [`Gauss']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The spectrum or set of spectra in which emission features are
         to be located. This must be a base NDF, the spectroscopic axis
         must be the first axis. No spectroscopic values or widths must
         exist in the Specdre Extension. The pixel centres along the
         first axis must be NDF pixel coordinates. Update access is
         necessary, the results structure in the Specdre Extension will
         be modified, possibly re-created.
      }
      \sstsubsection{
         FWHM = \_REAL (Read)
      }{
         The guessed full width at half maximum of the features to be
         located. This is used to estimate the maximum number of
         features that might be located, to locate baseline ranges next
         to suspected features, and as a guess for the line fit.
      }
      \sstsubsection{
         THRESH = \_REAL (Read)
      }{
         The threshold. While scanning a pixel must exceed this
         threshold to initiate a line fit. The fitted peak also must
         exceed the threshold in order that the feature location be
         accepted. This parameter is significant only for automatic
         location of features.
      }
      \sstsubsection{
         DEVICE = GRAPHICS (Read)
      }{
         The graphics device to be used. This is unused if DIALOG is
         false.
      }
      \sstsubsection{
         ROWNUM = \_INTEGER (Read)
      }{
         In graphics dialogue this parameter is used to switch to a
         different row (spectrum).
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         arclocat in 4.\ 20.\ mode=triangle dialog=f
      }{
         This will scan through (all rows of) the NDF called ``in''. It
         looks out for features of 4 pixels full width at half maximum
         and with a peak value of at least 20 above the local
         background. The features are fitted as triangles. The search is
         automatic. Thus a new results structure in the input NDF's
         Specdre Extension is created with the locations (centres) and
         peaks of located features.
      }
      \sstexamplesubsection{
         arclocat in 4.\ mode=Gauss dialog=g rownum=5
      }{
         This will use the graphic dialogue. Starting with the fifth row
         the user can use the mouse cursor to choose features that are
         to be deleted from or added to the list of located features.
         This can be used to improve on an automatic run, or when no
         features have been located so far. If you try to add a feature
         to the list, a Gauss fit is tried in the vicinity of the
         cursor-selected position.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
}

\sstroutine{
   ASCIN
}{
   Read a 1-D or N-D data set from an ASCII table
}{
   \sstdescription{
      This routine reads axis values, pixel widths, data values, and
      data errors from an ASCII table into an NDF data structure.
      Most of these items are optional, mandatory are only
      axis values for each axis and data values. Pixel widths can be
      read only in the one-dimensional case.

      The user specifies in which columns the different items are to be
      found. A range of line numbers to be used can be specified.
      Comment lines may be interspersed in this line range, if they are
      marked by an exclamation mark in the first or second character.
      All columns leftward of the rightmost used column must be
      numeric, non-numeric data may follow in further columns.
      Up to 132 characters are read from table lines. Numbers are read
      as \_REAL.

      If the result is one-dimensional, the axis values will be taken
      literally to define a grid, which in general may be non-linear and
      non-monotonic. If the result is multi-dimensional, the routine
      will guess from the table a grid that is linear in all directions.
      The parameter system is consulted to confirm or modify the
      suggested grid.

      The data value read from a line will be stored into exactly one
      output pixel, if and only if the table coordinates match that
      pixel's coordinate to within a specified fraction of the pixel
      step. Pixels for which no data are in the table are assigned the
      bad value. Table data equal to a specified ``alternative bad value''
      are replaced by the bad value before insertion into the data set.
      Where more than one table line corresponds to the same pixel, the
      pixel is assigned the last value from the table. That is, later
      specifications of the same pixel override previous ones.
   }
   \sstusage{
      ascin in lines colaxes=?\ coldata=?\ [start=?\ step=?\ end=?]\ out=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. This parameter is of significance only
         if the output is multi-dimensional. [YES]
      }
      \sstsubsection{
         TOL = \_REAL (Read)
      }{
         The tolerated fraction of the pixel size by which the table
         coordinates may deviate from the pixel coordinates. For a line
         read from the ASCII table, if any one of the axis values
         deviates by more than TOL times the pixel step, then the
         information from the table is disregarded. This parameter is of
         no significance, if the output is one-dimensional, since in
         that case the axis values found will define the exact
         (non-linear) grid. [0.2]
      }
      \sstsubsection{
         BAD = \_REAL (Read)
      }{
         The alternative bad value, i.e.\ the bad value used in the
         table. Any data or error value found in the table that is equal
         to BAD, is replaced by the bad value before insertion into the
         output. [-999999.]
      }
      \sstsubsection{
         IN = FILENAME (Read)
      }{
         The file containing the ASCII table.
      }
      \sstsubsection{
         LINES( 2 ) = \_INTEGER (Read)
      }{
         The line numbers of the first and last lines to be used from
         the table file. [1,9999]
      }
      \sstsubsection{
         COLAXES( 7 ) = \_INTEGER (Read)
      }{
         The column numbers where the axis values are to be found. All
         axes must be specified, i.e.\ at least one. The number of
         leading non-zero elements defines the number of axes in the
         output. [1,2]
      }
      \sstsubsection{
         COLWIDTH = \_INTEGER (Read)
      }{
         The column numbers where the pixel width values are to be
         found. This parameter is of significance only if the output is
         one-dimensional. Enter a 0 if no width information is
         available. [0]
      }
      \sstsubsection{
         COLDATA( 2 ) = \_INTEGER (Read)
      }{
         The column numbers where the data values (first element) and
         their associated error values (second element) are to be
         found. If no error information is available, enter 0 as second
         element. [3,0]
      }
      \sstsubsection{
         START( 7 ) = \_REAL (Read)
      }{
         The coordinates of the first pixel. This parameter is of
         no significance, if the output is one-dimensional, since in
         that case the axis values found will define the exact
         (non-linear) grid.
      }
      \sstsubsection{
         STEP( 7 ) = \_REAL (Read)
      }{
         The coordinate increments per pixel. This parameter is of
         no significance, if the output is one-dimensional, since in
         that case the axis values found will define the exact
         (non-linear) grid.
      }
      \sstsubsection{
         END( 7 ) = \_REAL (Read)
      }{
         The coordinates of the last pixel. This parameter is of
         no significance, if the output is one-dimensional, since in
         that case the axis values found will define the exact
         (non-linear) grid.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The NDF where to store the data.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         ascin in [1,9999]\ colaxes=[1,2]\ coldata=[3,4]
         start=[0,0]\ end=[2.5,5]\ step=[0.1,1]\ out=out
      }{
         This will read the data from the ASCII file IN, using line
         numbers 1 to 9999 (or till end of file if there are less lines
         in IN). The 1st axis data are taken from the first column, the
         2nd axis data from the second column. The image data are taken
         from the 3rd column and their errors from the 4th column. The
         routine tries to store the table data into a grid with the 1st
         axis running from 0 to 2.5 in steps of 0.1 (26 pixels) and the
         2nd axis running from 0 to 5 in steps of 1 (6 pixels). If a
         coordinate pair from columns 1\&2 matches any pixel centre well
         enough, the data from columns 4\&5 are entered into the
         corresponding element of the data and errors array. The data
         file is OUT.
      }
      \sstexamplesubsection{
         ascin in out [25,39]\ colaxes=5 coldata=[3,0]
      }{
         Here the output is one-dimensional and without errors array
         (thus the zero in COLDATA). Only lines 25 to 39 from IN are
         used. The axis data are from the 5th column and the spectrum
         data from the 3rd column. (Note that columns 1, 2 and 4 must
         contain numeric data.) The axis grid need not be specified. The
         axis values from the table will be taken literally to form a
         grid that is in general non-linear and non-monotonic.
      }
   }
   \sstimplementationstatus{
      It is not possible to read axis values from the table in double
      precision or create a double precision axis array.
   }
}

\sstroutine{
   ASCOUT
}{
   Write an NDF to an ASCII table
}{
   \sstdescription{
      This routine takes an NDF (section) and writes it to an ASCII
      table. The first part of the output file is a header giving
      textual information and a head for the table. These lines start
      with a blank carriage return control character followed by an
      exclamation mark as the first printed character. The table itself
      has to the left all the axis values and optionally the pixel
      widths, and to the right the data value and its error if known.
      The spectroscopic axis is written with higher precision (12
      significant digits instead of 7) if its storage type is \_DOUBLE.
      The total number of table columns can be 8 at most. All pixel
      widths are written if and only if requested, regardless of whether
      there is explicit information in the input file. Each width
      occupies the column to the right of the corresponding centre
      value.
   }
   \sstusage{
      ascout in out
   }
   \sstparameters{
      \sstsubsection{
         WIDTH = \_LOGICAL (Read)
      }{
         True if pixel widths are to be written, too. [NO]
      }
      \sstsubsection{
         BAD = \_REAL (Read)
      }{
         The alternative bad value. Where the data or variance array has
         bad values, BAD is written to the ASCII table.
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF.
      }
      \sstsubsection{
         OUT = FILENAME (Read)
      }{
         The ASCII output file.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         ascout in(1.5:2.5) out
      }{
         This expects a 1-D data set in IN and will write to the ASCII
         file OUT the information for axis values between 1.5 and 2.5.
         Should IN be more than 1-D, the first hyper-row would be used.
      }
      \sstexamplesubsection{
         ascout in(1.5:2.5,10:15) out
      }{
         This will accept a 2-D data set in IN and write to OUT the
         information for 1st axis coordinate values between 1.5 and 2.5
         and for 2nd axis pixel number between 10 and 15. Note that
         integers in the section specification are interpreted as pixel
         numbers.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.
   }
}

\sstroutine{
   BBODY
}{
   Calculate a black body spectrum
}{
   \sstdescription{
      This routine calculates for a given (vacuum) wavelength or
      frequency axis the intensity of a black body at given temperature.
      The intensity is the energy per unit time, per unit area, per unit
      solid angle, and per unit frequency (and for all polarisations):

      \[B_{\nu}={{2h\nu^3}\over{c^2}}
         {{1}\over{\exp{\left({{h\nu}/{kT}}\right)}-1}}\]

      where {\it c} is the velocity of light, and {\it h} and {\it k} are the
      Planck and Boltzmann constants.
   }
   \sstusage{
      bbody temp in=?\ xstart=?\ xstep=?\ xend=?\ xlabel=?\ xunit=?\ out=?
   }
   \sstparameters{
      \sstsubsection{
         LOGAR = LOGICAL (Read)
      }{
         True if the common logarithm of intensity is to be written
         rather than the intensity itself. [NO]
      }
      \sstsubsection{
         TEMP = REAL (Read)
      }{
         The black body temperature in Kelvin.
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The file holding axis data to be used. Enter the null value (!)
         to read axis data parameters from keyboard.
      }
      \sstsubsection{
         XSTART = REAL (Read)
      }{
         The spectroscopic value (pixel centre) for the first output
         pixel.
      }
      \sstsubsection{
         XSTEP = REAL (Read)
      }{
         The spectroscopic step (pixel distance) for the output pixels.
      }
      \sstsubsection{
         XEND = REAL (Read)
      }{
         The spectroscopic value (pixel centre) for the last output
         pixel.
      }
      \sstsubsection{
         XLABEL = CHARACTER (Read)
      }{
         The label for the spectroscopic axis. Allowed values are
         ``wavelength'' and ``frequency''. [wavelength]
      }
      \sstsubsection{
         XUNIT = CHARACTER (Read)
      }{
         The unit for the spectroscopic axis.
         If the label is ``wavelength'' then the unit can basically be ``m''
         for metre, ``micron'' for micrometre, or ``Angstrom'' for
         Angstr\"om. If the label is ``frequency'' then the unit must be
         basically ``Hz'' for Hertz.
         Any of these units may be preceded by a power of ten, so it
         could be ``10**1*Angstrom'' if you want to use nanometre as unit,
         or ``10**-9*m'' to the same effect. The power must be an
         integer.
         You can achieve a logarithmic axis by specifying something like
         ``log10(10**-3*micron)''. In this example the axis values will be
         the common logarithms of the wavelength in nanometres.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output file.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         bbody 5500 in=in out=out
      }{
         This calculates the black-body spectrum for 5500 K. The
         spectrum is written to file OUT. The routine tries to find all
         necessary information for the 1st (and only) axis in OUT from
         the spectroscopic axis of the file IN. Since LOGAR is left at
         its default value of FALSE, the data are intensity in Jy/sr.
      }
      \sstexamplesubsection{
         bbody 2.7 logar=true in=!\ xstart=0 xstep=0.05 
           xend=6 xlabel=wavelength xunit=log(micron) out=out
      }{
         This calculates the black-body spectrum for 2.7 K. The spectrum
         is written to OUT. No input file is specified. The axis
         contains the logarithms of wavelengths in micron, which run
         from 0 (1 micron) to 6 (1 metre). Since LOGAR=TRUE, the data
         are the logarithms of intensity in Jy/sr.
      }
      \sstexamplesubsection{
         bbody 1e6 logar=true in=!\ xstart=-1 xstep=0.05 
         xend=2 xlabel=frequency xunit=log10(10**15*Hz) out=out
      }{
         This calculates the black-body spectrum for 1 million K. This
         time the axis is logarithms of frequency, the units used are
         10{\tt\^{}}15 Hz. The frequency range covered is from 10{\tt\^{}}14 Hz to
         10{\tt\^{}}17 Hz.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.
   }
   \sstdiytopic{
      References
   }{
      Lang, K.R., 1980, Astrophysical Formulae, Springer, Heidelberg,
      Berlin, New York, p. 21
   }
}

\sstroutine{
   CORREL
}{
   Correlate two or three data sets
}{
   \sstdescription{
      This routine correlates two or three data sets. Either pair is
      subjected to a linear fit and the third data set is subjected to a
      two-parameter linear fit (i.e.\ regarded as a linear function of
      the first and second data sets). Each data set may be an NDF
      section. All must have the same dimensions.
   }
   \sstusage{
      correl inlist out logfil
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, input variances are ignored. [YES]
      }
      \sstsubsection{
         INLIST = LITERAL (Read)
      }{
         The group of input NDFs. Two or three NDFs must be specified.
         A complicated INLIST could look something like

         M\_51(25:35,-23.0,-24.0),M101,{\tt\^{}}LISTFILE.LIS

         This example NDF group specification consists of
         \sstitemlist{
            \sstitem
            one identified NDF from which a subset is to be taken,
            \sstitem
            one identified NDF,
            \sstitem
            an indirection to an ASCII file containing more NDF group
               specifications. That file may have comment lines and in-line
               comments, which are recognised as beginning with a hash ({\tt\#}).
         }
      }
      \sstsubsection{
         OUT = FILENAME (Read)
      }{
         The ASCII output file where the data points are written into a
         table. A new file will be opened. No file will be opened, if
         "!" is entered.
         The table in OUT is without any information else than the
         values from the 1st, 2nd, 3rd data array and errors from the
         1st, 2nd, 3rd variance array in that order. [!]
      }
      \sstsubsection{
         LOGFIL = FILENAME (Read)
      }{
         The ASCII log file where fit results are written to. This will
         be opened for append, if such a file exists.
      }
   }
}

\sstroutine{
   EDITEXT
}{
   Edit the Specdre Extension
}{
   \sstdescription{
      This routine allows the user to modify the Specdre Extension. See
      the topic ``Requests'' for details. Users should also consult the
      description of the Specdre Extension in SUN/140.
   }
   \sstusage{
      editext request in
   }
   \sstparameters{
      \sstsubsection{
         REQUEST = \_CHAR (Read)
      }{
         The action required. This consists of blank separated words.
         The following is a brief reminder of the syntax and permissible
         requests. For the full details refer to the ``Requests'' topic.
         \sstitemlist{
            \sstitem
            LIST
            \sstitem
            CREATE
            \sstitem
            CREATE RESULTS type1 type2 type3 int1 int2
            \sstitem
            DELETE
            \sstitem
            DELETE struct
            \sstitem
            SET ndf-struct
            \sstitem
            SET SPECVALS.comp value
            \sstitem
            SET COORD1.comp value
            \sstitem
            SET COORD2.comp value
            \sstitem
            SET scalar value
            \sstitem
            SET vector element value
            \sstitem
            TYPE scalar type
            \sstitem
            TYPE ndf-struct type
            \sstitem
            TYPE RESULTS type1 type2 type3
            \sstitem
            SHAPE RESULTS int1 int2
         }
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The NDF the Specdre Extension of which is to be modified. The
         modification is done in situ, i.e.\ there is no separate output
         NDF. In most modes, the routine requires update access. Only in
         list mode is read access sufficient.
      }
      \sstsubsection{
         LOGFIL = FILENAME (Read)
      }{
         The filename for the ASCII output file in list mode. If this
         file exists, it is opened for append access. A null value for
         this parameter will signal that no file is to be used. The
         output will then be directed to the standard output device (the
         user's screen).
         [!]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         editext list in accept
      }{
         This will look for the Specdre Extension to the main NDF called IN
         and list the Extension's contents to the default output device
         (usually the user's screen). Some character strings that may be
         up to 32 characters long are truncated to 16 characters in
         order to fit on the screen.
      }
      \sstexamplesubsection{
         editext list in logfil=out
      }{
         This will look for the Specdre Extension to the main NDF called IN
         and list the Extension's contents to the ASCII file out.
         This happens without string truncation.
      }
      \sstexamplesubsection{
         editext delete in
      }{
         This will look for the Specdre Extension to the main NDF called IN
         and delete the Extension.
      }
      \sstexamplesubsection{
         editext "set restframe heliocentric" in
      }{
         This will access the main NDF called IN, find or create its Specdre
         Extension, find or create the RESTFRAME structure in the
         Extension, and put the string ``heliocentric'' into the RESTFRAME
         structure.
      }
      \sstexamplesubsection{
         editext "set frequnit 6"
      }{
         This will access the main NDF called IN, find or create its Specdre
         Extension, find or create the FREQUNIT structure in the
         Extension, and put the value 6 into the FREQUNIT structure.
         This is to mean that reference and laboratory frequencies will
         be expressed in MHz (10{\tt\^{}}6 Hz).
      }
      \sstexamplesubsection{
         editext "set labfreq 5 1420" in
      }{
         This will access the main NDF called IN, find its Specdre Extension
         and find the RESULTS structure in the Extension (which is an
         NDF). If this is successful the routine will find the LABFREQ
         extension of the result NDF and set its fifth element to 1420.
         This is the laboratory frequency of the fifth spectral
         component. In conjunction with a FREQUNIT of 6, this is
         (very roughly) the frequency of the 21 cm ground state
         hyperfine transition of neutral atomic hydrogen.
      }
      \sstexamplesubsection{
         editext "set npara 5 3" in
      }{
         This will access the main NDF called IN, find its Specdre Extension
         and find the RESULTS structure in the Extension (which is an
         NDF). If this is successful the routine will find the NPARA
         extension of the result NDF and set its fifth element to 3.
         This is to mean that the fifth spectral component is allocated
         space for three parameters in the result NDF. Changing this
         number may require to increase the total number of parameters
         which in turn affects the shape of the result NDF and of the
         PARATYPE extension to the result NDF. Changing NPARA(5) also
         makes it necessary to shift information in the result NDF's
         data and variance structures as well as in the PARATYPE
         extension to the result NDF. All this is handled consistently by
         this routine.
      }
      \sstexamplesubsection{
         editext "shape results 6 20" in
      }{
         This will access the main NDF called IN, find or create its Specdre
         Extension, find or create the RESULTS structure in the
         Extension, and shape it to provide for six spectral components
         and a total of 20 parameters. If results existed before, it
         will be expanded or contracted ``at the end''. That is, existing
         components 1 to 6 and parameter 1 to 20 would be retained.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 1.1.

      This routine works in situ and modifies the input file.
   }
   \sstdiytopic{
      Requests
   }{
      The request or action required consists of blank-separated
      words. The first word is a verb specifying the kind of action.
      The verb can be LIST, CREATE, DELETE, SET, TYPE or SHAPE. The
      verb is case-insensitive. The length of the request is
      restricted to 130 characters.

      There may or may not follow a second word specifying the
      structure affected. This can be any of the scalar structures in
      the Specdre Extension, i.e.\ SPECAXIS, RESTFRAME, INDEXREFR,
      FREQREF, FREQUNIT. It can also be any of the NDF-type structures
      in the Specdre Extension, i.e.\ SPECVALS, SPECWIDS, COORD,
      COVRS, RESULTS.
      Finally it can be any structure which is an extension to the
      (NDF-)structure RESULTS. These latter structures are all HDS
      vectors, their names are LINENAME, LABFREQ, COMPTYPE, NPARA,
      MASKL, MASKR, PARATYPE. The structure specification is
      case-insensitive.

      Further words contain parameter values, usually one word per
      parameter. But if the last parameter is a string, it may
      consist of several words. No quotes are necessary.

      There is only one LIST request, namely the sole word LIST. This
      will cause the complete Specdre Extension -- except the contents of
      NDF arrays -- to be listed to the log file or to the screen.

      There are two possible CREATE requests.
      \sstitemlist{
         \sstitem
         ``CREATE'' on its own will create an empty Specdre Extension,
            or fail if a Specdre Extension already exists.
         \sstitem
         ``CREATE RESULTS type1 type2 type3 int1 int2'' needs five
            parameters. Three parameters are case-insensitive HDS data
            types. These are either \_DOUBLE or assumed to be \_REAL. The
            result structure is an NDF-type structure and the different
            type specifications apply to (i) the data and variance
            structures of the NDF, (ii) the laboratory frequency
            extension to the result NDF, (iii) the left and right mask
            extensions to the result NDF. All extensions to the result
            NDF are HDS vectors. Some of these have one element for
            each spectral component, their created length is specified
            by the fourth (last but one) request parameter, i.e.\ the
            sixth word. This word must convert to an integer greater
            than zero. Other HDS vectors in the extension to the result
            NDF have one element for each result parameter, their
            created length is specified by the fifth (last) request
            parameter, i.e.\ the seventh word. This word must convert to
            an integer greater than zero. ``CREATE RESULTS'' fails if the
            result NDF already exists.
      }
      ``DELETE'' on its own will delete the whole Specdre Extension.
      ``DELETE struct'' will delete the specified structure. This can be
      any of the NDF-type structures SPECVALS, SPECWIDS, COORD, COVRS, RESULTS.
      Deleting a structure does not include deleting the whole
      Extension, even if it becomes empty.

      All SET request will create the Specdre Extension, even if the
      request is not recognised as a valid one.

      ``SET ndf-struct'', where the second word specifies an NDF-type
      structure, will set the values of the specified structure to
      bad values. This does not work for COVRS, since it defaults to
      non-existence. The structure is created if it does not already
      exist. For SPECVALS and SPECWIDS only the NDF's data structure
      is affected. For RESULTS the NDF's data and variance structures
      are set to bad values, but all the vectors in the result NDF's
      extension remain unchanged.
      \sstitemlist{
         \sstitem
         ``SET SPECVALS'' will set the values in the data array of
            spectroscopic values to the default values. These are copies
            of the spectroscopic axis centres in the main NDF.
         \sstitem
         ``SET SPECWIDS'' will set the values in the data array of
            spectroscopic widths to the default values. These are copies
            of the spectroscopic axis widths in the main NDF.
         \sstitem
         ``SET COORD'' will set the values in the data array of
            COORD1 and COORD2 to the default values. These are copies
            axis centres for the first and second non-spectroscopic axes
            in the main NDF.
         \sstitem
         ``SET RESULTS'' will set the values in the data and variance
            arrays of the result NDF to bad values.
      }
      ``SET SPECVALS.comp value'' can be used to set the label and unit
      components of the spectroscopic values' NDF.
      \sstitemlist{
         \sstitem
         ``SET SPECVALS.LABEL label'' will set the value of the label
            of the spectroscopic values' NDF.
         \sstitem
         ``SET SPECVALS.UNITS unit'' will set the value of the unit of
            the spectroscopic values' NDF.
         \sstitem
         ``SET COORD1.LABEL label1'' will set the value of the label
            of the COORD1 NDF. Similarly for COORD2.
         \sstitem
         ``SET COORD1.UNITS unit1'' will set the value of the units of
            the COORD1 NDF. Similarly for COORD2.
      }
      ``SET scalar value'' will convert the third word to a value and
      put it in the scalar structure specified by the second word.
      \sstitemlist{
         \sstitem
         ``SET SPECAXIS int'' will try to convert the third word into
            an integer. It must be between 1 and the number of axes in
            the NDF to which this Specdre Extension is an extension. If
            the value is actually changed, then this command will also
            delete the NDF-type structures SPECVALS, COVRS and RESULTS.
            This is because the contents of those structures depends on
            the choice of spectroscopic axis and become invalid when the
            value is changed. This command will also create the Specdre
            Extension and spectroscopic axis structure if they do not
            yet exist.
         \sstitem
         ``SET RESTFRAME more words'' will put the third and following
            words (case-sensitive) into the reference frame structure.
            This command will also create the Specdre Extension and
            reference frame structure if they do not yet exist.
         \sstitem
         ``SET INDEXREFR value'' will try to convert the third word
            into a real or double value, depending on the current type
            of the refractive index structure. This command will also
            create the Specdre Extension and refractive index structure
            if they do not yet exist.
         \sstitem
         ``SET FREQREF value'' will try to convert the third word
            into a real or double value, depending on the current type
            of the reference frequency structure. This command will also
            create the Specdre Extension and reference frequency
            structure if they do not yet exist.
         \sstitem
         ``SET FREQUNIT int'' will try to convert the third word into
            an integer. This command will also create the Specdre
            Extension and frequency unit structure if they do not yet
            exist.
      }
      ``SET vector element value'' will change the value of the
      specified element in the specified vector. The vector must be
      one of the extensions of the result NDF. The result NDF must
      exist beforehand, which implies the existence of the vector.
      The vector must also be long enough to contain the element
      specified and the element number must be integer and greater
      than zero. There are two kinds of vectors, those indexed by
      spectral component and those indexed by result parameter.
      \sstitemlist{
         \sstitem
         ``SET LINENAME comp more words'' will put the forth and
            following words (case-sensitive) into the comp-th element
            of the line name structure.
         \sstitem
         ``SET LABFREQ comp value'' will try to convert the fourth
            word into a real or double value, depending on the current
            type of the laboratory frequency structure. It will then put
            the value into the comp-th element of the laboratory
            frequency structure.
         \sstitem
         ``SET COMPTYPE comp more words'' will put the forth and
            following words (case-sensitive) into the comp-th element
            of the component type structure.
         \sstitem
         ``SET NPARA comp npara'' will try to convert the fourth word
            into an integer greater than or equal to zero. This is the
            new number of parameters allocated to the comp-th component.
            Changing this value will affect several parts of the result
            structure both in their shapes and values. If the comp-th
            spectral component is allocated more parameters than before,
            then it may be necessary to provide for a higher total
            number of parameters, which implies increasing the size of
            .MORE.SPECDRE.RESULTS.DATA\_ARRAY and VARIANCE and of
            .MORE.SPECDRE.RESULTS.MORE.PARATYPE. At any rate, the
            information about spectral components with indices higher
            than comp must be relocated within those arrays.
         \sstitem
         ``SET MASKL comp value'' and ``SET MASKR comp value'' will try
            to convert the fourth word into a real or double value,
            depending on the current type of the mask structures. It
            will then put the value into the comp-th element of the
            relevant mask structure.
         \sstitem
         ``SET PARATYPE para more words'' will put the forth and
            following words (case-sensitive) into the para-th element
            of the parameter type structure.
      }
      A TYPE request can be applied to \_REAL or \_DOUBLE structures,
      and of these to scalars and NDF-type structures. Changing the
      type(s) of the result NDF needs specification of three separate
      types.
      \sstitemlist{
         \sstitem
         ``TYPE scalar type'' can be applied to INDEXREFR and FREQREF.
            The type specification is case-insensitive. If it is not
            \_DOUBLE, then \_REAL is assumed.
         \sstitem
         ``TYPE ndf-struct type'', will change the type of the
            specified NDF. The type specification is case-insensitive. It
            must be \_DOUBLE or is assumed to be \_REAL. This command can
            be applied to SPECVALS, SPECWIDS, COORD and COVRS. SPECVALS,
            SPECWIDS, COORD1 and COORD2 will be created if necessary,
            COVRS will not be created.
         \sstitem
         ``TYPE RESULTS type1 type2 type3'' will change the types of
            (i) the NDF's data and variance, (ii) the NDF's laboratory
            frequency extension, (iii) the NDF's mask extensions. the
            parameters are case-insensitive. They must be \_DOUBLE or are
            assumed to be \_REAL. This command includes creation of the
            result structure if necessary.
      }
      ``SHAPE RESULTS int1 int2'' will change the shape of the result
      structure. The two command parameters must convert to integers
      greater than zero. The first is the number of spectral
      components to be provided for, the second is the total number
      of parameters. If the result structure does not exist, then it
      is created. If it exists, then existing values are retained
      unless they were stored outside the new bounds.
   }
}

\sstroutine{
   EVALFIT
}{
   Evaluate fit results
}{
   \sstdescription{
      This routine turns components in the result structure of the
      Specdre Extension into a fake data set representing those results.
      Such a data set is necessary to perform arithmetic operations
      between the result (otherwise expressed only as a set of
      parameters) and the original data.

      The routine takes as input a base NDF (a section is not
      acceptable). The output is a copy of the input, except for the
      main NDF data and variance components. These are re-calculated from
      certain components in the result structure of the Specdre
      Extension. Thus the output contains the fit results both in the
      result structure and in the main NDF. The main NDF can then be
      compared pixel by pixel with the original data.

      If the input main NDF has a variance component, the output
      variances will be set to zero.

      This routine recognises result components created by FITCHEBY (the
      precursor of FITPOLY),
      FITGAUSS, FITPOLY, or FITTRI. Unrecognised components are ignored,
      i.e.\ not added into the data. A warning to that effect is given.
      If a component in any particular position has bad values as
      parameters, then that component is ignored on that position. No
      warning to this effect is given.

      A component is accepted as 7th order series of Chebyshev
      polynomials if the component type is 'Chebyshev series' and it has
      11 parameters. These are assumed to be order, xmin, xmax, coeff0,
      ... coeff7.

      A component is accepted as 7th order polynomial if the component
      type is 'polynomial' and it has 9 parameters. These are assumed to
      be order, coeff0, ... coeff7.

      A component is accepted as Gauss or triangle if the component type
      is 'Gauss' or 'triangle' and it has 4 parameters. The first three
      are assumed to be centre, peak, FWHM.

      The string comparison to check the component type is
      case-insensitive.
   }
   \sstusage{
      evalfit in out comp=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, this routine will issue only error messages and no
         informational message. [YES]
      }
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If `T', the routine can evaluate several sets of components.
         After a set of components has been evaluated, the user will be
         asked whether she wants to specify another set. [`T']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. This must be a base NDF. If you need only a
         section of an NDF, you use SUBSET first to create the section
         permanently.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output NDF.
      }
      \sstsubsection{
         COMP = \_INTEGER (Read)
      }{
         The numbers of up to 6 components to be added into the output
         data component. If you are not sure which component is which,
         you should inspect the result structure of the data first with
         EDITEXT.
      }
      \sstsubsection{
         REPLY = \_LOGICAL (Read)
      }{
         Set true to work on another set of components. This parameter
         is relevant only if DIALOG is true.
         [NO]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         evalfit in out comp=[2,5,1,2] accept
      }{
         This will take the input NDF IN and create an equally shaped
         NDF called OUT. The specified components stored in IN's (and
         OUT's) Specdre Extension are evaluated and added up to make up
         the main data in OUT. Note that component no. 2 is added twice.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.
   }
}

\sstroutine{
   FILLCUBE
}{
   Copy one NDF into part of another
}{
   \sstdescription{
      This routine copies data, variance etc. from one NDF into another
      existing NDF. By successive calls the output NDF can be filled
      with data from a number of input NDFs. The target area in the
      output is identified by matching axis data (not pixel indices).
      Data are copied from input to output only if the input data value
      is not bad, apart from that existing data in the output are
      overwritten.

      This application is more akin to ASCIN than to GROW. The main
      differences to ASCIN are that FILLCUBE updates an existing output
      and that its input is an NDF rather than an ASCII table.
      Its main advantage over GROW is that input and output may
      (actually must) have the same dimensionality, but any dimensions
      or axis data can differ. Also it is not necessary that target
      pixels form a contiguous subset in the output: The input pixels
      could match, say, every second or third output pixel.
      The disadvantages are that results and spectroscopic values in the
      Specdre Extension are not handled, and that the coordinates along
      each axis in input and output must be linear.

      For each input pixel, FILLCUBE looks for the output pixel that is
      nearest in the space of axis data coordinates. Data are copied
      only if the output pixel is hit close to its centre. However, if
      an axis is degenerate (has only one pixel) in both input and
      output, then the coordinates are assumed to match.

      No indication is given as to how many input pixels did not match
      any output pixel.
   }
   \sstusage{
      fillcube in out
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         True if informational messages are to be issued.
      }
      \sstsubsection{
         TOL = \_REAL (Read)
      }{
         The tolerated fraction of the pixel size by which the input
         coordinates may deviate from the output coordinates. If any one
         of the axis values deviates more than TOL times the coordinate
         step, then the input data are ignored and the output data left
         unchanged. [0.2]
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output NDF. This must already exist, update access is
         required.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7, although
      it is largely ignored.

      This routine works in situ on an existing output file.

      Spectroscopic values must not exist in the Extension of either
      the input or the output NDF: A unique coordinate axis is
      required for all axes, including the spectroscopic one, in
      order to locate the target pixels by matching coordinates
      between input and output. If this is inconvenient, GROW may be
      a more suitable application for your purpose.

      Spectroscopic widths must not exist in the Extension of the
      output NDF and are ignored in the input NDF: This information
      is likely to be present only when spectroscopic values are
      present as well.

      Covariance row sums must not exist in the Extension of the
      output NDF: The validity of this information is difficult to
      assess when only parts of spectra might be copied from one cube
      to another, and when these parts are contiguous in the input
      but might not be in the output. Input covariance row sums are
      ignored.

      The results in the input Extension are ignored, and results
      must not exist in the output Extension.
   }
}

\sstroutine{
   FITBB
}{
   Fit diluted Planck curves to a spectrum
}{
   \sstdescription{
      This routine fits up to six diluted Planck curves to a
      one-dimensional data set. This can be specified as an NDF section.
      The data set must extend along the spectroscopic axis. The fit is
      done on a double logarithmic representation of the data. The axis
      data must be the common logarithm of frequency in Hertz. The data
      themselves must be the common logarithm of intensity or flux
      density in arbitrary units.

      A diluted Plank component is defined as

      \[10^{f_j} = 10^{\Theta_j}
           \left({{\nu}\over{\rm Hz}}\right)^{\alpha_j}
           {{2h\nu^3}\over{c^2}}
           {{1}\over{\exp{\left({{h\nu}/{kT_j}}\right)}-1}}\]

      This assumes that the optical depth is small and the emissivity is
      proportional to the frequency to the power of alpha. 10**Theta is
      the hypothetical optical depth at frequency 1 Hz.

      If the optical depth is large, a single simple Planck function
      should be fitted, i.e.\ only one component with alpha = 0. In this
      case 10**Theta is the conversion factor from the Planck function in
      Jy/sr to the (linear) data values. If for example the data are the
      common logarithm of the calibrated flux density of a source in Jy,
      then Theta is the logarithm of the solid angle (in sr) subtended
      by the source.

      The fit is performed in double logarithmic representation,
      i.e.\ the fitted function is

      \[f = \lg{\left[\sum_j 10^{f_j}\right]}\]

      The choice of Theta, alpha and lg(T) as fit parameters is
      intuitive, but makes the fit routine ill-behaved. Very often alpha
      cannot be fitted at all and must be fixed. Theta and alpha usually
      anti-correlate completely. Even with fixed alpha do Theta and lg(T)
      anti-correlate strongly.

      Furthermore, Theta is difficult to guess. From any initial guess
      of Theta one can improve by using Theta plus the average
      deviation of the data from the guessed spectrum.

      After accessing the data and the (optional) plot device, the data
      will be subjected to a mask that consists of up to six abscissa
      intervals. These may or may not overlap and need not lie within
      the range of existing data. The masking will remove data which are
      bad, have bad variance or have zero variance. The masking will
      also provide weights for the fit. If the given data have no
      variances attached, or if the variances are to be ignored, all
      weights will be equal.

      After the data have been masked, guessed values for the fit are
      required. These are
      \sstitemlist{
         \sstitem
         the number of components to be fitted,
         \sstitem
         the components' guessed scaling constants Theta,
         \sstitem
         emissivity exponents alpha and
         \sstitem
         common logarithms of colour temperatures in Kelvin. Finally,
         \sstitem
         fit flags for each of the parameters are needed.
      }
      The fit flags specify whether any parameter is fixed, fitted, or
      kept at a constant offset to another fitted parameter.

      The masked data and parameter guesses are then fed into the fit
      routine. Single or multiple fits are made. Fit parameters may be
      free, fixed, or tied to the corresponding parameter of another
      component fitted at the same time. They are tied by fixing the
      offset, Up to six components can be fitted simultaneously.

      The fit is done by minimising chi-squared (or rms if variances are
      unavailable or are chosen to be ignored). The covariances between
      fit parameters -- and among these the uncertainties of parameters --
      are estimated from the curvature of psi-squared. psi-squared is
      usually the same as chi-squared. If, however, the given data are
      not independent measurements, a slightly modified function
      psi-squared should be used, because the curvature of chi-squared
      gives an overoptimistic estimate of the fit parameter uncertainty.
      In that function the variances of the given measurements are
      substituted by the sums over each row of the covariance matrix of
      the given data. If the data have been re-sampled with a Specdre
      routine, that routine will have stored the necessary additional
      information in the Specdre Extension, and this routine will
      automatically use that information to assess the fit parameter
      uncertainties. A full account of the psi-squared function is given
      in Meyerdierks, 1992a/b. But note that these covariance row sums
      are ignored if the main variance is ignored or unavailable.

      If the fit is successful, then the result is reported to
      the standard output device and plotted on the graphics device. The
      final plot view port is saved in the AGI data base and can be used
      by further applications.

      The result is stored in the Specdre Extension of the input NDF.
      Optionally, the complete description (input NDF name, mask used,
      result, etc.) is written (appended) to an ASCII log file.

      Optionally, the application can interact with the user. In that
      case, a plot is provided before masking, before guessing and
      before fitting. After masking, guessing and fitting, a screen
      report and a plot are provided and the user can improve the
      parameters. Finally, the result can be accepted or rejected, that
      is, the user can decide whether to store the result in the Specdre
      Extension or not.

      The screen plot consists of two view ports. The lower one shows the
      data values (full-drawn bin-style) overlaid with the guess or fit
      (dashed line-style). The upper box shows the residuals (cross
      marks) and error bars. The axis scales are arranged such that
      all masked data can be displayed. The upper box displays a
      zero-line for reference, which also indicates the mask.

      The Extension provides space to store fit results for each
      non-spectroscopic coordinate. Say, if you have a 2-D image each
      row being a spectrum, then you can store results for each row. The
      whole set of results can be filled successively by fitting one row
      at a time and always using the same component number to store the
      results for that row. (See also the example.)

      The components fitted by this routine are specified as follows:
      The line names and laboratory frequencies are the default values
      and are not checked against any existing information in the
      input's Specdre Extension. The component types are `Planck'. The
      numbers of parameters allocated to each component are 3, the
      three guessed and fitted parameters. The parameter types are in
      order of appearance: `Theta', `alpha', `lg(T)'.
   }
   \sstusage{
      fitbb in device=?\ mask1=?\ mask2=?\ ncomp=?\\
      theta=?\ alpha=?\ lgtemp=?\ sf=?\ af=?\ tf=?\ comp=?\ logfil=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, this routine will issue only error messages and no
         informational message. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, input variances are ignored. [YES]
      }
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If `T', the routine offers in general more options for
         interaction. The mask or guess can be improved after
         inspections of a plot. Also, the routine can resolve
         uncertainties about where to store results by consulting the
         user. [`T']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. This must be a one-dimensional (section of an)
         NDF. You can specify e.g.\ an image column as IN(5,) or part of
         an image row as IN(2.2:3.3,10). Update access is necessary to
         store the fit result in the NDF's Specdre Extension.
      }
      \sstsubsection{
         REPAIR = \_LOGICAL (Read)
      }{
         If DIALOG is true, REPAIR can be set true in order to change
         the spectroscopic number axis in the Specdre Extension. [NO]
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the plot device. Enter the null value (!) to
         disable plotting. [!]
      }
      \sstsubsection{
         MASK1( 6 ) = \_REAL (Read)
      }{
         Lower bounds of mask intervals. The mask is the part(s) of the
         spectrum that is (are) fitted and plotted. The mask is put
         together from up to six intervals:
         \begin{eqnarray*}
            mask&=&[MASK1(1);MASK2(1)]\cup[MASK1(2);MASK2(2)]\\
                &\cup& ... \cup[MASK1(MSKUSE);MASK2(MSKUSE)]
         \end{eqnarray*}
         The elements of the MASK parameters are not checked for
         monotony. Thus intervals may be empty or overlapping. The
         number of intervals to be used is derived from the number of
         lower/upper bounds entered. Either MASK1 or MASK2 should be
         entered with not more numbers than mask intervals required.
      }
      \sstsubsection{
         MASK2( 6 ) = \_REAL (Read)
      }{
         Upper bounds of mask intervals. See MASK1.
      }
      \sstsubsection{
         NCOMP = \_INTEGER (Read)
      }{
         The number of Planck curves to be fitted. Must be between 1
         and 6. [1]
      }
      \sstsubsection{
         THETA( 6 ) = \_REAL (Read)
      }{
         Guess scaling constant for each diluted Planck component.
      }
      \sstsubsection{
         ALPHA( 6 ) = \_REAL (Read)
      }{
         Guess emissivity exponent for each diluted Planck component.
      }
      \sstsubsection{
         LGTEMP( 6 ) = \_REAL (Read)
      }{
         Guess common logarithm of colour temperature in Kelvin for
         each diluted Planck component.
      }
      \sstsubsection{
         SF( 6 ) = \_INTEGER (Read)
      }{
         For each component I, a value SF(I)=0 indicates that
         THETA(I) holds a guess which is free to be fitted.
         A positive value SF(I)=I indicates that THETA(I) is fixed.
         A positive value SF(I)=J{\tt<}I indicates that THETA(I) has to
         keep a fixed offset from THETA(J).
      }
      \sstsubsection{
         AF( 6 ) = \_INTEGER (Read)
      }{
         For each component I, a value AF(I)=0 indicates that
         ALPHA(I) holds a guess which is free to be fitted.
         A positive value AF(I)=I indicates that ALPHA(I) is fixed.
         A positive value AF(I)=J{\tt<}I indicates that ALPHA(I) has to
         keep a fixed offset to ALPHA(J).
      }
      \sstsubsection{
         TF( 6 ) = \_INTEGER (Read)
      }{
         For each component I, a value TF(I)=0 indicates that
         LGTEMP(I) holds a guess which is free to be fitted.
         A positive value TF(I)=I indicates that LGTEMP(I) is fixed.
         A positive value TF(I)=J{\tt<}I indicates that LGTEMP(I) has to
         keep a fixed ratio to LGTEMP(J).
      }
      \sstsubsection{
         REMASK = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the mask.
         [NO]
      }
      \sstsubsection{
         REGUESS = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the guess and
         fit. [NO]
      }
      \sstsubsection{
         FITGOOD = \_LOGICAL (Read)
      }{
         Reply YES to store the result in the Specdre Extension. [YES]
      }
      \sstsubsection{
         COMP = \_INTEGER (Read)
      }{
         The results are stored in the Specdre Extension of the data.
         This parameter specifies which existing components are being
         fitted. You should give NCOMP values, which should all be
         different and which should be between zero and the number of
         components that are currently stored in the Extension. Give a
         zero for a hitherto unknown component. If a COMP element is
         given as zero or if it specifies a component unfit to store the
         results of this routine, then a new component will be created
         in the result storage structure. In any case this routine will
         report which components were actually used and it will deposit
         the updated values in the parameter system. [1,2,3,4,5,6]
      }
      \sstsubsection{
         LOGFIL = FILENAME (Read)
      }{
         The file name of the log file. Enter the null value (!) to
         disable logging. The log file is opened for append. [!]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitbb in device=xw mask1=10.5 mask2=14.5
            ncomp=1 theta=0.5 alpha=0 lgtemp=3.5 sf=0 af=1 tf=0
            comp=1 logfil=planck
      }{
         This fits a Planck curve to the range of frequencies between
         about 30 GHz and 3E14 Hz. The temperature is guessed to be
         3000 K. The fit result is reported to the text file PLANCK and
         stored as component number 1 in the input file's Specdre
         Extension.
         Since DIALOG is not turned off, the user will be prompted for
         improvements of the mask and guess, and will be asked whether
         the final fit result is to be accepted (stored in the Extension
         and written to planck).
         The xwindows graphics device will display the spectrum before
         masking, guessing, and fitting. Independent of the DIALOG
         switch, a plot is produced after fitting.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
   \sstdiytopic{
      References
   }{
      Meyerdierks, H., 1992a, Covariance in resampling and model fitting,
      Starlink, Spectroscopy Special Interest Group

      Meyerdierks, H., 1992b, Fitting resampled spectra, in P.J.
      Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data Analysis
      Workshop, Garching, 13 -- 14 May 1992, ESO Conference and Workshop
      Proceedings No. 41, Garching bei M\"unchen, 1992
   }
}

\sstroutine{
   FITGAUSS
}{
   Fit Gauss profiles to a spectrum
}{
   \sstdescription{
      This routine fits up to six Gauss profiles at a time to a
      one-dimensional data set. This can be specified as an NDF section.
      The data set must extend along the spectroscopic axis.

      After accessing the data and the (optional) plot device, the data
      will be subjected to a mask that consists of up to six abscissa
      intervals. These may or may not overlap and need not lie within
      the range of existing data. The masking will remove data which are
      bad, have bad variance or have zero variance. The masking will
      also provide weights for the fit. If the given data have no
      variances attached, or if the variances are to be ignored, all
      weights will be equal.

      After the data have been masked, guessed values for the fit are
      required. These are
      \sstitemlist{
         \sstitem
         the number of components to be fitted,
         \sstitem
         the value of any underlying constant continuum (this must be an
            a-priori known constant),
         \sstitem
         the components' guessed centre positions,
         \sstitem
         peak heights and
         \sstitem
         full widths at half maxima. Finally,
         \sstitem
         fit flags for each of the Gauss parameters are needed.
      }
      The fit flags specify whether any parameter is fixed, fitted, or
      kept at a constant ratio or offset to another fitted parameter.

      The masked data and parameter guesses are then fed into the fit
      routine. Single or multiple Gauss fits are made to line features.
      Gauss fit parameters may be free, fixed, or tied to the
      corresponding parameter of another Gauss component fitted at the
      same time. Peak and width are tied by fixing the ratios, the
      centre is tied by fixing the offset. Up to six Gauss components
      can be fitted simultaneously.

      The fit is done by minimising chi-squared (or rms if variances are
      unavailable or are chosen to be ignored). The covariances between
      fit parameters -- and among these the uncertainties of parameters --
      are estimated from the curvature of psi-squared. psi-squared is
      usually the same as chi-squared. If, however, the given data are
      not independent measurements, a slightly modified function
      psi-squared should be used, because the curvature of chi-squared
      gives an overoptimistic estimate of the fit parameter uncertainty.
      In that function the variances of the given measurements are
      substituted by the sums over each row of the covariance matrix of
      the given data. If the data have been re-sampled with a Specdre
      routine, that routine will have stored the necessary additional
      information in the Specdre Extension, and this routine will
      automatically use that information to assess the fit parameter
      uncertainties. A full account of the psi-squared function is given
      in Meyerdierks, 1992a/b. But note that these covariance row sums
      are ignored if the main variance is ignored or unavailable.

      If the fit is successful, then the result is reported to
      the standard output device and plotted on the graphics device. The
      final plot view port is saved in the AGI data base and can be used
      by further applications.

      The result is stored in the Specdre Extension of the input NDF.
      Optionally, the complete description (input NDF name, mask used,
      result, etc.) is written (appended) to an ASCII log file.

      Optionally, the application can interact with the user. In that
      case, a plot is provided before masking, before guessing and
      before fitting. After masking, guessing and fitting, a screen
      report and a plot are provided and the user can improve the
      parameters. Finally, the result can be accepted or rejected, that
      is, the user can decide whether to store the result in the Specdre
      Extension or not.

      The screen plot consists of two view ports. The lower one shows the
      data values (full-drawn bin-style) overlaid with the guess or fit
      (dashed line-style). The upper box shows the residuals (cross
      marks) and error bars. The axis scales are arranged such that
      all masked data can be displayed. The upper box displays a
      zero-line for reference, which also indicates the mask.

      The Extension provides space to store fit results for each
      non-spectroscopic coordinate. Say, if you have a 2-D image each
      row being a spectrum, then you can store results for each row. The
      whole set of results can be filled successively by fitting one row
      at a time and always using the same component number to store the
      results for that row. (See also the example.)

      The components fitted by this routine are specified as follows:
      The line names and laboratory frequencies are the default values
      and are not checked against any existing information in the
      input's Specdre Extension. The component types are `Gauss'. The
      numbers of parameters allocated to each component are 4, the
      three guessed and fitted parameters and the line integral. The
      parameter types are in order of appearance: `centre', `peak',
      `FWHM', `integral'.
   }
   \sstusage{
      fitgauss in device=?\ mask1=?\ mask2=?\ ncomp=?\\
      cont=?\ centre=?\ peak=?\ fwhm=?\ cf=?\ pf=?\ wf=?\ comp=?\ logfil=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, this routine will issue only error messages and no
         informational message. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, input variances are ignored. [YES]
      }
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If `T', the routine offers in general more options for
         interaction. The mask or guess can be improved after
         inspections of a plot. Also, the routine can resolve
         uncertainties about where to store results by consulting the
         user. [`T']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. This must be a one-dimensional (section of an)
         NDF. You can specify e.g.\ an image column as IN(5,) or part of
         an image row as IN(2.2:3.3,10). Update access is necessary to
         store the fit result in the NDF's Specdre Extension.
      }
      \sstsubsection{
         REPAIR = \_LOGICAL (Read)
      }{
         If DIALOG is true, REPAIR can be set true in order to change
         the spectroscopic number axis in the Specdre Extension. [NO]
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the plot device. Enter the null value (!) to
         disable plotting. [!]
      }
      \sstsubsection{
         MASK1( 6 ) = \_REAL (Read)
      }{
         Lower bounds of mask intervals. The mask is the part(s) of the
         spectrum that is (are) fitted and plotted. The mask is put
         together from up to six intervals:
         \begin{eqnarray*}
            mask&=&[MASK1(1);MASK2(1)]\cup[MASK1(2);MASK2(2)]\\
                &\cup& ... \cup[MASK1(MSKUSE);MASK2(MSKUSE)]
         \end{eqnarray*}
         The elements of the MASK parameters are not checked for
         monotony. Thus intervals may be empty or overlapping. The
         number of intervals to be used is derived from the number of
         lower/upper bounds entered. Either MASK1 or MASK2 should be
         entered with not more numbers than mask intervals required.
      }
      \sstsubsection{
         MASK2( 6 ) = \_REAL (Read)
      }{
         Upper bounds of mask intervals. See MASK1.
      }
      \sstsubsection{
         NCOMP = \_INTEGER (Read)
      }{
         The number of Gauss profiles to be fitted. Must be between 1
         and 6. [1]
      }
      \sstsubsection{
         CONT = \_REAL (Read)
      }{
         This value indicates the level of the continuum. Any constant
         value for CONT is acceptable. [0]
      }
      \sstsubsection{
         CENTRE( 6 ) = \_REAL (Read)
      }{
         Guess centre position for each Gauss component.
      }
      \sstsubsection{
         PEAK( 6 ) = \_REAL (Read)
      }{
         Guess peak height for each Gauss component.
      }
      \sstsubsection{
         FWHM( 6 ) = \_REAL (Read)
      }{
         Guess full width at half maximum for each Gauss component.
      }
      \sstsubsection{
         CF( 6 ) = \_INTEGER (Read)
      }{
         For each Gauss component I, a value CF(I)=0 indicates that
         CENTRE(I) holds a guess which is free to be fitted.
         A positive value CF(I)=I indicates that CENTRE(I) is fixed.
         A positive value CF(I)=J{\tt<}I indicates that CENTRE(I) has to
         keep a fixed offset from CENTRE(J).
      }
      \sstsubsection{
         PF( 6 ) = \_INTEGER (Read)
      }{
         For each Gauss component I, a value PF(I)=0 indicates that
         PEAK(I) holds a guess which is free to be fitted.
         A positive value PF(I)=I indicates that PEAK(I) is fixed.
         A positive value PF(I)=J{\tt<}I indicates that PEAK(I) has to
         keep a fixed ratio to PEAK(J).
      }
      \sstsubsection{
         WF( 6 ) = \_INTEGER (Read)
      }{
         For each Gauss component I, a value WF(I)=0 indicates that
         FWHM(I) holds a guess which is free to be fitted.
         A positive value WF(I)=I indicates that FWHM(I) is fixed.
         A positive value WF(I)=J{\tt<}I indicates that FWHM(I) has to
         keep a fixed ratio to FWHM(J).
      }
      \sstsubsection{
         REMASK = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the mask.
         [NO]
      }
      \sstsubsection{
         REGUESS = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the guess and
         fit. [NO]
      }
      \sstsubsection{
         FITGOOD = \_LOGICAL (Read)
      }{
         Reply YES to store the result in the Specdre Extension. [YES]
      }
      \sstsubsection{
         COMP = \_INTEGER (Read)
      }{
         The results are stored in the Specdre Extension of the data.
         This parameter specifies which existing components are being
         fitted. You should give NCOMP values, which should all be
         different and which should be between zero and the number of
         components that are currently stored in the Extension. Give a
         zero for a hitherto unknown component. If a COMP element is
         given as zero or if it specifies a component unfit to store the
         results of this routine, then a new component will be created
         in the result storage structure. In any case this routine will
         report which components were actually used and it will deposit
         the updated values in the parameter system. [1,2,3,4,5,6]
      }
      \sstsubsection{
         LOGFIL = FILENAME (Read)
      }{
         The file name of the log file. Enter the null value (!) to
         disable logging. The log file is opened for append. [!]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitgauss in device=xw mask1=-1.5 mask2=2.5
         ncomp=1 cont=1.0 centre=0.5 peak=-0.5 fwhm=1.5 cf=0 pf=0 wf=0
         comp=1 logfil=line
      }{
         This fits a single Gauss profile to the x range [-1.5,2.5]. The
         continuum is assumed to be constant at 1.0. The Gauss is
         guessed to be centred at 0.5 with width 1.5. It is guessed to
         be an absorption line with an amplitude of -0.5.
         All Gauss parameters are free to be fitted. The fit result is
         reported to the text file line and stored as component
         number 1 in the input file's Specdre Extension.
         Since DIALOG is not turned off, the user will be prompted for
         improvements of the mask and guess, and will be asked whether
         the final fit result is to be accepted (stored in the Extension
         and written to line).
         The xwindows graphics device will display the spectrum before
         masking, guessing, and fitting. Independent of the DIALOG
         switch, a plot is produced after fitting.
      }
      \sstexamplesubsection{
         fitgauss in(,5) device=!\ mask1=-1.5 mask2=2.5
         ncomp=1 cont=0.0 centre=0.5 peak=13.0 1.5 cf=0 pf=0 wf=1
         comp=0 logfil=!\ dialog=false
      }{
         This fits a single Gauss profile to the x range [-1.5,2.5] of
         the 5th row in the 2-D image IN. The baseline is assumed to be
         constant at 0.0. The Gauss is guessed to be centred at 0.5 with
         width 1.5. It is guessed to be an emission line with an
         amplitude of 13. Centre position and peak height are free to be
         fitted, but the width is fixed to 1.5. User interaction
         (DIALOG) and plotting (DEVICE) are de-selected. There is also no
         log file where to the results are written. If INFO were also
         switched off, no report whatsoever would be made. However, the
         results are stored as a new component (COMP=0) in the Specdre
         Extension of the input file.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
   \sstdiytopic{
      References
   }{
      Meyerdierks, H., 1992a, Covariance in resampling and model fitting,
      Starlink, Spectroscopy Special Interest Group

      Meyerdierks, H., 1992b, Fitting resampled spectra, in P.J.
      Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data Analysis
      Workshop, Garching, 13 -- 14 May 1992, ESO Conference and Workshop
      Proceedings No. 41, Garching bei M\"unchen, 1992
   }
}

\sstroutine{
   FITPOLY\xlabel{FITCHEBY}
}{
   Fit a polynomial to a spectrum
}{
   \sstdescription{
      This routine fits a polynomial to a one-dimensional data set. This
      can be specified as an NDF section. The data set must extend along
      the spectroscopic axis.

      After accessing the data and the (optional) plot device, the data
      will be subjected to a mask that consists of up to six abscissa
      intervals. These may or may not overlap and need not lie within
      the range of existing data. The masking will remove data which are
      bad, have bad variance or have zero variance. The masking will
      also provide weights for the fit. If the given data have no
      variances attached, or if the variances are to be ignored, all
      weights will be equal.

      The masked data are then fed into the fit routine. The highest
      polynomial order possible is 7. The fit weights data points
      according to their errors. The coefficients reported are those of
      an ordinary polynomial. Let (x,y) be the measurements, y(x) be the
      polynomial of order n fitting the measurements, c\_i (i = 1, ...,
      n+1) be the fitted coefficients. Then y(x) can be calculated as

      \[y(x) = c_1 + c_2 x + c_3 x^2 + ... + c_{n+1} x^n\]

      If the fit is successful, then the result is reported to the
      screen and plotted on the graphics device. The final plot view port
      is saved in the AGI data base and can be used by further
      applications.

      The result is stored in the Specdre Extension of the input NDF.
      Optionally, the complete description (input NDF name, mask used,
      result, etc.) is written (appended) to an ASCII log file.

      Optionally, the application can interact with the user. In that
      case, a plot is provided before masking and before specifying the
      polynomial order. After masking and fitting, a screen report and a
      plot (optional) are provided and the user can improve the
      parameters. Finally, the result can be accepted or rejected, that
      is the user can decide whether to store the result in the Specdre
      Extension or not.

      The screen plot consists of two view ports. The lower one shows the
      data values (full-drawn bin-style) overlaid with the fit (dashed
      line-style). The upper box shows the residuals (cross marks)
      and error bars. The axis scales are arranged such that
      all masked data can be displayed. The upper box displays a
      zero-line for reference, which also indicates the mask.

      The Extension provides space to store fit results for each
      non-spectroscopic coordinate. Say, if you have a 2-D image each
      row being a spectrum, then you can store results for each row. The
      whole set of results can be filled successively by fitting one row
      at a time and always using the same component number to store the
      results for that row. (See also the example.)

      The component fitted by this routine is specified as follows: The
      line name and laboratory frequency are the default values and are
      not checked against any existing information in the input's
      Specdre Extension. The component type is 'polynomial'. The
      number of parameters allocated to the component is 9. The
      parameter types are in order of appearance: 'order', 'coeff0',
      ... 'coeff7'. Unused coefficient are stored as zero.
   }
   \sstusage{
      fitpoly in device=?\ mask1=?\ mask2=?\ order=?\ comp=?\ logfil=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, input variances are ignored. [YES]
      }
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If 'T', the routine offers in general more options for
         interaction. The mask or guess can be improved after
         inspections of a plot. Also, the routine can resolve
         uncertainties about where to store results by consulting the
         user. ['T']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. This must be a one-dimensional (section of an)
         NDF. You can specify e.g.\ an image column as IN(5,) or part of
         an image row as IN(2.2:3.3,10). Update access is necessary to
         store the fit result in the NDF's Specdre Extension.
      }
      \sstsubsection{
         REPAIR = \_LOGICAL (Read)
      }{
         If DIALOG is true, REPAIR can be set true in order to change
         the spectroscopic number axis in the Specdre Extension. [NO]
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the plot device. Enter the null value (!) to
         disable plotting. [!]
      }
      \sstsubsection{
         MASK1( 6 ) = \_REAL (Read)
      }{
         Lower bounds of mask intervals. The mask is the part(s) of the
         spectrum that is (are) fitted and plotted. The mask is put
         together from up to six intervals:
         \begin{eqnarray*}
            mask&=&[MASK1(1);MASK2(1)]\cup[MASK1(2);MASK2(2)]\\
                &\cup& ... \cup[MASK1(MSKUSE);MASK2(MSKUSE)]
         \end{eqnarray*}
         The elements of the MASK parameters are not checked for
         monotony. Thus intervals may be empty or overlapping. The
         number of intervals to be used is derived from the number of
         lower/upper bounds entered. Either MASK1 or MASK2 should be
         entered with not more numbers than mask intervals required.
      }
      \sstsubsection{
         MASK2( 6 ) = \_REAL (Read)
      }{
         Upper bounds of mask intervals. See MASK1.
      }
      \sstsubsection{
         ORDER = \_INTEGER (Read)
      }{
         The polynomial order of the fit. Must be between 0 and 7. [1]
      }
      \sstsubsection{
         REMASK = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the mask.
         [NO]
      }
      \sstsubsection{
         REGUESS = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the guess and
         fit. [NO]
      }
      \sstsubsection{
         FITGOOD = \_LOGICAL (Read)
      }{
         Reply YES to store the result in the Specdre Extension. [YES]
      }
      \sstsubsection{
         COMP = \_INTEGER (Read and Write)
      }{
         The results are stored in the Specdre Extension of the data.
         This parameter specifies which existing component is being
         fitted. It should be between zero and the number of components
         that are currently stored in the Extension. Give zero for a
         hitherto unknown component. If COMP is given as zero or if it
         specifies a component unfit to store the results of this
         routine, then a new component will be created in the result
         storage structure. In any case this routine will report which
         component was actually used and it will deposit the updated
         value in the parameter system. [1]
      }
      \sstsubsection{
         LOGFIL = FILENAME (Read)
      }{
         The file name of the log file. Enter the null value (!) to
         disable logging. The log file is opened for append. [!]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitpoly in device=!\ mask1=2.2 mask2=3.3 order=3 comp=1 logfil=!
      }{
         IN is a 1-D NDF. A 3rd order fit is made to the abscissa range
         between 2.2 and 3.3. The result is stored in component number 1
         of the result structure in the Specdre Extension of IN. The
         plot device and ASCII log file are de-selected.
      }
      \sstexamplesubsection{
         fitpoly in(,15)\ device=xw mask1=[2.0,2.3,3.4]\ mask2=[2.1,3.2,4.0]\ 
            order=2 comp=0 logfil=myfil
      }{
         Here IN is 2-D and the 15th row is selected as the 1-D input
         for the fit. The mask consists of three intervals
         [2.0;2.1] U [2.3;3.2] U [3.4,4.0]. The fit is a parabola. Space
         for a new component is created for storage in the Specdre
         Extension. The plot device is xwindows.
      }
      \sstexamplesubsection{
         fitpoly in(,20)\ device=xw mask1=[2.0,2.3,3.4]\ mask2=[2.1,3.2,4.0]\ 
            order=4 comp=2 logfil=myfil
      }{
         In a follow-up from the previous example, now the 20th row is
         fitted with 4th order. If in the previous run the routine told
         us that it had used component number 2, then COMP=2 is what we
         want to use to store a similar fit for a different row.
         The first time round, the description of component 2 was
         created, saying that it is a Chebyshev series with order of 7
         or less etc. And the fit result for the 15th row was stored in
         an array that has space for all rows in the input file.
         So the second time round, FITPOLY checks whether component 2
         is suitable, whether it is a Chebyshev series with maximum
         order 7. It then stores the new result for the 20th row in the
         place reserved for this row.
         Gradually all rows can be fitted and their results stored in
         the Extension. Possibly this could be automated by writing a
         looping ICL procedure or shell script.
         In the end the corresponding results for all rows are stored in
         one data structure, and could for example be converted into a
         plot of the n-th parameter value versus row number.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
}

\sstroutine{
   FITTRI
}{
   Fit triangular profiles to a spectrum
}{
   \sstdescription{
      This routine fits up to six triangular profiles at a time to a
      one-dimensional data set. This can be specified as an NDF section.
      The data set must extend along the spectroscopic axis.

      After accessing the data and the (optional) plot device, the data
      will be subjected to a mask that consists of up to six abscissa
      intervals. These may or may not overlap and need not lie within
      the range of existing data. The masking will remove data which are
      bad, have bad variance or have zero variance. The masking will
      also provide weights for the fit. If the given data have no
      variances attached, or if the variances are to be ignored, all
      weights will be equal.

      After the data have been masked, guessed values for the fit are
      required. These are
      \sstitemlist{
         \sstitem
         the number of components to be fitted,
         \sstitem
         the value of any underlying constant continuum (this must be an
            a-priori known constant),
         \sstitem
         the components' guessed centre positions,
         \sstitem
         peak heights and
         \sstitem
         full widths at half maxima. Finally,
         \sstitem
         fit flags for each of the triangle parameters are needed.
      }
      The fit flags specify whether any parameter is fixed, fitted, or
      kept at a constant ratio or offset to another fitted parameter.

      The masked data and parameter guesses are then fed into the fit
      routine. Single or multiple triangle fits are made to line
      features. Triangle fit parameters may be free, fixed, or tied to
      the corresponding parameter of another triangle component fitted
      at the same time. Peak and width are tied by fixing the ratios,
      the centre is tied by fixing the offset. Up to six triangle
      components can be fitted simultaneously.

      The fit is done by minimising chi-squared (or rms if variances are
      unavailable or are chosen to be ignored). The covariances between
      fit parameters - and among these the uncertainties of parameters -
      are estimated from the curvature of psi-squared. psi-squared is
      usually the same as chi-squared. If, however, the given data are
      not independent measurements, a slightly modified function
      psi-squared should be used, because the curvature of chi-squared
      gives an overoptimistic estimate of the fit parameter uncertainty.
      In that function the variances of the given measurements are
      substituted by the sums over each row of the covariance matrix of
      the given data. If the data have been re-sampled with a Specdre
      routine, that routine will have stored the necessary additional
      information in the Specdre Extension, and this routine will
      automatically use that information to assess the fit parameter
      uncertainties. A full account of the psi-squared function is given
      in Meyerdierks, 1992a/b. But note that these covariance row sums
      are ignored if the main variance is ignored or unavailable.

      If the fit is successful, then the result is reported to
      the standard output device and plotted on the graphics device. The
      final plot view port is saved in the AGI data base and can be used
      by further applications.

      The result is stored in the Specdre Extension of the input NDF.
      Optionally, the complete description (input NDF name, mask used,
      result, etc.) is written (appended) to an ASCII log file.

      Optionally, the application can interact with the user. In that
      case, a plot is provided before masking, before guessing and
      before fitting. After masking, guessing and fitting, a screen
      report and a plot are provided and the user can improve the
      parameters. Finally, the result can be accepted or rejected, that
      is, the user can decide whether to store the result in the Specdre
      Extension or not.

      The screen plot consists of two view ports. The lower one shows the
      data values (full-drawn bin-style) overlaid with the guess or fit
      (dashed line-style). The upper box shows the residuals (cross
      marks) and error bars. The axis scales are arranged such that
      all masked data can be displayed. The upper box displays a
      zero-line for reference, which also indicates the mask.

      The Extension provides space to store fit results for each
      non-spectroscopic coordinate. Say, if you have a 2-D image each
      row being a spectrum, then you can store results for each row. The
      whole set of results can be filled successively by fitting one row
      at a time and always using the same component number to store the
      results for that row. (See also the example.)

      The components fitted by this routine are specified as follows:
      The line names and laboratory frequencies are the default values
      and are not checked against any existing information in the
      input's Specdre Extension. The component types are `triangle'. The
      numbers of parameters allocated to each component are 4, the
      three guessed and fitted parameters and the line integral. The
      parameter types are in order of appearance: `centre', `peak',
      `FWHM', `integral'.
   }
   \sstusage{
      fittri in device=?\ mask1=?\ mask2=?\ ncomp=?\ cont=?\\
         centre=?\ peak=? fwhm=?\ cf=?\ pf=?\ wf=?\ comp=?\ logfil=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, this routine will issue only error messages and no
         informational message. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, input variances are ignored. [YES]
      }
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         If `T', the routine offers in general more options for
         interaction. The mask or guess can be improved after
         inspections of a plot. Also, the routine can resolve
         uncertainties about where to store results by consulting the
         user. [`T']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. This must be a one-dimensional (section of an)
         NDF. You can specify e.g.\ an image column as IN(5,) or part of
         an image row as IN(2.2:3.3,10). Update access is necessary to
         store the fit result in the NDF's Specdre Extension.
      }
      \sstsubsection{
         REPAIR = \_LOGICAL (Read)
      }{
         If DIALOG is true, REPAIR can be set true in order to change
         the spectroscopic number axis in the Specdre Extension. [NO]
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the plot device. Enter the null value (!) to
         disable plotting. [!]
      }
      \sstsubsection{
         MASK1( 6 ) = \_REAL (Read)
      }{
         Lower bounds of mask intervals. The mask is the part(s) of the
         spectrum that is (are) fitted and plotted. The mask is put
         together from up to six intervals:
         \begin{eqnarray*}
            mask&=&[MASK1(1);MASK2(1)]\cup[MASK1(2);MASK2(2)]\\
                &\cup& ... \cup[MASK1(MSKUSE);MASK2(MSKUSE)]
         \end{eqnarray*}
         The elements of the MASK parameters are not checked for
         monotony. Thus intervals may be empty or overlapping. The
         number of intervals to be used is derived from the number of
         lower/upper bounds entered. Either MASK1 or MASK2 should be
         entered with not more numbers than mask intervals required.
      }
      \sstsubsection{
         MASK2( 6 ) = \_REAL (Read)
      }{
         Upper bounds of mask intervals. See MASK1.
      }
      \sstsubsection{
         NCOMP = \_INTEGER (Read)
      }{
         The number of triangle profiles to be fitted. Must be between 1
         and 6. [1]
      }
      \sstsubsection{
         CONT = \_REAL (Read)
      }{
         This value indicates the level of the continuum. Any constant
         value for CONT is acceptable. [0]
      }
      \sstsubsection{
         CENTRE( 6 ) = \_REAL (Read)
      }{
         Guess centre position for each triangle component.
      }
      \sstsubsection{
         PEAK( 6 ) = \_REAL (Read)
      }{
         Guess peak height for each triangle component.
      }
      \sstsubsection{
         FWHM( 6 ) = \_REAL (Read)
      }{
         Guess full width at half maximum for each triangle component.
      }
      \sstsubsection{
         CF( 6 ) = \_INTEGER (Read)
      }{
         For each triangle component I, a value CF(I)=0 indicates that
         CENTRE(I) holds a guess which is free to be fitted.
         A positive value CF(I)=I indicates that CENTRE(I) is fixed.
         A positive value CF(I)=J{\tt<}I indicates that CENTRE(I) has to
         keep a fixed offset from CENTRE(J).
      }
      \sstsubsection{
         PF( 6 ) = \_INTEGER (Read)
      }{
         For each triangle component I, a value PF(I)=0 indicates that
         PEAK(I) holds a guess which is free to be fitted.
         A positive value PF(I)=I indicates that PEAK(I) is fixed.
         A positive value PF(I)=J{\tt<}I indicates that PEAK(I) has to
         keep a fixed ratio to PEAK(J).
      }
      \sstsubsection{
         WF( 6 ) = \_INTEGER (Read)
      }{
         For each triangle component I, a value WF(I)=0 indicates that
         FWHM(I) holds a guess which is free to be fitted.
         A positive value WF(I)=I indicates that FWHM(I) is fixed.
         A positive value WF(I)=J{\tt<}I indicates that FWHM(I) has to
         keep a fixed ratio to FWHM(J).
      }
      \sstsubsection{
         REMASK = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the mask.
         [NO]
      }
      \sstsubsection{
         REGUESS = \_LOGICAL (Read)
      }{
         Reply YES to have another chance for improving the guess and
         fit. [NO]
      }
      \sstsubsection{
         FITGOOD = \_LOGICAL (Read)
      }{
         Reply YES to store the result in the Specdre Extension. [YES]
      }
      \sstsubsection{
         COMP = \_INTEGER (Read)
      }{
         The results are stored in the Specdre Extension of the data.
         This parameter specifies which existing components are being
         fitted. You should give NCOMP values, which should all be
         different and which should be between zero and the number of
         components that are currently stored in the Extension. Give a
         zero for a hitherto unknown component. If a COMP element is
         given as zero or if it specifies a component unfit to store the
         results of this routine, then a new component will be created
         in the result storage structure. In any case this routine will
         report which components were actually used and it will deposit
         the updated values in the parameter system. [1,2,3,4,5,6]
      }
      \sstsubsection{
         LOGFIL = FILENAME (Read)
      }{
         The file name of the log file. Enter the null value (!) to
         disable logging. The log file is opened for append. [!]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fittri in device=xw mask1=-1.5 mask2=2.5
         ncomp=1 cont=1.0 centre=0.5 peak=-0.5 fwhm=1.5 cf=0 pf=0 wf=0
         comp=1 logfil=line
      }{
         This fits a single triangular profile to the x range
         [-1.5,2.5]. The continuum is assumed to be constant at 1.0. The
         triangle is guessed to be centred at 0.5 with width 1.5. It is
         guessed to be an absorption line with an amplitude of -0.5.
         All triangle parameters are free to be fitted. The fit result
         is reported to the text file LINE and stored as component
         number 1 in the input file's Specdre Extension.
         Since DIALOG is not turned off, the user will be prompted for
         improvements of the mask and guess, and will be asked whether
         the final fit result is to be accepted (stored in the Extension
         and written to line).
         The xwindows graphics device will display the spectrum before
         masking, guessing, and fitting. Independent of the DIALOG
         switch, a plot is produced after fitting.
      }
      \sstexamplesubsection{
         fittri in(,5) device=! mask1=-1.5 mask2=2.5
            ncomp=1 cont=0.0 centre=0.5 peak=13.0 1.5 cf=0 pf=0 wf=1
            comp=0 logfil=!\ dialog=false
      }{
         This fits a single triangular profile to the x range [-1.5,2.5]
         of the 5th row in the 2-D image IN. The baseline is assumed to
         be constant at 0.0. The triangle is guessed to be centred at
         0.5 with width 1.5. It is guessed to be an emission line with
         an amplitude of 13. Centre position and peak height are free to
         be fitted, but the width is fixed to 1.5. User interaction
         (DIALOG) and plotting (DEVICE) are de-selected. There is also no
         log file where to the results are written. If INFO were also
         switched off, no report whatsoever would be made. However, the
         results are stored as a new component (COMP=0) in the Specdre
         Extension of the input file.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
   \sstdiytopic{
      References
   }{
      Meyerdierks, H., 1992a, Covariance in resampling and model fitting,
      Starlink, Spectroscopy Special Interest Group

      Meyerdierks, H., 1992b, Fitting resampled spectra, in P.J.
      Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data Analysis
      Workshop, Garching, 13 -- 14 May 1992, ESO Conference and Workshop
      Proceedings No. 41, Garching bei M\"unchen, 1992
   }
}

\sstroutine{
   GOODVAR
}{
   Replace negative, zero and bad variance values
}{
   \sstdescription{
      This routine checks the variance component of an NDF for values that
      are bad, negative, or zero and replaces them by values specified
      by the user. The specified value can be the null value (``!'') which
      is translated into the bad value.
   }
   \sstusage{
      goodvar in out bad neg zero
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output NDF.
      }
      \sstsubsection{
         BAD = \_REAL (Read)
      }{
         The value which replaces bad values. Enter an exclamation mark
         to keep bad values.
         Bad values in VARIANCE or ERRORS are not allowed by Figaro. If
         DSA has to convert these arrays while mapping them, floating
         overflows or square roots of negative numbers may occur, and
         the application is liable to crash. [!]
      }
      \sstsubsection{
         NEG = \_REAL (Read)
      }{
         The value which replaces negative values. Enter an exclamation
         mark to replace negative values with the bad value. Negative
         errors or variances are nonsense. Negative variances often will
         cause an application to crash because it takes the square root
         to calculate the error. [!]
      }
      \sstsubsection{
         ZERO = \_REAL (Read)
      }{
         The value which replaces zeros. Enter an exclamation mark
         to replace zeros with the bad value.
         Errors of zero sometimes are reasonable or necessary for error
         propagation. In other instances they cause problems, because
         statistical weights often are the reciprocal of the variance.
         [!]
      }
   }
}

\sstroutine{
   GROW
}{
   Copy an N-dimensional cube into part of an (N+M)-dimensional one
}{
   \sstdescription{
      This routine increases the number of axes of a data set by
      duplicating pixels along some axes while retaining other axes.
      A simple and common example is turning a single row into a set of
      identical rows or a set of identical columns. This routine copies
      an N-dimensional cube into (part of) an (N+M)-dimensional one. The
      input cube is in general copied several times into the output, but
      need not fill the output cube. If the output file is new, its size
      has to be given. If it is an existing file, it cannot be reshaped,
      the axes of input and output have to be consistent.
   }
   \sstusage{
      grow in expand stapix endpix size=?\ out=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [YES]
      }
      \sstsubsection{
         NEW = \_LOGICAL (Read)
      }{
         True if a new output file is to be created. [NO]
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         Input NDF.
      }
      \sstsubsection{
         EXPAND( 7 ) = \_INTEGER (Read)
      }{
         For each axis in OUT a 0 indicates that this is an axis with a
         correspondent in IN. A 1 indicates that it is an new (or expanded
         axis without correspondent in IN.
      }
      \sstsubsection{
         STAPIX( 7 ) = \_INTEGER (Read)
      }{
         There is an EXPAND vector parameter that indicates which axes in
         OUT are new or have a corresponding axis in IN. Here, for each
         axis in OUT the value indicates where the multiple copy of input
         should start. Only the values for new axes in OUT are relevant,
         but a value for each axis in OUT must be supplied. The number of
         STAPIX elements given must match the number of axes in OUT.
      }
      \sstsubsection{
         ENDPIX( 7 ) = \_INTEGER (Read)
      }{
         There is an EXPAND vector parameter that indicates which axes in
         OUT are new or have a corresponding axis in IN. Here, for each
         axis in OUT the value indicates where the multiple copy of input
         should end. Only the values for new axes in OUT are relevant,
         but a value for each axis in OUT must be supplied. The number of
         ENDPIX elements given must match the number of axes in OUT.
      }
      \sstsubsection{
         SIZE( 7 ) = \_INTEGER (Read)
      }{
         For each axis in OUT a 0 indicates that the axis is to be taken
         from IN, an integer greater than 1 indicates that the axis is
         a new one and that the SIZE value is to be the length of that
         axis. The number of SIZE elements given must match the number
         of axes in OUT. The number of zeros given must be the number of
         axes in IN.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         Output NDF, containing the expanded data set.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         grow spectrum [0,1]\ [0,1]\ [0,5]\ size=[0,5]\ out=image new=t info=f
      }{
         Grows a spectrum into an image of 5 identical rows. It forces
         the creation of a new output file even if IMAGE exists.
         Informational messages are suppressed.
      }
      \sstexamplesubsection{
         grow spectrum [1,0]\ [2,0]\ [4,0]\ out=image
      }{
         Grows a spectrum into an image of 3 identical columns. Column 1
         and columns beyond 4 in IMAGE remain unchanged. Since NEW is
         not specified, IMAGE must already exist. Its second axis must
         match the first axis of SPECTRUM, and its first axis must be
         at least 4 pixels long.
      }
      \sstexamplesubsection{
         grow spectrum [0,1,1]\ [0,1,1]\ [0,2,4]\ out=cube size=[0,4,8]\ new=t
      }{
         Grow the spectrum into a cube with the spectral axis the 1st
         cube axis.
      }
      \sstexamplesubsection{
         grow spectrum [1,0,1]\ [1,0,1]\ [2,0,4]\ out=cube size=[4,0,8]\ new=t
      }{
         Grow the spectrum into a cube with the spectral axis the 2nd
         cube axis.
      }
      \sstexamplesubsection{
         grow spectrum [1,0,1]\ [1,1,0]\ [2,4,0]\ out=cube size=[4,8,0]\ new=t
      }{
         Grow the spectrum into a cube with the spectral axis the 3rd
         cube axis.
      }
      \sstexamplesubsection{
         grow image [0,0,1]\ [0,0,1]\ [0,0,5]\ out=cube size=[0,0,5]\ new=t
      }{
         Grow an image into a cube, using the image as an xy-plane.
      }
      \sstexamplesubsection{
         grow image [0,1,0]\ [0,1,0]\ [0,5,0]\ out=cube size=[0,5,0]\ new=t
      }{
         Grow an image into a cube, using the image as an xt-plane.
      }
      \sstexamplesubsection{
         grow image [1,0,0]\ [1,0,0]\ [5,0,0]\ out=cube size=[5,0,0]\ new=t
      }{
         Grow an image into a cube, using the image as a yt-plane.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7. This
      routine does not propagate any other extensions even when a new
      output file is created.

      This routine may work in situ on an existing output file.

      When IN is given as a subset of lower actual dimensionality
      than its base NDF, the dimensionality will formally be the same
      as that of the base NDF with interspersed dimensions (axis
      lengths) of 1. If this is inconvenient, use the application
      SUBSET to create the subset in advance and without degenerate
      axes.
   }
}

\sstroutine{
   MOMENTS
}{
   Calculate moments of spectra in a cube
}{
   \sstdescription{
      This routine calculates the moments for each spectrum in a cube. For
      most of the calculated moments each spectrum y(x) is regarded as a
      probability distribution of x-values. That is to say that y(x)-b is
      proportional to the probability of the value x. The results for each
      spectrum are stored in the result structure of the Specdre Extension
      of the input NDF. The component stored is of type `moments'. The
      numbers calculated are:
      \sstitemlist{
         \sstitem
         minimum pos.: The smallest x found.
         \sstitem
         maximum pos.: The largest x found.
         \sstitem
         data minimum: The smallest data minus bias found.
         \sstitem
         data maximum: The largest data minus bias found.
         \sstitem
         sum of data: The sum of all values of data minus bias. This
            value is bad if any addend or its variance is bad.
         \sstitem
         pos. of minimum: The x value where the minimum data value was
            found.
         \sstitem
         pos. of maximum: The x value where the maximum data value was
            found.
         \sstitem
         median: The median is currently not calculated. The stored
            value is the bad value.
         \sstitem
         centroid: The mean x value. Contrary to the sum of data, this
            is calculated using only from data points where data and
            variance are not bad.
         \sstitem
         variance: The variance of the x values. This is calculated in a
            second pass after the centroid is known. An approximate
            rounding error correction is made according to Press et
            al. 1992, p. 607.
         \sstitem
         mean abs. dev.: The mean absolute deviation of the x values
            from the centroid. This is calculated in a second pass after
            the centroid is known.
         \sstitem
         skewness: The skewness gives a measure of the asymmetry of the
            profile of data minus bias versus x. It is positive when the
            profile has a tail towards large x, negative when the profile
            has a wing at small x.
         \sstitem
         kurtosis: The kurtosis gives a measure of the ``peakedness'' of
            the profile. It is zero for a Gaussian profile, positive if the
            profile peak is more pronounced, negative if the profile is
            flatter at the centre.
         \sstitem
         momentum: If x is radial velocity and data minus bias is a
            measure of mass, then this is a measure of the radial momentum
            (inertia). This value is bad if any addend or its variance is
            bad.
         \sstitem
         energy: If x is radial velocity and data minus bias is a
            measure of mass, then this is a measure of the kinetic
            energy. This value is bad if any addend or its variance is bad.
      }
      Note that the higher moments (variance, skewness, kurtosis) are
      rather unreliable unless the spectral features are very strong. For
      further discussion see Press et al. 1992, chapter 14.1.
   }
   \sstusage{
      moments in comp
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, this routine will issue only error messages and no
         informational message. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, input variances are ignored. [YES]
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. Update access is necessary to store the results
         in the Specdre Extension. The NDF can be a section, as in
         myndf(5:9,). The spectroscopic axis must be the first
         non-degenerate axis.
      }
      \sstsubsection{
         COMP = \_INTEGER (Read and Write)
      }{
         The component number to be used to store the results. This
         should be either an existing component of type 'moments' or
         zero. If it is zero, or if the component specified does not
         exist, or if it is not of type 'moments', then a new component
         will be created in the results structure. In any case, if INFO
         is true this routine will report which component number has
         actually been used. [0]
      }
      \sstsubsection{
         BIAS = \_REAL (Read)
      }{
         y(x) is not itself used as the probability of x, but y(x)-bias.
         Thus for a spectrum that was normalised to the continuum level,
         give BIAS=1.0. A bias of zero is suitable for baseline-corrected
         spectra. [0]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         moments in(-25.:+25.,,) 5
      }{
         The NDF is probably three-dimensional. Analysis is restricted
         to pixels between the pixels nearest to x=-25 and +25,
         according to the AXIS(1) information. If there are five or more
         components in the results structure and if the fifth is of type
         'moments' then it will be used to store the results. Otherwise
         a new component will be created for storage.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine works in situ and modifies the input file.
   }
   \sstdiytopic{
      References
   }{
      Press W.H., Teukolsky S.A., Vetterling W.T., Flannery B.P., 1992,
      Numerical recipes, Second edition, Cambridge University Press
   }
}

\sstroutine{
   MOVIE
}{
   Browse through slices of a cube
}{
   \sstdescription{
      This routine takes a three-dimensional NDF and displays its
      two-dimensional slices sequentially on a grey or colour graphics
      device.

      The colour table of the display is unaltered so that a previously
      loaded colour table will be used. Bad values will be displayed in
      the display background colour, which in general is distinct from
      the colour for the lowest (or highest) data value.

      This routine is quite primitive. It does not use axis data or
      spectroscopic values from the Specdre Extension. Pixels and slices
      in the cube are addressed by their NDF pixel indices, which are
      integer numbers, usually starting at 1.

      The routine also does not pay much attention to the precise timing
      of the display. The following list gives activities that the routine
      spends time on and how the user can exert some control over the
      timing.
      \sstitemlist{
         \sstitem
         Before a frame can be displayed it must be extracted from the
            cube. The time taken for this depends greatly on whether the
            frame counting axis is the first or last axis. Taking slices is
            fastest if AXIS=3 and can be very slow if AXIS=1, so it may be
            useful to re-arrange the axes of a cube that will be viewed
            often with the same frame-counting axis. Another way to reduce
            the time for taking slices from the cube is to use as small a
            cube as possible: If it is a-priori known that only a certain
            range of frames will be looked at, or that only a certain part
            of all frames is interesting, then the input cube can be given
            as an appropriate subset of the actual disk file.
         \sstitem
         Also before a frame can be displayed it must be converted
            according to the colour capabilities of the display.
         \sstitem
         Each frame needs to be extracted and converted only once and
            can be viewed several times, converted frames are kept in a
            workspace until the routine exits.
         \sstitem
         In the sequence displays each frame is converted and displayed
            before the routine goes on to the next frame.
         \sstitem
         When a specific frame is requested it is extracted, converted
            and displayed (unless it has been viewed before).
         \sstitem
         When the next or previous frame relative to the displayed one
            is requested, it is extracted and converted if necessary. Then
            it is displayed. In anticipation of another request of the same
            type the next or previous frame is extracted and converted
            immediately.
         \sstitem
         Even if a frame has been converted before, it takes some time
            to re-sample it from cube pixels to display pixels. This time
            can be minimised by choosing the fast mode, where a cube pixel
            is only one display pixel.
         \sstitem
         Disruptions occur in the display of a sequence of frames due to
            the unpredictable need for the machine to page memory.
         \sstitem
         Display may be over a network and bandwidth has to be shared
            with other users. This too causes disruptions of frame
            sequences.
      }
      In summary, it may be best to
      \sstitemlist{
         \sstitem
         put on your spectacles and settle for the fast (and tiny)
            display,
         \sstitem
         decide which part of the cube is interesting and specify
            only that sub-cube as input,
         \sstitem
         begin the forward sequence to convert the whole input sub-cube,
         \sstitem
         have a cup of tea if AXIS=3 and the cube is not small,
         \sstitem
         use the options 'I', 'P', 'N' to look at individual frames in
            your own time.
      }
      It is not possible to write the cube as converted for display.
      Such a cube would be of limited use, since it might contain only
      part of the input cube and since its scaling depends on the colour
      capabilities of the display used.
   }
   \sstusage{
      movie in axis low=?\ high=?
   }
   \sstparameters{
      \sstsubsection{
         DIALOG = \_CHAR (Read)
      }{
         The dialogue mode. If false ('F' or 'N') all frames of the cube
         will be displayed once in forward order. If true ('T' or 'Y')
         the routine will not display anything initially, but repeatedly
         ask for a menu option via the MENU parameter. 'G' for graphic
         is also permitted, but has the same meaning as 'T'. ['F']
      }
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If true, informational messages are given. Such as, which frame
         is currently displayed. [YES]
      }
      \sstsubsection{
         MODE = \_CHAR (Read)
      }{
         'Fast', 'Fill', or 'Square' for (i) a tiny but quick display,
         (ii) to use the whole display area available, (iii) the biggest
         display with square pixels that is possible in the area
         available. The mode can be abbreviated to two characters and is
         case-insensitive. ['Fast']
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. It must be three-dimensional - not counting
         degenerate axes.
      }
      \sstsubsection{
         DEVICE = GRAPHICS (Read)
      }{
         The graphics display device. It must be a screen device, not a
         printer device.
      }
      \sstsubsection{
         AXIS = \_INTEGER (Read)
      }{
         The number of the movie axis. Of the three axes in the input
         cube this is the one not visible in the display. This is the
         axis to count the frames of the movie. [3]
      }
      \sstsubsection{
         LOW = \_REAL (Read)
      }{
         The minimum data value from the cube to be displayed. Values
         less than this are displayed in the same colour.
      }
      \sstsubsection{
         HIGH = \_REAL (Read)
      }{
         The maximum data value from the cube to be displayed. Values
         greater than this are displayed in the same colour.
      }
      \sstsubsection{
         MENU = \_CHAR (Read)
      }{
         The application will ask repeatedly for the menu option, until
         'Q' is chosen. These options are also available from the
         keyboard if dialogue is graphic. ['F']
         \sstitemlist{
            \sstitem
            F: Display each frame in forward order.
            \sstitem
            B: Display each frame in backward order.
            \sstitem
            I: Ask for FRAME parameter and display specified frame.
            \sstitem
            P: Display previous frame.
            \sstitem
            N: Display next frame.
            \sstitem
            Q: Quit.
            \sstitem
            ?: Help.
         }
      }
      \sstsubsection{
         FRAME = \_INTEGER (Read)
      }{
         The frame number to be displayed next. Note that frames are
         counted in NDF pixel indices, i.e. from the NDF's lower bound
         to its upper bound.
      }
   }
}

\sstroutine{
   RESAMPLE
}{
   Re-sample and average several spectra
}{
   \sstdescription{
      Depending on the operation mode this routine either
      \sstitemlist{
         \sstitem
         takes a list of one-dimensional NDFs as input, re-samples them
            to a common linear grid of axis values, and averages them into
            a single one-dimensional NDF, or
         \sstitem
         takes a single N-dimensional NDF as input and re-samples each
            row into a new row of a similar output NDF; re-sampling is
            along the first axis, all further axes are retained.
      }
      The re-sampling creates an interdependence between pixels of the
      output NDF. Only limited information on this interdependence is
      stored in the output and ignored by most applications. Data input
      to this routine is assumed to have no such interdependence.

      The re-sampling algorithm is reintegration (Meyerdierks, 1992a)
      and it is applied to each input NDF separately. Any re-sampled data
      value is a weighted sum of the input values. The weights are the
      normalised overlaps between the output and input pixels. The
      re-sampled spectra are averaged pixel by pixel.

      If input variances are to be ignored it is assumed that the
      variance is a global constant, i.e. equal in all pixels of all
      input NDFs. The re-sampling may still result in different weights
      for different pixels. In the averaging process the global input
      variance is calculated and reported. The output variance will be
      derived on a pixel-by-pixel basis from the data scatter in the
      averaging process.

      In any input NDF, this routine recognises axis centres (pixel
      positions), pixel widths, data values, and data variances. (This
      routine also recognises the Specdre Extension and will use it
      where relevant.) Any other information is propagated from the
      first input NDF.

      Labels and units are checked for consistency, but only a warning
      is given. In interpreting the data all labels and units are
      assumed to be the same as in the first input NDF.

      All input NDFs must have a variance component (unless VARUSE is
      set false). NDFs without variances are ignored. A warning to that
      effect is issued. If VARUSE is set false, input NDFs may or may
      not have variances, such information will be ignored at any rate.

      The output NDF is based primarily on the first input NDF. There
      will be no pixel widths, since the pixel positions are linear and
      the pixels contiguous. The pixel positions, data values, and
      data variances will be affected by the re-sampling process. The
      output Specdre Extension will be based on the first input NDF or
      will be created.

      The vector of row sums of the covariance matrix (Meyerdierks,
      1992a/b) will be created in the Specdre Extension. This is an NDF
      structure with only a data component of the same shape as the main
      data array. If such a structure is found in one of the input
      NDFs, a warning is issued but such information is ignored.
   }
   \sstusage{
      resample mode inlist out start step end
   }
   \sstparameters{
      \sstsubsection{
         MODE = \_CHAR (Read)
      }{
         The operating mode. This can be abbreviated to one character,
         is case-insensitive and must be one of the following:
         \sstitemlist{
            \sstitem
            'SPECTRA': Average several 1-D input NDFs into a single
               1-D output NDF. Re-sample before averaging.
            \sstitem
            'CUBE': Accept only one - but N-D - input NDF. Re-sample each
               row (1-D subset extending along first axis) separately.
         }
            Note that a single spectrum could be handled by both modes; it
            is more effective to treat it as a 1-D cube than as an N=1
            average.
            ['Cube']
      }
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, informational and warning messages are suppressed.
         [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If true, input NDFs without variance information are skipped.
         If false, variance information in the input is ignored.
         [YES]
      }
      \sstsubsection{
         INLIST = LITERAL (Read)
      }{
         The group of input NDFs. In a complicated case this could be
         something like

            M\_51(25:35,-23.0,-24.0),M101,{\tt\^{}}LISTFILE.LIS

         This NDF group specification consists of
         \sstitemlist{
            \sstitem
            one identified NDF from which a subset is to be taken,
            \sstitem
            one identified NDF,
            \sstitem
            an indirection to an ASCII file containing more NDF group
               specifications. That file may have comment lines and in-line
               comments, which are recognised as beginning
               with a hash ({\tt\#}).
         }
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output NDF.
      }
      \sstsubsection{
         START = \_DOUBLE (Read)
      }{
         The first pixel position in the output NDF. The prompt value is
         derived from the first valid input NDF.
      }
      \sstsubsection{
         STEP = \_DOUBLE (Read)
      }{
         The pixel position increment in the output NDF. The prompt
         value is derived as the average increment in the first valid
         input NDF.
      }
      \sstsubsection{
         END = \_DOUBLE (Read)
      }{
         The last pixel position in the output NDF. The prompt value is
         derived from the first valid input NDF.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         resample spectra \^{}inlist out 3.5 0.0254902 10.0
      }{
         The names of input NDFs are read from an ASCII list file called
         INLIST. The result will be stored in OUT which has 256
         pixels covering the coordinates from 3.5 to 10.0
      }
      \sstexamplesubsection{
         resample spectra \^{}inlist out varuse=false accept
      }{
         The names of input NDFs are read from an ASCII list file called
         INLIST. The input NDFs either have no variance, or their
         variance information is to be ignored. The output will be in
         OUT. The start and end pixel positions for OUT are the same as
         in the first input NDF. OUT also has the same number of pixels.
         The pixel spacings are constant in OUT while they may not be in
         the input NDF.
      }
      \sstexamplesubsection{
         resample cube \^{}inlist out 3.5 0.0254902 10.0
      }{
         INLIST contains only one NDF probably with more than one
         dimension. OUT will have the same dimensions except the first,
         which is the re-sampled one.
      }
   }
   \sstnotes{
      The axis normalisation flag is ignored.

      This routine recognises the Specdre Extension v. 0.7.
   }
   \sstdiytopic{
      Pitfalls
   }{
      This routine uses pixel widths. If there is no width array in the
      input NDF, the widths default as described in SUN/33. This may
      have undesired effects on re-sampling spectra that cover several
      non-adjacent coordinate ranges and where the missing ranges are
      not covered by bad pixels. Such spectra have highly non-linear
      pixel positions and the default pixel widths will not be as
      desired. To illustrate this consider the following spectrum with
      four pixels, the intended extents of the pixels and the defaulted
      extents:

{\ssttt
\ \\
.........x.......x.........................x.......x......... \\
\ \\
.....|1111111|2222222|.................|3333333|4444444|..... \\
\ \\ 
.....|1111111|............|333333333333333333333333333333333| \\ 
|222222222222222222222222222222222|............|4444444|.....
\ \\
}

      Since this routine uses the overlap between input and output
      pixels as weights for re-sampling, non-bad pixels next to such a
      gap in data will affect too many output pixels with too much
      weight.
      Users should be aware that spectra as illustrated here are
      somewhat pathologic and that they should be given an explicit
      width array.

      The routine accesses one input NDF at a time and needs not hold
      all input NDFs at the same time. However, The routine needs
      temporary workspace. If KMAX is the number of pixels in an input
      NDF and LMAX the number of output pixels, the routine needs
      \sstitemlist{
         \sstitem
         one vector of length LMAX,
         \sstitem
         one matrix of size KMAX by LMAX,
         \sstitem
         two matrices of size LMAX by LMAX.
      }
      These workspaces are usually of type \_REAL. All (!) are of type
      \_DOUBLE if the first valid input NDF has type \_DOUBLE for either
      of the following:
      \sstitemlist{
         \sstitem
         pixel position,
         \sstitem
         pixel width,
         \sstitem
         data value,
         \sstitem
         data variance.
      }
      In addition one integer vector of length LMAX is needed.

      There is an oddity about this routine if only one input NDF is
      used and its variance array is used and some or all variance
      values are zero. In this case the output will formally still be an
      average of input NDFs using 1/variance as weights. Data with zero
      variance cannot be weighted and are regarded as bad. If this is a
      problem, users can set VARUSE false to ignore all the input
      variances. (Note that zero variances always cause that pixel to be
      ignored by this routine. But where it really calculates an average
      of two or more spectra, this is considered proper procedure.)
   }
   \sstdiytopic{
      Timing
   }{
      The time used by this routine is about proportional to the number
      of input NDFs. It is proportional to the square of the number of
      output pixels. Timing can be optimized, if the input NDFs cover
      about the same coordinate range as the output NDF rather than
      include a lot of data irrelevant for the output.
   }
   \sstdiytopic{
      References
   }{
      Meyerdierks, H., 1992a, Covariance in resampling and model fitting,
      Starlink, Spectroscopy Special Interest Group

      Meyerdierks, H., 1992b, Fitting resampled spectra, in P.J.
      Grosb\o l, R.C.E. de Ruijsscher (eds), 4th ESO/ST-ECF Data Analysis
      Workshop, Garching, 13 -- 14 May 1992, ESO Conference and Workshop
      Proceedings No. 41, Garching bei M\"unchen, 1992
   }
}

\sstroutine{
   SPECCONT
}{
   Contour a two-dimensional cut
}{
   \sstdescription{
      This routine displays a two-dimensional cut as a contour plot. The
      cut can be an ordinary image, a long-slit spectrum, or any cut
      through a spectroscopic data cube. The plot area is the current
      (AGI) picture of the graphics device.

      The plot can be an overlay over the most recent data picture
      inside the current picture, or a new plot inside the current
      picture. (The current picture after SPECCONT is the same as
      before.)

      The screen contents of the current picture can be erased or not.
      If the plot is not an overlay, then the space to be left for axis
      labels as well as the exact labelling can be specified.

      The labelling consists of axes, axis ticks, numeric labels at the
      major ticks, and text labels. The axes are counted from bottom
      clockwise. Each axis can be drawn or not. Each drawn axis can have
      ticks or not. Each axis can have numeric labels or not. The left
      and right axes can have either horizontal (orthogonal) or vertical
      (parallel) numeric labels. Each axis can have a text label or not.

      The kind of labelling is controlled by several 4-character
      strings. Each character is the switch for axis 1, 2, 3, 4
      respectively.  ``0'' turns an option off for that axis, ``+'' turns it
      on. For the ticks and for numeric labels of axes 2 and 4, ``-'' is
      also allowed. It yields inward ticks and vertical numeric labels.

      The attributes of the plot can be selected. These are
      \sstitemlist{
         \sstitem
         colour
         \sstitem
         line thickness
         \sstitem
         character height
         \sstitem
         simple or roman font
         \sstitem
         dash pattern
      }
      Most parameters default to the last used value.
   }
   \sstusage{
      speccont in overlay=?\ world=?\ start=?\ step=?\ end=?
   }
   \sstparameters{
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         If true the plot area will be cleared before plotting. [FALSE]
      }
      \sstsubsection{
         OVERLAY = \_LOGICAL (Read)
      }{
         If true then the last (AGI) data picture inside the current
         (AGI) picture is used to define the plot area and its world
         coordinates. Only that area will be cleared if CLEAR is true.
         No new labelling of the plot will occur. [FALSE]
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. It must be two-dimensional - not counting
         degenerate axes.
      }
      \sstsubsection{
         DEVICE = GRAPHICS (Read)
      }{
         The graphics display device.
      }
      \sstsubsection{
         FILL = \_LOGICAL (Read)
      }{
         If false then the plot window will be adjusted to give the same
         plot scale horizontally and vertically. If true, scaling is
         independent in each direction and the plot will fill the area
         available. This parameter is used only if OVERLAY is false.
         [FALSE]
      }
      \sstsubsection{
         ROMAN = \_LOGICAL (Read)
      }{
         If true, PGPLOT's roman font is used for drawing text.  If
         false, the normal (single-stroke) font is used. [FALSE]
      }
      \sstsubsection{
         HEIGHT = \_REAL (Read)
      }{
         The height of the characters measured in units of PGPLOT
         default text height, which is approximately 1/40 of the height
         of the (AGI) base picture (i.e. 1/40 the height of the
         work station window, screen or paper). HEIGHT will be used for
         labelling the plot box. The contour labels are always half that
         size. [1.]
      }
      \sstsubsection{
         COLOUR = \_INTEGER (Read)
      }{
         The PGPLOT colour index to be used for the plot. This can be
         formally between 0 and 255, but not all devices support all
         colours. The default colour representation is:
         \sstitemlist{
            \sstitem
            0: Background,
            \sstitem
            1: Foreground (default),
            \sstitem
            2: Red,
            \sstitem
            3: Green,
            \sstitem
            4: Blue,
            \sstitem
            5: Cyan,
            \sstitem
            6: Magenta,
            \sstitem
            7: Yellow,
            \sstitem
            8: Orange,
            \sstitem
            9: Green/Yellow,
            \sstitem
            10: Green/Cyan,
            \sstitem
            11: Blue/Cyan,
            \sstitem
            12: Blue/Magenta,
            \sstitem
            13: Red/Magenta,
            \sstitem
            14: Dark grey,
            \sstitem
            15: Light grey.
         }
      }
      \sstsubsection{
         THICK = \_INTEGER (Read)
      }{
         The PGPLOT line thickness. Can be between 1 and 21. [1]
      }
      \sstsubsection{
         DASH = \_INTEGER (Read)
      }{
         The PGPLOT dash pattern:
         \sstitemlist{
            \sstitem
            1: Full line,
            \sstitem
            2: Long dash,
            \sstitem
            3: Dash-dot-dash-dot,
            \sstitem
            4: Dotted,
            \sstitem
            5: Dash-dot-dot-dot.
         }
      }
      \sstsubsection{
         AXES = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of either of
         the four box sides. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be
         ``0'' or ``+''. E.g. `00++' would switch off the bottom and left
         axes and switch on the top and right axes. ['++++']
      }
      \sstsubsection{
         TICK = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of ticks along
         either axis. Ticks are drawn only if the corresponding axis is
         also drawn. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be
         ``0'', ``+'' or ``-''. E.g.\ `00+-' would switch off the bottom and
         left ticks and switch on the top and right ticks. The top axis
         would have ticks outward, the right axis would have ticks
         inward.
         Note that with the current plot software, ticks are drawn only
         if the axis is drawn as well. ['{\tt----}']
      }
      \sstsubsection{
         NUML = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of numeric
         labels along either axis. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be
         ``0'' or ``+''; the second and fourth switch can also be ``-''.
         E.g.\ `0+0-' would switch off the bottom and top labels and switch
         on the left and right labels. The left axis would have labels
         horizontal (orthogonal), the right axis would have labels
         vertical (parallel). ['++00']
      }
      \sstsubsection{
         TEXT = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of text labels
         along either axis. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be
         ``0'' or ``+''. E.g.\ `0++0' would switch off the bottom and right
         labels and switch on the left and top labels. ['++++']
      }
      \sstsubsection{
         MAJOR = \_REAL (Read)
      }{
         The distance in world coordinates between major tick marks. The
         first element is for the horizontal direction, the second for
         the vertical direction. This is also the distance along the
         axis between numeric labels. Values of 0 cause PGPLOT to choose
         the major tick interval automatically. [0.,0.]
      }
      \sstsubsection{
         MINOR = \_INTEGER (Read)
      }{
         The number of minor tick intervals per major tick interval. The
         first element is for the horizontal direction, the second for
         the vertical direction. Values of 0 for MINOR or MAJOR cause
         PGPLOT to choose the minor tick interval automatically. [0,0]
      }
      \sstsubsection{
         BOTTOM = \_CHAR (Read)
      }{
         The text label for the bottom axis. Give null to construct the
         label from the input NDF axis label and unit. Within the string,
         you can use the following escape sequences:
         \sstitemlist{
            \sstitem
            $\backslash$fn Normal (single stroke) font,
            \sstitem
            $\backslash$fr Roman font,
            \sstitem
            $\backslash$fi Italic font,
            \sstitem
            $\backslash$fs Script font,
            \sstitem
            $\backslash$u  Superscript (use only paired with $\backslash$d),
            \sstitem
            $\backslash$d  Subscript (use only paired with $\backslash$u),
            \sstitem
            $\backslash$b  Backspace,
            \sstitem
            $\backslash\backslash$  Backslash,
            \sstitem
            $\backslash$A  Danish umlaut (\AA ngstr\"om),
            \sstitem
            $\backslash$g  Any greek letter.
         }
      }
      \sstsubsection{
         LEFT = \_CHAR (Read)
      }{
         The text label for the left axis. See also BOTTOM.
      }
      \sstsubsection{
         TOP = \_CHAR (Read)
      }{
         The text label for the top axis. See also BOTTOM.
      }
      \sstsubsection{
         RIGHT = \_CHAR (Read)
      }{
         The text label for the right axis. See also BOTTOM.
      }
      \sstsubsection{
         LABSPC( 4 ) = \_REAL (Read)
      }{
         If the plot is not an overlay then this specifies the space left
         at the bottom, left, top, and right between the plot window and
         the current (AGI) picture. The space is measured as a fraction
         of the current picture. Ticks and numeric labels are drawn
         outward from the plot window, but text labels are drawn inward
         from the current picture. The labelling space must be between
         zero and 0.5. [0.1,0.1,0.05,0.05]
      }
      \sstsubsection{
         WORLD( 4 ) = \_REAL (Read)
      }{
         The world coordinates of the plot window. Give null to use the
         extent of the input NDF instead. The four elements are the
         bounds on the left, right, bottom, and top in that order. Left
         and right bound must not be equal, neither must bottom and top.
         [!]
      }
      \sstsubsection{
         START = \_REAL (Read)
      }{
         The first contour level.
      }
      \sstsubsection{
         STEP = \_REAL (Read)
      }{
         The step between successive major contour levels. If zero is given
         then only one contour at value START will be drawn. STEP can be
         negative. In addition to the major contours, three minor contours
         will be drawn between successive major contours. The major contours
         are thick and labelled with the contour value. The minor contours
         are thin and labelled with an arrow pointing counter-clockwise
         around a local maximum.
      }
      \sstsubsection{
         END = \_REAL (Read)
      }{
         The last contour level. This may be adjusted slightly so as to
         comply with START and STEP. If given equal to START then only
         one contour at value START will be drawn. END can be smaller
         than START.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         speccont cube(,5,) start=2 step=2 end=10 accept
      }{
         This takes the fifth xz-cut from the input cube. It draws thick
         labelled contours at values 2, 4, 6, 8, and 10. Each of these
         is accompanied by three minor contours at 2.5, 3.0, 3.5; 4.5,
         5.0, 5.5; ... 10.5, 11.0, 11.5.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine recognises and uses coordinate transformations in
      AGI pictures.
   }
}

\sstroutine{
   SPECGRID
}{
   Plot spectra on position grid
}{
   \sstdescription{
      This routine takes an NDF and displays all the spectra (rows) in
      it. Each spectrum occupies a cell in the plot which is positioned
      according to its coordinates. The coordinates for each spectrum
      are normally stored in the Specdre Extension, but for an NDF of at
      least three dimensions the first two non-spectroscopic axes can be
      used instead. The plot area is the current (AGI) picture of the
      graphics device.

      The plot can be an overlay in coordinate space to a previous plot,
      or a new plot. A plot can be combined from spectra in different
      NDFs, or it can overlay spectra on an image of the same or a
      similar region in coordinate space.

      The previous contents of the plot area can be erased or not.
      If the plot is not an overlay, then the space to be left for
      axis labels as well as the exact labelling can be specified.

      The labelling (in coordinate space) consists of axes, axis ticks,
      numeric labels at the major ticks, and text labels. The axes are
      counted from bottom clockwise. Each axis can be drawn or not. Each
      drawn axis can have ticks or not. Each axis can have numeric
      labels or not. The left and right axes can have either horizontal
      (orthogonal) or vertical (parallel) numeric labels. Each axis can
      have a text label or not.

      The kind of labelling is controlled by several 4-character
      strings. Each character is the switch for axis 1, 2, 3, 4
      respectively. ``0'' turns an option off for that axis, ``+'' turns
      it on. For the ticks and for numeric labels of axes 2 and 4, ``-''
      is also allowed. It yields inward ticks and vertical numeric
      labels.

      Labelling in spectroscopic space (e.g.\ wavelength-intensity space)
      is optional. It is in the form of an empty spectrum cell at a
      specified position in coordinate space. This legend cell is
      labelled with the ranges in spectroscopic space that is covered by
      each cell. It also has text labels to indicate what quantities are
      displayed.

      The attributes of the plot can be selected. These are
      \sstitemlist{
         \sstitem
         colour
         \sstitem
         line thickness
         \sstitem
         character height
         \sstitem
         simple or roman font
         \sstitem
         dash pattern
      }
      Most parameters default to the last used value.
   }
   \sstusage{
      specgrid in overlay=?\ bottom=?\ left=?\ top=?\ right=?\\
         labspc=?\ cworld=?\ legend=?\ sworld=?\ xlegend=?\ ylegend=?
   }
   \sstparameters{
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         If true the plot area will be cleared before plotting. When
         plotting to a printer file, set this false. Otherwise the
         output may be preceded by an empty page. [FALSE]
      }
      \sstsubsection{
         OVERLAY = \_LOGICAL (Read)
      }{
         If true then the last (AGI) data picture inside the current
         (AGI) picture is used to define the plot area and its world
         coordinates. Only that area will be cleared if CLEAR is
         true. No new labelling of the plot will occur. [FALSE]
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF. If it does not have explicit components COORD1
         and COORD2 in its Specdre Extension, then the NDF must have at
         least three dimensions (one spectroscopic and two positional
         axes). In any case, the spectroscopic axis must be the first
         non-degenerate axis.
      }
      \sstsubsection{
         DEVICE = GRAPHICS (Read)
      }{
         The graphics display device.
      }
      \sstsubsection{
         LIN = \_LOGICAL (Read)
      }{
         If true, the spectral data points will be connected by a
         line-style polygon. [TRUE]
      }
      \sstsubsection{
         BIN = \_LOGICAL (Read)
      }{
         If true, the spectral data points will be connected by a
         bin-style (or histogram-style) polygon. [FALSE]
      }
      \sstsubsection{
         MARK = \_INTEGER (Read)
      }{
         This parameter determines the kind of marker to be drawn at
         each spectral data point [0]:
         \sstitemlist{
            \sstitem
            0: No markers drawn,
            \sstitem
            1: Diagonal cross,
            \sstitem
            2: Asterisk,
            \sstitem
            3: Open circle,
            \sstitem
            4: Open square,
            \sstitem
            5: Filled circle,
            \sstitem
            6: Filled square.
         }
      }
      \sstsubsection{
         ERROR = \_LOGICAL (Read)
      }{
         If true and variance information available, error bars will be
         drawn for each spectral data point. [FALSE]
      }
      \sstsubsection{
         WIDTH = \_LOGICAL (Read)
      }{
         If true, the pixel width will be indicated by horizontal bars
         for each spectral data point. [FALSE]
      }
      \sstsubsection{
         FRAME = \_LOGICAL (Read)
      }{
         If true, each spectral cell gets a plain box drawn around
         it. [T]
      }
      \sstsubsection{
         FILL = \_LOGICAL (Read)
      }{
         If false then the plot window will be adjusted to give the same
         plot scale horizontally and vertically. If true, scaling is
         independent in each direction and the plot will fill the area
         available. This parameter is used only if OVERLAY is false. [F]
      }
      \sstsubsection{
         ROMAN = \_LOGICAL (Read)
      }{
         If true, PGPLOT's roman font is used for drawing text. If
         false, the normal (single-stroke) font is used. [FALSE]
      }
      \sstsubsection{
         HEIGHT = \_REAL (Read)
      }{
         The height of the characters measured in units of PGPLOT
         default text height, which is approximately 1/40 of the height
         of the (AGI) base picture (i.e. 1/40 the height of the
         work station window, screen or paper). HEIGHT will be used for
         labelling the plot box (coordinate space). The legend cell
         labels are always half that size. [1.]
      }
      \sstsubsection{
         COLOUR = \_INTEGER (Read)
      }{
         The PGPLOT colour index to be used for the plot. This can be
         formally between 0 and 255, but not all devices support all
         colours. The default colour representation is:
         \sstitemlist{
            \sstitem
            0: Background,           
            \sstitem
            1: Foreground (default),
            \sstitem
            2: Red,                  
            \sstitem
            3: Green,
            \sstitem
            4: Blue,                 
            \sstitem
            5: Cyan,
            \sstitem
            6: Magenta,              
            \sstitem
            7: Yellow,
            \sstitem
            8: Orange,               
            \sstitem
            9: Green/Yellow,
            \sstitem
            10: Green/Cyan,           
            \sstitem
            11: Blue/Cyan,
            \sstitem
            12: Blue/Magenta,         
            \sstitem
            13: Red/Magenta,
            \sstitem
            14: Dark grey,            
            \sstitem
            15: Light grey.
         }
      }
      \sstsubsection{
         THICK = \_INTEGER (Read)
      }{
         The PGPLOT line thickness. Can be between 1 and 21. [1]
      }
      \sstsubsection{
         DASH = \_INTEGER (Read)
      }{
         The PGPLOT dash pattern [1]:
         \sstitemlist{
            \sstitem
            1: Full line,
            \sstitem
            2: Long dash,
            \sstitem
            3: Dash-dot-dash-dot,
            \sstitem
            4: Dotted,
            \sstitem
            5: Dash-dot-dot-dot.
         }
      }
      \sstsubsection{
         AXES = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of either of
         the four box sides. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be ``0'' or
         ``+''. E.g. `00++' would switch off the bottom and left axes and
         switch on the top and right axes. [`++++']
      }
      \sstsubsection{
         TICK = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of ticks along
         either axis. Ticks are drawn only if the corresponding axis is
         also drawn. The sides are counted from bottom clockwise:
         bottom, left, top, right. Any switch can be ``0'', ``+'' or
         ``-''. E.g. `00+-' would switch off the bottom and left ticks and
         switch on the top and right ticks. The top axis would have
         ticks outward, the right axis would have ticks inward.
      }
      \sstsubsection{
         NUML = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of numeric
         labels along either axis. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be ``0'' or
         ``+''; the second and fourth switch can also be ``-''. E.g. `0+0-'
         would switch off the bottom and top labels and switch on the
         left and right labels. The left axis would have labels
         horizontal (orthogonal), the right axis would have labels
         vertical (parallel). [`++00']
      }
      \sstsubsection{
         TEXT = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of text labels
         along either axis. The sides are counted from bottom clockwise:
         bottom, left, top, right. Any switch can be ``0'' or ``+''.
         E.g. `0++0' would switch off the bottom and right labels and
         switch on the left and top labels. [`++++']
      }
      \sstsubsection{
         MAJOR = \_REAL (Read)
      }{
         The distance in world coordinates between major tick marks. The
          first element is for the horizontal direction, the second for
          the vertical direction. This is also the distance along the
          axis between numeric labels. Values of 0 cause PGPLOT to choose
          the major tick interval automatically. [0.,0.]
      }
      \sstsubsection{
         MINOR = \_INTEGER (Read)
      }{
         The number of minor tick intervals per major tick interval. The
         first element is for the horizontal direction, the second for
         the vertical direction. Values of 0 for MINOR or MAJOR cause
         PGPLOT to choose the minor tick interval automatically. [0,0]
      }
      \sstsubsection{
         BOTTOM = \_CHAR (Read)
      }{
         The text label for the bottom axis. Give null to construct the
         label from the input NDF axis label and unit. Within the
         string, you can use the following escape sequences [!]:
         \sstitemlist{
            \sstitem
            $\backslash$fn Normal (single stroke) font,
            \sstitem
            $\backslash$fr Roman font,
            \sstitem
            $\backslash$fi Italic font,
            \sstitem
            $\backslash$fs Script font,
            \sstitem
            $\backslash$u  Superscript (use only paired with $\backslash$d),
            \sstitem
            $\backslash$d  Subscript (use only paired with $\backslash$u),
            \sstitem
            $\backslash$b  Backspace,
            \sstitem
            $\backslash\backslash$  Backslash,
            \sstitem
            $\backslash$A  Danish umlaut (\AA ngstr\"om),
            \sstitem
            $\backslash$g  Any greek letter.
         }
      }
      \sstsubsection{
         LEFT = \_CHAR (Read)
      }{
         The text label for the left axis. Give null to construct the
         label from the input NDF axis label and unit. See also BOTTOM.
         [!]
      }
      \sstsubsection{
         TOP = \_CHAR (Read)
      }{
         The text label for the top axis. Give null to use the title
         from the input NDF. See also BOTTOM. [!]
      }
      \sstsubsection{
         RIGHT = \_CHAR (Read)
      }{
         The text label for the right axis. Give null to construct the
         label from the input NDF label and unit. See also BOTTOM. [!]
      }
      \sstsubsection{
         LABSPC( 4 ) = \_REAL (Read)
      }{
         This is a measure for the distance of the text labels from the
         coordinate view port. The elements are for the bottom, left,
         top, and right edge respectively. In the first instance the
         whole plot is inside the current (AGI) picture and LABSPC
         specifies the fraction of this view surface to be reserved for
         labelling. However, if FILL is false, then the view port will
         shrink further either horizontally or vertically to give equal
         plot scales. The labelling area will then move inwards as
         well. The labelling space is measured as a fraction of the
         current picture. The values must be between zero and 0.5.
         [0.1,0.1,0.05,0.05]
      }
      \sstsubsection{
         CWORLD( 4 ) = \_REAL (Read)
      }{
         The world coordinates of the plot window. Give null to use the
         coordinate extent of the input NDF instead. The elements are
         \sstitemlist{
            \sstitem
            1: Left bound in coordinate space,
            \sstitem
            2: Right bound in coordinate space,
            \sstitem
            3: Bottom bound in coordinate space,
            \sstitem
            4: Top bound in coordinate space,
         }
         Left and right bound must not be equal, neither must bottom and
         top. [!]
      }
      \sstsubsection{
         LEGEND( 2 ) = \_REAL (Read)
      }{
         The coordinates of the legend cell. Each spectral cell has
         coordinates according to the NDF's Specdre Extension (or
         positional axes). LEGEND is in the same units the position of
         the empty cell that explains the spectral and intensity
         coverage of all cells. Give null to avoid the legend cell being
         drawn. [!]
      }
      \sstsubsection{
         CELLSZ( 2 ) = \_REAL (Read)
      }{
         The size of the spectral cells, specified in coordinate units.
         These must be positive. [1.,1.]
      }
      \sstsubsection{
         SWORLD( 4 ) = \_REAL (Read)
      }{
         The world coordinates within the spectrum cells. Give null to
         use the spectral extent and data range of the input NDF
         instead. The elements are
         \sstitemlist{
            \sstitem
            1: Left bound in spectroscopic space,
            \sstitem
            2: Right bound in spectroscopic space,
            \sstitem
            3: Bottom bound in spectroscopic space,
            \sstitem
            4: Top bound in spectroscopic space.
         }
         Left and right bound must not be equal, neither must bottom and
         top. [!]
      }
      \sstsubsection{
         XLEGEND = \_CHAR (Read)
      }{
         The text label for the bottom axis of the legend cell. Give
         null to construct the label from the input NDF axis label and
         unit. Within the string, you can use the following escape
         sequences [!]:
         \sstitemlist{
            \sstitem
            $\backslash$fn Normal (single stroke) font,
            \sstitem
            $\backslash$fr Roman font,
            \sstitem
            $\backslash$fi Italic font,
            \sstitem
            $\backslash$fs Script font,
            \sstitem
            $\backslash$u  Superscript (use only paired with $\backslash$d),
            \sstitem
            $\backslash$d  Subscript (use only paired with $\backslash$u),
            \sstitem
            $\backslash$b  Backspace,
            \sstitem
            $\backslash\backslash$  Backslash,
            \sstitem
            $\backslash$A  Danish umlaut (\AA ngstr\"om),
            \sstitem
            $\backslash$g  Any greek letter.
         }
      }
      \sstsubsection{
         YLEGEND = \_CHAR (Read)
      }{
         The text label for the left axis of the legend cell. Give null
         to construct the label from the input NDF label and
         unit. See also XLEGEND. [!]
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         specgrid in accept
      }{
         Let's assume the given NDF is three-dimensional and has neither
         a Specdre Extension, nor any axis information. This implies
         that the first axis is spectroscopic with pixel centres 0.5,
         1.5, ..., NX-0.5. The second and third axes will thus be used
         to position the plots of the individual spectra horizontally
         and vertically. The positions will also be (0.5,0.5),
         (1.5,0.5), ..., (NY-0.5,NZ-0.5). Each cell has the default size
         of 1.0 by 1.0, thus neighbouring pixels in the Y-Z plane of the
         NDF will be in adjacent cells on the plot. All cells use
         internally the same scales for the spectroscopic value and the
         data value. Since these are not specified in parameters, each
         cell goes from 0.5 to NX-0.5 horizontally and from the minimum
         data value to the maximum data value vertically. There will be
         no legend cell, since its position was not given.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 1.1.

      This routine recognises and uses coordinate transformations in
      AGI pictures.
   }
}

\sstroutine{
   SPECPLOT
}{
   Plot a spectrum
}{
   \sstdescription{
      This routine plots a spectrum (or any one-dimensional NDF section)
      in the current (AGI) picture of the graphics device.

      The plot can basically be an overlay over the most recent data
      picture inside the current picture, or a new plot inside the
      current picture. (The current picture after SPECPLOT is the
      same as before.)

      The screen contents of the current picture can be erased or not.

      The plot location and size is governed by the outer and the
      inner box. The inner box is the area where data are plotted,
      the outer box contains the inner box and the plot labels.

      In the overlay case the inner box and its world coordinates are
      identified with the most recent data picture inside the current
      picture. No labelling is done in the overlay case, so the outer
      box has no meaning in this case.

      In the case of a new plot, the outer box will be identified
      with the current picture, although the plot labels are allowed
      to extend beyond this area. Depending on the choice of
      labelling, a sensible location for the inner box is offered.
      After the inner box is specified, its world coordinates are
      enquired. The prompt values correspond to the extreme values
      found in the data. The location and world coordinates of the inner
      box are saved as a data picture in the AGI data base.

      The labelling consists of axes, axis ticks, numeric labels at
      the major ticks, and text labels. The axes are counted from
      bottom clockwise. Each axis can be drawn or not. Each
      drawn axis can have ticks or not. Each axis can have numeric
      labels or not. The left and right axes can have either
      horizontal (orthogonal) or vertical (parallel) numeric labels.
      Each axis can have a text label or not.

      The kind of labelling is controlled by several 4-character
      strings. Each character is the switch for axis 1, 2, 3, 4
      respectively. ``0'' turns an option off for that axis, ``+'' turns
      it on. For the ticks and for numeric labels of axes 2 and 4,
      ``-'' is also allowed. It yields inward ticks and vertical
      numeric labels.

      The data can be plotted as a set of markers, as a line-style
      polygon connecting the data points, or as a bin-style polygon.
      In addition error bars or pixel width bars can be plotted. Each
      of the options can be selected independent of the others, i.e.\
      several (or all) options can be selected at the same time. If
      no variance information is available, error bars are de-selected
      automatically. Bad data are omitted from the plot. If error
      bars are selected, bad variances cause the corresponding data
      also to be omitted.

      The attributes of the plot can be selected. These are
      \sstitemlist{
         \sstitem
         colour
         \sstitem
         line thickness
         \sstitem
         character height (equivalent to marker size)
         \sstitem
         simple or roman font
         \sstitem
         dash pattern for polygon connections
      }
      Most parameters default to the last used value.
   }
   \sstusage{
      specplot in overlay=?\ bottom=?\ left=?\ top=?\ right=?\\
         labspc=?\ world=?
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [TRUE]
      }
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         If true, the part of the graphics device corresponding to the
         current (AGI) picture is erased before the plot is drawn.
         [FALSE]
      }
      \sstsubsection{
         OVERLAY = \_LOGICAL (Read)
      }{
         If true, the plot will be an overlay on the most recent (AGI)
         data picture within the current (AGI) picture.
         If false, the plot will be user-defined, but the inner box is
         restricted to the current (AGI) picture. [FALSE]
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF.
      }
      \sstsubsection{
         LIN = \_LOGICAL (Read)
      }{
         If true, the data points will be connected by a line-style
         polygon. [TRUE]
      }
      \sstsubsection{
         BIN = \_LOGICAL (Read)
      }{
         If true, the data points will be connected by a bin-style (or
         histogram-style) polygon. [FALSE]
      }
      \sstsubsection{
         MARK = \_INTEGER (Read)
      }{
         This parameter determines the kind of marker to be drawn at
         each data point [0]:
         \sstitemlist{
            \sstitem
            0: No markers drawn,
            \sstitem
            1: Diagonal cross,
            \sstitem
            2: Asterisk,
            \sstitem
            3: Open circle,
            \sstitem
            4: Open square,
            \sstitem
            5: Filled circle,
            \sstitem
            6: Filled square.
         }
      }
      \sstsubsection{
         ERROR = \_LOGICAL (Read)
      }{
         If true and variance information available, error bars will be
         drawn. [FALSE]
      }
      \sstsubsection{
         WIDTH = \_LOGICAL (Read)
      }{
         If true, the pixel width will be indicated by horizontal bars.
         [FALSE]
      }
      \sstsubsection{
         ROMAN = \_LOGICAL (Read)
      }{
         If true, PGPLOT's roman font is used for drawing text. If
         false, the normal (single-stroke) font is used. [FALSE]
      }
      \sstsubsection{
         HEIGHT = \_REAL (Read)
      }{
         The height of the characters. This also affects the size of the
         markers. Markers are about half the size of characters. The
         height is measured in units of PGPLOT default text heights,
         which is approximately 1/40 of the height of the (AGI) base
         picture (i.e.\ 1/40 the height of the work station window, screen
         or paper). [1.]
      }
      \sstsubsection{
         COLOUR = \_INTEGER (Read)
      }{
         The PGPLOT colour index to be used for the plot. This can be
         formally between 0 and 255, but not all devices support all
         colours. The default colour representation is:
         \sstitemlist{
            \sstitem
            0: Background,           
            \sstitem
            1: Foreground (default),
            \sstitem
            2: Red,                  
            \sstitem
            3: Green,
            \sstitem
            4: Blue,                 
            \sstitem
            5: Cyan,
            \sstitem
            6: Magenta,              
            \sstitem
            7: Yellow,
            \sstitem
            8: Orange,               
            \sstitem
            9: Green/Yellow,
            \sstitem
            10: Green/Cyan,           
            \sstitem
            11: Blue/Cyan,
            \sstitem
            12: Blue/Magenta,         
            \sstitem
            13: Red/Magenta,
            \sstitem
            14: Dark grey,            
            \sstitem
            15: Light grey.
         }
      }
      \sstsubsection{
         THICK = \_INTEGER (Read)
      }{
         The PGPLOT line thickness. Can be between 1 and 21. [1]
      }
      \sstsubsection{
         DASH = \_INTEGER (Read)
      }{
         The PGPLOT dash pattern [1]:
         \sstitemlist{
            \sstitem
            1: Full line,
            \sstitem
            2: Long dash,
            \sstitem
            3: Dash-dot-dash-dot,
            \sstitem
            4: Dotted,
            \sstitem
            5: Dash-dot-dot-dot.
         }
      }
      \sstsubsection{
         AXES = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of either of
         the four box sides. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be
         ``0'' or ``+''. E.g.\ `00++' would switch off the bottom and left
         axes and switch on the top and right axes. [`++++']
      }
      \sstsubsection{
         TICK = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of ticks along
         either axis. Ticks are drawn only if the corresponding axis is
         also drawn. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be
         ``0'', ``+'' or ``-''. E.g.\ `00+-' would switch off the bottom and
         left ticks and switch on the top and right ticks. The top axis
         would have ticks outward, the right axis would have ticks
         inward. [`{\tt----}']
      }
      \sstsubsection{
         NUML = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of numeric
         labels along either axis. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be
         ``0'' or ``+''; the second and fourth switch can also be ``-''.
         E.g.\ `0+0-' would switch off the bottom and top labels and switch
         on the left and right labels. The left axis would have labels
         horizontal (orthogonal), the right axis would have labels
         vertical (parallel). [`++00']
      }
      \sstsubsection{
         TEXT = \_CHAR (Read)
      }{
         Array of switches to turn on or off the drawing of text labels
         along either axis. The sides are counted from bottom
         clockwise: bottom, left, top, right. Any switch can be
         ``0'' or ``+''. E.g.\ `0++0' would switch off the bottom and right
         labels and switch on the left and top labels. [`+++0']
      }
      \sstsubsection{
         NORTHO = \_REAL (Read)
      }{
         If orthogonal numeric labels have been selected, you must
         specify how much space there must be between the
         axis and the text label, i.e.\ how long the longest numeric
         label along the left or right axis will be. The unit is character
         heights. [1]
      }
      \sstsubsection{
         MAJOR( 2 ) = \_REAL (Read)
      }{
         The distance in world coordinates between major tick marks. The
         first element is for the horizontal direction, the second for
         the vertical direction. This is also the distance along the
         axis between numeric labels. Values of 0 cause PGPLOT to choose
         the major tick interval automatically. [0.,0.]
      }
      \sstsubsection{
         MINOR( 2 ) = \_INTEGER (Read)
      }{
         The number of minor tick intervals per major tick interval. The
         first element is for the horizontal direction, the second for
         the vertical direction. Values of 0 for MINOR or MAJOR cause
         PGPLOT to choose the minor tick interval automatically. [0,0]
      }
      \sstsubsection{
         BOTTOM = \_CHAR (Read)
      }{
         The text label for the first axis. Within the label string, you can
         use the following escape sequences:
         \sstitemlist{
            \sstitem
            $\backslash$fn Normal (single stroke) font,
            \sstitem
            $\backslash$fr Roman font,
            \sstitem
            $\backslash$fi Italic font,
            \sstitem
            $\backslash$fs Script font,
            \sstitem
            $\backslash$u  Superscript (use only paired with $\backslash$d),
            \sstitem
            $\backslash$d  Subscript (use only paired with $\backslash$u),
            \sstitem
            $\backslash$b  Backspace,
            \sstitem
            $\backslash\backslash$  Backslash,
            \sstitem
            $\backslash$A  Danish umlaut (\AA ngstr\"om),
            \sstitem
            $\backslash$g  Any greek letter.
         }
      }
      \sstsubsection{
         LEFT = \_CHAR (Read)
      }{
         The text label for the second axis. See also BOTTOM.
      }
      \sstsubsection{
         TOP = \_CHAR (Read)
      }{
         The text label for the third axis. See also BOTTOM.
      }
      \sstsubsection{
         RIGHT = \_CHAR (Read)
      }{
         The text label for the fourth axis. See also BOTTOM.
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device for the plot.
      }
      \sstsubsection{
         LABSPC( 4 ) = \_REAL (Read)
      }{
         The space between outer box (AGI current picture) and inner box
         measured in units of character heights. The four numbers are
         for the bottom, left, top, right labelling space in that order.
         The dynamic default offered is based on the space requirements
         for the axis labelling, and can in general be accepted.
      }
      \sstsubsection{
         WORLD( 4 ) = \_REAL (Read)
      }{
         The world coordinates that the left, right, bottom and top ends
         of the inner box should correspond to.
         The dynamic default is based on the coordinates of the first
         and last pixel of the selected subset and on the extreme data
         values of the selected subset. Reverse axes can be achieved by
         giving WORLD(1) {\tt >} WORLD(2) and/or WORLD(3) {\tt >} WORLD(4).
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         specplot spectrum accept
      }{
         This is the simplest way to plot a 1-D data set in its full
         length.
      }
      \sstexamplesubsection{
         specplot imagerow(-100.:50.,15.) accept
      }{
         This will take a 2-D data set IMAGEROW and plot part of the
         row specified by the second coordinate being 15. The part of
         the row plotted corresponds to the first coordinate being
         between -100 and +50. Note that the decimal point forces use of
         axis data. Omitting the period would force use of pixel
         numbers.
      }
      \sstexamplesubsection{
         specplot imagecol(15.,-100.:50.) accept
      }{
         This will take a 2-D data set IMAGEROW and plot part of the
         column specified by the first coordinate being 15. The part of
         the row plotted corresponds to the second coordinate being
         between -100 and +50. Note that the decimal point forces use of
         axis data. Omitting the period would force use of pixel
         numbers.
      }
      \sstexamplesubsection{
         specplot spectrum lin=false bin=true accept
      }{
         Replace direct connections between data points by bin-style
         connections.
      }
      \sstexamplesubsection{
         specplot spectrum mark=1 accept
      }{
         Mark each data point by a diagonal cross.
      }
      \sstexamplesubsection{
         specplot spectrum error=true width=true accept
      }{
         Draw an error bar and a pixel width bar for each data point.
      }
      \sstexamplesubsection{
         specplot spectrum roman=true height=1.5 colour=3 accept
      }{
         Draw text with the roman font, draw text and makers 1.5 times
         their normal size, and plot the whole thing in green colour.
      }
      \sstexamplesubsection{
         specplot spectrum bottom=Weekday left="Traffic noise [dBA]" accept
      }{
         Specify text labels on the command line instead of constructing
         them from the file's axis and data info.
      }
      \sstexamplesubsection{
         specplot spectrum overlay=true clear=false accept
      }{
         The position and scale of the plot are determined by the
         previous plot (which might have been produced by a different
         application).
      }
      \sstexamplesubsection{
         specplot spectrum world=[0.,1.,-1.,1.]\ accept
      }{
         Use plot limits different from the extreme data values.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.

      This routine recognises and uses coordinate transformations in
      AGI pictures.
   }
}

\sstroutine{
   SUBSET
}{
   Take a subset of a data set
}{
   \sstdescription{
      Takes a rectangular subset of a data set. The given data set and
      the resulting subset may have up to seven axes. Axes that become
      degenerate by sub-setting -- i.e.\ along which only one pixel is
      chosen -- are deleted from the subset. Thus the subset may have
      smaller dimensionality than the original.
   }
   \sstusage{
      subset in out
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input file.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         The output file.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         subset in(1.5:2.5,10:12) out
      }{
         This takes the data from IN and writes the subset to OUT. The
         subset is specified as having 1st axis coordinates between 1.5
         and 2.5 and 2nd axis pixel numbers between 10 and 12.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7.
   }
}

\sstroutine{
   XTRACT
}{
   Average an N-dimensional cube into an (N-M)-dimensional one
}{
   \sstdescription{
      This routine reduces the number of axes of a data set by averaging
      pixels along some axes while retaining other axes. A simple and
      common example is averaging all or a certain range of rows (or
      columns) of an image to create a single row, e.g.\ an averaged
      spectrum from a 2-D slit spectrum. Input pixels with bad or zero
      variance are treated as bad, i.e.\ disregarded in the averaging
      (unless NOVAR is true).
   }
   \sstusage{
      xtract in colaps out
   }
   \sstparameters{
      \sstsubsection{
         INFO = \_LOGICAL (Read)
      }{
         If false, the routine will issue only error messages and no
         informational messages. [YES]
      }
      \sstsubsection{
         VARUSE = \_LOGICAL (Read)
      }{
         If false, data variance in the input is ignored and output
         variance is calculated from the scatter of averaged values.
         If true, data variance in the input is used to weight mean
         values and to calculate output variance. [YES]
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         Input file.
      }
      \sstsubsection{
         COLAPS( 7 ) = \_INTEGER (Read)
      }{
         For each axis in IN a 0 indicates that the axis is to be
         retained in OUT, a 1 indicates that along that axis pixels
         from IN are to be averaged.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         Output file, containing the extracted data set.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         xtract cube(-30.:30.,1.5:2.5,10:20) [0,0,1]\ xyplane
      }{
         This first takes a subset from the 3-D data cube extending
         from -30 to +30, 1.5 to 2.5, 10 to 20 along the 1st, 2nd, 3rd
         axes respectively. (Coordinates are used along the 1st and 2nd
         axes, pixel indices along the 3rd.) From that sub-cube all the
         x-y-planes are averaged to create a 2-D image.
         (E.g.\ this averages the channel maps between 10 and 20 into an
         integrated map.)
      }
      \sstexamplesubsection{
         xtract cube(-30.:30.,1.5:2.5,10:20) [1,1,0]\ spectrum
      }{
         This averages each x-y-plane into a single point of the output
         row. The subset used is the same as above. (E.g.\ this averages
         the cube of channel maps into a mean spectrum.)
      }
      \sstexamplesubsection{
         xtract image(-30.:30.,1.5:2.5) [0,1]\ spectrum info=no varuse=no
      }{
         This averages all rows between 1.5 and 2.5 into a spectrum. The
         spectrum extends from -30 to +30. Informational messages are
         suppressed, and data variances in the image are ignored. The
         variances in the spectrum are calculated from the row-to-row
         scatter in each column.
      }
   }
   \sstnotes{
      This routine recognises the Specdre Extension v. 0.7. However, no
      extraction is performed on NDFs in the input Specdre Extension. If
      the spectroscopic axis is retained, then the scalar components in
      the Extension are propagated. If the spectroscopic axis is
      collapsed, the Extension is not propagated at all.
   }
}

\sstroutine{
   SPE\_HELP
}{
   Browse through a Starlink help library
}{
   \sstdescription{
      This application is an interactive browser to display the contents of a
      Starlink help library on an alphanumeric terminal. The user can navigate
      through the library with the following responses to the prompt:
      \sstitemlist{
         \sstitem
         A blank response gets you one level up in the topic hierarchy.
         \sstitem
         A question mark (?) re-displays the current topic.
         \sstitem
         An end-of-file character exits. This is usually Ctrl-d.
         \sstitem
         Any normal text specifies (sub-) topics to look for.
         \sstitem
         Each blank-separated word stands for one topic in the
            hierarchy. E.g.\ three blank-separated words go down three
            levels in the hierarchy.
         \sstitem
         Each underscore-separated word stands for an underscore-separated
            word in a single topic.
         \sstitem
         Words (whether separated by blanks or underscores) that are not
            unique topics or include wild card characters are expanded and
            help is given on all matching topics. Wild card characters are
            \% for a single character and * for any number of characters
            including none. In the word expansion A\_G\_N would match
            active\_galactic\_nuclei, which is one topic. The same is true
            for A*\_G*\_N* or active or active*.
      }
      When the help text to be printed is longer than the terminal page,
      then the user is asked to press the Return key before proceeding
      with output. At this point, too, can an end-of-file character be
      given to exit immediately.
   }
   \sstusage{
      helpc [-l library.shl] [topic [subtopic [subsubtopic ...]]]
   }
   \sstparameters{
      \sstsubsection{
         -l:
      }{
         Next parameter is the name of the Starlink help library to be opened.
         If given, this parameter must be the first, the library name the
         second.
      }
      \sstsubsection{
         library.shl:
      }{
         The name of the Starlink help library. These names usually end in
         ``.shl''. If given, ``-l'' must be the first and this the second
         parameter. If no library is given then the library
         \$\{SPECDRE\_HELP\}.shl, i.e.\ the Specdre help library, is used.
      }
      \sstsubsection{
         topic, subtopic etc.:
      }{
         The initial entry point in the hierarchy of topics and subtopics in
         the help library.
      }
   }
}

\normalsize

% -----------------------------------------------------------------------------

\newpage
\section{\label{changes}\xlabel{changes}Changes from version 1.0 to 1.1}

\begin{itemize}
\item Specdre can now be run from the ICL command line interface, which is now
    preferred over the Unix shell (C or Tc).

\item Specdre 1.1 uses the new PDA library of Public Domain Algorithms instead
    of  the proprietary  NAG  library.   As  a  consequence the  application
    FITCHEBY is discontinued and replaced by FITPOLY.

\item Revised  documentation (Starlink User Note 140),  which is now available
    in printed form (LaTeX 2.09) and on the World Wide Web (HTML).

\item Five new applications have been added:

   \begin{itemize}
   \item FITPOLY replaces FITCHEBY and fits an ordinary polynomial.

   \item MOMENTS performs a moments analysis on all rows of a hypercube.

   \item MOVIE is a  grey/colour display  application to browse through planes
       of a cube.

   \item SPECCONT  is  - yet  another -  contouring application. What distin-
       guishes it from  KAPPA's contouring routines is  that it can  overlay
       images with different sampling rates.

   \item SPECGRID displays all rows of a cube as small spectra, each in a cell
       that is positioned according to the row's position in the cube. Users
       of SPECX will know this  kind of plot  from the command GRID-SPECTRA.
       Instead of  the second and   third cube axis,  independent coordinate
       arrays can be used to position the cells on the plot.
   \end{itemize}

\item The help  browser SPE\_HELP is a  stand-alone programme, it can  open any
    Starlink help library  and opens  the Specdre help  library  only as the
    default.

\item The  definition of the Specdre Extension  has been expanded from version
    0.7 to  version 1.1. (The Specdre Extension  is the extension to the NDF
    data format as used by Specdre.)  In version 1.1 an additional component
    may exist to store two coordinate values. This part  of the Extension is
    supported only by the applications SPECGRID and EDITEXT.

\item A small number of bugs have been fixed, namely in RESAMPLE and GROW.

\item Revised layering of Specdre's internal subroutine libraries.

\item Revised makefile (Mark V).
\end{itemize}

\end{document}
