\documentstyle{article}
\pagestyle{myheadings}

%------------------------------------------------------------------------------
\newcommand{\stardoccategory}  {Starlink User Note}
\newcommand{\stardocinitials}  {SUN}
\newcommand{\stardocnumber}    {16.4}
\newcommand{\stardocauthors}   {T N Wilkins \& D J Axon}
\newcommand{\stardocdate}      {22 September 1994}
\newcommand{\stardoctitle}     {TWODSPEC --- Some Additions To Figaro}
%------------------------------------------------------------------------------

\newcommand{\stardocname}{\stardocinitials /\stardocnumber}
\markright{\stardocname}
\setlength{\textwidth}{160mm}
\setlength{\textheight}{240mm}
\setlength{\topmargin}{-5mm}
\setlength{\oddsidemargin}{0mm}
\setlength{\evensidemargin}{0mm}
\setlength{\parindent}{0mm}
\setlength{\parskip}{\medskipamount}
\setlength{\unitlength}{1mm}

%------------------------------------------------------------------------------
% Add any \newcommand or \newenvironment commands here
\newcommand{\igref}[2]{{\item{#1} {\it\ #2}}}
\newcommand{
\jref}[5]{
 \item[#5] { #1}. \mbox{{\it#2}}, \mbox{{\bf#3}},
\mbox{{ #4.}}  }
\newcommand{\cref}[4]{
     {\item{#4} { #1}. {\it#2},{ #3.} }}
%------------------------------------------------------------------------------

\begin{document}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
\thispagestyle{empty}
DRAL / {\sc Rutherford Appleton Laboratory} \hfill {\bf \stardocname}\\
{\large Particle Physics \& Astronomy Research Council}\\
{\large Starlink Project\\}
{\large \stardoccategory\ \stardocnumber}
\begin{flushright}
\stardocauthors\\
\stardocdate
\end{flushright}
\vspace{-4mm}
\rule{\textwidth}{0.5mm}
\vspace{5mm}
\begin{center}
{\Large\bf \stardoctitle}
\end{center}
\vspace{5mm}

%------------------------------------------------------------------------------
%  Add this part if you want a table of contents
\setlength{\parskip}{0mm}
\tableofcontents
\setlength{\parskip}{\medskipamount}
\markright{\stardocname}
%------------------------------------------------------------------------------

\newpage
\section{Introduction}

TWODSPEC is a package of programs for the reduction and analysis of
long-slit and optical fibre array spectra, implemented within the FIGARO
package (Shortridge 1988).
If you wish to use the ADAM version of TWODSPEC you should look at the
ADAM documentation or at least ensure you are familiar with the ADAM
parameter system.
If you wish to use the native Figaro TWODSPEC you should read the
FIGARO documentation.
However, a number of the functions will be found useful outside the area
of spectroscopy.

LONGSLIT is a program for the analysis of calibrated long-slit spectra.
It has, among other things, the capability to fit Gaussians, either
manually or automatically---in batch.
It can, however, handle data with two spatial dimensions, such as
obtained using TAURUS.
The program FIBDISP provides further options useful for such data,
although this is primarily designed for fibre array data.
An extensive range of options is available, especially in terms of
output.
ARC2D is a two-dimensional arc calibration program.
COMB and ARCSDI are for the correction of data for S-distortion and line
curvature respectively.
A number of miscellaneous programs are also included, including programs
to convert between FIGARO and IRAF data formats, and to display data.
Any comments on these programs or documentation should be addressed to
T. N. Wilkins (T.N.Wilkins@UK.AC.DURHAM) or D. J. Axon
(RLESIS::STSCIC::DJA or AXON@STSCI.EDU).
Note that these programs are designed to do as much as possible for the
user, to assist quick reduction and analysis of data---LONGSLIT can fit
multiple Gaussians to line profiles in batch for example, and will
decide how many components to fit.
More information on the aims behind the package are given in Wilkins
1988 and Wilkins \& Axon 1991.
\vspace*{3mm}
\rule{\textwidth}{0.5mm}
\begin{center}
{\large\bf Users of this software are requested to acknowledge the
authors of the software in any publications resulting from its use.}
\end{center}
\rule{\textwidth}{0.5mm}
\vspace*{3mm}

\subsection{Accessing These Programs}

To use the Unix ADAM version, you need to type ``twodspec'' and then the
programs should be ready to run. This is at present available for Suns running
SunOS (4.x or 5.x), DECStations running Ultrix and DEC Alphas running OSF/1.

To use the Sun Figaro 2.4 version, you should set the environment variable
FIGARO\_PROG\_U to point to the directory containing the executables,
and include it in your PATH, for example:
\begin{quote}\begin{verbatim}
% setenv FIGARO_PROG_U /soft/twodspec/
% setenv PATH ${FIGARO_PROG_U}:${PATH}
\end{verbatim}\end{quote}
You would probably want these commands in your .cshrc.
You may prefer to create links to the programs you require if you wish
to your own programs, but you will also need to include the auxiliary
files (e.g. line lists)---it is the parameter files and other auxiliary
files which are located using the environment variable FIGARO\_PROG\_U.
After that the use is much the same as for the DCL version.

The X version of the (non-ADAM) Figaro 2.4 parameter handling routines
(i.e. those which come with Figaro) are not particularily useful for
interactive programs, and should probably be avoided, by setting the
environment variable INHIBIT\_XPAR to anything, e.g.
\begin{quote}\begin{verbatim}
% setenv INHIBIT_XPAR "yes"
\end{verbatim}\end{quote}

If you wish to use the X11 versions of the programs you will need to tell the
programs where to find the help files---see appendix section~\ref{x-deflts}.

On VMS these routines are accessed from FIGARO, as with ``normal'' FIGARO
commands or from ICL (Bailey 1989, Chipperfield 1989, Lawden 1989).
On VMS the latter is based on the method used to create the ADAM monolith
version of FIGARO.
When this is started logical names are set up in the job logical name
table to control plotting of hardcopy output.
These are taken from a file TWODSPEC\_GRAPH.COM, and private copies (to
change the default print queues for example) can be kept in the users
FIGARO\_PROG\_U.
\begin{quote}\begin{verbatim}
$ ADAMSTART
$ ICL

Interactive Command Language   -   Version 1.5-3

  - Type HELP package_name for help on specific Starlink packages
  -   or HELP PACKAGES for a list of all Starlink packages
 Command ASTERIX redefined
  - Type HELP [command] for help on ICL and its commands

ICL> TWODSPEC
Creating DCL subprocess

 ---------- Initialised for TWODSPEC ---------

        Version: February 15th, 1991

ICL> LONGSLIT
Loading FIGARO_PROG_N:TWODSPEC into 2225TWODSPEC
IM - (IMage) Name of image for input /'2064SCR'/ >
\end{verbatim}\end{quote}
for example.
It is quicker to access these programs from ICL, at least as regards
entering and leaving the programs.
The same limitations apply as with the ADAM version of FIGARO (see
Shortridge 1989).
Note that the programs IRAF2HDS, HDS2IRAF and BATCH are not available
from ICL (BATCH is not a FIGARO program, and IRAF2HDS and HDS2IRAF both
require special linking).
Beware that batch queue CPU limits are effectively reduced by running
from ICL.

To use from DCL, the you would enter FIGARO as normal, and then type the
name of the command, e.g.
\begin{quote}\begin{verbatim}
$ FIGARO

  F I G A R O       Version 3.0-8  (April 1993)

  Some help may be obtained by typing HELP FIGARO

      This is a new release of the Figaro data reduction system.

        Any complaints, suggestions, problems, etc., should be
        forwarded to -

              Horst Meyerdierks (REVAD::HME)

    -----------------------------------------------------------------

        National (UK/Starlink) Figaro version 3.0-8  (April 1993)

                 Help may be obtained with HELP NFIGARO.

    -----------------------------------------------------------------

 Running DU:[L_FIG]FIGARO.COM

 Symbol CREPAR defined for Figaro developement

XBCLEAN defined

MEDSP defined

$ TWODSPEC
$ LONGSLIT ....
This version of LONGSLIT is from $1$DRB3:[FIGPACK.TWODSPEC.TWODSPEC]
(IMage) Name of image for input [2063] -
\end{verbatim}\end{quote}
The examples in this document are for the DCL version, and the prompting
will be different for the ICL version.

\section{Line Profile Analysis}

\subsection{Introduction}

In order to parameterise a long-slit spectrum, it is often
convenient to fit a function to the spectral lines.
The most commonly used function is a Gaussian, defined as:
\[
     f(x) = H e^{-((x-x_{0})^{2}/2\sigma^{2})}
\]
where $\sigma$ is the standard deviation of the line, $x_{0}$ is its
centre, and $H$ is its height.
If the gas consists of a number of discrete clumps, moving at the same
velocity, then a series of Gaussians would normally accurately represent
the line profile (although the instrumental profile may alter this).

In many cases it is evident that more than one
Gaussian is required, whereas some profiles are asymmetrical, but not
necessarily of more than one component.
For the latter the skew Gaussian (Frazer and Suzuki 1969) is useful:
\[
     f(x) = H e^{-(\ln 2(\ln (1+2s(x-x_{0})/w)/s)^{2})}
\]
where $s$ is positive or negative, depending on the direction of the
skew.
$w$ is the width.
As $s$ tends to zero, this tends to a symmetrical Gaussian.
Fitting a skew Gaussian is also useful when two components are present,
but one is too weak to allow a double Gaussian to be fitted.
This function also represents the line profiles found in many Seyfert
galaxies (see for example the profiles shown by Whittle 1985).

The Cauchy function may also be used:
\[
     f(x) = \frac{H}{1+[2(x-x_{0})/w]^{2}}
\]
The instrumental line profiles from a Fabry-Perot interferometer are
approximate Cauchy functions.
The precise form (Bland 1986) is:
\[
A(x,y,z) =
[1+(2N/\pi )^{2}\sin^{2}((2\pi \mu l(z)\cos Q)/\lambda )]^{-1}
\]
where $N$ is the finess of the etalon, $\mu$ the refractive index of
air, $l(z)$ the etalon spacing, $Q$ the angle to the optical axis, and
$\lambda$ the wavelength of the light concerned.

Frazer and Suzuki (1969) give a function which varies smoothly between a
Gaussian and a Cauchy function:
\begin{equation}
     f(x) = \frac{H}{(1+[2^{a^{2}}-1][2(x-x_{0})/w]^{2})}
\label{eq.varcauchy}
\end{equation}
where as $a$ tends to 0, this function tends to a Gaussian, and for
$a=1$, it is a Cauchy function.

The fitting consists of optimising the values of the free parameters,
that is $H$, $x_{0}$, etc.
Normally one would also allow the fitting routines to vary the base
level.

The programs LONGSLIT and FIBDISP can fit Gaussians (single or
multiple), skew Gaussians and Cauchy functions (actually the function of
equation \ref{eq.varcauchy}) to line profiles and LONGSLIT can also
carry out shape analysis to produce the Whittle (1982, 1985) and Heckman
(Heckman {\it et al.} 1981) asymmetry parameters (the latter is also
evaluated at heights other than 20\%)---these are described below.
From the fitting (not the shape analysis), velocity plots may be
produced by the programs, as can plots of line width and line flux (not
FIBDISP), for each line against cross-section (greyscale in FIBDISP).
It is also possible to output greyscale or contour velocity plots.
LONGSLIT is designed to operate in batch as much as possible.

\subsection{LONGSLIT}
\label{sec.long}

LONGSLIT is designed primarily for the analysis of long-slit spectra,
although this is mainly a matter of the display options available---the
fitting routines, for example, can cope with three-dimensional data.

The basic method of operation of LONGSLIT is as follows:

\newcounter{longcntr}
\begin{list}{(\roman{longcntr})}{\usecounter{longcntr}}
\item Locate lines.
This is done by extracting part (or all) of the image and displaying
it.
The user is then required to mark to either side of the lines with a
cursor and to identify the lines so located.
Alternatively CLONE (see section~\ref{long.id}) may be used.
A great advantage with CLONE here is that, since the data
is calibrated, for a lot of data the tram positions will be the same.
This makes CLONING in batch very useful, since only one spectrum
need be calibrated before submitting a job to calibrate the rest and
then perform Gaussian fitting.
This part is the same as for ARC2D (see section~\ref{sc.arc2d}).
\item Choose the fit type.
The type of fit to be performed is entered into the array .RES.CONTROL
(see appendix~\ref{ap.res} for a description of the results structure),
for the relevant line and cross-section.
This defaults to a single Gaussian with a base.
\item Perform the fit.
The fit type is read from .RES.CONTROL, the fit is performed, and the
results are stored in .RES.DATA.
\item Output results.
This can be in the form of plots, tables, or a file for input into the
statistics program EXTATIC, also running in FIGARO\@.
EXTATIC is derived from a program by W. Sparks, but has been extensively
re-written.
This permits more detailed statistical analysis of the results than is
possible using LONGSLIT alone.
\end{list}

The fact that LONGSLIT uses its own structure in the data file
(the .RES structure, see appendix~\ref{ap.res}), means that the different stages above
can easily be carried out at separate times, without needing to stay in
the program between them.

There are two principle modes of operation for line fitting:

\begin{description}

\item[AUTO mode] In this the fits, blocking etc.\ are defined
beforehand, and then performed without any further reference to the
user (except for interactive multiple fits).
This can be carried out in batch, thus saving time at the terminal.

\item[MANUAL mode] In this the user can choose the blocking and fit
type for each line as the fits are made, or the fit types may be
defined in manual mode, but actually performed in AUTO mode, in batch
for example.
It is also possible to combine any range of cross-sections for a fit.
Any previous fits may be displayed (this is the default), and it is
possible to scan though the data, either individual cross-sections or
with whole window's being averaged up for each plot.

\end{description}

If you are unfamiliar with LONGSLIT you should first read
sections~\ref{long.id}, \ref{long.mb} and \ref{long.out}.
If you get stuck try looking at section~\ref{long.main}.

\subsubsection{Line Identification}
\label{long.id}

Since ARC2D and ARCSDI have much in common with LONGSLIT as regards
line selection, we will only describe this part once, noting the
differences when they occur.

\begin{quote}\begin{verbatim}
$ LONGSLIT
This version of LONGSLIT is from $1$DRB3:[FIGPACK.TWODSPEC.TWODSPEC]
(IMage) Name of image for input [1052-5VIG] -
1052-5VIG[2040,171] HH 1-2 HALPHA/[N II]

=========< A r c _ O p t s >=========

New    : Set up line identifications from scratch
Repeat : Use existing line identifications
Clone  : Use line identifications from another file
Fit option [REPEAT] - NEW
\end{verbatim}\end{quote}
The options here are:
\begin{description}
\item[NEW] Used on a file LONGSLIT has not been used on before, a
.RES structure is created and the user is asked to locate and identify
the lines.
\item[CLONE] As NEW, but used where line identifications are to be
copied from another file.
\item[REPEAT] Used when a .RES structure already exists, that is if
LONGSLIT (or ARC2D) has already been run on the file.
\end{description}
NEW mode is needed to set up line identifications.
\begin{quote}\begin{verbatim}
(MAXLines) Maximum number of lines to allow room for [10] -
\end{verbatim}\end{quote}
This only affects the size of the arrays to be used, so you are not
committed to 10 lines!
You cannot, however, later have more than the number set here.
\begin{quote}\begin{verbatim}
(MAXGauss) Maximum number of Gaussians to allow room for [5] -
\end{verbatim}\end{quote}
Similarly this only sets the maximum allowed number of components in a
fit.
Note that this is the only ``hard'' limit to the number of components to
be fitted when guesses are made for multiple component fits.
A look at the data should allow a sensible value to be selected, five is
a reasonable value (data rarely needs more components than this).
\begin{quote}\begin{verbatim}
(YStart) analysis lower limit [160] -
(YEnd) analysis upper limit [200] -
\end{verbatim}\end{quote}
A one-dimensional spectrum is extracted from the two-dimensional
spectrum, and used for line selection.
For LONGSLIT it is probably best to include all the data as here (the
example data has 171 cross-sections), but for ARC2D and ARCSDI a central
portion should be chosen (e.g.\ cross-sections 70--80 for a total of 171
cross-sections).
At this stage the actual lines are located by marking to either side
with a cursor.
Be sure to include some base to either side, so that the optimisation
routines can determine the base level precisely.
\begin{quote}\begin{verbatim}


Locate Line :   1
2nd tram line
Locate Line :   2
2nd tram line
Locate Line :   3
\end{verbatim}\end{quote}
A list of cursor options can be obtained by hitting the ``?' key (hitting
any key will then return to displaying the spectrum).
Note that the options change depending upon whether the display is
zoomed, and also upon whether any lines are identified (see below).
If the key ``Y'' is pressed, this causes the program to set a new
maximum Y value for the display based on the current cursor position.
This is used to select weak lines, where the default display is such
that they are small on the display, due to the presence nearby of much
stronger line(s).
The remaining lines are located.

Alternatively we could have used an automatic line-finding algorithm, by
hitting key A. This option is available only while no lines have been found,
or those found have only been located using this option (in which case the
previously identified lines are discarded if the option is selected).
This uses a routine adapted from SPICA, the predecessor to FIGARO.
The automatic line finding algorithm checks for a
minimum acceptable height for a line and for the values of the data
dropping off to either side. It then finds the centroids and checks
that the possible lines are above a minimum width. A check is also made
for the line's being sufficiently above the noise, and for not being a
side-peak to another line.

The lines have now been located, LONGSLIT and ARC2D now need to know the
identifications of these lines.
Line lists are provided and users may create their owns lists.
Alternatively the ``ID'' option in the ``Identification Menu'' can be
used, in which a wavelength and name are used as given.
Since ARCSDI only aims to straighten the lines, you would leave ARCSDI
here.
\begin{quote}\begin{verbatim}

=========< L i n e   L i s t   M e n u >=========

Emission   : Emission lines
Absorption : Absorption lines
Neon       : Neon arc
Cuar       : Copper-argon arc
Helium     : Helium arc
Iron       : Iron arc
SKy        : Sky lines
STored     : User supplied list in file
Ok         : All tables read in
Line List Menu - e

 Emission line list.
 Current version August 1990
  78 lines read from EMISSION
\end{verbatim}\end{quote}
The user can create his/her own file for use here.
No responsibility is accepted for the wavelengths of the lines in the
lists supplied.
The format is as below, and the files can be created using an editor.
The header consists of a number (as many as you want) of lines with an
asterix in the first column, the only thing that is done with
these is that they are written to the terminal (without the asterix).
Blank lines are ignored, and all remaining lines are taken as data lines.
The data lines have the wavelength (read free-format) followed by the
(optional) name, the array for the names is character*10 line\_name(number
of line slots).
These files can be placed in the default directory (the program will
search there first, so they can have the same names as those supplied).
The first few lines of the emission line list supplied are given here:
\begin{quote}\begin{verbatim}
* Emission line list.
* Current version August 1990

 3726.1600 [OII]
 3728.9100 [OII]
 4101.0000 HDELTA
 4276.8300 [FeII]
 4287.4000 [FeII]
 4319.6200 [FeII]
\end{verbatim}\end{quote}
It is possible to specify the FIGARO .ARC files, as alternatives, but if
there is a file included with TWODSPEC with the same name, excluding the
extension, then the extension must be given (note that most of these do
not include line names, so these will be left blank).
When you have read in all the required line lists, you reply ``ok'' to
the above menu.
\begin{quote}\begin{verbatim}

L I N E   I D E N T I F I C A T I O N
-------------------------------------
Identify line number  1
\end{verbatim}\end{quote}
The line with some of the surrounding spectrum is displayed so the user
can, it is hoped, recognise the line!
If the displayed width is not satisfactory, it can be changed using the
WIDTH option of the menu shown below.
A default response is given which is the value of the X array at the
peak in intensity within the line boundaries.
If, however, you already have two or more lines identified, the default
will be interpolated or extrapolated from these (again using the
positions of the maximum intensities within the line boundaries).
It is possible to leave lines unidentified, although at present the only
way to get back to the menu is by identifying at least one more
line---you can edit the line list, but you then don't have the advantage
of the interpolation to help guessing (the fitting of the dispersion
relationship in ARC2D can be used to help line identification).
You can keep looping around the line list, identifying the lines in any
order.
\begin{quote}\begin{verbatim}

=========< I d e n t i f i c a t i o n   M e n u >=========

[number]               : Line wavelength
Width         [number] : Redraw with different width
Next                   : Go to next line
Display                : Start/stop displaying tables of wavelengths
Id [wavelength] [name] : Give line ID/wavelength (just taken as given)
Quit                   : Exit leaving lines unidentified
Identification Menu [ 6548.4] -
6548.1001 [NII]6548 ok? [YES]
Identify line number  2

=========< I d e n t i f i c a t i o n   M e n u >=========

[number]               : Line wavelength
Width         [number] : Redraw with different width
Next                   : Go to next line
Display                : Start/stop displaying tables of wavelengths
Id [wavelength] [name] : Give line ID/wavelength (just taken as given)
Quit                   : Exit leaving lines unidentified
Identification Menu [ 6563.1] -
6562.8169 HALPHA ok? [YES]
\end{verbatim}\end{quote}
When the program has 2 or more lines displayed, it will give a guess to
remaining lines.
The ``fit'' is a Chebyshev polynomial, then converted to a ``normal''
polynomial (a warning is given if the line is outside the limits of the
Chebyshev polynomial).
The ``interpolated'' wavelength is obtained by using spline
interpolation (up to 3rd order).
The wavelength required here is the {\em REST} wavelength of the line.

If you enter just a wavelength the nearest line in the lists will be
located, and you will be asked if this is what you want (as shown).

\begin{quote}\begin{verbatim}
Identify line number  3
Warning, line outside Cheby limits
Fit (order=1) : 6583.45
Interpolated : 6583.45

=========< I d e n t i f i c a t i o n   M e n u >=========

[number]               : Line wavelength
Width         [number] : Redraw with different width
Next                   : Go to next line
Display                : Start/stop displaying tables of wavelengths
Id [wavelength] [name] : Give line ID/wavelength (just taken as given)
Quit                   : Exit leaving lines unidentified
Identification Menu [ 6583.4463] -
6583.6001 [NII]6584 ok? [YES]
Identify line number  4
Warning, line outside Cheby limits
Fit (order=2) : 6721.03
Interpolated : 6719.23

=========< I d e n t i f i c a t i o n   M e n u >=========

[number]               : Line wavelength
Width         [number] : Redraw with different width
Next                   : Go to next line
Display                : Start/stop displaying tables of wavelengths
Id [wavelength] [name] : Give line ID/wavelength (just taken as given)
Quit                   : Exit leaving lines unidentified
Identification Menu [ 6719.23] -
6716.4702 [SII]6717 ok? [YES]
Identify line number  5
Warning, line outside Cheby limits
Fit (order=3) : 6730.11
Interpolated : 6730.57

=========< I d e n t i f i c a t i o n   M e n u >=========

[number]               : Line wavelength
Width         [number] : Redraw with different width
Next                   : Go to next line
Display                : Start/stop displaying tables of wavelengths
Id [wavelength] [name] : Give line ID/wavelength (just taken as given)
Quit                   : Exit leaving lines unidentified
Identification Menu [ 6730.5752] -
6730.8501 [SII]6731 ok? [YES]

 C U R R E N T   L I N E   L I S T
 _________________________________
line No     identification      wavelength
   1           [NII]6548          6548.1001
   2           HALPHA             6562.8169
   3           [NII]6584          6583.6001
   4           [SII]6717          6716.4702
   5           [SII]6731          6730.8501
Edit line list? [NO]
\end{verbatim}\end{quote}
Errors can be corrected using this edit option.
The user now reaches the {\it Main Menu}:

\begin{quote}\begin{verbatim}
Current value of iteration is   1

=========< M a i n   m e n u >=========

ADd       : Add more lines
It        : Change iteration
TEmplates : Input data for template based fits
AUto      : Fixed window & fits type menu (use DEFINE to setup models)
Manual    : Interactive setup of window & fits
Define    : Define fits for AUTO
EDit      : Edit/look at results structures
TOls      : Apply tolerances
Output    : Create output plots, etc.
SKy       : Sky subtraction and data vignetting corrections
SYnthetic : Generate synthetic spectra from input data and/or fits
EXit      : Leave program
Main menu [MANUAL] -
\end{verbatim}\end{quote}

Once you have located and identified lines in one file, this information
may be {\em CLONED} to another file.
A powerful feature of ARC2D and LONGSLIT
is the ability to {\em CLONE} a set of line locations from another
file, rather than, or in addition to (but preceding), the normal line
selection. For this, the two arcs are plotted one above the other
and, from three points marked by the user on each
arc, the locations of the lines are interpolated for the new arc. The
wavelengths and identifications are then also copied over. This means
that if two spectra of the same wavelength range are to be used,
without co-adding, then the whole process of line identification can be
circumvented. If the spectra are very similar, then the information can
be copied straight over, even in batch mode.
The parts of the images to be extracted for the plots are prompted for.

\subsubsection{Note on Use of LONGSLIT for Large Data Sets}

When using LONGSLIT to create a .RESULTS structure for a file
resulting from an instrument such as TAURUS the following points should
be born in mind to avoid hitting system limits:
\newcounter{warnings}
\begin{list}{(\roman{warnings})}{\usecounter{warnings}}
\item The data array is always mapped with data type real.
\item Two arrays are created, of type real, with the dimensions of
(3$\times$(number of Gaussians allowed for) + 6) $\times$ (number of
lines) $\times$ (dimensions of data excluding wavelength dimension).
This can be quite large!
\end{list}

\subsubsection{MAIN MENU}
\label{long.main}

This controls the interactive use of LONGSLIT, apart from the initial
line identification.

The options are:
\begin{description}
\item[ADD] Add more lines
\item[IT] Change iteration.
This allows reduction of iteration, in order to allow previous fits to
be over-ridden (they can also be over-ridden in MANUAL mode fitting).
\item[AUTO] In this mode one defines fits beforehand, and then goes
though the lines and cross-sections in order performing the fits.
This is the mode of fitting in batch, and ideally should not be carried
out interactively.
\item[MANUAL] In this mode one does the fits in whatever order is
preferred.
This can also be used for checking fits performed in batch mode.
The line selected is indicated above the menu.
Initially this is line one in the list.
MANUAL mode is also driven by a menu:\\
\begin{description}
\item[Line name] to go to that line.
By stating the line name, or any unique abbreviation, the line will be
set to that selected.
\item[LAST] Move back to previous line in line list.
\item[NEXT] Move on to next line in line list.
\item[TRIM] Limit the range of cross-sections.
This is useful for altering the block start/end in window mode (see
below)
\item[CHECK] This displays 20 profile plots at a time.
At present this will start at the current line, cross-section one, and
scan through.
When a screen is full, the user is asked whether he/she wants to see the
next plots.
This is designed for a quick look, before more detailed checking and
re-fitting, if that is required.
It is also useful to provide a convenient record of the line profiles to
refer to, in which case the hardcopy option should be used. Note that
if the ``quick plot'' option here (which gives plots of similar quality
to softcopy plots) is not used the plots files can be {\em very} large.
\item[FIT] Fit this window as it stands
\item[ADVANCE] Go to next WINDOW
\item[BACK] Go to previous window
\item[SEE] Look at individual elements of window
\item[WIDTH] Change window width
\item[SCAN] Scan through windows
\item[OLD] Start/stop plotting old fits
\item[DELETE] Delete fits in this range
\item[HARD] Produce hardcopy of data with fit
\item[EXIT] Return to main menu
\end{description}
\item[DEFINE] Define fits for AUTO mode, for batch or interactive use.
This defines fits, either for all the lines at a time, or for each line
separately.
To define fits differently within a line, use MANUAL mode as described
above.
\item[LOOK] Look at values of data cube.
This just lists the values in the results array.
\item[TOLS] Apply tolerances.
This will reject fits which have, for example, too great a width.
\item[OUTPUT] These options are the main ways of producing output
from the program.
Selection of this puts the user into the ``OUTPUT MENU'', with the
following options:\\
\begin{description}
\item[HARDCOPY] Produce plots of profile fits.
The line profile is displayed, together with the fit.
It is possible to preview the fits first in softcopy (unless all fits
are being plotted, using the option ``plot whole cube'').
If the option ``plot whole cube?'' is not being used, then the X-axis of
the plots can be in km\,s$^{-1}$ or {\AA}ngstroms, otherwise they are in
{\AA}ngstroms.
\item[TABLE] Print table of profile parameters
\item[SHAPE] Analyse the shape of the lines.
This will give plots and tables of the Whittle and Heckman asymmetry
parameters.
The profiles must have been fitted with Gaussians etc., and the same
blocking is used here as for fitting.
\item[PLOT] Plot rotation curves.
Also width v. cross-section and flux v. cross-section plots can be made.
All these can be made in softcopy or hardcopy.
Also a plot of average velocities v. cross-section will be plotted, if
the plot is in hardcopy, and there is more than one line.
\item[STATIC] Write results as EXTATIC FIGARO image.
EXTATIC is a statistics program.
\item[PRINT] Print rotation curves
\item[RATIO] Evaluate line ratios
\item[GREY] Produce greyscale plot of data.
This is for one line at a time, using the wavelength limits for the line
from the initial line location.
A velocity scale is plotted on the X-axis.
\item[CONTOUR] This is similar to the GREY option, but uses contours
rather than a greyscale image.
\item[FULL] This produces a table suitable for input to a FORTRAN
program, including the information from both PRINT and PLOT.
\item[CHECK] This produces an array of line profiles to which successful
(or NAG error failed fits if allowed) have been made, with the fits
superimposed.
\item[SOFT] This says whether plots are in softcopy (as opposed to
hardcopy). This only affects the PLOT, CONTOUR and CHECK options.
\item[VELOCITY] Plot with velocity scale for X axis. This affects the PLOT
and HARDCOPY options above.
\item[EXIT] Exit and accept current setting of the options.
When any selected plots etc.\ have been carried out, the user is
returned to the main menu.

If the PLOT option is selected the user will be presented with the
following menu, to select precisely which plots are required:\\
\begin{description}
\item[VELOCITY] Radial velocities
\item[WIDTH] Widths
\item[FLUX] Fluxes
\item[AVERAGE] Average velocities
\item[ALL] Velocities on 1 plot
\item[EXIT] Exit menu and perform plotting
\end{description}
\end{description}
The output options (except GREY and RATIO) can be given in the command
line as keywords.
An example of the use of the OUTPUT option is given in
section~\ref{long.out}.
\item[SKY] Not yet implemented
\item[SYNTHETIC] Not yet implemented
\item[EXIT] Leave program
\item[CUBAN] Display array of profiles (3-dimensional data only), and move
around under cursor control. This is the same as the option in the main menu
of FIBDISP (see below).
\end{description}

\subsubsection{Use of LONGSLIT in Batch Mode}

Since fitting Gaussians takes a fair time it is desirable to use
LONGSLIT in batch mode wherever possible.
To do so the lines must first be located as described in
section~\ref{long.id}.
The desired fit type must then be defined, by selecting the DEFINE
option at the main menu, or alternatively the parameter FIT\_TYPE can be
used (in which case the model can be set in batch).
To fit a data set using skew Gaussians in batch, a command file
containing the line
\begin{quote}\begin{verbatim}
$ LONGSLIT RUN1 REPEAT 1 386 5 0 FIT_MODEL=SK
\end{verbatim}\end{quote}
might be used.

We will assume however that the fit model is to be defined
interactively, in which case the user is then asked if the same
model is to be used for all lines (if there is more than one line
present), and to give the required model(s).
In the following example all lines are to be fitted with double
Gaussians (``untied'' means that the two Gaussians are independent of
each other).

\begin{quote}\begin{verbatim}

=========< M a i n   m e n u >=========

ADd       : Add more lines
It        : Change iteration
TEmplates : Input data for template based fits
AUto      : Fixed window & fits type menu (use DEFINE to setup models)
Manual    : Interactive setup of window & fits
Define    : Define fits for AUTO
EDit      : Edit/look at results structures
TOls      : Apply tolerances
Output    : Create output plots, etc.
SKy       : Sky subtraction and data vignetting corrections
SYnthetic : Generate synthetic spectra from input data and/or fits
EXit      : Leave program
Main menu [MANUAL] - DEFINE
Use same fit model for all lines? [YES] YES

=========< F i t   M e n u >=========

GUess       : Alter guessing ...... Multiples - SCRATCH, Singles - MAXIMUM
BAse        : Alter base model .................................. CONSTANT
BImodal     : Test for bimodality before fitting
Instant     : Perform fits immediately
ABsorption  : Fit ABSORPTION lines
TEsts       : Alter tests for auto ......... AIC after fits (max cmps 5)
Routines    : Select fitting routines ......... Sing -LM, Dble - N1, Mult - LM
Nofit       : Don't fit this position
GAussian    : Fit Gaussian(s) ...................................... *******
SKew        : Fit a single Skew Gaussian
Cauchy      : Fit a single Cauchy function
Lorentzian  : Fit Lorentzian(s)
SIngle      : Fit single(s)
TSep        : Fit two Gaussians with fixed separation
TWidth      : Fit two Gaussians with fixed width ratio
THeight     : Fit two Gaussians with fixed height ratio
Double      : Fit double(s)
Manual      : Perform manual fitting
AUto        : Automatic multiple Gaussian/Lorentzian ............. *******
Pcygni      : P Cygni profile
TRansfer %F : Start from fit to another line
Exit        : Exit menu and perform any fitting
Fit Menu [EXIT] - DO
\end{verbatim}\end{quote}
Normally you would fit lines with the same model along all of
the slit length, but in the following case line 2 is to be fitted with a
double Gaussian (unconstrained) for the first 90 cross-sections, and a
skew Gaussian for the rest.
The default fit type is a Gaussian with a base.

DEFINE can be used if you want to fit a whole line with a given type of
fit e.g.\ a skew Gaussian.
The ADD option puts you back into the line selection as above, but
without the option of automatic line location.
The option is given to edit the line list again.
\begin{quote}\begin{verbatim}


Lines identified:
[NII]6584 (6583.6)       [NII]6548 (6548.1)       HALPHA (6562.817)

Enter starting line number [1] - 2
(YBlock) Analysis x-sect width [5] - 90
Number of blocks = 2
Enter starting cross-section number [1] -
\end{verbatim}\end{quote}
The line is displayed.

If a previous fit has been made on the same data, this is displayed
at this stage (but this can be suppressed).
\begin{quote}\begin{verbatim}
No previous fit

=========< M a n u a l   M o d e >=========

[name]  : Line name to go to that line
Last    : Move back to previous Line
Next    : Move on to next line in list
Trim    : LIMIT the range of X-sects
Check   : Check previous fits (20 to a screen)
FIt     : Fit this window as it stands
Advance : Go to next WINDOW
Back    : Go to previous window
SEe     : Look at individual elements of window
Width   : Change window width
SCan    : Scan through windows
Old     : Start/stop plotting old fits
Delete  : Delete fits in this range
Hard    : Produce hardcopy of data with fit
Exit    : Return to main menu
Manual Mode [FIT] -

=========< F i t   M e n u >=========

GUess       : Alter guessing ........... Multiples - SCRATCH, Singles - MAXIMUM
BAse        : Alter base model ....................................... CONSTANT
BImodal     : Test for bimodality before fitting
Instant     : Define fits only
ABsorption  : Fit ABSORPTION lines
TEsts       : Alter tests for auto ................ AIC after fits (max cmps 5)
Routines    : Select fitting routines ......... Sing - N1, Dble - N1, Mult - N2
Nofit       : Don't fit this position
GAussian    : Fit Gaussian(s) ......................................... *******
SKew        : Fit a single Skew Gaussian
Cauchy      : Fit a single Cauchy function
Lorentzian  : Fit Lorentzian(s)
SIngle      : Fit single(s) ........................................... *******
TSep        : Fit two Gaussians with fixed separation
TWidth      : Fit two Gaussians with fixed width ratio
THeight     : Fit two Gaussians with fixed height ratio
Double      : Fit double(s)
Manual      : Perform manual fitting
AUto        : Automatic multiple Gaussian/Lorentzian
Pcygni      : P Cygni profile
TRansfer    : Start from fit to another line
Exit        : Exit menu and perform any fitting
Fit Menu [EXIT] - D

=========< F i t   M e n u >=========

GUess       : Alter guessing ........... Multiples - SCRATCH, Singles - MAXIMUM
BAse        : Alter base model ....................................... CONSTANT
BImodal     : Test for bimodality before fitting
Instant     : Define fits only
ABsorption  : Fit ABSORPTION lines
TEsts       : Alter tests for auto ................ AIC after fits (max cmps 5)
Routines    : Select fitting routines ......... Sing - N1, Dble - N1, Mult - N2
Nofit       : Don't fit this position
GAussian    : Fit Gaussian(s) ......................................... *******
SKew        : Fit a single Skew Gaussian
Cauchy      : Fit a single Cauchy function
Lorentzian  : Fit Lorentzian(s)
SIngle      : Fit single(s)
TSep        : Fit two Gaussians with fixed separation
TWidth      : Fit two Gaussians with fixed width ratio
THeight     : Fit two Gaussians with fixed height ratio
Double      : Fit double(s) ........................................... *******
Manual      : Perform manual fitting
AUto        : Automatic multiple Gaussian/Lorentzian
Pcygni      : P Cygni profile
TRansfer    : Start from fit to another line
Exit        : Exit menu and perform any fitting
Fit Menu [EXIT] -

=========< M a n u a l   M o d e >=========

[name]  : Line name to go to that line
Last    : Move back to previous Line
Next    : Move on to next line in list
Trim    : LIMIT the range of X-sects
Check   : Check previous fits (20 to a screen)
FIt     : Fit this window as it stands
Advance : Go to next WINDOW
Back    : Go to previous window
SEe     : Look at individual elements of window
Width   : Change window width
SCan    : Scan through windows
Old     : Start/stop plotting old fits
Delete  : Delete fits in this range
Hard    : Produce hardcopy of data with fit
Exit    : Return to main menu
Manual Mode [FIT] - ADVANCE
\end{verbatim}\end{quote}
The line is displayed.
\begin{quote}\begin{verbatim}
No previous fit

=========< F i t   M e n u >=========

GUess       : Alter guessing ........... Multiples - SCRATCH, Singles - MAXIMUM
BAse        : Alter base model ....................................... CONSTANT
BImodal     : Test for bimodality before fitting
Instant     : Define fits only
ABsorption  : Fit ABSORPTION lines
TEsts       : Alter tests for auto ................ AIC after fits (max cmps 5)
Routines    : Select fitting routines ......... Sing - N1, Dble - N1, Mult - N2
Nofit       : Don't fit this position
GAussian    : Fit Gaussian(s) ......................................... *******
SKew        : Fit a single Skew Gaussian
Cauchy      : Fit a single Cauchy function
Lorentzian  : Fit Lorentzian(s)
SIngle      : Fit single(s) ........................................... *******
TSep        : Fit two Gaussians with fixed separation
TWidth      : Fit two Gaussians with fixed width ratio
THeight     : Fit two Gaussians with fixed height ratio
Double      : Fit double(s)
Manual      : Perform manual fitting
AUto        : Automatic multiple Gaussian/Lorentzian
Pcygni      : P Cygni profile
TRansfer    : Start from fit to another line
Exit        : Exit menu and perform any fitting
Fit Menu [EXIT] - SK
\end{verbatim}\end{quote}
Just reply with ``EXIT'' from now on to leave the program.
\begin{quote}\begin{verbatim}

=========< M a n u a l   M o d e >=========

[name]  : Line name to go to that line
Last    : Move back to previous Line
Next    : Move on to next line in list
Trim    : LIMIT the range of X-sects
Check   : Check previous fits (20 to a screen)
FIt     : Fit this window as it stands
Advance : Go to next WINDOW
Back    : Go to previous window
SEe     : Look at individual elements of window
Width   : Change window width
SCan    : Scan through windows
Old     : Start/stop plotting old fits
Delete  : Delete fits in this range
Hard    : Produce hardcopy of data with fit
Exit    : Return to main menu
Manual Mode [FIT] - EXIT
 Fits defined =    2
Current value of iteration is   1

=========< M a i n   m e n u >=========

ADd       : Add more lines
It        : Change iteration
TEmplates : Input data for template based fits
AUto      : Fixed window & fits type menu (use DEFINE to setup models)
Manual    : Interactive setup of window & fits
Define    : Define fits for AUTO
EDit      : Edit/look at results structures
TOls      : Apply tolerances
Output    : Create output plots, etc.
SKy       : Sky subtraction and data vignetting corrections
SYnthetic : Generate synthetic spectra from input data and/or fits
EXit      : Leave program
Main menu [MANUAL] - EXIT
\end{verbatim}\end{quote}

You have now left the program

The setting up of lines in ARCSDI is similar to the above, except that
the program is exited after the ``OK?'' question above (before line
identification).

The program should then be submitted as a batch job, the following
command line will fit cross-sections 1 to 180 blocking together in
blocks of 2.
REPEAT indicates that the lines are already identified---as above---the
0 is iteration, this may be used later, but should be set to zero.
Always use REPEAT mode when you have already identified one or more
lines.
\begin{quote}\begin{verbatim}
$ LONGSLIT T4 REPEAT 1 180 2 0
\end{verbatim}\end{quote}
The following is a complete command file to run longslit in batch,
in this case REPEAT is abbreviated to R.
\begin{quote}\begin{verbatim}
$ FIGARO
$ SET DEFAULT SCRATCH:[TNW]
$ LONGSLIT T4 R 1 180 2 0
$ EXIT
\end{verbatim}\end{quote}
To run any FIGARO function in batch, it will probably be found
convenient to use the program BATCH, in (see section~\ref{batch}).

\paragraph{Fitting Multiple Components in Batch}
\label{long.mb}

If LONGSLIT, when in batch mode, comes across a fit defined as multiple,
it will attempt to fit it with as many lines as it thinks are needed!
It uses the tolerance settings for minimum and maximum width, and
minimum height.
These are used for the guesses and the final answers, and will probably
need some tweaking.
Also it will of course still need checking afterwards.
It may well have more than one attempt at fitting. This checking is
easily carried out using the CHECK option in MANUAL mode---it
is best to use this to produce hardcopy plots which can be kept beside
the terminal while you go through checking (you may find 20\% of the
profiles need re-fitting, but this will depend upon the line profiles,
as well as the values of the tolerances). You may also need to
investigate cross-sections for which no fit is given---this may be
because there is no emission at that point, or the fit may have failed
for some reason, even with strong emission (it may guess too many
components for example, and then crash in the fitting).
Note that LONGSLIT outputs to the batch log file the reason why the next
component was not accepted when it searches for components. The log file
will also contain details of any re-fitting.
Thus if the fits are not acceptable after the first attempt at using
this, the user can inspect the log file and alter the tolerances
accordingly.
Because of this information the batch log file can get quite large (the
log file can be made smaller if the keyword PRFITS is set to false, but
there is then no information about the fitting).

The keyword AIC determines whether Akaike's information criterion
(Akaike 1973) is used during batch multiple fitting to decide how many
components to fit.
This is true by default.
The guesses are made in the same way, whether or not this is true,
except that the widths are allowed to be a little larger---in both cases
the guesses to the widths are allowed to the value in the tolerances
array, to allow for errors in guessing (the fits are only accepted if
they are actually within the tolerances), this  ratio is larger if AIC
is true.
The value of the criterion is then calculated, and the best value is
selected after progressively removing components to the fit.
Thus if the guessing produces three components, the program will also
``see what happens'' with two components, one component and just a base.
This is fairly heavy on cpu usage (it can take several hours cpu), and
the batch log file is even larger than if AIC is false (unless PRFITS is
false).
The criterion is
\[
C = M \times \ln S + 2 \times N
\]
where $C$ is Akaike's information criterion, $M$ is the number of data
points, $S$ is the weighted sum of squares, and $N$ is the number of fit
parameters.
The fit with the lowest value of $C$ is used.
For fitting the tolerances should be as close as possible to the
expected values of the parameters.

The user is {\em strongly} encouraged to learn to use this option, since
it can give considerable time savings.

If the keyword BOUNDS is specified then the fits will be bounded on
widths as in the TOLS array, all other parameters will be bounded
within the window.

N.B. To refit a cube which has already been fitted, but the values
changed, e.g.\ flux-calibrated, use the keyword COPY, which will use
scaled existing values as first guesses, repeating all the fits in the
``cube''.
If used with CLONE, it enables similar spectra to be easily fitted.

\paragraph{Tolerances in Batch}

Tolerances can be applied in batch.
The values must be set up beforehand, the tolerances to be used are
selected by name (as when using normally), using a colon to delimit more
than one name (for names and description of tolerances, see
section~\ref{sec.tols}).
The parameter TOLS is used, set it ``HEIGHT:S/N'' to for example to
apply tolerances on height and signal to noise.

When tolerances are used like this, the current values of the tolerances
are used-there is no facility in LONGSLIT to change their values in
batch (although the FIGARO function LET can be used). This facility is
also available in ARC2D, ARCSDI and COMB (although not all the
tolerances are meaningful).

The user may wish to refit weaker areas with a larger blocking, and the
use of tolerances allows such fits to be rejected to allow this.
The user would then run LONGSLIT again to perform the fits at the new
blocking, and this process may be repeated several times, gradually
increasing the blocking.

\paragraph{Other Facilities in Batch Mode}

The output options are also available in batch (see
sections~\ref{long.out} and~\ref{long.fit}).
Cloning is available in batch, but only if no shift is required (i.e.
the lines can be copied straight over).

\subsubsection{Fitting Gaussians etc. Interactively}

The main method of fitting these is the WINDOW option of MANUAL mode.
First select MANUAL at the main menu.
\begin{quote}\begin{verbatim}

=========< M a i n   m e n u >=========

ADd       : Add more lines
It        : Change iteration
TEmplates : Input data for template based fits
AUto      : Fixed window & fits type menu (use DEFINE to setup models)
Manual    : Interactive setup of window & fits
Define    : Define fits for AUTO
EDit      : Edit/look at results structures
TOls      : Apply tolerances
Output    : Create output plots, etc.
SKy       : Sky subtraction and data vignetting corrections
SYnthetic : Generate synthetic spectra from input data and/or fits
EXit      : Leave program
Main menu [MANUAL] - MANUAL
\end{verbatim}\end{quote}
Say which line you wish to start working on, give the required blocking
(number of cross-sections added together) and the starting
cross-section.
\begin{quote}\begin{verbatim}

Lines identified:
[NII]6584 (6583.6)       [NII]6548 (6548.1)       HALPHA (6562.817)

Enter starting line number [1] - 3
(YBlock) Analysis x-sect width [5] - 10
Number of blocks = 2
Enter starting cross-section number [1] - 110
\end{verbatim}\end{quote}
The line is displayed.
This then puts you into MANUAL menu. Select FIT and then select the fit
type required.
MG allows the user to set the guesses required for
optimisation, and to constrain the fits if required. TG allows the
ratio of the heights or widths, or the separation of the lines, to be
fixed. The other fit types are entirely automatic. Note that the
``Cauchy'' function is actually a function which varies smoothly
between a true Cauchy function and a Gaussian
(equation~\ref{eq.varcauchy}).
\begin{quote}\begin{verbatim}
No previous fit

=========< M a n u a l   M o d e >=========

[name]  : Line name to go to that line
Last    : Move back to previous Line
Next    : Move on to next line in list
Trim    : LIMIT the range of X-sects
Check   : Check previous fits (20 to a screen)
FIt     : Fit this window as it stands
Advance : Go to next WINDOW
Back    : Go to previous window
SEe     : Look at individual elements of window
Width   : Change window width
SCan    : Scan through windows
Old     : Start/stop plotting old fits
Delete  : Delete fits in this range
Hard    : Produce hardcopy of data with fit
Exit    : Return to main menu
Manual Mode [FIT] -

=========< F i t   M e n u >=========

GUess       : Alter guessing ........... Multiples - SCRATCH, Singles - MAXIMUM
BAse        : Alter base model ....................................... CONSTANT
BImodal     : Test for bimodality before fitting
Instant     : Define fits only
ABsorption  : Fit ABSORPTION lines
TEsts       : Alter tests for auto ................ AIC after fits (max cmps 5)
Routines    : Select fitting routines ......... Sing - N1, Dble - N1, Mult - N2
Nofit       : Don't fit this position
GAussian    : Fit Gaussian(s) ......................................... *******
SKew        : Fit a single Skew Gaussian
Cauchy      : Fit a single Cauchy function
Lorentzian  : Fit Lorentzian(s)
SIngle      : Fit single(s) ........................................... *******
TSep        : Fit two Gaussians with fixed separation
TWidth      : Fit two Gaussians with fixed width ratio
THeight     : Fit two Gaussians with fixed height ratio
Double      : Fit double(s)
Manual      : Perform manual fitting
AUto        : Automatic multiple Gaussian/Lorentzian
Pcygni      : P Cygni profile
TRansfer    : Start from fit to another line
Exit        : Exit menu and perform any fitting
Fit Menu [EXIT] - MG

=========< F i t   M e n u >=========

GUess       : Alter guessing ........... Multiples - SCRATCH, Singles - MAXIMUM
BAse        : Alter base model ....................................... CONSTANT
BImodal     : Test for bimodality before fitting
Instant     : Define fits only
ABsorption  : Fit ABSORPTION lines
TEsts       : Alter tests for auto ................ AIC after fits (max cmps 5)
Routines    : Select fitting routines ......... Sing - N1, Dble - N1, Mult - N2
Nofit       : Don't fit this position
GAussian    : Fit Gaussian(s) ......................................... *******
SKew        : Fit a single Skew Gaussian
Cauchy      : Fit a single Cauchy function
Lorentzian  : Fit Lorentzian(s)
SIngle      : Fit single(s)
TSep        : Fit two Gaussians with fixed separation
TWidth      : Fit two Gaussians with fixed width ratio
THeight     : Fit two Gaussians with fixed height ratio
Double      : Fit double(s)
Manual      : Perform manual fitting .................................. *******
AUto        : Automatic multiple Gaussian/Lorentzian
Pcygni      : P Cygni profile
TRansfer    : Start from fit to another line
Exit        : Exit menu and perform any fitting
Fit Menu [EXIT] - MG
\end{verbatim}\end{quote}
We choose MG for this example. The program plots the line profile with
the current guesses (either taken from a previous fit or ``guessed''
from scratch).
\begin{quote}\begin{verbatim}
HALPHA (line = 2)
Previous fit
Single emission    Gaussian with base
successful
Will fit a multiple Gaussian
Number of gaussians in previous fit = 2
Guesses:-
Component     Centre        fwhm       Height    Base
   1       6564.599       0.8904        255.1        2.934
   2       6561.983        1.485        23.98
\end{verbatim}\end{quote}
The user is free to alter the parameters and to add or delete
components. When satisfactory, the parameters are optimised. Hit ``F''
or click in the rectangle labelled ``FIT''. If the fit routine used is
E04KDF (N2 in the menus), then you have the option of a constrained fit:
\begin{quote}\begin{verbatim}

=========< B o u n d s   M e n u >=========

None     : No Bounds (unconstrained)
Equal    : All L_bounds/U_bounds same
Positive : all bounds >= 0
General  : Supply all bounds
Fix      : Fix parameters
Window   : Bound inside current window
Lock     : Lock order of centres of components
Bounds Menu [NONE] -
\end{verbatim}\end{quote}
The default of no bounds is selected in this case.
\begin{quote}\begin{verbatim}
Fitting 2 Gaussians
Final values & errors :-
Component     Centre        fwhm       Height    Base
   1       6564.625       0.9160        251.3        23.05
          6.6107502E-03   1.4509E-02    3.816       0.6862
   2       6562.003        1.781        65.55
          3.7648823E-02   7.1038E-02    2.682

=========< F i t   O k >=========

Store  : Store the fit in the cube
Repeat : Repeat fit
Quit   : Abandon this fit
Fit Ok [STORE] - STORE
redraw current data? [NO]
\end{verbatim}\end{quote}

\subsubsection{Background Models}

The default model is a flat background, which is quite sufficient for
many objects such as many emission nebulae where there is not a strongly
varying background.
For some objects, however, this will not be sufficient, and allowance
must be made for a varying background (e.g.\ a stellar continuum).
LONGSLIT provides three solutions to this problem
\newcounter{backopts}
\begin{list}{\bf\arabic{backopts} ---}{\usecounter{backopts}}
\item
Fit a Chebyshev polynomial to the base immediately before fitting the
line profile.
The polynomial it fitted to the whole of the spectrum, except for those
parts within the boundaries of the identified lines.
The order for the polynomial is stored in the results block, and this is
used to re-create the fit as required (e.g.\ for profile plots).
Although this cuts down on storage requirements, it does mean that if
you identify any further lines, these will cause the program to make
incorrect assumptions.
For this the blocking is, of course the same as for the profile
fitting.
The order can be varied as required for different lines.
If this option is used, it is essential that the spectrum is not trimmed
(subsetted) too close to the line.
\item
Use a cubic spline to interpolate the values in the range of fitting.
As with the Chebyshev polynomial the blocking is as for the fit, and the
areas within the line boundaries are not used to constrain the splines.
\item
Use Chebyshev polynomials, but perform their fitting using FITCONT.
LONGSLIT can then pick up the coefficients (from a special coefficient
structure), and use them to obtain the base.
Since the blocking to be used by LONGSLIT is not known at this stage (by
FITCONT at least), the fits are to single cross-sections, and the
results are added when required.
The areas to be excluded from the fitting are marked with a cursor (this
is more versatile than when the fitting is performed by LONGSLIT).
\end{list}
To select a model other than a flat base, use the DEFINE option in the
main menu, in which case you will be asked for the background model.
Note that once set up, the model remains in force for fitting until the
user leaves the program.

If you select option (i) above at the MANUAL menu, LONGSLIT gets you to
decide upon the order to fit at that time---various plots are available
to help.
If you select this from DEFINE, then you be asked to decide upon the
order before the first fit, unless you have already given it the order
(in WINDOW for example).
For this reason, this option cannot yet be used in batch mode.

\subsubsection{Producing Output Plots, Tables etc.}
\label{long.out}

When all the desired fits have been done (or
earlier to get a check on them), you should used the standard FIGARO
function HARD to set the hardcopy device (like SOFT for a softcopy
device), for example
\begin{quote}\begin{verbatim}
$ HARD CANON
\end{verbatim}\end{quote}

Then enter the program and at the main menu select ``OUTPUT''. Then
enter the options required e.g.\
\begin{quote}\begin{verbatim}
......
......
Output   : Create output plots, etc.
Exit     : Leave program
Main menu [MANUAL] - OUTPUT

=========< O u t p u t   O p t i o n s >=========

Hardcopy : Plots of profile fits.............. F
Table    : Table of profile parameters........ F
SHape    : Profile shape analysis............. F
PLot     : Plot rotation curves............... F
STatic   : Write results in form for EXTATIC.. F
PRint    : Print rotation curves.............. F
Ratio    : Output line ratios (with plots).... F
Grey     : Greyscale plot of velocity......... F
COntour  : Contour plot of velocity........... F
Full     : Large table........................ F
CHeck    : Profile array...................... F
SOft     : Plot in softcopy (* above) ...... = T
Velocity : Use velocity scale (+ above) .... = T
Exit     : Exit menu
Output Options - TABLE
\end{verbatim}\end{quote}
The selection of options here is to ``toggle'' the option on or off,
so entering an option once will select it, twice will cancel the
selection. The options are performed when this menu is left.
\begin{quote}\begin{verbatim}

=========< O u t p u t   O p t i o n s >=========

Hardcopy : Plots of profile fits.............. F
Table    : Table of profile parameters........ T
SHape    : Profile shape analysis............. F
PLot     : Plot rotation curves............... F
STatic   : Write results in form for EXTATIC.. F
PRint    : Print rotation curves.............. F
Ratio    : Output line ratios (with plots).... F
Grey     : Greyscale plot of velocity......... F
COntour  : Contour plot of velocity........... F
Full     : Large table........................ F
CHeck    : Profile array...................... F
SOft     : Plot in softcopy (* above) ...... = T
Velocity : Use velocity scale (+ above) .... = T
Exit     : Exit menu
Output Options [TABLE] - PLOT

=========< O u t p u t   O p t i o n s >=========

Hardcopy : Plots of profile fits.............. F
Table    : Table of profile parameters........ T
SHape    : Profile shape analysis............. F
PLot     : Plot rotation curves............... T
STatic   : Write results in form for EXTATIC.. F
PRint    : Print rotation curves.............. F
Ratio    : Output line ratios (with plots).... F
Grey     : Greyscale plot of velocity......... F
COntour  : Contour plot of velocity........... F
Full     : Large table........................ F
CHeck    : Profile array...................... F
SOft     : Plot in softcopy (* above) ...... = T
Velocity : Use velocity scale (+ above) .... = T
Exit     : Exit menu
Output Options [PLOT] - PRINT

=========< O u t p u t   O p t i o n s >=========

Hardcopy : Plots of profile fits.............. F
Table    : Table of profile parameters........ T
SHape    : Profile shape analysis............. F
PLot     : Plot rotation curves............... T
STatic   : Write results in form for EXTATIC.. F
PRint    : Print rotation curves.............. T
Ratio    : Output line ratios (with plots).... F
Grey     : Greyscale plot of velocity......... F
COntour  : Contour plot of velocity........... F
Full     : Large table........................ F
CHeck    : Profile array...................... F
SOft     : Plot in softcopy (* above) ...... = T
Velocity : Use velocity scale (+ above) .... = T
Exit     : Exit menu
Output Options [PRINT] - EXIT

=========< P l o t   O p t i o n s >=========

Velocity : Radial velocities .. = T
Width    : Widths ............. = T
Flux     : Fluxes ............. = T
AVerage  : Average velocities . = T
ALl      : Velocities on 1 plot = T
Exit     : Exit menu
Plot Options [] - EXIT
Evaluate correction for Earth motion etc. [YES]
 DEC -6 48 0
 RA 5 33 56.8
Day of observation? [339] -
 Date:- 339/1/1984
 UT 17 51 13
 Observatory position 149 3 57.91 -31 16 37.34
\end{verbatim}\end{quote}

If the program cannot find information on the observatory, then it will
ask.
The reply can be the position, or an observatory name (type
``HELP'') for help.
Likewise the position of the object, date of observation etc. will be
prompted for if required.
Note that it is satisfactory to give the date as (for example) the 60th
day of the year, giving the month as 1, rather than giving the true
month, if this is preferred.

The velocity may be corrected to heliocentric, local standard of rest,
galactic, or local group.
This uses the STARLINK SLA library (Wallace 1990) and is based on the
STARLINK program RV (Wallace 1987).

\begin{quote}\begin{verbatim}
Is longitude ok, taking longitude as +ve west [YES] n
 V_HEL=-4.16896E+00 V_LSR= 1.40857E+01
 V_GAL= 1.33324E+02 V_LGROUP= 1.38917E+02

=========< V e l o c i t y   C o r r e c t i o n >=========

Hel     : Suns ref frame
Lsr     : Local standard of rest
GAl     : Galaxy ref frame
GRoup   : Local galactic group ref frame
Other   : Other - enter from terminal
Velocity Correction [LSR] -
OK? [YES]
Show fits which had NAG errors [NO]
Is data flux calibrated? [NO]
Printing fits
Indicate presence of failed fits [NO]

=========< F l u x   M a r k i n g >=========

No           : Don't mark points according to flux
Continuous   : Mark using a continuous scale
Three        : Mark by grading into 3 divisions
Flux Marking [CONTINUOUS] -
Plotting Velocities
Line number  1
Line number  2
Line number  3
Plotting average velocities
Plot flux v. xsect? [YES] n
Plot width v. xsect? [YES] n
Job 123 entered on queue SYS_LASER
Print of velocities
\end{verbatim}\end{quote}
Unless NOFIT was specified (in which case the user would miss out
the main menu entirely), the user is now returned to the main menu.

The option to mark the points can either put the markers so that the
most intense 3rd of the points have a filled circle, the next 3rd an
open one, and the remainder are not marked, or the area can be
proportional to the logarithm of the flux.
The latter is recommended.

Note that if NOLABEL is given in the command line the titles will be
omitted for many of the output plots.

In some cases LONGSLIT may not have been able to obtain errors on the
fits---this involves inverting a matrix.
For such cases the errors will be given as zero (obviously not a correct
value!) when the fits are listed.

\subsubsection{The Keyword FIT}
\label{long.fit}

This will cause the program to run though the fitting part of the
program, by default it is true, so may be ignored. It is, however,
possible to specify NOFIT in a command line, to run LONGSLIT in batch
without performing fits (or just to skip the middle part of the program
when running interactively) e.g.\
\begin{quote}\begin{verbatim}
$ FIGARO
$ SET DEFAULT SCRATCH:[TNW]
$ LONGSLIT T1 REPEAT TABLE NOFIT
$ EXIT
\end{verbatim}\end{quote}
This will produce a table of the fit results.
It may be used for other output options (except RATIO), e.g.\
\begin{quote}\begin{verbatim}
$ LONGSLIT T1 REPEAT PRINT TABLE PLOT HARDCOPY NOFIT VCORR=10
\end{verbatim}\end{quote}

  In batch correction may be made for the Earth's motion by using the
parameter VCORR as above, this will be subtracted from the radial
velocities calculated.

\subsubsection{TRANSFER Option}

The OL option in select fit type is for use with the TRANSFER option to
transfer fits between different lines. For this to work the fits must
be defined using the DEFINE option, before fitting in AUTO mode,
preferably in batch. The fits are copied so that one line becomes a
copy of another as regards fit type, blocking, etc., but of course the
fits are optimised on the data for the current line. If the fits are
not re-defined, then other modes will not work for that line (except
COPY). TRANSFER does NOT override masking as COPY does, and is selected
by specifying OL as the fit type.
If this option is used, all the cross-sections of a line should be so
defined.

\subsubsection{INHERIT Option}

This is an option to do any fitting in batch, provided the program has
a fit to one block, and the data is fairly continuous. Value $-1$ to
inherit from previous block, +1 to inherit from next.
The fit to this block used as the guess for the next fit.
Likewise INHERIT {\em must} be given on the command line.

\subsubsection{Use of Tolerances}
\label{sec.tols}

The option to apply tolerances interactively is selected at the main
menu.
First of all the user is asked to confirm (and alter if required) the
values of the tolerances to apply, and then the user can select which
tolerances to use.
It is possible merely to set the values, but not to apply them at the
time, for example if you wanted to set them up for use in batch mode.

In the following example the maximum allowed height is set to $10^{7}$,
and then tolerances are applied on height, centre and width.

\begin{quote}\begin{verbatim}
Main menu [MANUAL] - TOLS
Tolerances on position are relative to the line's rest wavelength

=========< E d i t   T o l e r a n c e s >=========

V_Tol %F : Absolute error of line centre .... = 1.8994141E-02
V_MAx %F : Max allowed line centre .......... = 3
V_MIn %F : Min line centre .................. = -3
W_Tol %F : Absolute error of line width ..... = 1.8994141E-02
W_MAx %F : Max allowed line width ........... = 4
W_MIn %F : Min allowed line width ........... = 0.1899414
W_S_n %F : Width signal/noise ............... = 5
H_MAx %F : Max allowed line height .......... = 1000
H_MIn %F : Min allowed line height .......... = 20
H_S_n %F : Height signal/noise .............. = 5
C_tol %F : Absolute error of Cauchy parameter = 2.0000001E-03
S_tol %F : Absolute error of Skew parameter . = 2.0000001E-03
H_Tol %F : Absolute error of height ......... = 3
Apply    : Apply tolerances now
Exit     : Exit but don't apply tolerances now
Edit Tolerances - H_MAX 1E7

=========< E d i t   T o l e r a n c e s >=========

V_Tol %F : Absolute error of line centre .... = 1.8994141E-02
V_MAx %F : Max allowed line centre .......... = 3
V_MIn %F : Min line centre .................. = -3
W_Tol %F : Absolute error of line width ..... = 1.8994141E-02
W_MAx %F : Max allowed line width ........... = 4
W_MIn %F : Min allowed line width ........... = 0.1899414
W_S_n %F : Width signal/noise ............... = 5
H_MAx %F : Max allowed line height .......... = 1.E7
H_MIn %F : Min allowed line height .......... = 20
H_S_n %F : Height signal/noise .............. = 5
C_tol %F : Absolute error of Cauchy parameter = 2.0000001E-03
S_tol %F : Absolute error of Skew parameter . = 2.0000001E-03
H_Tol %F : Absolute error of height ......... = 3
Apply    : Apply tolerances now
Exit     : Exit but don't apply tolerances now
Edit Tolerances - APPLY

=========< S e t   R e j e c t i o n   C r i t e r i a >=========

Height      : Test on HEIGHT....... = F
Centre      : Test on CENTRE....... = F
Width       : Test on WIDTH........ = F
Errors      : Test on ERRORS....... = F
S/n         : Test on S/N.......... = F
SHape       : Test on SHAPE........ = F
SEparations : Test on SEPARATIONS.. = F
Log         : Log failures ........ = F
Apply       : Apply tolerances now
Set Rejection Criteria - CENTRE

=========< S e t   R e j e c t i o n   C r i t e r i a >=========

Height      : Test on HEIGHT....... = F
Centre      : Test on CENTRE....... = T
Width       : Test on WIDTH........ = F
Errors      : Test on ERRORS....... = F
S/n         : Test on S/N.......... = F
SHape       : Test on SHAPE........ = F
SEparations : Test on SEPARATIONS.. = F
Log         : Log failures ........ = F
Apply       : Apply tolerances now
Set Rejection Criteria - HEIGHT

=========< S e t   R e j e c t i o n   C r i t e r i a >=========

Height      : Test on HEIGHT....... = T
Centre      : Test on CENTRE....... = T
Width       : Test on WIDTH........ = F
Errors      : Test on ERRORS....... = F
S/n         : Test on S/N.......... = F
SHape       : Test on SHAPE........ = F
SEparations : Test on SEPARATIONS.. = F
Log         : Log failures ........ = F
Apply       : Apply tolerances now
Set Rejection Criteria - WIDTH

=========< S e t   R e j e c t i o n   C r i t e r i a >=========

Height      : Test on HEIGHT....... = T
Centre      : Test on CENTRE....... = T
Width       : Test on WIDTH........ = T
Errors      : Test on ERRORS....... = F
S/n         : Test on S/N.......... = F
SHape       : Test on SHAPE........ = F
SEparations : Test on SEPARATIONS.. = F
Log         : Log failures ........ = F
Apply       : Apply tolerances now
Set Rejection Criteria - APPLY
Testing on: HEIGHT CENTRE WIDTH
75 fits rejected
\end{verbatim}\end{quote}
The option to log the failures lists, for each fit at each
cross-section which fails the tolerances, the reason why it failed, and
the value of the relevant parameter.
For a large data set this can produce a large amount of output!
Returned fits will also be listed (if you relaxed your rejection
criteria).
Otherwise the number of fits rejected is all that is listed.
Note that the testing ignores blocking, so each cross-section for each
line is counted as a separate fit.

The tolerances are as follows, and apply to each component of the fit
unless otherwise stated (i.e. if one component fails, the fit is
rejected):
\begin{description}
\item[HEIGHT] Fits are accepted only if their height is greater than
H\_MIN and and less than H\_MAX.
\item[CENTRE] Fits are accepted only if their centre is greater than
V\_MIN and and less than V\_MAX.
\item[WIDTH] Fits are accepted only if their width is greater than
w\_MIN and and less than W\_MAX.
\item[ERRORS] Fits are accepted only if the error on the height is less
than H\_TOL, that on width less than W\_TOL and that on centre less than
V\_TOL.
\item[S/N] Fits are accepted only if the ratio of Height to Error on
height is greater than H\_S\_N and if the ratio of width to its error is
greater than W\_S\_N.
\item[SHAPE] The fit is rejected if the error on the Skew is greater
than S\_TOL or that on Cauchy greater than C\_TOL (these are of course
only relevant for Skew and Cauchy fits).
\item[SEPARATIONS] For the moment this is ignored!
\end{description}

\subsubsection{Summary of Parameters}

\begin{description}
\item[IMage] (char) Name of image for input, This is the data and should
be a FIGARO format data file.
This should also have a .X axis array which contains the wavelengths of
the lines. For the identification files supplied with the program the
units should be {\AA}ngstroms. However, if the user supplies his/her own
files, this need not apply, although some plots may have the wrong
labels.
\item[ARC\_OPTS] (char) Fit option\\
NEW    (N) Set up a new analysis\\
REPEAT (R) Iterate on  previous analysis\\
CLONE  (C) CLONE an analysis from another file
\item[YStart] (int) Analysis lower limit
\item[YEnd] (int) Analysis upper limit
\item[YBlock] (int) Analysis cross-section width
\item[ITeration] (int) New value of iteration
\item[MAXLines] (int) Maximum number of lines to allow room for
\item[CLfile] (char) Name of image for {\em CLONING} from
\item[OUtable] (char) Name for EXTATIC file
\item[VCorr] (real) Correction to apply to radial velocities
\item[TOls] (char) For use in batch only
\item[INherit] (int) Number to control inheritance of  previous fits\\
If zero no inheritance of fits\\
If one then inherited from next block\\
If minus one then inherited from previous block
\item[DEvice] (char) Device to use for plotting (greyscale)
\item[FITRat] (real) Ratio of widths, heights, or separation, for
double fits
\item[CAlrat] (int) Ratio of number of iteration to default
\item[WHite] (float) Level to plot as white (greyscale option)
\item[BLack] (float) Level to plot as black (greyscale option)
\item[MAXGauss] (int) Maximum number of Gaussians that can be fitted
to a profile.
\item[TStart] (int) Analysis lower limit
\item[TEnd] (int) Analysis upper limit
\item[TBlock] (int) Analysis width in T direction
\item[FIT\_MODEL] (char) Model for fitting
\item[PLOTLim] (float) Limits of plot (world coordinates).
This is to allow velocity plots to be forced to all have the same scale,
making comparison easier.
\item[HArdcopy] (key) Produce hardcopy plots of fits from cube
\item[TAble] (key) Produce table of fits from cube
\item[PLOT] (key) Produce plots of rotation curves
\item[PRInt] (key) Produce print out of rotation curves
\item[SHape] (key) Carry out shape analysis
\item[KEEP\_ITT] (key) Keep iteration files. These files contain
details of the fitting process. If a fit succeeds the file is always
deleted, if it crashed it is always kept, this keyword controls
whether it is deleted if a NAG error occurs.
\item[FIT] (key) Perform fitting
\item[COPY] (key) Copy previous fits This will repeat all the fits
previously made, which is likely to be of use if data is co-added after
one file has been analysed. Also, when used with CLONE the entire .RES
structure is copied without any change. For the new fits the previous
fits (suitably scaled) are used as first guesses.
\item[ABsorption] (key) Allow fitting of absorption lines.
This allows absorption fits to be defined---once defined they can be
fitted whatever the value of this.
\item[BOunds] (key) Perform bounded fits to lines (in batch).
This only bounds the widths.
\item[LAbel] (key) Put labels on plots.
This is true by default, but it may be preferable if plots are to be
used in a paper to not have labels.
\item[CONtour] (key) Produce contour plots
\item[GRey] (key) Produce greyscale plots
\item[LOG] (key) Use log scale for greyscale and contour plots
\item[WEIghts] (key) Use weighted fitting (default is true).
\item[PRFits] (key) Print out details of fitting.
This is true by default, but if you wish to avoid large log files set it
to false.
\item[FULL] (key) Print out full details of fits in table.
This provides the information in a form which is easier to read into a
FORTRAN program, and includes the information given in PRINT and TABLE.
\item[CHECK] (key) This produces an array of line profiles with the fits
superimposed.
\end{description}
The options HARDCOPY, TABLE, PLOT, PRINT, SHAPE and FULL are the same
as for the OUTPUT MENU, except that in batch mode HARDCOPY and SHAPE
will work on all line profiles/blockings which have a successful fit.
If OUTABLE is given in the command line, the STATIC option is
selected.
CONTOUR and GREYSCALE work on all the lines in batch mode.

Note that the use of TSTART and TEND is with 3-dimensional data files.
At present LONGSLIT only partially supports these, but some of the same
code is used as in FIBDISP.

The keyword PRFITS is true by default but the user may set it to false
to avoid large log files when running in batch.


\subsection{FIBDISP---Analyise 3-d Data (spectral)}

This performs a similar function to LONGSLIT, but is optimised for
three-dimensional data arrays.
In interactive use a greyscale softcopy device such as a GWM (the X
window you get from GKS) is used, and profiles are selected using a cursor.
Alternatively an
array of profiles may be displayed using line graphics. If run in batch
mode the whole data block may be fitted with Gaussians etc. The same
fitting options are available as for LONGSLIT (see
section~\ref{sec.long}). The file should already have a results
structure (unlike LONGSLIT, FIBDISP does not create one), this can be
created by FIB2CUBE or LONGSLIT.

FIBDISP can handle arrays in which the pixels correspond to a
rectangular or hexagonal grid on the sky.

On entry to the program the user is prompted for the name of the data
cube and then enters the {\em main menu}.

\subsubsection{Main Menu}

The options in the main menu are as follows:
\begin{description}
\item[RESULTS] Display a plane of the results block.
The plane is accessed by name, and this is one of only two places in the
whole of TWODSPEC that cares about the case of the answer given by the
user (the other is MODPARAMS).
For example, if you wanted to display the centre of the first component,
you would enter ``Centre\_1''.
\item[DATA] Display a plane of the data
\item[PROFILE] Examine line profiles.
The profiles are selected using a cursor.
If more than one is selected they are added together. Gaussians  etc.\
may be fitted to the profile, and the results stored.
The fitting is similar to that of LONGSLIT.
Hard-copy plots of the profile may also be made.
\item[XCUT] Take a cut through the data in the X direction, and
display on a greyscale device.
The position is marked with a cursor, and the nearest pixels are chosen
(i.e. no interpolation).
\item[YCUT] Take a cut through the data in the Y direction, and
display on a greyscale device.
Similar to XCUT.
\item[XOUT] Output to file X direction cut through data.
This is like XCUT, but the output is to a file, rather than an image
display.
\item[YOUT] Output to file Y direction cut through data.
Similar to YOUT.
\item[IT] Reduce iteration
\item[CHECK] Display an array of line profiles---this can be in soft
or hard-copy
\item[TOTAL] Display the summed intensity through the image
\item[LIMIT] Limit X and Y range for display.
This acts as a toggle, so if you have limited plots and select this
option again, you will return to full range.
\item[TOLS] Apply/set tolerances
\item[LOOK] Look at values in results cube
\item[DELETE] Delete fits from results cube
\item[DEFINE] Define fits for batch mode. The fits are defined and
stored in the control array.
\item[OUTPUT] This allows the user to list all the fits onto the
line printer, or to produce hardcopy plots of all the fits.
\item[EXIT] Leave the program
\item[CUBAN] Cuban-style display/motion.
This is better than CHECK for large data-sets (such as from TAURUS).
An array of profiles is plotted, with a small greyscale representation
of the total intensity in the top right-hand corner---the centre of the
array plot is marked on this with a cross.
The user has the following options (decided by cursor keys):
\begin{description}
\item[A] Add profiles to fit (end with F)
\item[C] Centre here
\item[D] Down
\item[E] Exit
\item[F] Make fit to point
\item[H] Make hardcopy of current plot
\item[J] Jump to new area (using greyscale plot)
\item[L] Left
\item[P] Indicate position
\item[R] Right
\item[S] Set scaling for profile plots
\item[U] Up
\item[X] Erase fit
\item[?] Help
\end{description}
\end{description}
Note that the following options require the use of a greyscale display:
RESULTS; DATA; XCUT; YCUT and TOTAL, although PGPLOT is capable of
producing a simulated greyscale on line graphics devices, if the array
is rectangular.

\subsubsection{Summary of Parameters}

\begin{description}
\item[CUbe] (file) Cube for display
\item[YStart] (int) analysis lower limit
\item[YEnd] (int) analysis upper limit
\item[YBlock] (int) Enter analysis x-sect width
\item[TStart] (int) analysis lower limit
\item[TEnd] (int) analysis upper limit
\item[TBlock] (int) Enter analysis blocking width in 3rd dimension
\item[DEvice] (char) Device for display
\item[ITeration] (short) New value of iteration
\item[OUTABLE] (file) Name for EXTATIC file
\item[VCorr] (float) Correction to apply to radial velocities
\item[TOls] (char) For use in batch only
\item[FITRat] (real) Ratio of widths, heights, or separation, for
double fits
\item[CAlrat] (int) Ratio of number of iteration to default
\item[OUTPut] file: Name for output file
\item[FIT\_MODEL] (char) Model for fitting
\item[LOw] (float) Minimum value for display
\item[HIgh] (float) Maximum value for display
\item[ABsorption] (key) Allow fitting of absorption lines
\item[BOunds] (key) Perform bounded fits to lines (in batch)
\item[HArdcopy] (key) Produce hardcopy plots of fits from cube
\item[TAble] (key) Produce table of fits from cube
\item[PRInt] (key) Produce print out of radial velocities
\item[SHape] (key) Carry out shape analysis
\item[KEEP\_ITT] (key) Keep iteration files
\item[FIT] (key) Perform fitting
\item[WEIghts] (key) Use weighted fitting (default is true).
This only applies if the file contains an error array.
\item[PRFits] (key) Print out details of fitting.
When fitting large data-sets in batch this would normally be set to
false, otherwise a very large log file will be created.
\end{description}
SHAPE is not yet implemented.

\section{ARC2D---Wavelength Calibration}
\label{sc.arc2d}
\subsection{Introduction}

Once the data has been corrected for distortions or has been
``cleaned'', it must be converted to a linear (calibrated) scale in
wavelength, or at least the relationship must be defined.
In practice it is much easier to handle a re-binned file.

The ``traditional'' way of doing this is to take one or more spectra
of an arc lamp (which has lines of known wavelength), using the same
spectrometer arrangement, fairly close in time to the exposures of the
objects. This spectrum will contain information on the wavelengths
corresponding to different channels. The user will then indicate the
locations of the lines in some way to a program, and then this program
will find the centroid of each line at each cross-section, or group of
cross-sections---a {\em block}). These positions are then used to obtain
the relationships of the channel number to wavelength, by fitting
polynomials to the line positions and wavelengths. The coefficients of
these are then used for the re-binning onto a linear scale.

This reduction method has several disadvantages:
\begin{itemize}
\item The process of finding centroids is often less accurate than
fitting Gaussians, since is is more prone to errors due to poor signal
to noise, or a strong base (especially if this is not constant).
\item Finding centroids provides no information on the likely error
in the centre. Ideally the strongest lines should be given the highest
weights. Conventional programs use unweighted polynomial fitting.
\item In treating each cross-section or block separately, the fact that
the line centres will vary smoothly (for ``normal'' data) across the
image is ignored. This can give rise to steps in what should be smooth
lines, for the re-binned image. If the data from successive
cross-sections in blocked together then, even if all the centres lie on
a smooth curve, steps will occur in the calibrated data.
\end{itemize}

ARC2D is a two-dimensional arc calibration program. It makes use of a
calibration arc as above, but Gaussians are fitted to the arc-lines in
order to locate them with maximum precision, as well as to give
information on errors. The continuity of the arc lines is taken into
account. Rather than locating the line centres immediately prior to
fitting the polynomials (to determine the dispersion relationship)
whilst travelling up the spectrum, ARC2D finds all the line centres,
and the fit to any cross-section can be checked, and poor lines
rejected. Then all the polynomials can be fitted and, if satisfactory,
written to the output file. Conventional programs require the order of
the polynomial and the arc lines to be used to be guessed, before any
fits are made (or at best when only one cross-section or block has been
fitted)!

The line identification part of ARC2D is the same as for LONGSLIT (see
section~\ref{long.id}).

The line positions ARC2D has at this stage are approximate since they
are only lower and upper limits in channels for the optimisation.

The lines are then accurately located (automatically) at each
cross-section by fitting Gaussians to them, assuming a Gaussian can
successfully be fitted. For this the data from successive
cross-sections can be blocked together, to improve the signal to noise
ratio. In our case we typically co-add ten successive cross-sections.
This fitting is a least squares optimisation, including a flat base,
and is the same as used for LONGSLIT.

ARC2D creates a file for use with ISCRUNCH.
Weighted fitting may be used for the
polynomials to determine the dispersion relation. In addition
polynomials may be fitted to the relation of line centre to
cross-section for each line, and the polynomial values from this
fitting used instead of the Gaussian centres, to give a smooth
variation of the dispersion relation across the spectrum.

  To use ARC2D you should do the following:-
\newcounter{cntr}
\begin{list}{(\roman{cntr})}{\usecounter{cntr}}
\item Set up tram arrays---to to tell the program where the lines are
and to identify them
\item Fit Gaussians to lines (usually in batch mode).
\item Apply continuity correction (this is not essential).
\item Fit polynomials to define the correction to be applied to
``scrunch'' the data (you automatically end up here unless you answer
``YES'' to leave program now...).
\item If you are satisfied with the fits, create a calibration file and
use it as input to ISCRUNCH.
\end{list}

Parts (i) and (ii) above are as for LONGSLIT, except that only AUTO
mode is available for (ii) and the program will follow any curvature
of the lines during fitting.

With ARC2D it may be found useful to copy the .RES.ARC array across
from one file to another (this is not done in CLONE, unless COPY is
specified).
This may be done by:
\begin{quote}\begin{verbatim}
$ LET FILE2.RES.ARC = FILE1.RES.ARC
\end{verbatim}\end{quote}
to copy it from file1 to file2.

In the main menu LOOK, TOLS,EXIT and ADD are the same as in LONGSLIT,
and GAUS is similar to the AUTO option in LONGSLIT. POLY and DISP are
described in the next two sections. SOFT and HARD produce plots of use
in determining which lines to include in the fitting. These are of line
centre against cross-section, line width against centre and error on
centre v. height.

\subsection{Continuity Correction}
\label{arc2d.continuity}

In order to avoid ``steps'' in the scrunched data, it is advisable to
use this option for long-slit spectra (of course this isn't applicable
to data such as form fibre arrays where a smooth curve is not to be
expected).
Select the POLY option in the main menu:
\begin{quote}\begin{verbatim}

=========< M a i n   M e n u >=========

Look   : Look at values of data cube
Gaus   : Fit Gaussians to line profiles
Soft   : Produce soft-copy plots of diagnostics
Hard   : Produce hard-copy plots of diagnostics
Tols   : Apply tolerances
Disp   : Evaluate dispersion relation
Add    : Add more lines
Poly   : Fit polynomials in X-Sect direction
Exit   : Leave the program
Main Menu [EXIT] - POLY
keep fits with nag errors for poly fitting? [NO]
Weight fits? [YES]
Performing weighted fit
\end{verbatim}\end{quote}
A plot of the sum of the squares of the residuals against order is
displayed.
\begin{quote}\begin{verbatim}
Go onto next plot? [YES]
 S E E K   M E N U
\end{verbatim}\end{quote}
The residuals are plotted against cross-section. When these appear to
be due to noise only, answer ``NO'' to leave the loop, and reply with
the order required. A plot of the fit over the centres is given, and
similar plots are displayed (two to a page) for the remaining lines,
but can be suppressed.
\begin{quote}\begin{verbatim}
Y for next plot;N to stop [YES]
Y for next plot;N to stop [YES]
Y for next plot;N to stop [YES] NO
(ORder) order for polynomial fitting [3] -
Order returned =   3
Further plotting may be suppressed by typing ``N''
Go onto next plot? [YES]
Performing weighted fit
Go onto next plot? [YES]
Performing weighted fit
Go onto next plot? [YES]
Performing weighted fit
Go onto next plot? [YES] NO
Performing weighted fit
Produce hardcopies of line centre & residual plots [NO]

=========< C o n t i n u i t y   F i t s >=========

Accept   : Accept fits
Retry    : Try again
Quit     : Give up
Continuity Fits [ACCEPT] -
\end{verbatim}\end{quote}
The results are now stored and the user returned to the main menu.

\subsection{Evaluating the Dispersion Relation}

Select the DISP option at the main menu:
\begin{quote}\begin{verbatim}

=========< M a i n   M e n u >=========

Look   : Look at values of data cube
Soft   : Produce soft-copy plots of diagnostics
Hard   : Produce hard-copy plots of diagnostics
Tols   : Apply tolerances
Exit   : Leave the program
Disp   : Evaluate dispersion relation
Gaus   : Fit Gaussians to line profiles
Add    : Add more lines
Poly   : Fit polynomials in X-Sect direction
Main Menu [EXIT] - DISP

You now are given a list of the lines on the graphics screen. You can
hit ``?'' to see what the options are. This part of the program is
designed to be used with a cursor, rather than using menus. You can
edit line lists, set parameters, etc. Typing ``f'' will then perform a
fit and you will be given plots of the fit and residuals.

When you have the fit/residuals plots, the lines used are shown with
filled-in markers, and open markers indicate unused lines. Note that,
although the lines used are updated on the lower plot, only when you
hit ``f'' again will new fits be performed, so until then the
residuals etc. will refer to the old fit.

If you are not using continuity-corrected data, you have the additional
option to include fits with NAG errors (this may be satisfactory,
depending upon the cause of the error, you should examine the log file
before using such fits).
\begin{quote}\begin{verbatim}

 ARCFIT :   Order = 3,  cross-section 85

Coefficients of fit are -

  8.27062E-16 -3.97077E-12  3.86509E-09
  4.17852E-06  0.00000E+00  0.00000E+00

Start wavelength = 4827.310,     End wavelength = 5152.441
Central wavelength = 4989.621,   Mean dispersion (per channel) = 0.1601613

          Line    Wavelength    Calculated   Discrepancy
                                Wavelength

        133.35       4847.81       4847.81          0.00
        396.13       4889.04       4889.06         -0.02
\end{verbatim}\end{quote}
A list of line wavelengths with fitted wavelengths is written to
the terminal. If there are any lines not used in the fit, these are
still output (after those which are used), so that, if required,
estimates of the wavelengths of unidentified lines can be obtained.
\begin{quote}\begin{verbatim}
       1646.52       5090.50       5090.51         -0.01
       1971.84       5141.78       5141.78          0.00

Chi-squared(ang**2) = 1.1019063E-04
\end{verbatim}\end{quote}
The fits should be checked at several positions across the data and,
when satisfactory, the tables etc.\ should be suppressed, and the
results written to a file (the ``A'' option will accept the fits).
If ``continuity corrected'' data is used, then each cross-section is
treated separately, otherwise each block is treated as one fit.
\begin{quote}\begin{verbatim}
Minimum start wavelength = 4824.331, maximum end wavelength = 5153.783
Copy any coefficients from one line to another? [NO]
Fits OK? [YES]

Summary of Image Arc Fit Results -
-----------------------------------

Image dimensions  2040 by   171
Number of rows that could not be fitted =     0
Maximum chi-Squared error =       0.00
Maximum degree polynomial used =   3

Fit results written to file DISK$USER3:[SCRATCH.TNW]EXAMPLE.IAR;1


=========< M a i n   M e n u >=========

Look   : Look at values of data cube
Gaus   : Fit Gaussians to line profiles
Soft   : Produce soft-copy plots of diagnostics
Hard   : Produce hard-copy plots of diagnostics
Tols   : Apply tolerances
Disp   : Evaluate dispersion relation
Add    : Add more lines
Poly   : Fit polynomials in X-Sect direction
Exit   : Leave the program
Main Menu [EXIT] -
\end{verbatim}\end{quote}

\subsection{Summary of Parameters}

\begin{description}
\item[IMage] (file) Name of image for input. This should be a file
containing an arc spectrum.
\item[ARC\_OPTS] (char) Enter arc fit option\\
NEW    (N) set up a new wavelength calibration\\
REPEAT (R) Iterate on previous calibration.\\
CLONE  (C) CLone a previous calibration.
\item[YStart] (int) analysis lower limit The data between the
limits ystart and yend is extracted and the resultant spectrum is used
to locate the lines.
\item[YEnd] (int) analysis upper limit. The data between the
limits ystart and yend is extracted and the resultant spectrum is used
to locate the lines.
\item[YBlock] (int) Enter analysis x-sect width Each window is
of this width (except perhaps the final one).
\item[ITeration] (short) New value of iteration
\item[ORder] (int) order for polynomial fitting This is for the
continuity correction of the data. Ideally the arc should have been
pre-processed with ARCSDI, so a low order e.g.\ 2 should be used.
\item[MAXLines] (int) Maximum number of lines to allow room for.
This must be greater than or equal to the number of lines fitted, so
room should be allowed in case any more are to be added later.
\item[CLfile] (file) Name of image for cloning from. This should
be a file containing an arc spectrum.
\item[TOls] (char) For use in batch only
\item[KEEP\_ITT] (key) keep iteration files
\item[PRFits] (key) Print out details of fitting
\end{description}

\section{COMB and ARCSDI-Correction for Geometrical Distortions}

\subsection{COMB---Correction for S-Distortion}

This program takes a long-slit spectrum with one or more continua, and
creates a file which allows it to apply a correction to further files,
such that the continua are made straight.
This performs a similar function to the FIGARO function SDIST, but is
more automatic (it may be run completely in batch if required).

Since the philosophy of all the software we have written is to be as
automatic as possible, COMB will automatically find the ``teeth'' of
the comb, and can be run completely in batch, if so required. This
enables quicker processing of data, since in practice the order
required for the Chebyshev polynomials is
found to be 3 for all combs on which the program has been run. It is
probably best, however, to initially locate the teeth interactively
(for setting the values in the tram arrays, which define the edges of
the line at the centre of the data), since some experimentation may be
required. The operation of COMB is as follows:
\newcounter{combcntr}
\begin{list}{(\roman{combcntr})}{\usecounter{combcntr}}
\item
Locate `teeth'. A cut is taken from the data, along the slit direction,
from the central 20 channels of the data.
The algorithm looks for
the highest point in this data, and searches outwards to find other
teeth, whether a new ``tooth'' is accepted depends upon the value of the
parameter LEVEL, which is the minimum acceptable ratio of an new
tooth's height to that of one already accepted. If too many or too few teeth are found, a
different value of LEVEL may be used, or the teeth may be selected
manually. To confirm the correct location of teeth, these are output to
the user (or batch log file), at the start of the program. Thus a check
of the teeth locations may also be made if they are located in batch.
\item
The program then follows the ``teeth'' along the image (in the
channel direction), finding the line centres by fitting Gaussians, or
alternatively by finding centroids. Finding centroids is quicker than
fitting Gaussians, and may be more suitable for some data. Fitting
Gaussians is often more accurate, and gives estimates of the errors.
For this the program will co-add a number of channels (say 20) to
obtain a higher signal to noise (and reduce the time taken to find the
centres).
The teeth are located in the centre of the data (in the channel
direction), and followed out from there in both directions (they are
likely to be strongest in the centre).
Since the variation along the image may be considerable, the tram
positions (the Y values of the range used for fitting) must be updated
using the centre from the last fit, before being used.
This enables the program to follow the continua.
\item
The points so obtained are fitted with Chebyshev polynomials
(for each tooth). If the parameter ORDER, the order of the polynomial to be fitted is
specified in the command line, then it is taken as that. Otherwise the
program plots the residuals for each order until told to stop, and then
prompts for the order.
In batch mode the program will exit at this stage unless ORDER is given
in the command line.
This is similar to the {\em continuity correction} part of ARC2D (see
section~\ref{arc2d.continuity}).
\item
The Chebyshev polynomials are then evaluated at each channel for
each ``tooth'', within the limits used for fitting the polynomials, and
the points outside these limits are evaluated using (local) cubic
splines (Akima 1972). When a value for the cross-section position of
each ``tooth'' has been obtained, these are used for interpolating
(again using cubic splines), the values of the data at the positions at
the current channels corresponding to each cross-section at the
reference position (the central channel). The correction is such that
the data is lined up with this reference channel.
\end{list}

COMB outputs plots of the locations of the teeth at the start (i.e.\ at
the central channel) and a few other points along the data, and also
outputs a plot of the positions of all the points on the teeth found by
the Gaussian fitting, enabling a check to be made as to whether the
results seem reasonable.
Plots of the polynomial fits to the teeth and of the residuals on these
fits (typically up to about 0.2 cross-sections) are also output. If
running interactively these are in soft-copy, otherwise in hard-copy.
To apply the correction, use OLD mode, the correction file (i.e. the
file with the polynomial coefficients) is COMB.GMC.

This is a program to correct data for S-distortion by moving data in
the cross-section direction to line it up for a {\em comb} of continua
spectra. This correction is then applied to the data itself. A comb
dekker is used to produce about ten continuum spectra across an image
(this is done at the telescope). This image is then used by the
program:- The program locates the teeth and follows them along the
image (in the channel direction), finding the line centres by fitting
Gaussians. The points so obtained are fitted with Chebyshev polynomials
(for each tooth). The intermediate positions are interpolated from
these, which are then used to evaluate the required movement for each
data point. The coefficients are written to a file which may then be
read by the program to apply correction to the actual data.

Alternatively, if QUICK is specified, centroids are used rather than
the fitting of Gaussians.
Since fitting Gaussians is generally quite fast, this is only really
likely to be needed when the profiles are significantly different from
Gaussian.

\subsubsection{Summary of Parameters}
\begin{description}
\item[IMage] (file) Name of image for input. This should be a file
containing continua spectra.
\item[ARC\_OPTS] (char) Enter arc fit option\\
NEW    (N) set up a new wavelength calibration\\
REPEAT (R) Iterate on previous calibration\\
CLONE  (C) CLone a previous calibration
OLD    (O) Correct using previous results
\item[OUtput] (file) Name of output file. File to contain
corrected data.
\item[XStart] (int) analysis lower limit. The data between the
limits xstart and xend is extracted and the resultant spectrum is used
to locate the lines.
\item[XEnd] (int) analysis upper limit. The data between the
limits xstart and xend is extracted and the resultant spectrum is used
to locate the lines.
\item[XBlock] (int) Enter averaging width in channels. Each
window is of this width (except perhaps the final one).
\item[ITeration] (short) New value of iteration
\item[LEvel] (float) Level of edge of tooth
\item[ORder] (int) order for polynomial fitting This is for the
continuity correction of the data.
\item[MAXLines] (int) Maximum number of lines to allow room for
This must be greater than or equal to the number of lines fitted, so
room should be allowed in case any more are to be added later.
\item[CLfile] (file) Name of image for cloning from. This should
be a file containing an comb spectrum.
\item[TOls] (char) For use in batch only
\item[KEEP\_ITT] (key) keep iteration files?
\item[QUick] (key) Centroid rather than fit Gaussians?
\item[PRFits] (key) Print out details of fitting
\item[PLOtcorr] (key) Plot correction?
\end{description}

\subsubsection{Main Menu}

\begin{description}
\item[LOOK] Look at values of data cube
\item[SOFT] Produces soft copy plots of diagnostics
\item[HARD] Produces hard copy plots of diagnostics
\item[TOLS] Apply tolerances
\item[POLY] Fit polynomials to results
\item[CENTRES] Find line centres-takes a long time.
This is similar to the GAUS option of ARC2D.
\item[ADD] Add more lines
\item[EXIT] Exit program
\end{description}

\subsection{ARCSDI---Correction for Line Curvature}

This corrects data for line curvature, using an arc spectrum.
The function is the same as for comb, but it works in the perpendicular
direction (again this is for long-slit spectra).

The aim is to give ARC2D a spectrum with fairly straight arc lines, so
it can ``block'' more cross-sections together to obtain a higher signal
to noise.
The operation is similar to parts (i) to (iii) of ARC2D, except that
ARCSDI does not need to know the line identifications.
To apply the correction, use OLD mode, the correction file (i.e. the
file with the polynomial coefficients) is ARCSDI.GMC.
The main menu is the same as for COMB.

The arc lines are located using much of the same code as in ARC2D
described below, including the fitting of Gaussians. The reason for
doing preliminary correction of this nature, rather than using ARC2D
straight away, is that for data which is fairly noisy, it is desirable
to give ARC2D as many cross-sections at a time for the fitting. It is
intended that ARCSDI be used with a long exposure from a {\em chimney
arc}. Another problem overcome by this is that of vignetting which
occurs in the comparison arc optics of the RGO spectrograph at the
AAT\@. `Chimney' arcs are reflected off the dome, and do not suffer
from this problem (the same optical path is used as for the
observations). It is often an advantage to use an arc from another
wavelength range, if this has more suitable lines, or if they are
better distributed along the spectrum.

Once the lines are located, they are treated in much the same way as
the positions of continua in COMB. Chebyshev polynomials are fitted to
these and the results used to determine the required shifts.

\subsubsection{Summary of Parameters}
\begin{description}
\item[IMage] (file) Name of image for input. This should be a file
containing an arc spectrum.
\item[ARC\_OPTS] (char) Enter arc fit option\\
NEW    (N) set up a new wavelength calibration\\
REPEAT (R) Iterate on previous calibration\\
CLONE  (C) CLone a previous calibration
OLD    (O) Correct using previous results
\item[OUtput] (file) Name of output file
File to contain corrected data.
\item[YStart] (int) analysis lower limit
The data between the limits ystart and yend is extracted
and the resultant spectrum is used to locate the lines.
\item[YEnd] (int) analysis upper limit
The data between the limits ystart and yend is extracted
and the resultant spectrum is used to locate the lines.
\item[YBlock] (int) Enter analysis x-sect width
Each window is of this width (except perhaps the final one).
\item[ITeration] (short) New value of iteration
\item[ORder] (int) order for polynomial fitting
This is for the continuity correction of the data. Ideally the
arc should have been pre-processed with ARCSDI, so a low
order e.g.\ 2 should be used.
\item[MAXLines] (int) Maximum number of lines to allow room for
This must be greater than or equal to the number of lines
fitted, so room should be allowed in case any more are
to be added later.
\item[CLfile] (file) Name of image for cloning from
This should be a file containing an arc spectrum.
\item[TOls] (char) For use in batch only
\item[KEEP\_ITT] (key) keep iteration files
\item[PRFits] (key) Print out details of fitting
\item[PLOtcorr] (key) Plot correction?
This is used with the OLD option.
\end{description}

\section{Display Programs}

\subsection{ISCAN---Plot Spectra Extracted From an Image}

This produces plots of 1-d spectra which it extracts from a long-slit
spectrum (i.e. it acts as if it were a wrap-up of the FIGARO functions
EXTRACT and SPLOT).
To use ISCAN just set the hard or softcopy device using HARD or SOFT,
then type ISCAN, and answer the questions. To use in batch follow the
instructions given by the program BATCH.

\subsubsection{Summary of Parameters}

\begin{description}
\item[IMage] (file) Name of image for input
\item[XStart] (float) display lower limit
\item[XEnd] (float) display upper limit
\item[YStart] (int) display lower limit
\item[YEnd] (int) display upper limit
\item[YBlock] (int) Enter display x-sect width
\item[SCan] (key) Scan through data
\item[HArd] (key) use hard graphics device for display
\end{description}

\subsection{HIMAGE---Greyscale Display}

This uses GKS to plot a greyscale image of an image (in the FIGARO
sense). It can plot on a GKS greyscale device such as a GWM or
Postscript laser printer.
The plot can include a key. Unfortunately this
program takes a fair time to scale the image, so will ``sit there''
for a minute or so before anything appears on the screen (this does, of
course, depend upon the image size). The parameters are as follows:
\begin{description}
\item[IMage] (file) Name of image to be displayed
\item[YStart] (int) First Y value to be displayed
\item[YEnd] (Int) Last Y value to be displayed
\item[XStart] (int) First X value to be displayed
\item[XEnd] (int) Last X value to be displayed
\item[Low] (int) Minimum count level for display
\item[High] (int) Maximum count level for display
\item[PLOTdev] The device to plot on (translated using GNS, so
should be ``xwindows'' etc.).
\item[ASPect] This is the aspect ratio (X/Y) of the plot (excluding
any key). The default is set to the value which gives square pixels.
\item[SHRINK] This gives a wide margin to the plot.
\item[LOG] (key) Display using logarithmic scaling
\item[KEY] This plots a key giving the data values corresponding to
given greyscale values.
\end{description}

\subsection{CSCAN---Plot Array of Line Profiles from Data ``Cube''}

This produces as array of line profiles from a 3-d data array, where
the first dimension corresponds to wavelength. The data is scaled using
the maximum and minimum intensities in the whole data cube, rather than
of the individual profiles.

\subsubsection{Summary of Parameters}

\begin{description}
\item[CUBE] (file) Name of CUBE for input
\item[YStart] (float) display lower limit
\item[YEnd] (float) display upper limit
\item[TStart] (int) display lower limit
\item[TEnd] (int) display upper limit
\item[HArd] (key) use hard graphics device for display
\end{description}
As regards the parameters, Y is the first spatial dimension and T the
second.

\section{Removing Continua}
\subsection{FITCONT---Fit Polynomial to Continuum}

The optimisation in LONGSLIT allows for a flat base.
Although this is generally satisfactory, for some data the base varies
significantly.
The user then has various options: the base can be subtracted from the
using CSUB (which subtracts a polynomial fit to the base from the
data); the data can be divided by the base; or the base can be left as
it is, but a polynomial fit can be subtracted during the actual fitting.
The latter option has the advantage that any plots of line profile etc.
will still show the base as in the original data.
This option is available by the use of the routine FITCONT, or totally
within LONGSLIT.
In general it is better to use the version in LONGSLIT, but this will
not always cope with ``difficult'' data sets, so FITCONT may be
required.
The main difference here which allows FITCONT to deal with these cases
is that it allows the weighting to be set using a cursor (in LONGSLIT it
is done simply by excluding the areas within line boundaries, which is
less versatile).
This is similar to CSUB in fitting a Chebyshev polynomial to the base.
However the data itself is not altered.
When LONGSLIT fits a line it can subtract this base.
Alternatively the base may be interpolated over the line using cubic
splines (FITCONT is not needed for this).

\subsubsection{Summary of Parameters}

\begin{description}
\item[IMage] The file with the data in
\item[XSect] The cross-section to use for the first fit
\end{description}


\subsection{CSUB---Subtract Continuum}

This is similar in function to FITCONT, except that is performs the
continuum subtraction itself.

\subsection{CADD---Add Back Continuum Previously Subtracted}

This add back a continuum previously subtracted using CSUB, making use
of the information CSUB stores in the data file.

\section{Getting 3-D Data into the Correct Form for FIBDISP/LONGSLIT}

These programs were designed for use with a fibre array on the
Manchester Echelle Spectrometer.
The fibres effectively make it a multi-slit spectrometer, with the
fibres gathered into three (for example) rows at the input to the
spectrometer.
The first task is to extract the data from the individual fibres, the
data is then wavelength calibrated and combined into a three-dimensional
array, simulating the relative positions of the input ends to the
fibres.
FIBSEP extracts the data from the individual fibres into what looks like a
long-slit spectrum for each row of fibres (at their output end).
ARC2D can then be used for the wavelength calibration, and FIB2CUBE will
re-arrange them into the format required for FIBDISP (FIB2CUBE also
created the results structure).

\subsection{FIBSEP---Extract Data from Individual Fibres}

This separates out the individual fibres from a 2-d spectrum. It is
assumed that the data had previously been corrected for S-distortion.
This first produces a greyscale plot of the data (in a GWM window for
example), and then the user is asked to mark two fibres and the limits
(in X) that the program is to consider.
If there is more than one ``slit'', then each must be dealt with by
separate runs of the program.
The method is to find the centres of the fibres by fitting Gaussians,
and to divide the data half-way between these points. Since the
separation of the fibres may vary slightly, the user is recommended
to use CSUB on the data first. This method is fairly crude and the user
may well wish to improve upon it, but if all that is required is radial
velocities this should be adequate.

\subsubsection{Summary of Parameters}

\begin{description}
\item[IMage] (file) Name of image to be displayed
\item[OUTput] (file) Name of image to create
\item[FILE] (char) Name of file for positions of fibres
\item[YStart] (int) First Y value to be displayed
\item[YEnd] (int) Last Y value to be displayed
\item[XStart] (int) First X value to be displayed
\item[XEnd] (int) Last X value to be displayed
\item[Low] (int) Minimum count level for display
\item[High] (int) Maximum count level for display
\item[DEVICE] (char) Plotting device (normally something like ``xwindows'')
\item[NFIB] (int) Approximate number of fibres expected
\item[LOG] (key) Display using logarithmic scaling
\item[OLD] (key) Use previous results to extract data
\end{description}

\subsection{FIB2CUBE---Convert Fibre Data to ``Cube''}

This converts calibrated long-slit spectra from a fibre array into a
3-d data cube for use by FIBDISP.
Data from separate slits should be in different files, prompted for as
IMAGE1, IMAGE2, etc.
CUBE is the output file.
FILE is a file listing the X, Y positions of each fibre in the output
array (default type .DAT), probably produced by ENCODE.
Note that, although CSCAN can produce plots from a file produced by
FIB2CUBE for types ``HEX'' and ``RECT'', these are only correct for type
``RECT''.
FIB2CUBE create files with all the structures required by FIBDISP.
The emission line is delimited using the same code as used in LONGSLIT.
The code only allows for one spectral line per file.
\begin{description}
\item[IMAGE1] (file) Input image 1
\item[IMAGE2] (file) Input image 2
\item[IMAGE3] (file) Input image 3
\item[IMAGE4] (file) Input image 4
\item[IMAGE5] (file) Input image 5
\item[CUBE] (file) Output data cube
\item[FILE] (char) Name of file with coordinates for output cube This
file must contain the two spatial dimensions of the output cube followed
by the number of input images in the
first record. After that the X, Y coordinates of the input spectra (a
cross-section at a time, starting at 1) in the output cube.
After that the contents of the X displacement array (if any).
Comments lines start with an exclamation mark.
\item[MAXGAUSS] (int) Maximum number of Gaussians that can be fitted
to a profile.
\end{description}

\subsection{CUBEIN}

This performed the same function as the NEW mode of LONGSLIT (see
section~\ref{long.id}), and has therefore been withdrawn (use LONGSLIT
instead).

\section{IRAF--FIGARO file conversion}

The FIGARO functions HDS2IRAF and IRAF2HDS allow conversion between
the two formats. Both of these access the .FITS, and not the .OBS
structure (except for the object name), within the FIGARO file, so it
may be a good idea to rename any .OBS structure (or elements from it) to
.FITS before using HDS2IRAF. Both of these are linked using the
non-shareable versions of most of the libraries (IRAF has it's own
versions of C run time library routines which gives rise to problems
when used with HDS, which is written in C). For access to the IRAF
format files the IRAF library IMFORT is used, which at present only
supports OIF format (the default).
Access to FIGARO files is via the DSA library.

Note that IRAF does not support an axis array, but instead uses a
starting point and increment in the same manner as FITS.
These programs will convert between the two, but a non-linear FIGARO
axis cannot be converted to IRAF format.

\subsection{IRAF2HDS}

This converts an IRAF file to a FIGARO format file.

\subsubsection{Parameters}

\begin{description}
\item[FILE] Name of input IRAF format file
\item[OUTPUT] Name of output HDS file
\end{description}

\subsection{HDS2IRAF}

This converts a FIGARO format file to an IRAF format file.

\subsubsection{Parameters}

\begin{description}
\item[IMAGE] Name of input file
\item[OUTfile] Name of output IRAF format file
\end{description}

\section{Miscellaneous Programs}

\subsection{CUBE2LONG---Extract a Spectrum from a Spectral ``Cube''}

This extracts a long-slit-type spectrum from a 3-d data array, which
has the dimension corresponding to wavelength last. The values are
obtained using local cubic spline interpolation. The program requires
a position (xpoint, ypoint) on the ``slit'', and an angle. It will then
produce a cut through the data, following this ``slit'' until it reaches
the edge of the data in each direction from the given point. Variation
of the parameter PIXLEN eases comparison of (for example) TAURUS data
with long-slit data. Cubic spline interpolation is used to obtain the
data values.

\subsubsection{Summary of Parameters}

\begin{description}
\item[CUBE] (file) Name of cube for input.
\item[OUTput] (file) This is the name of the resulting image.
\item[YPOINT] (int) Y point on slit
\item[XPOINT] (int) X point on slit
\item[ANGLE] (float) pa of slit
\item[PIXLEN] (float) Pixel length
\end{description}

\subsection{VIG---Correct For Vignetting}

This fits a Chebyshev polynomials to templates taken from both
directions across an image, and divides the image by the product of the
polynomials, normalising so that the data in the centre in unchanged.
A file is produced containing the coefficients of this polynomial so that
other data files can have the correction applied.

\subsection{BATCH---Creating Batch Command Files}
\label{batch}

This program does not actually run in FIGARO, but produces command
files to run FIGARO routines in batch. These may be for the DCL or ICL
version of FIGARO. It is currently located in FIGARO\_PROG\_N, and is
run as a normal program. It will prompt for the name of the command file
to create (default type .COM or .ICL as relevant), and then enters
command level, where the terminal input is copied to the output file. If
this is run it will give a list of the parameters of the function to
remind the user of the order/parameters required. The option is given to
edit the file before submission. BATCH creates a command file, and will
submit it if required. It will set the default directory to the
directory the file is created in.

Note that, as with DCL, a dash may be used to continue a command on
another line.
For compatability with ICL a ``\~{ }'' may also be used.
If a FIGARO command is recognised, then a list of parameters is output
(keywords in square brackets).
Anything entered after that will go onto the end of the current command
line, until a blank string is entered. BATCH takes care of continuation
lines in the output file.

N.B. This program will exit when the word ``EXIT'' is entered in full,
if you wish to enter ``EXIT'' as a command, but not to leave the program
(e.g.\ for a DCL subroutine), then enter ``EXI'' or precede it with a
tab. BATCH also recognises the following commands as commands to itself,
and they are not written to the command file:
\begin{description}
\item[EXIT] Exit, retaining the command file.
\item[QUIT] Exit, deleting the command file.
\item[HELP] Output elementary help information.
\item[LIST] List FIGARO commands with their parameters.
\item[DCL] Create a spawned subprocess to perform a DCL command.
\end{description}

It is possible to create your own private versions of the input files,
so as to include your own software (the versions supplied include
the``standard'' FIGARO functions plus those of TWODSPEC).
For more details see appendix~\ref{append.install}.

Note that this will not trap commands which are available from DCL, but
not from ICL (for example IRAF2HDS).

\subsection{IDIMS---List Files with Dimensions and Object Names}

This will list the dimensions and object names of a list of files
satisfying the template given in the parameter FILES. This may include
(and usually will) wildcard symbols (e.g.\ ``*''), but shouldn't include
the file extension. For example to list the files SEP22000*.DST:
\begin{verbatim}
$ IDIMS SCRATCH:[TNW.RAQ]SEP22000*
This version of IDIMS is from $1$DRB2:[TNW.FIGARO.TWODSPEC.TWODSPEC]
  $1$DRB0:[TNW.RAQ]SEP220002[576,386]  H1-36
  $1$DRB0:[TNW.RAQ]SEP220003[576,386]  ARC
  $1$DRB0:[TNW.RAQ]SEP220004[576,386]  H1-36
  $1$DRB0:[TNW.RAQ]SEP220005[576,386]  H1-36 PA0
  $1$DRB0:[TNW.RAQ]SEP220006[576,386]  HE2-390
  $1$DRB0:[TNW.RAQ]SEP220007[576,386]  HE2-390 PA 90
\end{verbatim}

\subsection{ALTDEF---Edit HDS file}
\label{altdef}

This is an interactive program for editting an HDS file, and it has one
parameter, INFILE, the name of the file to work on.

At present it is only possible to alter the values of elements (it is
not possible to add elements for example).
To drop to a lower level just specify the name of that level (this can
be abbreviated).
On reaching an array or scaler element, you have the option to set its
value(s).
The example shows how to set the element OPT\_STEP to 0.9.
\begin{quote}\begin{verbatim}
$ ALTDEF
(INFILE) Input file [ ] - TWODSPEC_DEFAULTS

=========< E d i t   M e n u >=========

Fit_handler
Opt_step
Tols
Usepeak
Vacuum
AX1_Log
AX1_Units
Exit
Edit Menu - OPT_STEP
DEFAULTS.OPT_STEP FLOAT 1
\end{verbatim}\end{quote}
(the data type and current value of the element is displayed)
\begin{quote}\begin{verbatim}

=========< A l t e r   E l e m e n t >=========

Set     : Set value
Range   : Set range (if array)
Exit    : Exit
Alter Element - SET
Value [0] - 0.9

=========< A l t e r   E l e m e n t >=========

Set     : Set value
Range   : Set range (if array)
Exit    : Exit
Alter Element - EXIT

=========< E d i t   M e n u >=========

Fit_handler
Opt_step
Tols
Usepeak
Vacuum
AX1_Log
AX1_Units
Exit
Edit Menu - EXIT
\end{verbatim}\end{quote}
This was primarily designed for altering the defaults file (see
section~\ref{deffile}), but is not tied to any particular structure.

\subsection{MODPARAMS--Modify Parameter Names Array}

This program alters the parameter names array in the results structure
used by LONGSLIT etc.
The file name is given by the parameter IMAGE and the string given by
parameter OLDPAR is changed to that given by NEWPAR.
This allows data files to be altered to support extra parameters,
assuming that one of the existing parameters can be sacrificed (this
parameter should not have been used).
Note that the cases of the values given for OLDPAR and NEWPAR are
significant, and they will need to be quoted if lowercase characters are
present.

\subsection{TAU2HDS-Read TAURUS Format file}

This provides an alternative to the NDPROGS program TAU2FIG (this is
only available for the VMS version of TWODSPEC).

\subsubsection{Summary of Parameters}

\begin{description}
\item[TAUfile] TAURUS format data file to be converted
\item[NDims] Dimensionality of data set (2 or 3)
\item[DIMensions] Data dimensions
\item[INForm] The format of the input data (I2 or R)
INFORM specifies the format of the input data set.
Most TAURUS raw 3d data is Integer*2 (16 bits/pixel - I2).
2d images are  either REAL (32 bits/pixel, floating point - R) or
Integer*2.
In the 2d case the output will always be REAL*4 but in the 3d case the
output will have the same format as the input.
\item[OUTfile] The name for the Output Data Structure
\end{description}

% We don't want to number the references as a section

\section*{References}
\addcontentsline{toc}{section}{References}

\begin{description}
\cref{Akaike, H., 1973}{in 2nd int. Symp. Information Theory}
{P.267, Eds. Petrov, B. N. \& Csaki, K., Akademai Kiado, Budapest}{}
\igref{Bailey, J., 1989}{ICL --- The New ADAM Command Language}
\igref{Bland, J., 1986}{Ph. D. Thesis, Univ. of Sussex}
\igref{Chipperfield, A. J., 1989}{SUN/35.2, ICL --- Interactive Command
Language: An Introduction}
\jref{Frazer, R. D. B. \& Suzuki, E., 1969}{Anal. Chem.}{41}{37}{}
\jref{Heckman, T. M., Miley, G. K., van Breugel, W. J. M. \& Butcher,
H. R., 1981}{Astrophys. J.}{247}{403}{}
\igref{Lawden, M. D., 1989}{SG/4.1 ADAM --- The Starlink Software
Environment}
\igref{Shortridge, K., 1988}{FIGARO 2.4}
\igref{Shortridge, K., 1989}{Running FIGARO Under ADAM and ICL}
\igref{Wallace, P. T., 1987}
{SUN/78.4 RV --- Radial Components of Observer's Velocity}
\igref{Wallace, P. T., 1990}{SUN/67.14 SLALIB --- a Library of
Subprograms}
\igref{Whittle, M., 1982}{Ph. D. Thesis, Univ. of Cambridge}
\jref{Whittle, M., 1985}{Mon. Not. R. Ast. Soc.}{213}{1}{}
\igref{Wilkins, T. N., 1988}{Ph. D. Thesis, Univ. of Manchester}
\item{Wilkins, T. N. \& Axon, D. J. 1991, }{\it Astronomical Data Analysis
Software and Systems 1, Astronomical Society of the Pacific Conference Series }
{\bf 25,}{ 427}
\end{description}

\appendix

\section{Changes Since Previous Releases}

A number of bugs have been fixed since previous releases, and there have
been a large number of minor changes.
The following significant changes to the features of the package have occurred
since the March 1991 Release.

\newcounter{changecntr2}
\begin{list}{(\roman{changecntr2})}{\usecounter{changecntr2}}
\item
The MANUAL and WINDOW menus in LONGSLIT have been combined.
Also the menu for setting the fit type now includes what was previously
the flags menu.

\item
Scaling for optimisation has been improved.

\item
CUBEIN is withdrawn (its function can be performed by LONGSLIT).

\item
The facility to ``edit'' the results cube is available.

\item
The parameter TRANSFER for LONGSLIT has been removed-the code checks
whether any such fits are defined, and performs them automatically.

\item
The extensive output when LONGSLIT and other programs were started up
has been replaced by something much briefer.

\item
CHECK is now an output option, and available via a command line
parameter (and hence can be accessed in batch).
The behavior of this option has changed slightly otherwise.

\item
The results structure has been changed.
The programs which use it will convert any ``old'' format structures to
``new'' format as they encounter them (this isn't done for CLONE files,
but running LONGSLIT on a file will convert it).
This means that the main results are stored as an NDF (see SGP/38) and
the fit status is now a separate array.
The fit status and control coding is now much more flexable, which means
that some parameters of the programs can be removed.

\item
A version of TWODSPEC is available to run on Suns.
Only a small number of routines are significantly different between the
two versions.
Since Sun FIGARO is FIGARO 2.4, and a few other libraries are not yet
generally available on Suns, a few ``substitute routines'' are included,
for example for the DSA FITS routines.

\item
TWODSPEC can also be used with the ADAM Figaro on UNIX  hosts, at
present Suns, DECStations and DEC Alphas.

\item
The programs can use STARLINK .SDF files (as used by KAPPA
for example) as well as .DST files.
These are of course supported by FIGARO 3, but a few changes had to be
made to TWODSPEC (these were incorporated in a ``mini-release'' of
TWODSPEC when FIGARO 3 was released).

\item
The fitting routines have been altered considerably, to make it easier
to add further models, fitting algorithms, etc.
This also means that you can use any combination of the various fitting
options, except that the tied doubles are only available with E04GBF.
A modified Levenberg-Marquardt algorithm is now available (which can be
faster than using the Nag routines), and is now the default.
The routines to use can be either set in the fits menu, or within the
multiple fitting.

\item
X11 (Motif) interfaces are now provided for longslit and arc2d.

\item
The limit of 800 lines for the line identification lists has been removed.

\item
The line-finding code has been completely re-organised, with virtually
everything being controlled by cursor hits.

\item
The dispersion bit of ARC2D now uses a graphical interface, with
cursor options replacing the menus. This uses PGPLOT so can be used on
any graphics device which PGPLOT can use which has a cursor. The same
is true of the manual altering of guesses in LONGSLIT/FIBDISP.
\end{list}

\section{Bugs/Limitations}

BATCH does not handle long (over 80 character) comment lines properly.

Sometimes odd behavior has occurred with large data frames (such as from
TAURUS), this seems to be due to something odd in VMS.

\section{Tailoring to Your Needs}

\subsection{Graphics}

As with FIGARO (from version 2.4), the VMS versions of the
programs use GNS to translate the
graphics device names, so the same user variables are used as for
FIGARO (the Sun version uses native PGPLOT following the example of
Sun FIGARO).
Thus the FIGARO commands SOFT and HARD can be used to set the devices
for LONGSLIT etc.
Checking is carried out by SOFT and HARD, but if for any reason
(e.g.\ attempting to use an xwindows device when DISPLAY isn't set)
the programs described here fail to open a device when running
interactively (except for greyscale), the user is prompted for a new
device name.

On VMS plot files are automatically sent to the plotter, being printed
PRINT/PASSALL/DELETE/NOTIFY (this is performed using SYS\$SNDJBCW for
those who like to know such things!).

Note that, for the cursor to work properly on the ARGS or IKON (other
than when used on greyscale or profile array plots in FIBDISP) the
device must be of the type IKON1\_VT, ARGS1\_VT etc.

All programs will correctly handle any connection identifier as regards
printing files, not just 0.
It is possible, for example to use the following set of commands:
\begin{quote}\begin{verbatim}
$ HARD "1200,2" FORCE
$ DEFINE GKS_1200_2 MYPLOT.LIS
$ LONGSLIT .....  !Produce hardcopy plots.
\end{verbatim}\end{quote}
 and get a file produced and printed called MYPLOT.LIS.
This may be of use if you have two programs working in one directory
producing plot files (e.g.\ one in batch), confusion will be avoided
(although there are unlikely to be problems even if they do have the
same name, since the programs check the full file name soon after
opening the file, including version number).

Some programs also use a greyscale display, prompted for as required.

For hardcopy devices the line width and font are set to give higher
quality plots.

\subsection{Note on Menus}

Many of the programs use menus to find out what the user requires next.
When a menu-type prompt is given, the answer may be abbreviated, as long
as a unique match is achieved (in many cases the most abbreviated form
is given in brackets).
Some menus also allow numbers or additional strings to follow (or even
replace) the menu options themselves.

The menu parser can accept the command ``\%$n$'' to alter the prompting
style.
At present $n$ can be from 0 (no menu, just menu name) to 3 (full menu,
abbreviations given in square brackets).
This will then apply to future calls of the menu routine (note that the
parameter menus (e.g.\ ARC\_OPTS) are dealt with separately to most of
the menus, and only support option 2.
Options 0 and 1 are for experienced users, especially if they are
working with slow terminal lines (or slow computers!).
The formats are as below:
\newcounter{menus}
% LATEX doesn't seem able to handle a counter of 0, so 1st item has to
% explicitly give the header!
\begin{list}{Option \arabic{menus}:}{\usecounter{menus}}
\setcounter{menus}{0}
\item[Option 0]
\hspace*{150 mm}
\begin{quote}\begin{verbatim}
Edit options [] -
\end{verbatim}\end{quote}
\item\samepage
\hspace*{150 mm}
\begin{quote}\begin{verbatim}
Look,Planes,Ascending,Descending,Exit
Edit options [] -
\end{verbatim}\end{quote}
\item (default)
\hspace*{150 mm}
\begin{quote}\begin{verbatim}

=========< E d i t   o p t i o n s >=========

Look       : Look at data values
Planes     : Move planes of results around
Ascending  : Sort components by wavelength
Descending : Sort components by wavelength
Exit       : Return to main menu
Edit options [] -
\end{verbatim}\end{quote}
\item
\hspace*{150 mm}
\begin{quote}\begin{verbatim}

=========< E d i t   o p t i o n s >=========

LOOK          [L] : Look at data values
PLANES        [P] : Move planes of results around
ASCENDING     [A] : Sort components by wavelength
DESCENDING    [D] : Sort components by wavelength
EXIT          [E] : Return to main menu
Edit options [] -
\end{verbatim}\end{quote}
\end{list}
For options 1 and 2 the minimum abbreviation is given in upper case.

The option is also provided here to spawn to a subprocess, useful if you
receive mail when running LONGSLIT for example:
\begin{quote}\begin{verbatim}
Window Menu [FIT] - %$
$ MAIL
...
$ LOGOUT
Window Menu [FIT] -
\end{verbatim}\end{quote}
This option is not implemented under Unix---use control z instead---and
doesn't work from ICL.
The command ``??'' will output help on the menu.

\subsection{The Defaults File}
\label{deffile}

This file (at present called TWODSPEC\_DEFAULTS.DST) is used to allow
the behaviour of programs to be varied.
The elements have the following meanings:
\begin{description}
\item[FIT\_HANDLER]
If this has the value 1 then a condition handler is used during the
fitting (this is in addition to the FIGARO default handler).
This condition handler is not available with the Sun version.
\item[OPT\_STEP]
This controls the size of jump between iterations in the fitting.
The value multiplies the default.
\item[TOLS]
These are the values copied to the TOLS array in the results structure
when a new results structure is created.
Where appropriate the values will be multiplied by the dispersion.
\item[USEPEAK]
If this is 1 then the single component fitting will default to taking
the peak as the centre, for guessing.
\item[VACUUM]
This is set to 1 to indicate wavelengths are values in vacuum.
\item[AX1\_LOG]
If this is 1 it indicates that the first axis has a log scale (by
default).
\item[AX1\_UNITS]
The default units for the first axis.
\item[RELATIVISTIC]
Type integer, if absent, or of value 1, then a full relativistic
furmula is used for Doppler shifts, otherwise the non-relativistic one
is used.
\item[DEC]
FITS keywords to search to find declination of object (not yet used)
\item[COR\_DEC]
Mutiplier for values from DEC to convert to degrees (not yet used)
\item[RA]
FITS keywords to search to find right ascension of object (not yet used)
\item[COR\_RA]
Mutiplier for values from RA to convert to hours (not yet used)
\item[EQUINOX]
FITS keywords to search to find equinox
\item[UT]
FITS keywords to search to find UT
\item[TELESCOP]
FITS keywords to search to find telescope name (in form acceptable to SLALIB)
\item[LONG\_OBS]
FITS keywords to search to find longitude of observatory
\item[COR\_LONG\_OBS]
Mutiplier for values from LONG\_OBS to convert to degrees
\item[LAT\_OBS]
FITS keywords to search to find latitude of observatory
\item[COR\_LAT\_OBS]
Mutiplier for values from LAT\_OBS to convert to degrees
\end{description}
The file is kept in FIGARO\_PROG\_N, but may be copied to another
directory to allow private versions.
The values may be set using the program ALTDEF (see
section~\ref{altdef}).
If the multipliers are absent they are taken as 1.0.

An example of a defaults file is given below (as shown by Figaro exam):
\begin{quote}\begin{verbatim}
twodspec_defaults   Struct
  .FIT_HANDLER   Int     0
  .OPT_STEP      Float   1
  .TOLS[13]      Float   0.10000 3 -3 0.2000 8 1 5 1000 20 5 2.000E-3
                                                      .... 2.000E-3 3
  .USEPEAK       Int     1
  .VACUUM        Int     0
  .AX1_LOG       Int     0
  .AX1_UNITS[20]  Char   Angtroms
  .Z             Struct
    .DATA[1]     Float   0
  .COR_LAT_OBS[1]  Float 1
  .LAT_OBS[8,1]  Char    LAT_OBS
  .LONG_OBS[8,1]  Char   LONG_OBS
  .COR_LONG_OBS[1]  Float -1
\end{verbatim}\end{quote}
In this case COR\_LONG\_OBS is used to correct for the value of
longitude in the file's being positive east, whereas we want positive
west. Note that you can have several values in each array, which would
act as a search list (at present up to 5 values).

\subsection{X defaults}
\label{x-deflts}
The X11 version of the interface may be tailored to individual needs using
X resources. In the table (\ref{tb.motif}) the names and the classes
are given (classes starting in upper case).
\begin{table}
\small
\begin{tabular}{|c|c|c|}
\hline
{\it command name}& main        & Menus       \\
                  & XmRowColumn & XmRowColumn \\ \cline{3-3}
                  &             & text        \\
                  &             & XmScrolledWindow, XmText \\
\hline
\end{tabular}
\caption{Widgets for Motif interface}
\label{tb.motif}
\end{table}
In general gadgets are used where possible, rather than full widgets
(this has speed advantages), and Motif convenience fucntions are
used. The latter means that not all widgets are explicitly given names
in the interface code. Given the above, in most cases it should be
obvious which widget/gadget is being used.

Note that although asterixes and full stops are interchangeable for many
purposes as delimiters, an asterix can ``stand'' for any number of other
widget names, which could produce unexpected results if you're not careful.

The resources for the above may be obtained from the appropriate documentation.
Typical resources include width, height (both in pixels), foreground and
background (both colours).

There are three additional resources, {\it helpdirectory}, {\it soft}
and {\it hard}.
{\it helpdirectory} should be set to the path where additional help
files are kept.
This might be given in a .Xdefaults file as
\begin{quote}\begin{verbatim}
Twodspec.help.helppath: /home1/tnw/figaro/twodspec/X/Help/
\end{verbatim}\end{quote}
Note that the trailing slash {\em must} be included.
{\it soft} and {\it hard} are the soft and hardcopy devices to use,
and should have the same values as you'd use for the FIGARO functions
SOFT and HARD (these programs use X resources instead of using
FIGARO variables).

\section{Note on Installing These Programs}
\label{append.install}

\subsection{VMS}

To run these additions to FIGARO, the .EXE, .PAR and .INF files should
be in a ``FIGARO directory'' (see below), as should the file FIGARO.COM
(for the latter the default directory is unsuitable, unless it also has
a logical name of the FIGARO\_PROG\_U... type).
For the purposes of this discussion a ``FIGARO directory'' is one which
programs will search for files-the same as used by other FIGARO
routines.
ARC2D and LONGSLIT also require .IDS files supplied may be used, or
private versions prepared using an editor.
The format of these files is given in section~\ref{long.id}.
The .IDS files may be in a ``FIGARO directory''.
To run BATCH one requires BATCH.EXE, BATCH.DAT, NOBATCH.DAT and
BATCH\$QUEUE.DAT. BATCH.DAT contains a list of FIGARO commands with
parameters (continued on more than one line by putting a ``-'' in the
first column of the second and any successive lines).
Each line of this file must start with the command name (in upper case),
followed by a comma.
The format of the remainder of the line is not important to the program
BATCH, but the format used is to list the parameters, with keywords in
square brackets.
Any local programs can be added to this if required (although too many
could overflow the work arrays).
A program BCON is supplied to create this file using the connection
files as input (this means that functions without connection files need
to be added manually, and if a function is not suitable for batch use it
will have to be removed).
The version of BATCH.DAT included includes the NDPROGS programs as well
as the ``system'' FIGARO and TWODSPEC.
If FIGARO or NDPROGS is updated, BCON should be run on the connection
files.
NOBATCH.DAT contains a list of commands unsuitable for batch use.
Both BATCH.DAT and NOBATCH.DAT require to be updated with a new version
of FIGARO.
BATCH\$QUEUE.DAT is simply a list of available batch queues, and should
be edited accordingly.

The programs are linked with the FGRSHR sharable image, which applies
to FIGARO versions {\em later} than 2.4. This is because of a bug in
the earlier version of DSA\_GET\_RANGE, the use of the DSA FITS
routines, and the large number of parameters used by LONGSLIT.
Problems may be found using FIBDISP if the file was created by an
earlier version of FIB2CUBE, although versions from the first release of
TWODSPEC will be okay.
The FGRSHR provided with FIGARO 3 works properly.

These programs are implemented using DSA, so should require little
alteration to work with future versions of FIGARO. There are some
routines, however, which make direct calls to DTA, and would need
re-coding if DTA were withdrawn at any time.

To run from ICL the command
\begin{quote}\begin{verbatim}
DEFSTRING TWODSPEC LOAD FIGARO_PROG_N:TWODSPEC.PRC
\end{verbatim}\end{quote}
(with the relevant directory substituted if TWODSPEC is not installed in
FIGARO\_PROG\_N) should be executed, preferably in an ICL startup file.

The libraries required are SOURCE, LIBMAIN, LIBOPT, LIBFIBRE, LIBUTIL
and LIBGRAPH.

In addition to the libraries specific to this package, use is made of
the following FIGARO libraries: DSA, DTA, FIG, GEN, ICH, PAR, the
following STARLINK libraries: CHR, SLA, HDS (only via DTA), SGS (HIMAGE
only), GNS, the graphics library PGPLOT (STARLINK version), the
following DEC libraries: LIB\$, SYS\$, and the following non-DEC
proprietary software: NAG, GKS.
The order of linking with the libraries is as follows (where
{\tt (directory)} is the directory containing the libraries):

\begin{quote}\begin{verbatim}
! Options file for linking with the libraries used for LONGSLIT etc.
! which are specific to LONGSLIT, etc.
!  T.N.Wilkins 5/12/88
(directory):source/library
(directory):libfibre/library
(directory):libmain/library
(directory):libopt/library
(directory):libmain/library
(directory):libgraph/library
(directory):libutil/library
\end{verbatim}\end{quote}
The logical names LIB\_MISC, LIB\_MAIN and LIB\_OPT are required for
compilation of many routines. These should point to the text libraries
LIBUTIL, LIBMAIN and LIBOPT respectively.

The package has been set up such that in order to compile and link it,
the only file which needs to be manually altered is
[.TWODSPEC.LIB]SETUP\_LOGICALS.COM. The command procedure
[.TWODSPEC.DEV]MAKE.COM will then create the whole package.
The sub-directories of the top level directory contain the following:
\begin{description}
\item[ADAM] Files required only for the ADAM monolith version of
TWODSPEC.
These are copies of those used for the ADAM monolith version of FIGARO.
\item[DEV] Files concerned with creating the run-time files, also
the source for the parameter files.
\item[FIGLIB] Private versions of FIGARO libraries. These are just
more up-to-date versions than included with the currently released
version of FIGARO, and will not be needed when the next version of
FIGARO is released.
\item[LIB] Source code and object libraries.
\item[TEST] Files to test the setup of the package.
\item[TWODSPEC] Run-time files.
\item[UPDATE] Used for scratch files during linking etc.
\end{description}

\subsection{Graphics}

The printing of hardcopy plot files requires the logical names as
follows to be set up:
\begin{quote}\begin{verbatim}
$ DEFINE GKS_(workstation type)_QUEUE (queue name)
\end{verbatim}\end{quote}
 Where (workstation type) is the GKS workstation type (a number) e.g.\
for the printronix 1200.
In fact what is used (for all programs excpet HIMAGE) is the string
returned by PGQINF as the device type (this is important when native
PGPLOT is used).
(queue name) is the name of the queue on which the file should be
printed, e.g.\
SYS\_PRINTRONIX. So for the printronix the following should be used
(with the correct GKS):
\begin{quote}\begin{verbatim}
$ DEFINE GKS_1200_QUEUE SYS_PRINTRONIX
\end{verbatim}\end{quote}
 This will have to done for each workstation type, twice where a device
can be addressed by 2 numbers, e.g.\ 1200 and 1201.
Note that the FIGARO.COM supplied assumes the printer queue for canon
laser printer files to be SYS\_LASER.
If there is more than one similar laser printer on a site, it will
probably be found convenient to set up commands to alter this logical
name to point to the required printers.
Alternatively printing may be suppressed by removing these logical
names.
The Sun version uses environment variables, and the translation is used
after the ``-L'' flag, so additional flags may be added if required (it
simply executes a shell command).

For use with the ADAM version the logical names have to be in the job
(or system) logical name table, not the process.
This is done in the file ADAM\_TWODSPEC.COM included with the package.
This file is executed when the command TWODSPEC is given.
The logical names set up by this may well not be applicable to a
particular site, so correct definitions should be given in either
FIGARO\_PROG\_L:TWODSPEC\_GRAPH.COM or
FIGARO\_PROG\_U:TWODSPEC\_GRAPH.COM.
This file might be as follows:
\begin{quote}\begin{verbatim}
$!
$! Definitions for plotting of hardcopy output (GKS 7)
$! (see documentation)
$!
$ define/nolog/job gks_1200_queue rgo_printronix
$ define/nolog/job gks_1201_queue rgo_printronix
$ define/nolog/job gks_2600_queue obs_laser
$ define/nolog/job gks_2601_queue obs_laser
$ define/nolog/job gks_1000_queue zeta
$ define/nolog/job gks_1001_queue zeta
\end{verbatim}\end{quote}

This is all done in FIGARO.COM. These routines use the user variables
`SOFT' and `HARD' for graphics device names (the same as FIGARO),
this makes them fully compatible with the GKS version of FIGARO.
These are accessed in the routines OPENHARD and OPENSOFT.

Substitute routines are provided to allow the use of TWODSPEC with
FIGARO 2.4, which are used in the Sun version. Environment variables
are used instead of logical names under UNIX.

\subsection{UNIX}

You first of all need to select the appropriate site.def for the system
you have, and edit it to reflect your setup (for example whether you have
IRAF). You then type
\begin{quote}\begin{verbatim}
% imake
% make init
\end{verbatim}\end{quote}
to create the Makefiles.
Running make will then create the libraries and executables, but it is
important to run the script to ``start'' Figaro before running make if you are
creating the ``native'' Figaro version.
Problems have been encountered in which there are complaints about system
libraries having unexpected end-of-files. If you repeat ```make -k''
you should eventually get everything made, if you do encounter any problems.
The main makefile assumes that libX11.so.?.? is to be picked up without
explicity having to give it's location (it should be in /usr/lib, which is the
standard system library).

If you are using one of the ADAM versions the software should then be
installed by typing ```make install'' (this isn't needed for the ``native''
Figaro version).

The README file in the top directory gives more information.

\section{The Contents of the Results Structure}
\label{ap.res}

COMB, ARCSDI, ARC2D and LONGSLIT create a .RES structure in which to
store their results.
This enables repeat fits etc.\ to be performed easily, as well as making
it unnecessary to do all the fits at one time.
Most of this structure is mapped and its arrays thus accessed directly
from the programs.
Data is mapped by the address of the first element of an array in the
file's being obtained by the program.
FORTRAN passes all arguments to subroutines and functions by giving
their address, although for character data this is the address of the
descriptor, which includes the address of the data.
Therefore, if this address is passed to the subroutine or function---its
value not address---then the called routine will treat the data as a
normal array.
For character data a descriptor must be constructed and passed by
address.
In the interests of portability it is better to use an array in common
to pass the address, the element of the array at that address is passed
(even though that is outside the bounds of the array). For character
strings the string must be passed as string(start:end). The arrays are
in common so that they can be referenced with the same offset from
different subroutines.
The same principle can be used to obtain virtual memory
(there are VAX library routines to obtain the addresses of virtual
memory and to `free' such memory when it is finished with).

Since COMB performs fitting of continua rather than lines, the structure
for COMB is different to that for the other programs in that the arrays
are dimensioned in channels where other programs would dimension them in
cross-sections.

 The elements of the structure are in table \ref{tb.res}:-
\begin{table}
\begin{quote}\begin{verbatim}
.RESULTS   Results
  .DATA_ARRAY[21,5,385,1]  Float 10.50 546515.9 6542.3 1.558 165959.0
                                           .... -1.701E+38 -1.701E+38
  .VARIANCE[21,5,385,1]  Float 100 2.069E+7 0.01732 0.08867 1.29E+8
                                           .... -1.701E+38 -1.701E+38
  .MORE          Struct
    .PARAMS[210]  Char   Space1_posBase      Centre_1  Width_1   Height_
    .REST_WAVE[5]  Float 6548.1 6562.8 6583.6 0 0
    .IDS[50]     Char    [NII]6548 HALPHA    [NII]6584?????????????????
    .TRAML[5]    Float   6542.0 6555.0 6575.2 0 0
    .TRAMR[5]    Float   6553.7 6568.1 6589.4 0 0
    .TWODSPEC    Struct
      .ITMASK[5,385,1]  Short 12 10 12 1 1 12 10 12 1 1 12 10 12 1 1 12
                                          .... 12 12 1 1 12 12 12 1 1
      .CONTROL[3,5,385,1]  Int 101110.0 11000 2000 101110.0 11000 3000
                                        .... 1100 101100.0 11000 1100
      .ITERATION   Short 12
      .FIT_STATUS[3,5,385,1]  Int 301114.0 11000 1000 101111.0 11000
                                          .... 11000 1000 0 0 0 0 0 0
      .SELECT[7,5]  Short 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
                                       .... 0 0 0 0 0 0 0 0 0 0 0 0 0
      .TOLS[13]  Float   0.01899 3 -3 0.03799 1.52 0.1899 5 100000.0 20
                                           .... 5 2.000E-3 2.000E-3 3
      .OPTSTATE   Struct
        .PAR_STATUS[21]  Int 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
                                                             .... 0 0
        .FREE_PARAMETERS[21]  Int 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
                                                     .... 0 0 0
        .LINK_INDEX[21]  Int 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
                                                             .... 0 0
        .LINK_CONSTANT[21]  Double 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
                                                         .... 0 0 0 0
        .LOWER_BOUND[21]  Double 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
                                                           .... 0 0 0
        .UPPER_BOUND[21]  Double 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
                                                            .... 0 0 0
        .PERIODIC_PARS[21]  Int 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
                                                             .... 0 0
        .PERIODS[21]  Double 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
                                                             .... 0 0
        .PERIOD_START[21]  Double 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
                                                           .... 0 0 0
      .TEMPLATE   Struct
        .DATA_ARRAY[560]  Float 0 0 0 0 0 0 0 0 0 0 -323.2 8736.9
                                            .... 7299.5 7346.9 7426.4
        .AXIS[1]   Struct
          .DATA_ARRAY[560]  Float 6513 6513.2 6513.4 6513.6 6513.8
                                            .... 6618.8 6619.0 6619.2
      .GROUPS    Struct
        .ALL     Struct
          .TYPE[20]  Char
          .NUMBER   Short 0
          .DATA_ARRAY[5]  Short 0 0 0 0 0
        .SKY     Struct
          .TYPE[20]  Char
          .NUMBER   Short 0
          .DATA_ARRAY[5]  Short 1 1 1 1 1
 \end{verbatim}\end{quote}
\caption[a]{The results structure}
\label{tb.res}
\end{table}
 The .TRAML and .TRAMR arrays store the line positions, that is
the limits considered for optimisation (assumed at
the centre by ARC2D, the .IDS array store their identifications and
the .REST\_WAVE their wavelengths. The .ARC array is used by ARC2D to
decide which lines are to be used in the evaluation of the relationship
between channel number and wavelength. If the element of .ARC
is 0 then the line is included under all circumstances, if it is 1 then
it is included for non-`continuity corrected' data only, otherwise it is
not included for any. A value of 4 indicates that no fits are present,
while 10 and 11 are the values if the user manually deletes a line which
previously had the value 0 or 1 respectively. If arc is 10 or 11 the fits
can be `undeleted'.

The TEMPLATE structure keeps a record of the one-dimensional spectrum
used for line identification.
Since the axis array is also kept, it is possible to CLONE from such an
array, even if the main data array has been scrunched.

The DATA\_ARRAY array is used to store the results of the Gaussian fitting,
and is also used by ARC2D to store the results for the continuity
correction. The errors on the results are stored as variances in the
VARIANCE array. The .PARAMS structure acts as an index to this (used by
the program to determine where to store results).
The .CONTROL array gives the fit type to be performed (in the same form
as the fit status element of the .RESULTS structure).
In the
above example, 171 is the number of cross-sections in the image and 10
is the maximum number of lines allowed for in the structure (this can
be up to 50). Originally the block number was stored, but this gives an
ambiguous way of determining where the block starts and ends. Therefore
the starting cross-section of the fit is now stored (it is possible to
change the blocking and only alter a small number of fits).

The fit is performed on data extracted from ``nwindow'' cross-sections
starting at ``first cross-section''.

\begin{table}[ht]
\small
\begin{center}
\tiny
\begin{tabular}{|l|l|l|l|l|l|} \hline
Element & Digit & Number & Name & Refer to as &Meaning\\ \hline
1 & 1 & 1 & Absorption flag & FIT\_ABS & 0 - Emission\\
  &   &   &                 & & 1 - Absorption\\ \cline{2-6}
  & 2 & 2 & Profile Model & FIT\_MODEL & 0 - none\\
  & 3 &   &               &            & 1 - Gaussian\\
  &   &   &               &            & 2 - Skew Gaussian\\
  &   &   &               &            & 3 - Cauchy/Gaussian\\
  &   &   &               &            & 4 - Centroid\\
  &   &   &               &            & 5 - Lorentzian\\ \cline{2-6}
  & 4 & 3 & Fit type      & FIT\_TYPE & 0 - none \\
  & 5 &   &               &           & 1 - single \\
  &   &   &               &           & 2 - double (fixed separation) \\
  &   &   &               &          & 3 - double (fixed width ratio) \\
  &   &   &               &         & 4 - double (fixed height ratio) \\
  &   &   &               &           & 5 - double (independent) \\
  &   &   &               &           & 6 - Multiple \\ \cline{2-6}
  & 6 & 4 & Number of Components & FIT\_NCMP & or may act as maximum\\
\cline{2-6}
  & 7 & 5 & Weights Method for Profiles & FIT\_WEIGH & 0 - Uniform\\
  &   &   & & & 1 - Variance\\
  &   &   & & & 2 - BIweights\\
  &   &   & & & 3 - Hubber - This is a Robust estimator\\
  &   &   & & & 4 - Cauchy\\
  &   &   & & & 5 - Not used \\
  &   &   & & & 6 - Not used\\
  &   &   & & & 7 - Not used\\
  &   &   & & & 8 - Not used\\
  &   &   & & & 9 - Not used\\ \cline{2-6}
  & 8 & 6 & Profile Fit Status & FIT\_STAT & 0 - No fit \\
  &   &   &                    & & 1 - Success \\
  &   &   &                    & & 2 - Nag error (not serious)\\
  &   &   &                    & & 3 - Nag error (serious)\\
  &   &   &                    & & 4 - Crash \\
  &   &   &                    & & 5 - Failed tols\\
  &   &   &                & & 6 - Failed tols (non-serious Nag)\\ \hline
2 & 1 & 7 & Manual guessing flag & FIT\_MAN & 0 - No manual guessing\\
  &   &   & & &  1 - Manual guessing (between below and fit)\\ \cline{2-6}
  & 2 & 8 & First Guess method & FIT\_GUES & 1 - Centroid \\
  & 3 &   &                    & &  2 - Peak\\
  &   &   &                    & &  3 - Bimodef\\
  &   &   &                    & &  4 - Inherit FORWARD\\
  &   &   &                    & &  5 - Previous answer at this place\\
  &   &   &                    & &  6 - Inherit BACKWARD\\
  &   &   &              & &  7 - REGION (2d for TAURUS -to be defined)\\
  &   &   &                    & &  8 - ROBUST estimator\\
  &   &   &                    & &  9 - MODEL (synthetic model eg
rotation curve)\\
  &   &   &                    & &  10 - P Cygni\\ \cline{2-6}
  & 4 & 9 & Optimization method & FIT\_OPT & Choice of routines\\ \cline{2-6}
  & 5 & 10 & Component number control & FIT\_STST &
                       0 - Fit up to number of components requested\\
  &   &   &  & &         1 - AIC after fitting\\
  &   &   &  & &         2 - AIC, before fitting (using guesses)\\ \cline{2-6}
  & 6 & 11 & Constraints Method & FIT\_CONSTR & 0 - No Constriants\\
  &   &   &  & & 1 - Bounds only\\
  &   &   &  & & 2 - General Constriants (read from constraints struc)\\
  &   &   &  & & 3 - EQUATIONS  -read from equations Struct\\ \cline{2-6}
  & 7 & 12 & Dynamic weights flag & FIT\_DYNWEI & 0 - no dynamic weights\\
  &   &   &  & & 1 - Use dynamic weights\\ \cline{2-6}
  & 8 & 13 & Fit group & FIT\_GROUP & Number of group \\
  & 9 &    & & & \\
\hline
3  & 1 & 1 &Method of Removal & BACK\_REMOV & 0 - subtract \\
  &   &   &                  & & 1 - divide\\
\cline{2-6}
  & 4 & 3 & Background Order & BACK\_ORDER & \\
  & 5 & & & & \\
\cline{2-6}
  & 6 & 4 & Weight Function to be used & BACK\_WEIGH & as for entry 5 above \\
\cline{2-6}
  & 2 & 2 &GLOBAL Background model & BACK\_MODEL & 0 - No base\\
  & 3 &   &                        & & 1 - Flat BAse\\
  &   &   &                        & & 2 - Cubic Spline\\
  &   &   &                        & & 3 - Chebyshev fit NOW\\
  &   &   &                        & & 4 - FITCONT cheby\\
  &   &   &                        & & 5 - Power law\\
  &   &   &                        & & 6 - Black Body\\
  &   &   &                        & & 7 - EMPIRICAL\\
  &   &   &    & & 8 - Black Body with Optical Depth Cutoff\\
\cline{2-6}
 & 7 & 5 & Optimization Method & BACK\_OPT & \\ \cline{2-6}
 & 8 & 6 & local Fit flag & BACK\_LOCAL & 0-9 Codes as above \\ \cline{2-6}
 & 9 & 7 &      Success of FIT & BACK\_STAT & \\
\hline
\end{tabular}
\caption[a]{The meaning of the fit status element}
\label{tb.fitstatus}
\end{center}

For background model 3 the polynomial is fitted immediately before the
line profile fitting is carried out.
For background model 4 the polynomial is evaluated using coefficients
previously stored in the data file and subtracted before fitting.
\end{table}

The FIT\_STATUS array contains information as to the type of fit
performed and as to the success or otherwise of the fit (see
table~\ref{tb.fitstatus}).

Only LONGSLIT and FIBDISP are able to fit multiple Gaussians, so ARC2D,
COMB, and ARCSDI use a smaller results structure.

The last element in this direction is the density scale factor, used for
scaling previous fits for use as first guesses to another fit.

The elements of the CONTROL array are the same as those of the
FIT\_STATUS array, except that they do not include information on the
success of the fit.
This array is used to set which type of fit is to be performed.

The .ITMASK array and .ITERATION are used in conjunction to prevent
accidental refitting of data.
Note that a check is made to prevent a fit's being accidently
overwritten by a simpler fit.
The order of increasing complexity is:- single Gaussian, skew
Gaussian, Cauchy function, double Gaussian, multiple Gaussian.
Both this type of checking and the checking using .MASK and .ITERATION
can be overridden in MANUAL mode.

The .TOLS array provides a way of retaining the values for tolerances
between successive work on the same file, and also (using the FIGARO
function LET, or the CLONE COPY option in LONGSLIT), provides a way of
transferring these from one file to another.
This retention enables tolerances to be applied in batch, since it is
much simpler to specify which tolerances to apply, rather than to list
all the values to use.
It is also used during multiple fitting in batch to determine the number
of components to fit.

The results structure for three--dimensional data arrays used by FIBDISP
is similar to the above, but has some extra elements.
The VARIANT element gives the form of the relationship between the array
elements of the main data array, and their actual positions.
For type=``HEX'' there is a .XDISP array: XDISP[IY] defines the
displacement in the X direction of the .Z.DATA[IX,IY], relative to the
value given by X.DATA[IX] (that is the element of the data of axis
number one).
ORDER (at present always ``SORT'') indicates whether the data is
arranged with the first or the last array index varying over wavelength.
SORT corresponds to the first axis being wavelength.
TOTAL\_INTENSITY stores the total intensity integrated along the
wavelength axis, and is useful for locating interesting areas of the
data.
\end{document}

