\documentstyle{article} 
\pagestyle{myheadings}

%------------------------------------------------------------------------------
\newcommand{\stardoccategory}  {Starlink General Paper}
\newcommand{\stardocinitials}  {SGP}
\newcommand{\stardocnumber}    {31.9}
\newcommand{\stardocauthors}   {M D Lawden}
\newcommand{\stardocdate}      {22 November 1990}
\newcommand{\stardoctitle}     {THE STARLINK PROJECT}
%------------------------------------------------------------------------------

\newcommand{\stardocname}{\stardocinitials /\stardocnumber}
\markright{\stardocname}
\setlength{\textwidth}{160mm}
\setlength{\textheight}{240mm}
\setlength{\topmargin}{-5mm}
\setlength{\oddsidemargin}{0mm}
\setlength{\evensidemargin}{0mm}
\setlength{\parindent}{0mm}
\setlength{\parskip}{\medskipamount}
\setlength{\unitlength}{1mm}

\begin{document}
\thispagestyle{empty}
SCIENCE \& ENGINEERING RESEARCH COUNCIL \hfill \stardocname\\
RUTHERFORD APPLETON LABORATORY\\
{\large\bf Starlink Project\\}
{\large\bf \stardoccategory\ \stardocnumber}
\begin{flushright}
\stardocauthors\\
\stardocdate
\end{flushright}
\vspace{-4mm}
\rule{\textwidth}{0.5mm}
\vspace{5mm}
\begin{center}
{\Large\bf \stardoctitle}
\end{center}
\vspace{5mm}

\section {INTRODUCTION}

This paper gives an overview of the Starlink Project.
It describes its objectives, history, and organisation; the user
population served; and the hardware and software available.
It concludes with a review of the main lessons that have been learnt during
the life of the Project.
Specific details about several aspects of the Project are given in the
appendices.
Such details rapidly get out of date, so for up-to-date information you should
consult the on-line information summaries stored on Starlink computers.

\section {OBJECTIVES}

The main objectives of the Project are to:
\begin{itemize}
\item Provide and coordinate interactive data reduction and analysis facilities
for use as a research tool by UK astronomers.
\item Encourage software sharing and standardisation to prevent unnecessary
duplication of effort.
\item Provide systems software support for astronomers.
\end{itemize}
Starlink also fills several other roles for UK astronomers such as providing
an electronic mail service and advising on the purchase and operation of
Starlink compatible computer equipment.

\section {HISTORY}

In the 1970's it became clear that the data processing facilities available to
UK astronomers were inadequate to deal with the anticipated flood of data in
digital form which would be generated by new data acquisition techniques.
For example, a photographic plate could produce a set of data comprising
$10^{8}$ pixels and data rates of $10^{10}$ bits/day were possible.
In April 1978, the Science Research Council (now SERC) set up a Panel on
Astronomical Image and Data Processing under the chairmanship of Professor M J
Disney to ascertain the computing needs of UK astronomers for the next 5 to 7
years.
This panel reported in April 1979 and recommended the installation of 6
super-minicomputers connected together in a star network; hence the name
{\em Starlink}.
The computer type chosen was the DEC VAX 11/780.
Between December 1979 and July 1980, VAXs were installed at Cambridge,
Manchester, RAL, RGO, ROE, and UCL.
Astronomers at other sites used Starlink facilities via network links.
These groups were called Remote User Groups.
The Project was inaugurated on 24th October 1980 by Mr D N MacFarlane,
Parliamentary Under Secretary at the Department of Education and Science.

The biggest remaining question concerned the plans for software development.
To consider this, a workshop was held at Appleton Laboratory in November 1979.
This made the following recommendations:
\begin{itemize}
\item An overall supervisory system should be written.
\item There should be a hierarchical data system.
\item There should be a command system with parameter handling, defaulting
mechanisms, symbols, help, prompting, multi-stream, and batch.
\item There should be a standard graphics system.
\item Data interchange on tape should be in FITS format.
\end{itemize}
The initial Project staff were appointed by mid 1980.
The administrative centre was located at RAL and each of the other sites had a
Site Manager to manage the computer, support the users, and implement Starlink
policies.
The first software release was made in March 1980, and in September 1980 the
initial version of the first Starlink software environment was distributed.
This formed the basis for much subsequent application software.

The network has been considerably extended since the initial six nodes were set
up, and there are now 24 interconnected computer systems based on VAXes which
are regarded as part of the Starlink network.
One of these (STADAT) differs from the rest in that it provides a central data
and software facility for all Starlink users.
Another, the `Project Cluster', is devoted mainly to providing facilities for
project staff.
The other 22 serve local astronomical communities at the sites listed in
Appendix B.
In 1990 the RGO node merged with the Cambridge node when RGO moved from
Sussex to Cambridge.

In 1989 a major refurbishment of the hardware began with the replacement of
the original 11/780 at RAL by a VAX 3500.
The other 780s and 750 have also been replaced by more modern VAXes.
In 1990 the Project started to install powerful UNIX-based scientific
workstations (mainly SUN Sparcstations) at some of its sites.
The networking has also changed from a star network of leased lines to the use
of a packet switched network.

In the spring of 1986 a major exercise was carried out to determine what the
users thought should be the future direction of the Starlink Project.
A 14 page questionnaire was distributed to 669 people (mostly users) of which
394 were returned.
These returns were carefully analysed and the results have influenced the
direction of the Project.
The analysis is available as Starlink paper SGP/35.

\section {ORGANISATION}

The position of Starlink in the context of the funding and organisation of
British science in general, and astronomy in particular, is shown in Figure 1.
All abbreviations are explained in Appendix A.
Starlink is controlled by the APS Board of the SERC, which is funded by the
Department of Education and Science.
The Board is advised on Starlink policy matters by the Starlink Users'
Committee.
At RAL, Starlink is part of the Planetary and Astronomical Data Division of
the Space Sciences Department.

The Starlink sites are grouped into nine areas as shown in Table 1.
Each of these areas has its own Area Management Committee serving a
geographical catchment area.
This committee monitors the use being made of the Starlink equipment in that
area, and advises the Project on resource allocation and user requirements.
Special Interest Groups are user groups concerned with software development
in specific application areas.
Currently, there are ten such groups (Database, Hubble Space Telescope, Image
Processing Environment, IRAS, IUE, Radio Astronomy, Solar Astrophysics,
Spectroscopy, Volumetric Imaging, and X-ray).
There is also a Hardware Advisory Group to advise the project on the selection
of suitable hardware.
The AMC's and SIG's formally {\em report} to SUC and {\em advise} RAL Starlink
management.

Except for the 3 nodes at SERC establishments, Starlink funds all the Site
Managers at the Major nodes via site contracts, and also a number
(currently 6) of Programmers on contract at University sites to work closely
with the SIGS.
Every four months all the Site Managers meet at one of the sites for general
discussions, and once a year the meeting is enlarged to include the
Programmers.
These meetings are recognised as vital for the cohesiveness of the Project.
Details of the Starlink sites are given in Appendix B.

\setlength{\unitlength}{1mm}
\begin{center}
\begin{picture}(110,120)
\thicklines
\put (40,105){\framebox(30,5){GDP}}
\put (40,90){\framebox(30,5){Government}}
\put (40,75){\framebox(30,5){DES}}
\put (40,60){\framebox(30,5){SERC}}
\put (40,45){\framebox(30,5){APSB}}
\put (40,30){\framebox(30,5){RAL ({\tiny APSB})}}
\put (80,30){\framebox(30,5){SUC}}
\put (40,15){\framebox(30,5){Starlink}}
\put (0,0){\framebox(30,5){AMCs}}
\put (40,0){\framebox(30,5){HAG}}
\put (80,0){\framebox(30,5){SIGs}}
\put (55,95){\line(0,1){10}}
\put (55,80){\line(0,1){10}}
\put (55,65){\line(0,1){10}}
\put (55,50){\line(0,1){10}}
\put (55,35){\line(0,1){10}}
\put (55,20){\line(0,1){10}}
\put (55,5){\line(0,1){10}}
\put (95,35){\line(0,1){12.5}}
\put (15,5){\line(0,1){5}}
\put (95,5){\line(0,1){5}}
\put (15,10){\line(1,0){80}}
\put (70,47.5){\line(1,0){25}}
\put (71,115){\pounds M}
\put (71,110){519,000}
\put (71,95){196,000}
\put (71,80){19,600}
\put (71,65){405}
\put (71,50){63.4}
\put (71,37){11.7}
\put (71,20){1.813}
\end{picture}
\end{center}
\begin{tabbing}
XXXXXXXX\=Starlink breakdown:XXXX\=Capital + RecurrentXXXXXXXX\=\kill
\>Starlink breakdown:\>Capital + Recurrent\>= 1.453 Million\\
\>\>Manpower: 12DMY (12x30K)\>= 0.360 Million
\end{tabbing}
\begin{center}
Figure 1. Starlink Management and Finances --- 1989/90
\end{center}

\section {USERS}

Currently (Nov 90) the Starlink user population stands at 1262; the number of
users per site is shown in Table 2\footnote{Figures for Kent and Hatfield are
not yet available}.
The total number of users grew steadily at about 140 a year until the end of
1984.
Then the growth rate declined as the influx of new users was balanced by the
departure of old ones.
Since the beginning of 1987 there has been a further significant increase in
user numbers.
The growth is illustrated in Figure 2.

\begin{table}[htb]
\begin{center}
\begin{tabular}{|l|l|l|l|l|} \hline
{\bf Area and AMC name} & {\bf Major nodes} & {\bf Minor nodes} &
{\bf RUGs (UK)} \\
\hline
\hline
Scotland & Edinburgh/ROE & St Andrews & Glasgow \\
\hline
Northern Ireland & QUB/Armagh & - & - \\
\hline
North East & Durham & - & - \\
\hline
North West & Jodrell Bank & Keele & Leeds \\
& Manchester & Lancs (Preston) & Bradford \\
\hline
Midlands West & Birmingham & - & Aberystwyth \\
\hline
Midlands East & Leicester & - & - \\
\hline
East Anglia & Cambridge/RGO & - & DAMTP Cambridge \\
\hline
South West & Oxford & Cardiff & Bristol \\
& RAL & Southampton & - \\
\hline
London \& South East & UCL & QMW London & Hatfield Poly \\
& & ICSTM London & MSSL (Dorking) \\
& & Sussex & - \\
& & Kent (Canterbury) & - \\
\hline
\end{tabular}
\caption {Starlink Areas (Nov 1990)}
\end{center}
\end{table}

Some users use more than one site, in which case a particular site is chosen as
{\em Primary} and the others as {\em Secondary} for this user.
This is to avoid double counting and to manage documentation distribution.

\begin{table}[htb]
\begin{center}
\begin{tabular}{||l|r|r|r||}
\hline
{\em Node} & {\em Primary} & {\em Secondary} & {\em Total}\\ 
\hline \hline
Armagh       &  22 &   2 &  24 \\
Belfast      &  28 &   4 &  32 \\
Birmingham   &  49 &  13 &  62 \\
Cambridge    & 255 &  26 & 281 \\
Durham       &  72 &  16 &  88 \\
Jodrell Bank &  79 &  23 & 102 \\
Leicester    &  73 &  25 &  98 \\
Manchester   &  34 &  26 &  60 \\
Oxford       &  70 &  18 &  88 \\
RAL (Project) & 32 &  27 &  59 \\
RAL (Astron) &  43 &  16 &  59 \\
ROE          & 142 &  29 & 171 \\
UCL          & 120 &  54 & 174 \\
\hline
Cardiff      &  38 &   6 &  44 \\
ICSTM        &  41 &   4 &  45 \\
Keele        &  10 &   3 &  13 \\
Preston      &  16 &   4 &  20 \\
QMWC         &  52 &   5 &  57 \\
Southampton  &  37 &   2 &  39 \\
St Andrews   &  20 &   7 &  27 \\
Sussex       &  29 &   5 &  34 \\
\hline \hline
{\em Totals} & 1262 & 315 & 1577\\
\hline
\end{tabular}
\caption {Starlink User Population (Nov 1990)}
\end{center}
\end{table}

\section {HARDWARE}

The hardware is based mainly on local area VAXclusters (LAVC) of MicroVAXs.
The potential speed problems of channelling disk traffic through the Ethernet
were avoided by using multiported disks from System Industries.
The results have been very satisfactory.

A benefit of the LAVC approach, as opposed to the provision of a single
large machine, has been that it is now possible to add on extra computer power
in relatively small increments.
The clusters offer considerable flexibility to system managers.

Current enhancements include VAXstations, X-terminals, and at some sites the
Avalon Vaccelerator board.
There are also about 20 Unix workstations (mostly SUN Sparc) and will be many
more in future years.

\setcounter{figure}{1}
\begin{figure}[htb]
\begin{center}
\begin{picture}(160,120)
\end{picture}
\caption {Starlink User Population Growth.}
\end{center}
\end{figure}

Image displays have been developing alongside the CPUs.
The original \verb+512x512+ Sigmex ARGS systems were replaced by
\verb+780x1024+ Digisolve Ikon systems a few years ago.
Recently, interest in separate displays has fallen away as colour workstations
have become affordable.
As well as providing a local and accessible CPU, the workstations offer
still higher resolution (typically \verb+1024x1280+) and are much faster.
These factors are felt to outweigh the drawback that most current workstations
do not offer any separately controllable `graphics overlay' planes; a function
that is difficult to provide satisfactorily in software.

\section{NETWORKS}

Starlink was the first astronomical data processing system to use networking
extensively; though one or two similar services have grown up since (the
Italian ASTRONET for example).
The early links ran at 4800 baud and used DEC protocols exclusively.
In due course, Starlink was integrated with the UK Joint Academic Network
(JANET), giving access not only to other Starlink VAXs but to thousands of
machines of all types at research establishments and educational
institutions throughout the UK.
JANET now provides Starlink with links an order of magnitude faster than
those available in 1980, and is extremely reliable.
It also provides gateways to international networks, including IPSS, SPAN, and
BITNET/EARN.

At each site the VAXclusters see each other as DECNET nodes, achieved by
running DECNET protocols over JANET's basic X.25 packet-switching service.
Other protocols, notably TCP-IP, are in use at various sites to support
network connections to Unix workstations and PCs.

The Starlink machines were networked originally in order to facilitate the
sharing of software and to allow rapid dissemination of enhancements and
bug fixes.
This is still a vital role; however, enormous and largely unforseen benefits
have come simply from the availability of electronic mail, both national and
international.
This has led to greatly increased levels of collaboration, both between
research astronomers and those writing software.
More recent benefits have been the provision of database services and DEC's
VAX Notes conferencing system.

\section {SOFTWARE}

The Project has installed a {\em Starlink Software Collection}.
This is a set of software items which is controlled centrally at RAL by the
Starlink Software Librarian.
It is implemented at every Starlink site under the control of the Site Manager.
The Librarian updates the Collection by first preparing a software
release in a special directory.
This includes a Starlink Software Change (SSC) notice and a command procedure
to automatically perform the update.
The SSC gives information about the release and describes how to implement it.
When the update is released, the SSC is sent over the network to the Site
Managers who then carry out its instructions.
Normally, the full contents of a release are copied over the network by each
Site Manager, although large releases may need to be distributed on magnetic
tape.
479 SSCs have been issued so far (Nov 90) and about 60 new ones are issued
yearly.

The Collection comprises 91 items and contains over two million lines of
source code.
It occupies 361 Mbyte of disc space and data catalogues occupy a further
322 Mbyte.
The categories of software include: data analysis packages, software
environments, graphics, data base, astronomical and general purpose utilities,
and libraries.
A summary of the items is given in Appendix C.
Besides the Starlink nodes, Starlink software has also been distributed to 132
non-Starlink sites of which 113 are abroad.
The sites which have received Starlink software are listed in Appendix D.

Most of the Starlink applications software is either imported from
international groups or written by programmers working under contract at
university sites, while the central team at RAL concentrates on the production
of `infrastructure' software.

The most important software issue is the {\em software environment}.
This term covers the following features:
\begin{itemize}
\item Programming languages
\item Job control
\item Command languages
\item Data systems
\item Graphics
\item Documentation aids
\item Utilities
\item Error handling
\end{itemize}
It is important both because of the investment of scarce software effort
required, and because of its central role in the organisation of Starlink
software.

Attempts to solve the environment problem lie on a continuum between two extreme
positions.
At one extreme (PRAGMATIC), the facilities provided by the computer manufacturer
are used together with a collection of ad hoc routines.
This approach tends to be favoured by users innocent of the real cost of
software who just want to get on with the job of analysing data as quickly as
possible.
Its dangers are:
\begin{itemize}
\item Huge monolithic programs offering facilities of limited flexibility.
\item A multiplicity of systems which are idiosyncratic to user and programmer.
\item Fixed, inflexible data formats.
\item Incompatibility between different systems.
\item Duplication.
\end{itemize}
At the other extreme (UTOPIAN), an ideal system is created which is machine and
operating system independent.
This approach tends to be favoured by computer science enthusiasts who sense
many years of fascinating research beckoning them on.
Its dangers are:
\begin{itemize}
\item Might not be what the users expected.
\item Takes a long time to develop --- at the expense of application
programming.
\item Inefficient and slow.
\end{itemize}
The first Starlink environment (INTERIM) is a data and parameter system towards
the pragmatic end of the spectrum.
It also has a fairly primitive command language (DSCL).
It is tolerably efficient, very easy to use and was available within 9 months of
the start of the Project.
It has served as the basis for a large amount of application software which is
widely used.

The second Starlink environment (SSE) was an ambitious concept residing near the
utopian end of the spectrum.
It was developed for 5 years but was never considered satisfactory and was
inadequately documented.
It was 44 times bigger than INTERIM (even though incomplete) and command
processing was unacceptably slow.
It contained some powerful and promising packages, in particular a good graphics
system based on GKS and a Hierarchical Data System (HDS) of considerable
elegance, power and efficiency.
The failure of the SSE was due to its development being badly affected by
catastrophic losses of key staff and, more generally, by lack of central
programming effort.
This serious situation was considered at two meetings at RAL in November 1985
and February 1986.
The result was that a new environment was adopted for development.

The new Starlink environment (ADAM) does not represent a fresh start
in the way that SSE was a radically different design from INTERIM.
In fact it derives from many sources taken from many different places.
It owes a lot to the SSE and in some respects is a re-implementation of it with
a greater emphasis on efficiency.
The initial focus for this work was the production of a real-time telescope and
instrument control system at ROE and RGO, but this has now broadened to
encompass Starlink's requirements.
The initial release was in September 1986 and it has since been extensively
developed.
ADAM is now properly supported by an ADAM Support Group of five people located
at RAL.
Comprehensive user and programmer guides are available.

A major challange and opportunity for the future is the arrival of powerful
Unix-based workstations.
It will be necessary to port selected Starlink software to these machines, in
particular a big effort is underway to port the ADAM environment.
Also, it will be necessary to establish a new collection of software
specifically written for these machines.

\section {WHAT HAVE WE LEARNT?}

The Starlink Project has completed nearly ten years of operation.
What have we learnt during this time?
\begin{itemize}
\item The VAX computer was an excellent initial choice.
It provided a powerful, well documented, user-friendly operating system.
For some time it was the standard hardware for the main stream of world-wide
astronomical computing.
Newer VAX hardware products such as microVAXes and LAVC provide a natural
growth path for the Project.
The choice of VAX hardware has made Starlink a success in spite of many
problems.
The arrival of powerful Unix-based scientific workstations has changed the
situation and these may replace the VAXs some time in the future.
\item Compatible hardware at each site allows software to be distributed easily
and avoids wasteful development of different versions for different hardware.
\item The network has been vital.
It enables a basic set of software to be centrally controlled and rapidly
distributed so that a common user environment exists at every site.
The electronic mail facilities are heavily used and extremely valuable.
They bind the astronomical community together.
\item Our first software environment was modest but successful.
The second proved to be over-ambitious in relation to our limited
resources and was just too slow for users to tolerate.
The third is our hope for the future.
Perhaps the biggest lesson is the importance of having a strong, central
programming team under firm control when embarking on this kind of project.
After prolonged efforts by Starlink management, this situation has now been
achieved by the creation of the ADAM Support Group at RAL.
\item Once a large collection of software has been distributed, the maintenance
and support requirements are very considerable.
This is not understood by some astronomers and is one of Starlink's most
intractable problems.
\item Central management of the preparation and distribution of software releases
by a Software Librarian is vital to the cohesiveness of the Project.
A properly integrated set of software, documentation and administrative
information installed at multiple sites is difficult to achieve and there
should be a specific person committed to and responsible for its realisation.
A related problem is the dispersion of different versions of specific software
items within different packages.
This leads to waste of storage space and difficulties with updates.
The solution lies in stronger central control of software development and the
use of programming support environments.
\item Software distribution has been very time consuming and should be made
as simple and automatic as possible.
Ideally, every site should be treated the same as special cases are a menace.
Unfortunately, the ever expanding size of the Collection tends to cause
fragmentation in the distribution because not every site can store it as a
whole.
The requirement to install and distribute software on Unix machines complicates
matters still further.

\end{itemize}

\newpage

\appendix

\section {ABBREVIATIONS}

\begin{tabbing}
XXXX\=ABCDEFGHIJKLM\=XX\kill
\>AMC \>Area Management Committee\\
\>APSB \>Astronomy \& Planetary Science Board of the SERC\\
\>DEC \>Digital Equipment Corporation\\
\>DECnet \>DEC network software\\
\>DES \>Department of Education \& Science\\
\>GDP \> Gross Domestic Product (UK)\\
\>HAG \> Hardware Advisory Group\\
\>JANET \>Joint Academic Network\\
\>LAVC \>Loca Area VAXcluster\\
\>RAL \>Rutherford Appleton Laboratory\\
\>RGO \>Royal Greenwich Observatory\\
\>ROE \>Royal Observatory Edinburgh\\
\>SERC \>Science \& Engineering Research Council\\
\>SIG \>Special Interest Group\\
\>SSC \>Starlink Software Collection/Starlink Software Change\\
\>SUC \>Starlink Users' Committee\\
\>UCL \>University College London\\
\>VAX \>Virtual Address Extension\\
\end{tabbing}

\section {STARLINK SITES}

\subsection{Postal Addresses}

N.B. RGO shares the CAM node, but its postal address is: Royal Greenwich
Observatory, Madingley Road, CAMBRIDGE, CB3 0EZ.

{\em Major Nodes:}

{\bf ARM}: Armagh Observatory, College Hill, Armagh, BT61 9DG\\
{\bf BEL}: Dept of Pure \& Applied Physics, Queen's University of Belfast,
             Belfast, BT7 1NN\\
{\bf BIR}: School of Physics \& Space Research, University of Birmingham,
             PO Box 363, Birmingham, B15~2TT\\
{\bf CAM}: Institute of Astronomy, University of Cambridge, Madingley Road,
             Cambridge, CB3 0HA\\
{\bf DUR}: Dept of Physics, Science Labs, University of Durham, South Road,
             Durham, DH1 3LE\\
{\bf JOD}: Nuffield Radio Astronomy Lab, Jodrell Bank, Macclesfield, Cheshire,
             SK11 9DL\\
{\bf LEI}: Dept of Physics \& Astronomy, University of Leicester,
             University Rd, Leicester, LE1 7RH\\
{\bf MAN}: Dept of Astronomy, University of Manchester, Oxford Road,
             Manchester, M13 9PL\\
{\bf OXF}: Dept of Astrophysics, Physics Building, Keble Rd, Oxford, OX1 3RH\\
{\bf PRO}: Rutherford Appleton Laboratory, Chilton, Didcot, Oxon, OX11 0QX\\
{\bf RAL}: Rutherford Appleton Laboratory, Chilton, Didcot, Oxon, OX11 0QX\\
{\bf ROE}: Royal Observatory Edinburgh, Blackford Hill, Edinburgh, EH9 3HJ\\
{\bf UCL}: Dept of Physics and Astronomy, University College London,
             Gower Street, London, WC1E 6BT

{\em Minor Nodes:}

{\bf CAR}: Dept of Physics, University of Wales College of Cardiff, PO Box 913,
             Cardiff, CF1 3TH\\
{\bf IMP}: Dept of Physics, Blackett Lab, ICSTM, Prince Consort Rd, London,
             SW7 2BZ\\
{\bf KEE}: Dept of Physics, University of Keele, Keele, Staffs, ST5 5BG\\
{\bf KEN}: Electronic Engineering Lab, University of Kent, Canterbury, Kent,
             CT2 7NT\\
{\bf PRE}: School of Physics \& Astronomy, Lancashire Polytechnic, Preston,
             PR1 2TQ\\
{\bf QMW}: Dept of Physics, Queen Mary \& Westfield College, Mile End Road,
             London, E1 4NS\\
{\bf SOU}: Dept of Physics, University of Southampton, Southampton, SO9 5NH\\
{\bf STA}: University Observatory, University of St Andrews, Buchanan Gardens,
             St Andrews, KY16 9LZ\\
{\bf SUS}: Astronomy Centre, Div of Physics \& Astronomy, University of Sussex,
             Brighton, BN1 9QH

{\em Remotely Managed Nodes:}

{\bf HAT}: Dept of Physics \& Astronomy, Hatfield Poly, College Lane, HATFIELD,
             Herts, AL10 9AB

{\em Central Data \& Software Facility:}

{\bf CEN}: Rutherford Appleton Laboratory, Chilton, Didcot, Oxon, OX11 0QX

\subsection{Network addresses, Telephone \& Telex numbers}

\begin{center}
\begin{tabular}{||l|l|l|l|l||}
\hline
Site & DECnet addr & JANET addr & Telephone & Telex \\
\hline
\hline
{\bf ARM} & ARVAD & UK.AC.QUB.ARM.STAR & 0861-522928 & 747937 ARMOBS G \\
{\bf BEL} & QUVAD & UK.AC.QUB.PHY.STAR & 0232-245133 X3648 & 74487 QUBADM G \\
{\bf BIR} & BHVAD & UK.AC.BHAM.SR.STAR & 021-414-6447 & 338938 SPAPHY G \\
{\bf CAM} & CAVAD & UK.AC.CAM.AST-STAR & 0223-337528 & 817297 ASTRON G \\
{\bf DUR} & DUVAD & UK.AC.DUR.STAR     & 091-374-2131 & 537351 DURLIB G \\
{\bf JOD} & JBVAD & UK.AC.MAN.JB.STAR  & 0477-71321 X293 & 36149 JODREL G \\
{\bf LEI} & LTVAD & UK.AC.LE.STAR      & 0533-523599 & 341664 LUXRAY G \\
{\bf MAN} & MAVAD & UK.AC.MAN.AST.STAR & 061-275-4236 & 668932 MCHRUL G \\
{\bf OXF} & OXVAD & UK.AC.OX.ASTRO     & 0865-273311 & 83295 NUCLOX G \\
{\bf PRO} & RLVAD & UK.AC.RL.STAR      & 0235-821900 X5609 & 83159 RUTHLB G \\
{\bf RAL} & RLSAC & UK.AC.RL.STAR.AST  & 0235-821900 X6497 & 83159 RUTHLB G \\
{\bf ROE} & REVAD & UK.AC.ROE.STAR     & 031-668-8377 & 72383 ROEDIN G \\
{\bf UCL} & ZUVAD & UK.AC.UCL.STAR     & 071-380-7147 & 28722 UCPHYS G \\
\hline
{\bf CAR} & CARDIF & UK.AC.CF.ASTRO.V1 & 0222-874000 X5282 & 498635 ULIBCF G \\
{\bf IMP} & ICVAD  & UK.AC.IC.PH.STAR  & 071-589-5111 X6658 & 261503 IMPCOL G \\
{\bf KEE} & KLVAD  & UK.AC.KL.PH.STAR  & 0782-621111 & 36113 UNKLIB G \\
{\bf KEN} &        &                   & 0227-764000 X3190 & \\
{\bf PRE} & LPVAD  & UK.AC.LANCSP.STAR & 0772-201201 & 677409 LANPOL G \\
{\bf QMW} & QMCMV  & UK.AC.QMC.STAR    & 071-975-5053 & 893750 QMCUOL G \\
{\bf SOU} & SOTON  & UK.AC.SOTON.PHASTR & 0703-592112 & 47661 SOTONU G \\
{\bf STA} & SASTAR & UK.AC.ST-AND.STAR & 0334-76161 X8301 & 9312110846 SA G \\
{\bf SUS} & SUSTAR & UK.AC.SUSX.STAR   & 0273-606755 X3053 & \\
\hline
{\bf HAT} &        &                   & 0707-279607 & \\
\hline
{\bf CEN} & STADAT &                   & 0235-821900 X6235 & 83159 RUTHLB G \\
\hline
\end{tabular}
\end{center}
Every Site Manager has username OPER except at IMP (STAR), and STA (SYSTEM).
\section {STARLINK SOFTWARE}

The items in the Starlink Software Collection are classified below.
Items marked `(*)' are proprietary.
\begin{center}
{\bf\large DATA REDUCTION \& ANALYSIS}
\end{center}

\begin{description}

\item [General Purpose] ---
\begin{description}
\item [ASPIC] : General data analysis (Old Starlink package)
\item [FIGARO] : General data analysis (New Starlink package) 
\item [IDL] : General data analysis (US Commercial package)
\item [KAPPA] : Kernel applications 
\item [MIDAS] : General data analysis (ESO)
\end{description}

\item [Specific Wavelengths] ---
\begin{description}
\item [AIPS] : Radio astronomy data analysis
\item [ASTERIX] : X-ray data analysis 
\item [NOD2] : Radio astronomy data analysis
\item [SPECX] :  Mm-wave data analysis
\end{description}

\item [Specific Instruments] ---
\begin{description}
\item [HXIS] : SMM (X-ray)
\item [IRAF] : HST (multi-wavelength)
\item [IRAS] : IRAS (Infra-red)
\item [IRCAM] : UKIRT (Infra-red)
\item [IUEDR] : IUE (Ultra-violet)
\end{description}

\item [Astrometry and Image Analysis] ---
\begin{description}
\item [ASTROM] : Basic astrometry
\item [COCO] : Celestial coordinate conversion
\item [PISA] : Object finding and analysis 
\item [RV] : Calculate radial components of observer's velocity
\end{description}

\item [Photometry] ---
\begin{description}
\item [DAOPHOT] : Stellar photometry 
\item [GASP] : Galaxy surface photometry
\item [PHOTOM] : Aperture photometry 
\end{description}

\item [Polarimetry] ---
\begin{description}
\item [TSP] : Time series and polarimetry analysis 
\end{description}

\item [Spectroscopy] ---
\begin{description}
\item [APIG] : Absorption profiles in the interstellar gas
\item [DIPSO] : Spectral analysis and plotting
\end{description}

\item [Statistics \& Database Management] ---
\begin{description}
\item [ASURV] : Statistical analysis of data with upper limits
\item [CLUSTAN] : Cluster analysis - (Restricted to 10 sites) (*)
\item [GENSTAT] : General statistical analysis (*)
\item [REXEC] : Relational database management system
\item [SCAR] : Catalogue data base system 
\end{description}

\end{description}

\begin{center}
{\bf\large ASTRONOMICAL UTILITIES}
\end{center}

\begin{description}

\item [Archive Access] ---
\begin{description}
\item [IUEDEARCH] : Access IUE archive data
\item [PATT] : Read PATT newsletter
\item [USSP] : Access to IUE uniform low-dispersion archive (ULDA) ---
 (STADAT only)
\end{description}

\item [Data Copying \& Format Conversion] ---
\begin{description}
\item [CONVERT] : Data format conversion (Figaro/BDF to NDF) 
\item [EDFITS] : Copy FITS tapes
\item [FORMCON] : Data format conversion (IPCS/VICAR to BDF)
\item [STARCON] : Data format conversion (BDF to HDS)
\end{description}

\item [Preparation] ---
\begin{description}
\item [AATGS] : Guide probe predictions for AAT
\item [APLATE] : Aperture plate preparation
\item [CHART] : Finding chart and stellar data system
\item [ECHWIND] : Plan use of UCL echelle spectrograph
\item [RPS] : Submit ROSAT proposals
\item [TPOINT] : Telescope pointing analysis
\end{description}

\end{description}

\begin{center}
{\bf\large NON-ASTRONOMICAL UTILITIES}
\end{center}

\begin{description}

\item [Document Preparation \& Search] ---
\begin{description}
\item [FIND] : Starlink document search
\item [GEROFF] : Document preparation
\item [HINDLEGS] : Lineprinter output from \TeX\ files
\item [HONEY] : Hardcopy output from the Honeywell camera
\item [IKONPAINT] : Ikon to inkjet hard-copy
\item [TEX] : Document preparation (includes \LaTeX)
\end{description}

\item [Graphics] ---
\begin{description}
\item [MONGO] : Interactive plotting
\item [VSHC] : Obtain hardcopy from the display of a VAXstation
\end{description}

\item [Magnetic Tape Handling \& Data Compression] ---
\begin{description}
\item [LZCMP] : File compression and de-compression
\item [TAPECOPY] : Copy magnetic tapes
\item [TPU] : Magnetic tape handling
\end{description}

\item [Mathematical] ---
\begin{description}
\item [MAPLE] : Mathematical manipulation language --- (STADAT only) (*)
\end{description}

\item [Operational] ---
\begin{description}
\item [NETWORK] : DECNET utilities
\item [NEWSMAINT] : News maintenance
\item [NOCBS] : Error reporting for CBS on satellites
\item [PAD\_AUDIT] : PAD auditing
\item [QUOTAS] : Other users' disk quotas
\end{description}

\item [Programming Support] ---
\begin{description}
\item [FORCHECK] : Fortran verifier and programming aid --- (STADAT only) (*)
\item [GENERIC] : Compile generic Fortran routines
\item [LIBMAINT] : Library maintenance
\item [LIBX] : Library maintenance
\item [SPAG] : Improve structure of Fortran source code --- (STADAT only) (*)
\item [SST] : Simple Software Tools package
\item [STARLSE] : Starlink language sensitive editor
\end{description}

\end{description}

\begin{center}
{\bf\large SUBROUTINE LIBRARIES}
\end{center}

\begin{description}

\item [ARGS Support] ---
\begin{description}
\item [ARGSLIB] : ARGS manipulation
\item [ARGSMAC] : ARGS programming system
\end{description}

\item [Astronomical \& Mathematical] ---
\begin{description}
\item [JPL] : Solar system ephemeris
\item [NAG] : Numerical mathematics \& statistics (*)
\item [SLALIB] : Mainly positional astronomy
\item [TRANSFORM] : Coordinate transformation
\end{description}

\item [Data Management] ---
\begin{description}
\item [ARY] : Accessing ARRAY data structures
\item [HDS] : Hierarchical data system
\item [PRIMDAT] : Processing primitive numerical data
\end{description}

\item [Graphics] ---
\begin{description}
\item [AGI] :  Graphics database
\item [GKS] : Low-level graphics (*)
\item [GNS] : Graphics workstation name service
\item [IDI] : Image display interface
\item [NCAR/SNX] : High-level graphics
\item [PGPLOT] : High-level graphics
\item [SGS] : Simple graphics
\end{description}

\item [Other] ---
\begin{description}
\item [CHR] : Character handling
\item [EMS] : Error message service 
\item [TAPEIO] : Magnetic tape handling
\end{description}
\end{description}

\begin{center}
{\bf\large SOFTWARE ENVIRONMENTS}
\end{center}

\begin{description}

\item [Starlink Environments] ---
\begin{description}
\item [ADAM] : Standard Starlink software environment
\item [INTERIM/DSCL/RUNSTAR] : Interim Starlink software environment
\end{description}
\end{description}

\section {STARLINK SOFTWARE DISTRIBUTION}

The following non-Starlink sites have been supplied with Starlink software:
\begin{tabbing}
XX \=X \kill
1. DIRECT:\\
\\
1.1 UK\\
\\
\>Daresbury Laboratory, Warrington\\
\>Hatfield Polytechnic\\
\>Imperial College of Science \& Technology, London\\
\>Institute of Cancer Research, Sutton\\
\>Meteorological Office, Bracknell\\
\>Mullard Radio Astronomy Observatory, Cavendish Laboratory, Cambridge\\
\>Mullard Space Science Laboratory, Dorking\\
\>Queen Elizabeth College, London\\
\>St Mary's Hospital, Portsmouth\\
\>University of Bristol\\
\>University of Leeds\\
\\
1.2 Overseas\\
\\
\>Anglo Australian Observatory, Epping, AUSTRALIA\\
\>Arizona State University, Tempe, USA\\
\>Beijing Astronomical Observatory, CHINA\\
\>California Association for Research in Astronomy, Pasadena, USA\\
\>Carnegie Institution, Washington, USA\\
\>Carter Observatory, Wellington, NEW ZEALAND\\
\>Chiang Mai University, THAILAND\\
\>Ciudad Universitaria, Madrid, SPAIN\\
\>CNRS, Marseille, FRANCE\\
\>Danish Space Research Institute, Lyngby, DENMARK\\
\>Dominion Astrophysical Observatory, Victoria, CANADA\\
\>Dunsink Observatory, Dublin, IRELAND\\
\>Electronics Research Lab, Salisbury, AUSTRALIA\\
\>ESA Tracking Station, VILSPA, Madrid, SPAIN\\
\>ESO, Garching bei M\"{u}nchen, WEST GERMANY\\
\>ESTEC, Noordwijk, NETHERLANDS\\
\>Indian Institute of Astrophysics, Bangalore, INDIA\\
\>Institute for Advanced Study, Princeton, USA\\
\>Instituto de Astrofisica de Andalucia, Granada, SPAIN\\
\>Instituto de Astrofisica de Canarias, La Laguna, Tenerife, CANARY ISLANDS\\
\>Instituto de Astronomia, MEXICO\\
\>IPAC, Pasadena, USA\\
\>IRAM, Grenoble, FRANCE\\
\>ISRO Satellite Centre, Bangalore, INDIA\\
\>Jet Propulsion Laboratory, Pasadena, USA\\
\>Joint Astronomy Center, Hilo, Hawaii, USA\\
\>Kapteyn Laboratory, Groningen, NETHERLANDS\\
\>Laboratoire d'Astronomie, Montpellier, FRANCE\\
\>Leander McCormick Observatory, Charlottesville, USA\\
\>Leiden Observatory, NETHERLANDS\\
\>Lick Observatory, Santa Cruz, USA\\
\>Lockheed Research Laboratory, Palo Alto, USA\\
\>Lunds Universitat, Lund, SWEDEN\\
\>Max-Planck-Institut f\"{u}r Extraterrestrischephysik, Garching-bei-M\"{u}nchen, WEST GERMANY\\
\>Max-Planck-Institut f\"{u}r Kernphysik, Heidelberg, WEST GERMANY\\
\>McDonald Observatory, Austin, USA\\
\>National Optical Astronomy Observatories, Tucson, USA\\
\>National Radio Astronomy Observatory, Socorro, USA\\
\>Netherlands Foundation for Radio Astronomy, Dwingeloo, NETHERLANDS\\
\>Observatoire de Geneve, Sauverny, SWITZERLAND\\
\>Observatoire de Haute Provence, FRANCE\\
\>Observatoire de Meudon, FRANCE\\
\>Observatoire de Nice, FRANCE\\
\>Osservatorio astronomico, Napoli, ITALY\\
\>Osservatorio astronomico di Trieste, ITALY\\
\>Owens Valley Radio Observatory, Caltech, Pasadena, USA\\
\>Pakistan Space \& Upper Atmospheric Research Commission, Karachi, PAKISTAN\\
\>Physikalisches Institut, Cologne, WEST GERMANY\\
\>Purple Mountain Observatory, Nanjing, CHINA\\
\>Radioastronomiches Institut, Bonn, WEST GERMANY\\
\>Smithsonian Institution, Cambridge, USA\\
\>South African Astronomical Observatory, Cape Town, SOUTH AFRICA\\
\>Space \& Astronomy Research Centre, Baghdad, IRAQ\\
\>Space Telescope Science Institute, Baltimore, USA\\
\>Tata Institute of Fundamental Research, Ootacamund, INDIA\\
\>Tokyo Astronomical Observatory, JAPAN\\
\>Universidade do Porto, PORTUGAL\\
\>Universitaet Kiel, WEST GERMANY\\
\>Universitats-Sternwarte, Munich, WEST GERMANY\\
\>University of Amsterdam, NETHERLANDS\\
\>University of California, Davis, USA\\
\>University of Cape Town, SOUTH AFRICA\\
\>University of Colorado, Boulder, USA\\
\>University of Crete, GREECE\\
\>University of Helsinki, FINLAND\\
\>University of Michigan, Ann Arbor, USA\\
\>University of Oslo, NORWAY\\
\>University of Tasmania, Hobart, AUSTRALIA\\
\>University of Tromso, NORWAY\\
\>University of Tubingen, WEST GERMANY\\
\>University of Valencia, SPAIN\\
\>University of Victoria, CANADA\\
\>University of Washington, Seattle, USA\\
\>University of Wollongong, AUSTRALIA\\
\\
2. INDIRECT:\\
\\
2.1 UK\\
\\
\>Clarendon Laboratory, Oxford\\
\>RAL, Atmospheric Science Group\\
\>RAL, A\&G Division, Didcot\\
\>RAL, Space Plasma Group, Didcot\\
\>RGO, La Palma development Group, Herstmonceux\\
\>ROE (UKIRT development), Edinburgh\\
\>University of Aberdeen \& Grampian Health Board, Aberdeen
\end{tabbing}

\begin{tabbing}
XX \=X \kill
2.2 Overseas\\
\\
\>Anglo Australian Telescope, Coonabarabran, AUSTRALIA\\
\>CSIRO, Epping, AUSTRALIA\\
\>Drake University, Des Moines, USA\\
\>High Energy Physics Institute, Beijing, CHINA\\
\>IESI/CNR, Bari, ITALY\\
\>IFCAI/CNR, Palermo, ITALY\\
\>IRA/CNR and Osservatorio Astronomico, Bologna, ITALY\\
\>IRAM, Granada, SPAIN\\
\>IRAM, St Martin d'Heres, FRANCE\\
\>Instituto de Astrofisica de Canarias, Tenerife, CANARY ISALNDS\\
\>Istituto Astronomico, Roma, ITALY\\
\>Istituto di Astronomia, Catania, ITALY\\
\>Kyoto Observatory, JAPAN\\
\>Los Alamos Meson Physics Facility, USA\\
\>Mount Stromlo \& Siding Spring Observatories, Canberra, AUSTRALIA\\
\>National Radio Astronomy Observatory, Tucson, USA\\
\>Osservatorio Astrofisico, Firenze, ITALY\\
\>Osservatorio Astronomico, Cagliari, ITALY\\
\>Osservatorio Astronomico, Milano, ITALY\\
\>Osservatorio Astronomico, Napoli, ITALY\\
\>Osservatorio Astronomico, Padova, ITALY\\
\>Osservatorio Astronomico, Palermo, ITALY\\
\>Osservatorio Astronomico, Torino, ITALY\\
\>RGO (La Palma), Tenerife, CANARY ISLANDS\\
\>Rhodes University, Grahamstown, SOUTH AFRICA\\
\>Tata Institute of Fundamental Research, Bombay, INDIA\\
\>Universidad de Cantabria, Santander, SPAIN\\
\>Universidade de Sao Paulo, BRAZIL\\
\>Universita degli studi di Udine, ITALY\\
\>Universita di Napoli, ITALY\\
\>Universita di Roma, ITALY\\
\>University of Alabama, USA\\
\>University of Canterbury, NEW ZEALAND\\
\>University of Patras, GREECE\\
\>University of Sydney, AUSTRALIA
\end{tabbing}
\end{document}
