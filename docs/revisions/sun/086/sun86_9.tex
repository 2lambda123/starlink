%\documentstyle[11pt]{article}      %1-col
 \documentstyle[twocolumn]{article} %2-col
\pagestyle{myheadings}

%------------------------------------------------------------------------------
\newcommand{\stardoccategory}  {Starlink User Note}
\newcommand{\stardocinitials}  {SUN}
\newcommand{\stardocnumber}    {86.9}
\newcommand{\stardocauthors}   {H.\ Meyerdierks}
\newcommand{\stardocdate}      {29 November 1993}
\newcommand{\stardoctitle}     {FIGARO --- A general data reduction system}
%------------------------------------------------------------------------------

\newcommand{\stardocname}{\stardocinitials /\stardocnumber}
\markright{\stardocname}
\setlength{\textwidth}{160mm}
\setlength{\textheight}{230mm}
\setlength{\topmargin}{-2mm}
\setlength{\oddsidemargin}{0mm}
\setlength{\evensidemargin}{0mm}
\setlength{\parindent}{0mm}
\setlength{\parskip}{\medskipamount}
\setlength{\unitlength}{1mm}

%------------------------------------------------------------------------------
% Add any \newcommand or \newenvironment commands here
%------------------------------------------------------------------------------

\begin{document}
 \onecolumn %2-col
\thispagestyle{empty}
SCIENCE \& ENGINEERING RESEARCH COUNCIL \hfill \stardocname\\
RUTHERFORD APPLETON LABORATORY\\
{\large\bf Starlink Project\\}
{\large\bf \stardoccategory\ \stardocnumber}
\begin{flushright}
\stardocauthors\\
\stardocdate
\end{flushright}
\vspace{-4mm}
\rule{\textwidth}{0.5mm}
\vspace{5mm}
\begin{center}
{\Large\bf \stardoctitle}
\end{center}

\vspace{5mm}

\begin{center}
{\Large\it Portable Figaro - version 3.2}
\end{center}

\vspace{5mm}

%------------------------------------------------------------------------------
%  Add this part if you want a table of contents
%  TOC should end either with a \newpage, a \twocolumn or with a \rule\vspace
\setlength{\parskip}{0mm}
\tableofcontents
\setlength{\parskip}{\medskipamount}
%\newpage   %1-col
 \twocolumn %2-col
%\rule{\textwidth}{0.5mm}\vspace{5mm}
%  \markright{\stardocname}
%------------------------------------------------------------------------------

 \sloppy %2-col

\section{Introduction}
\label{intro}

This document contains information on Portable Figaro 3.2, Starlink's
release of the Figaro package. The user will want to read Section
\ref{getstart} and consult the classified list of commands in Section
\ref{classif}. A detailed description of ways in which Figaro is
commonly used can be found in Section \ref{techno}.  If you are familiar
with Figaro 3.0, you may be interested in Section \ref{changes}. If you
want to link your own Figaro-ish applications with Figaro libraries,
refer to Section \ref{proghint}. This document is {\it not} concerned
with (VAX) Figaro 3.0; if you use version 3.0 please refer to the
relevant user guide instead.

Figaro is a set of general astronomical data reduction programs with a
certain inclination towards spectroscopy. The package was developed by
Keith Shortridge, originally at Caltech and later at AAO. The present
version -- Portable Figaro 3.2 -- is released and supported by Starlink.
It has evolved from Figaro 3.0 as distributed for VAXs by Starlink, with
some minor contributions from Sun Figaro 2.4.5 as produced by Samuel
Southard (Caltech).  Figaro 3.0 was a release from AAO, but it had been
adapted somewhat by Starlink (Figaro 3.0-3 and 3.0-5). Also some
modifications had been made under the term ``National Figaro'' (3.0-1
through 3.0-6).  Since Portable Figaro 3.1 and VAX Figaro / National
Figaro 3.0-6 the two releases evolve in parallel though not
independently.  Porting Figaro 3.0 to Unix -- resulting successively in
Figaro 3.1 and 3.2 -- was a joint effort by the ``Figaro Port Group''
Michael Ashley and Brad Carter (UNSW), Stephen Meatheringham (MSSSO),
Horst Meyerdierks (UoE, Starlink), and Keith Shortridge (AAO).

Portable Figaro 3.2 is available on Sun4 (SunOS 4.x), Sun4 (Solaris)
DECstation (Ultrix), and Alpha AXP (OSF/1).  It can be run in a
command-oriented way from the Unix shell (usually a shell similar to the
C shell).  It can also be run from Starlink's X/Motif graphical user
interface Xadam.  When Starlink's command language ICL is released for
Unix, Portable Figaro 3.2 can also be run from ICL.


\section{Getting Started}
\label{getstart}

In this section and in Section \ref{techno} we will assume that the
Figaro commands are run from the C shell, indicated by a {\tt \%} prompt
in the examples.  The backslashes that you find in the examples are
necessary to avoid the shell interpreting the next character as a meta
character.  The backslash escapes must be removed when you run Figaro
commands from ICL.

You start up the Figaro package with the command

\begin{verbatim}
   % figaro
\end{verbatim}

The commands available are listed in Section \ref{classif}.  An
application is available under its proper name and under an extended name,
e.g.

\begin{verbatim}
   % istat
   % fig_istat
\end{verbatim}

Before you can run any commands, you must have a directory where to
store command parameters.  Usually you will create a directory {\tt
\$HOME/adam} for this purpose.  Any other directory will do, if it is
pointed to by the environment variable {\tt ADAM\_USER}.

There is a small demonstration procedure you may want to inspect. It is
more of a test for successful installation than a user demo. But it
shows how you can issue commands and how you can edit C shell scripts to
get things done. To run the demo you will need a colour Xwindows
display:

\begin{verbatim}
   % more   $FIG_DIR/demo.csh
   % source $FIG_DIR/demo.csh
\end{verbatim}

You can find out the details of the applications from the on-line help
and run-time help. The information is stored in a hierarchical help
library. There are top level topics ``Figaro'' and ``Classified'', and
each application name is a top level, too.  And there may be one or more
top level topics beginning with ``News''. You might enter the on-line
help with

\begin{verbatim}
   % fighelp istat
\end{verbatim}

If there are sub-topics you are asked to choose one, a blank choice gets
you one level up in the hierarchy (and eventually out). (Sub)-topics can
be abbreviated and wild cards can be used. You can get the list of
sub-topics again by entering a question mark.
Run-time help is available while an application prompts for a parameter
value.  By replying with a question mark you get the on-line help for
this particular application and parameter and are then re-prompted. This
should enable you to respond sensibly to the re-prompt.

All applications will need further information, which they try to get from the
{\it parameter system}. For some parameters the application may be happy to get
some default value from the parameter system without the user knowing. Other
parameters will be prompted for. The prompt often offers a more or less
suitable default. Usually you will enter the proper value, but there are some
other answers to a prompt that make surprisingly much sense.

\begin{verbatim}
   PARNAME - Prompt text /Default/ > 
   PARNAME - Prompt text /Default/ > !!
\end{verbatim}

By just hitting the {\tt Return}, the default value is used for the
parameter.  {\tt !!} should cause the command to abort.

You need not wait for the application to prompt for a value but can set
parameter values on the command line. Actually some parameters are not
prompted for, they take default values unless you specify them on the
command line.  There are also special keywords for the command line:
{\tt accept, prompt, reset}.  With these keywords the applications will
use default values instead of prompt, prompt for all parameters, use
reset values instead of last used ones, respectively.

Most parameters are assigned positions on the command line, but all parameters
can be specified by their name and value. In the following example three
parameters are specified on the command line, all others default.

\begin{verbatim}
   % istat image=myimage accept
\end{verbatim}

Some parameters may be tricky to specify on the command line, namely
strings that contain spaces and vectors. The syntax on the command line
is more strict in these cases. And on Unix systems it is more obscure,
because one must take precautions against the shell interpreting meta
characters.

\begin{verbatim}
   % <command> <stringpar>=\"word1 word2\"
   % <command> <vectorpar>=\[1,2,3\]
\end{verbatim}

If a command cannot be completed on one line, it can be split into
several. The split character -- the last character before the splitting
{\tt Return} -- is a backslash $\backslash$ in Unix.  (You can think of
the backslash as escaping the new-line character that follows.  In ICL
the split character is a tilde {\~\ }.

You can find real-life examples in the demonstration script.

One advantage of command line parameters is that sometimes you need not
wait at the terminal for the last prompt, which might come only after
lengthy calculations. But more importantly you can edit commands into
shell scripts. This is exactly what was done for the demonstration. The
demo, however is simply a linear sequence of commands with static
parameters.  If you are a C-shell guru you can write more sophisticated
scripts than {\tt demo.csh}. There is a complication, however. The
demonstration is executed with a {\tt source} command. This means that
the script is taken as a list of commands for the current shell, no
child process is created to run the script. Thus all current {\tt
alias}es and environment variables are available to the commands in the
script. If you execute the script by using its name as the command, then
it is executed in a child process. That process is in general ignorant
of the {\tt alias}es set up in its parent. It has to do its own Figaro
initialisation.

The transfer of Starlink Data Files between different machines is very
simple.  The disks may be cross-mounted via {\tt nfs}. In this case the
``foreign'' disk looks like a normal directory. Any {\tt .sdf} files can
be read and written by any machine. If there is no such close connection
between the machines, you will have to transfer the data with {\tt ftp}.
Make sure that you transfer the files as ``binary''. The transferred
files can be used directly.  Note that this data portability is a
feature of HDS files, which include Starlink Data Files ({\tt .sdf}) and
Figaro's {\tt .dst} files. Other binary files -- especially if they
contain four- or eight-byte floating-point data -- cannot be used
directly on another machine architecture.

Another common source of data are FITS files.  Normally these should be
on tape, but there exists a quasi-standard for FITS files on disk.
Disk-FITS can be read with the {\tt rdfits} command, but Portable Figaro
3.2 cannot handle tapes.  It is possible to copy FITS files from
tape to disk before using {\tt rdfits}.

By default you will use NDF as preferred format and DST as supported
second format. You can change this by setting the environment variable
{\tt FIG\-ARO\-\_FOR\-MATS} to a different value. Usually its value is
``ndf,dst''.  You can change it to ``dst,ndf'' in order to change the
preference or to ``ndf'' or ``dst'' to support only one format and
reject the other. The command to change the environment variable would
be included in your {\tt \$HOME/.login} file:

\begin{verbatim}
   % setenv FIGARO_FORMATS dst,ndf
\end{verbatim}

Even if you accept both formats your data will usually not change from
the second choice to the preferred format. So you may need an explicit
conversion.  The applications {\tt dst2ndf} and {\tt ndf2dst} are
available in the Convert package (SUN/55).  The current data access
routines may fail in rare circumstances when on invocation of an
application has to cope with both formats, e.g.\ when an application is
given a mixture of NDF and DST input files.  For this reason you may
want to make a conscious decision about {\tt FIGARO\_FORMATS}.

Should you have been using a version of Figaro that supports neither of
these data formats and want to use old data, please refer to Section
\ref{flavours} to see how you can convert from those old DTA formats.
{\it You are strongly advised to convert all your data as soon as
possible to DST or NDF, because the conversions from DTA formats are
available only in Sun Figaro 2.4.5 and Figaro 3.0 respectively. These
flavours of Figaro may not be available for very long.}


\subsection{The Parameter System}

The applications use the parameter system to get the necessary
information from the outside world. The source of information is not
always the user's keyboard. The specification of a parameter on the
command line is slightly different from entering the value at the
prompt.

A good model to imagine the workings of the parameter system is as
follows.  The system is a pool of parameter values. On the command line
you can pass values to the parameter system (not the application). When
the application runs and needs a parameter value, it will ask the
parameter system (not the user terminal). For each parameter the system
has two sets of rules, one to arrive at a default value and one to work
out the value itself. If the value was specified on the command line,
the system will just pass that as the value to the application.
Otherwise the value is so far unknown to the parameter system and it
will construct a default value and a value according to the rules.
There are several possible sources for these two:

\begin{itemize}
\item the last used value of a global parameter (common to more than one
   application),
\item the last used value (as stored on a per application basis),
\item a dynamic default, set by the application at run-time,
\item a static default, set in the interface file,
\item response to a user prompt.
\end{itemize}

So asking the user is only one way of getting information from the
parameter system. You also see that the defaults offered -- or accepted
by {\tt accept} on the command line -- may be arrived at in a number of
ways.

What used to be Figaro variables up to version 3.0 are now global
parameters.  For example, {\tt istat} puts the average value into a
global parameter called {\tt stat\_mean}.  It can be used as a parameter
value in another command, but this is not very easy from the Unix shell.
You must know where the global parameters are stored.  If you have set
an environment variable {\tt ADAM\_USER} then the file will be {\tt
\$ADAM\_USER/GLOBAL.sdf}, otherwise it will be {\tt
\$HOME/\-adam/\-GLOBAL.sdf}.  You can use {\tt stat\_mean} as follows:

\begin{verbatim}
   % istat image
   % icdiv image \
      @$HOME/adam/GLOBAL.STAT_MEAN output
\end{verbatim}

Using ICL would avoid this clumsy syntax.  It would have a command to
get a global parameter into an ICL variable.  But it would also allow
you to get the {\tt istat} output parameter {\tt stat\_mean} directly
into an ICL variable.

\begin{verbatim}
   ICL> istat image stat_mean=(get_this)
   ICL> use_this = sqrt(get_this)
   ICL> icdiv image (get_this) output
\end{verbatim}


\subsection{Graphics}

You can use up to three different graphics devices.  They are chosen
with the commands {\tt soft, hard} and {\tt idev}, and are used for line
plots on the screen, line plot hard copies, and image display,
respectively.  If instead of the device name you give {\tt options} then
you will get a list of valid device names.  This list is the same for
all three commands, but may depend on the machine you use.  So formally
you can give the same device for all three.  And if you want the {\tt
image} command to produce a print file you will actually use {\tt idev}
with a printer device name.

You may be tempted to use the same device in {\tt soft} and {\tt idev}.
But most line plot commands in Figaro interfere with the colour table
that you are using for image display.  So there may be a point in using
two separate windows on an Xwindows display for line plots and image
display.

The use of Xwindows work stations will become more and more common.
Often the display server is a different machine than the client that
runs the application, say you use an X-terminal to connect to a Sun with
Figaro installed.  In this case you have to tell the remote client
machine which is the display server, by setting the {\tt DISPLAY}
environment variable.  This can conveniently be done with the {\tt
xdisplay} command.  Without argument it will try to find out from where
you logged in and use that information.  You can also give the display
host name as an argument, e.g.\ if that is not the machine you logged in
from.

Figaro uses a Starlink utility called GWM to administer the display
windows.  There is the command {\tt xmake} in GWM, which you can use to
tailor the display windows.  If you do not create the windows with {\tt
xmake} then they will be created with default properties when they are
needed to display something.  You can determine in your {\tt .Xdefaults}
file what these default settings are.  GWM is documented in SUN/130.
It is not much use to re-size the GWM window with the mouse, that will
only change the size of a central part of the graphics that you see.  If
you create the window with the desired size in the first place, all
plots will fit that size.
Here is an instructive example how to set up your Figaro graphics.

\begin{verbatim}
   % xmake xwindows  -colour 16  \
       -geom -0-0
   % xmake xwindows2 -colour 128 \
       -geom 512x512-0+0
   % soft xw
   % idev x2w
   % hard ps_l
   % colour standard_lut
\end{verbatim}

The first {\tt xmake} creates a display window in the bottom right
corner of the screen ({\tt -0-0}).  It is allocated 16 colours,
sufficient for an elaborate line plot.  Since no size is specified it
will be about 700 by 500 pixels.

The second {\tt xmake} creates another display window in the top right
corner with size 512 by 512 pixels.  It is allocated 128 colours with the
intention to display images in it.  By default GWM windows get 64
colours.  At any rate the first 16 colours are not used for image
display but are reserved for line plots.

{\tt soft} and its sibling commands use not the GWM names for windows,
but the GNS names for graphics devices.  Unfortunately these are
different.  So the GWM window ``xwindows'' is the GNS device
``xwindows'', which can be abbreviated to ``xw''.  But the GWM window
``xwindows2'' is the GNS device ``x2windows'', abbreviated to ``x2w''.

``ps\_l'' will cause the generation of a PostScript file that will print
in landscape mode.  Instead of a printable PostScript file you can
generate an encapsulated PostScript file by choosing the device
``epsf\_l''.  You can include such a file into a \TeX\ or \LaTeX\
document during conversion from {\tt dvi} to PostScript.  Or you can
merge several encapsulated PostScript files before printing with {\tt
psmerge} (SUN/\-164).

The {\tt colour} command loads a colour table into ``x2w''.  You can use
{\tt grey} or {\tt gray} instead of a colour table.  The colour table
used in the example is stored as the file {\tt
\$FIGARO\_PROG\_S/stan\-dard\-\_lut.sdf}.  To see what colour tables are
included in Figaro,

\begin{verbatim}
   % ls $FIGARO_PROG_S/*_lut.sdf
\end{verbatim}

You can also use Kappa colour tables:

\begin{verbatim}
   % ls $KAPPA_DIR/*_lut.sdf
   % colour $KAPPA_DIR/heat_lut
\end{verbatim}

For the actual display of images you will usually use {\tt image} which
in the example would go into ``x2w''.  {\tt icur} then lets you walk
through the display with the cursor, inspect pixels, end record
positions.  Alternatively you can use {\tt igrey} or {\tt icont} which
would display in ``xw''.  Then instead of {\tt icur} you would use {\tt
igcur}.  {\tt icont} and {\tt igcur} would also work on a monochrome
display or indeed on a graphics terminal.


%\footnote{
%   If {\tt ADAM\_USER} is not set, use {\tt \$HOME/adam} instead.  Also,
%   ICL has simpler mechanisms to transfer parameter values from one
%   application to the other.}
% @$ADAM_USER/GLOBAL.

\section{Techniques}
\label{techno}

{\it This Section is an updated version of Section 17 of the Figaro 3.0
User's Guide by Keith Shortridge, a guide to ways of using Figaro
functions to perform certain useful operations. Some Sub-sections are
not from that user guide itself but from other documentation that used
to be only on-line in Figaro 3.0. The update editing is not complete, in
that often upper case file names etc.\ are used in the text whereas
lower case would be appropriate for Unix system. Also no font changes
are made in the text to emphasise file names etc. Finally some
references to the internal structure of files are made assuming DST
format whereas NDF format should be preferred.  The examples are lower
case and written out for the Unix shell. They should work equally from
ICL except for escaping the shell meta characters.}


\subsection{Flats}

Flat field division should be relatively straightforward; you take
your raw data and divide it pixel for pixel by your flat field.  In
practice, this may be acceptable for image data, but there are additional
considerations to be  taken into account when the data involved is spectral.


\goodbreak
\vspace{12pt}
{\it A) Image Data}

In principle, you can simply divide your image by the appropriate
flat field and use the result.  For example:

\begin{verbatim}
   % idiv image=mydata image1=ff \
      output=flatdata
\end{verbatim}

In practice, you really don't usually want to divide your data by
the (typically) very large numbers found in flat fields.  Ideally, your
flat field should first be reduced to numbers around unity.  The easiest
way to do that is to divide the flat field by its mean value, and the
easiest way to find the mean value is with ISTAT.  The command

\begin{verbatim}
   % istat image=ff reset accept
\end{verbatim}

will give you the mean value over the whole image.  If your flat field has
strange effects round the edges, you may prefer to limit the range of
x and y values used by ISTAT.  For example, if your image is 800 by 800,

\begin{verbatim}
   % istat image=ff ystart=100 yend=700 \
      xstart=100 xend=700 accept 
\end{verbatim}

will only look at the central 600 by 600 part of the image.  ISTAT
prints out the value of the mean, and you can then divide the flat field
by that.  For example, supposing the mean value were 350, the command

\begin{verbatim}
   % icdiv image=ff factor=350 \
      output=ffdiv
\end{verbatim}

will generate a new flat field (FFDIV) that will have a mean value
around unity, as desired.  You may find it convenient to know that ISTAT
sets the Figaro user variable STAT\_MEAN to the mean value.  So you
could use\footnote{
   If {\tt ADAM\_USER} is not set, use {\tt \$HOME/adam} instead.  Also,
   ICL has simpler mechanisms to transfer parameter values from one
   application to the other.}

\begin{verbatim}
   % icdiv image=ff \
      factor=@$ADAM_USER/GLOBAL.STAT_MEAN\
      output=ffdiv
\end{verbatim}


\goodbreak
\vspace{12pt}
{\it B) Spectral Data}

Figaro expects you to have your spectral data oriented so that the
x dimension is the wavelength dimension.  If this is not the case, use
IROT90 to switch the axes.  In theory, Figaro does not care whether
wavelength increases or decreases with x value, but in practice routines
tend to be tested with data whose wavelength increases with x value, and
odd bugs may turn up with `reversed' data.  You are recommended to get
your data into the more common form, if necessary using IREVX.  Note that,
as always in Figaro, `x' means the horizontal axis when data is displayed
using IMAGE.

The main problem with spectral flat fields is that the flat field data
will usually vary in x, not because of the instrumental response, but 
simply because of the spectral response of the flat field lamp.  The
usual way of dealing with this is to fit this spectral response --
obtained by collapsing the flat field in y -- and then multiplying by
the fitted value before dividing by the flat field.  Another way to look
at this is to consider the result of dividing the flat field by the
fit to the spectral response.  The result would be an image which was
essentially flat in X, but which is `humped' in Y -- since most flat fields
fall off at the edges in Y due to instrumental vignetting.  Dividing by
this -- in a 2D manner -- will give you images where the pixel to pixel
variations in the detector have been corrected, along with any spatial
vignetting of the instrument, but where the overall wavelength response
of the instrument is not corrected.

(Of course, if you think your flat field lamp {\it is} flat - which may be
true at high dispersion - you can just divide by the raw flat field and
you're taking out the instrumental wavelength response as well.  However,
the better way to deal with that is with your standard stars.)

The only problem, usually, is deciding on what represents an acceptable
fit to the collapsed flat field.  One approach is to fit a polynomial to
it.  This at least has the advantage of being automatic, and if the result
is satisfactory this is probably the best thing to do.

So this recipe is as follows:

\begin{enumerate}
\item Collapse the flat field in Y to give a single spectrum.  This is the
average spectral response of the flat field lamp combined with that of the
detector.
\item Fit a low order polynomial to this, giving a smooth spectrum.  It may be
better to fit to the log of the data if large count numbers are involved, but
this is not usually critical.
\item Take each of the cross-sections of constant Y value in the flat field
and divide them by this smoothed spectrum.  The result is your corrected
flat field calibration image.
\item Divide each pixel of your image to be corrected by the corresponding
pixel of the flat field calibration image.
\end{enumerate}

The individual Figaro commands to do this might be the following\footnote{
   If {\tt ADAM\_USER} is not set, use {\tt \$HOME/adam} instead.  Also,
   ICL has simpler mechanisms to transfer parameter values from one
   application to the other.}

\begin{verbatim}
   % extract image=ff ystart=-1e30 \
      yend=1e30 spectrum=spres
   % icdiv   image=spres \
      factor=@$ADAM_USER/GLOBAL.YEND \
      output=spres
   % sfit    spectrum=spres order=2 \
      output=spfit  logs
   % isxdiv  image=ff spectrum=spfit \
      output=ffcal
   % idiv    image=mydata image1=ffcal \
      output=mydataff
\end{verbatim}

which, given a flat field exposure in FF, and an image in MYDATA, performs
a flat field calibration as described above and puts the result in MYDATAFF.

This is a sufficiently common way of proceeding that there is a single
Figaro command that performs all of this, without the overheads of running
four programs and generating the intermediate files.  This is the FF command.
The command

\begin{verbatim}
   % ff image=mydata flat=ff order=2 \
      output=mydataff
\end{verbatim}

is functionally equivalent to the above sequence.  However, the advantage
of splitting up the process is that you can compare SPRES and SPFIT to
see how good the polynomial fit to the collapsed data really is.  E.g.\ by

\begin{verbatim}
   % splot spres reset accept
   % splot spfit noaxes noerase accept
\end{verbatim}

(You also have the advantage that you can control the limits used
when collapsing the flat field; you do not have to use MIN and MAX as
the limits, although that is what FF does.)

If the fit is not satisfactory, what can you do instead?  Well,
the problem is essentially that of generating a smoothed spectrum, and
Figaro has a number of ways of doing that.

\begin{enumerate}
\item You can actually smooth the spectrum, using IXSMOOTH.
\item You can do it by hand, using CFIT to generate a spline fit between
points indicated interactively using the cursor, having first displayed
the spectrum with SPLOT.
\item A less interactive way of using spline fits would be to use MCFIT,
probably with a zero spectrum for a mask, although you could always use
MASK to generate a suitable mask should there be bad regions in the 
spectrum.  (You can generate a zero mask by multiplying the original
spectrum by zero, obviously.)
\end{enumerate}

One of these should enable you to get a satisfactory result, although
they all require more effort than does the simple FF.


\subsection{B-Stars}

The atmospheric features that mess up the red end of the spectrum may
be calibrated out by multiplying by a calibration spectrum obtained from an
observation of an object that is featureless in this region.  A B star is
usually used, hence the term `B star calibration'.

The essential Figaro command for this calibration is BSMULT.  This
multiplies the spectrum to be corrected by a B star calibration spectrum
-- which is essentially obtained by fitting the continuum of a B star and
then dividing that fitted continuum by the B star observation.  BSMULT
allows for the difference in air masses of the two observations, which is
what makes it necessary, rather than, say, the simpler IMULT.

BSMULT is a fairly straightforward operation.  The problems come in
generating the calibration spectrum.  At present, the simplest way is to
display the B star spectrum using SPLOT and then generate the continuum
by hand using CFIT.  An alternative is to generate a mask for the lines
in the spectrum using MASK and then fit the masked continuum using
MCFIT.  The main problem with the latter approach is that the chances are
that very little of the spectrum is actually uncontaminated either by
stellar lines or by the atmospheric bands.  

(The best solution is use an automatic program that fits
splines between points on the spectrum that are known to be uncontaminated,
but such a program is not available yet -- nor is a really good list of
such points that will apply for all wavelength ranges and dispersions.)

The calibration spectrum should really be exactly 1.0 at all points
not affected by the atmospheric bands, and it is probably worth displaying the
calibration spectrum using SPLOT and then using CSET to set such regions
to 1.0 interactively.  (This is something else that could be made automatic
eventually.  Fortunately, it isn't necessary to do this very often.)

Note that BSMULT requires that both the calibration and the object being
corrected have valid air masses.  Air masses are stored in NDF data
files as FILE.MORE.FIGARO.SECZ, and in DST data files as FILE.OBS.SECZ.
If necessary they may be set by hand using, e.g.

\begin{verbatim}
   % creobj object=file.MORE.FIGARO.SECZ \
      type=_REAL
   % setobj object=file.MORE.FIGARO.SECZ \
      value=1.4
\end{verbatim}

Also note that the correction applied by BSMULT is multiplicative --
this means that it is {\it not} suitable for data that is in logarithmic flux
units, such as magnitudes.


\subsection{Filters}

If the response of a filter has been tabulated, then the table of
values may be used to generate a `spike\-trum', which may then be turned
into a calibration spectrum.  See the section on `SPIKETRA' for details
about these things, which are essentially ways of turning sets of tabulated
values into spectra by interpolation.

Say the spectrum to be corrected for a filter response is called
SPECT.  A table for the filter response might look like -

\goodbreak
\begin{verbatim}
   *
   *     Filter transmission table
   *
       3000    .05
       4000    .10
       4200    .55
       4400    .90
       4600    .95
       5000    .95
       5500    .95
       6000    .95         and so on..
\end{verbatim}
\goodbreak

This is not a particularly realistic table.  A proper table should
have enough points to ensure that there are a reasonable number of values
tabulated over the region of the spectrum to be corrected.  With this table
in FILTER.TAB, a filter calibration spectrum can be produced and 
applied as follows -

\begin{verbatim}
   % gspike  spectrum=spect \
      table=filter.tab spiketrum=filter
   % interp  spiketrum=filter \
      spectrum=calib
   % idiv    image=spect image1=calib \
      output=spect
\end{verbatim}

Note that IDIV is used to apply the corrections, which is simply
a case of dividing the spectrum to be corrected by the interpolated
filter response.  The same calibration spectrum may be used for any
other spectra that cover the same wavelength range.


\subsection{Spiketra}

A `spike\-trum' is a half-way house between a
table of values and a spectrum.  It has an element for each wavelength
value in its range, but only a few of these elements -- those that correspond
to the table entries -- actually have non-zero values.  Obviously a spike\-trum
may be generated very simply given just a table of wavelengths and values
at those wavelengths, and a range of wavelength values to be covered.
Usually an existing spectrum is used as a template to indicate the
wavelength values, and the resulting spike\-trum has elements that match
those of the template spectrum exactly in wavelength.  If a spike\-trum is
plotted, the result is a set of spikes of varying height - hence the name.

A spike\-trum may be turned into a spectrum by interpolating between the
spike values.

The GSPIKE command will generate a spike\-trum from a table of 
wavelengths and data values, and the INTERP command will interpolate between
the points to generate a spectrum.  GSPIKE also records the nearest values
that are just outside the wavelength range covered by the template spectrum,
so that INTERP may make use of these as well as the actual spike\-trum values.
GSPIKE also allows some control to be exercised over the data structure of
the spike\-trum -- `SET' records included in the table file can cause the
UNITS or LABEL objects in the structure to be set to appropriate values.
As an example, see any of the supplied flux standard table files (files
intended for GSPIKE usually have a .TAB extension); these generally set the
units of the spike\-trum to match those used in the table, so that GSPIKE
can produce spiketra whose units are AB magnitudes, micro-Janskys,
or whatever, simply depending on what is in the table file used.

For more details about GSPIKE tables, see the section on standard files in
fluxing.

The main reason for the use of spiketra is that they enable 
what is essentially tabulated data -- instrumental response values at
certain wavelengths, as calculated by CSPIKE, for example -- to be
manipulated using the standard Figaro routines designed to manipulate
spectra.

Since spiketra are really just spectra, they can be plotted using 
SPLOT.  They may be modified, if necessary, using the fudge commands
such as TIP\-PEX or SETOBJ.  If SPIKE
is a spike\-trum and SMOOTH is the interpolated spectrum generated from it
by INTERP, the following sequence will generate a plot of the
two superimposed -

\begin{verbatim}
   % soft  xwindows draw=false
   % splot spike reset accept
   % splot smooth noaxes noerase accept
\end{verbatim}

Alternative commands to INTERP for interpolating between the
points of a spike\-trum are SPIFIT (which fits a global polynomial to
the points) and LINTERP (which uses linear interpolation).  The
SPIED command is designed to help modify spike\-trum points in order to
influence the interpolation result - i.e.\ to fudge the resulting spectrum.


\subsection{Fluxing}

{\it 1) Units}

First, a brief word about units.  Although it may not always be
obvious, the underlying philosophy of Figaro is to provide tools which 
you may use as you wish, and not to force you to reduce your data in
a way imposed by the author of the software.  It would be in keeping with
this for there to be no restrictions on the units that can be used for
flux calibration.  However, there are practical limitations.  

Magnitude units, such as the
AB79 system (Oke \& Gunn, 1983) give results that have a comfortable feel
for optical astronomers, and also fall into a numerically convenient range,
generally being numbers between 0 and 25.  Unfortunately, being logarithmic
(not to mention going backwards) they are, while not actually difficult to
handle, sufficiently different to linear units to make it impossible to
write software that can deal with them other than as a special case.

Rather than do that, Figaro compromises.  The flux calibration routines
insist on the calibration being performed using linear units.  When the
flux calibrated spectrum has been produced in such units, it may then be
converted to the AB79 scale.  This means that rather than have a lot of
software that operates on both linear and logarithmic data, we have one
routine (ABCONV) that will perform the conversion.

With that restriction, Figaro will allow you to use whatever linear
units you prefer.  As will become clearer soon, the units used are determined
entirely by entries in the original tables used by the routine GSPIKE, and
if you insist on ``Joules per minute per square foot per Angstrom'' you are
quite at liberty to prepare your own table giving the flux density for your
standard at the appropriate wavelengths in these units.  You will even find
that SPLOT will label your axes properly when you plot the data!

As far as linear units go, there are objections to using ``ergs per
sec per square cm per Hz'' and even more so to ``ergs per sec per square cm
per Angstrom'' on the grounds that the numbers involved are ridiculously
small (and in the latter case can easily go outside the floating point
range, which is a serious problem!).  So, mainly to be able to
avoid having to type commands such as ``SPLOT HIGH=2.5E-27 LOW=1.5E-27'',
the preferred units for Figaro files are Janskys, mJy, or micro-Janskys.

For most of the flux standards for which Figaro supplies tables, two
tables are provided: one in AB magnitudes, usually a direct copy of the
published data, and one in (milli- or micro-) Janskys, usually the result of
a semi-automatic conversion from the former.  If you particularly like
ergs/s/cm**2/A, then you can either provide your own flux tables in these
units and work from them directly, or you can use the FLCONV command to
convert from a spectrum calibrated in Janskys into these units.


\goodbreak
\vspace{12pt}
{\it 2) Standard Files}

The main Figaro directory (environment variable FIGARO\_PROG\_S)
contains a number of files giving the published flux densities of
standard stars, all with a .TAB extension.  It does, however not include
the Oke and HST standards made available by Jeremy Walsh in 1991.  These
need considerable disk space and can be accessed on the central Starlink
data base machine.

For example,
the file G158M100.TAB begins as follows -

\goodbreak
\begin{verbatim}
   *           G 1 5 8 - 1 0 0
   *
   *   Table file for G158-100, faint
   *   object flux standard, based on
   *   Filippenko and Greenstein (1984)
   *   (Preprint, submitted to P.A.S.P.)
   *   Note that these are fitted
   *   continuum fluxes, not directly
   *   measured fluxes, and should be
   *   used accordingly.  This file is
   *   designed for use with the Figaro
   *   routine GSPIKE.  The data here is
   *   given to 3 decimal places and was
   *   supplied directly by Alex
   *   Filippenko.
   *
   SET UNITS = "micro-Janskys"
   SET LABEL = "Flux"
   *
    3300   891.251
    3400   990.831
    3500  1135.534
    3600  1282.331
    3700  1432.187
    3800  1599.556
    3900  1770.110
\end{verbatim}
\goodbreak

The lines beginning with asterisks are treated as comments, and the
lines that begin with `SET' are used to set data objects in the file
created from this table.  An alternative version of this file is
G158M100A.TAB, which contains the lines

\goodbreak
\begin{verbatim}
   SET UNITS = "AB Magnitudes"
   SET LABEL = "Flux"
   SET MAGFLAG = 1
   *
    3300   16.525
    3400   16.410
    3500   16.262
    3600   16.130
    3700   16.010
    3800   15.890
\end{verbatim}
\goodbreak

The functions of the UNITS and LABEL lines should be fairly obvious.
Setting MAGFLAG to 1 in a file indicates to SPLOT that the data is in
magnitude units and so should be plotted with the flux scale reversed.
(Note that most .TAB files actually used by Figaro in fact use .Z.UNITS
rather than just UNITS; these were written for the original version of
GSPIKE where you were allowed to assume that data units always were held
in a file in an item called file.Z.UNITS.  Now that Figaro supports NDF
format files, this is no longer the case, and the abstract term UNITS is
preferred -- however, the original format files still work, since the
new GSPIKE uses a conversion table to handle these explicitly named
items.)  Tables based on data from, for example, Oke and Gunn's 1983
paper will also include the line

\begin{verbatim}
   SET BANDWIDTH = 40
\end{verbatim}

to indicate the 40 Angstrom bandwidth used by their data.  The Filippenko
and Greenstein data represents fitted continuum fluxes, so does not have
a bandwidth -- a point we shall have to return to very shortly.  From the .TAB
files already supplied, it should be possible for you to deduce how to
create your own, should that be necessary.

There is a Figaro convention regarding the
naming of the table files.  The directory command may be used to find
which files are available as tables.  The command

\begin{verbatim}
   % ls $FIGARO_PROG_S/*.tab
\end{verbatim}

will list all the table files supplied in the main directory.  Note that not 
all of these are intended for flux calibration; some may be extinction
tables, etc.  If in doubt, these are all text files, and should have
comments at the start describing their function.  So, the command

\begin{verbatim}
   % more $FIGARO_PROG_S/file.tab
\end{verbatim}

will list the file for you.  You should find that most flux files exist
in two incarnations, as implied above, one in a Jansky based unit, one
in AB magnitudes.  The name of the AB magnitude file ends with `A'.  So,
for example, the files L74546.TAB and L74546A\-.TAB both represent the
standard star L745-46A, but the former is in Jansky units -- and is
therefore probably the one you should use -- while the latter is in AB
magnitudes.  The fact that the name of the object itself ends in `A' 
is an unfortunate complication that may be misleading.


\goodbreak
\vspace{12pt}
{\it 3) The Final Step}

To anticipate for a moment, the final step in the flux calibration
process is carried out by the command SPFLUX.  SPFLUX multiplies an
observed spectrum by a calibration spectrum, to create a flux-calibrated
spectrum.  Each element of the calibration spectrum contains a value
which is effectively the instrumental response of the detector at a given
wavelength, the response being in units of ``Flux density units per
count per second per Angstrom''.  The ``Flux density units'' may be any
linear units, e.g.\ mJy.  SPFLUX assumes that the spectrum to be calibrated
is still in counts, and for each element calculates the wavelength range
covered by that element, and then combines that with the counts in that
element, the value of the calibration spectrum at that element, and the
exposure time for the spectrum, to generate a flux density for the
central wavelength of the element.  The result is, of course, a spectrum
in ``Flux density units'' -- whatever they happen to be.

SPFLUX is straightforward enough; the problem, as with similar
functions, is to generate the flux calibration spectrum.


\goodbreak
\vspace{12pt}
{\it 4) Published Standards}

Published standards fall into two distinct classes; they are
similar, but differ sufficiently that Figaro provides different sets
of functions for dealing with them.

Older, brighter, standards such as those in Oke and Gunn, (1983) 
are published as tables giving wavelengths and the corresponding flux
densities calculated by measuring the observed flux over a range
of wavelength centered on the tabulated wavelength.  (This is the
significance of the .Z.BANDWIDTH value shown in section {\it 2)}  These
tabulated values therefore correspond exactly to what the observed
spectrum should look like, even in the presence of absorption features.

More recently, Filippenko and Greenstein (1984) have published
tables for fainter stars where they fit a continuum to the observed
data and tabulate the value of this continuum at various wavelength values.
These values will not therefore represent the actual observed data in
regions where there is absorption, and the concept of a `bandwidth'
does not apply.


\goodbreak
\vspace{12pt}
{\it 5) First Step --- Turning the Table into a Spiketrum}

(You may find it useful to review the section on Spiketra before
reading on.)

Figaro usually deals with tables, which are tricky to manipulate
directly, by turning them into Spiketra, which can be manipulated like 
spectra, although
they only have non zero values at points that correspond to those 
tabulated.  No matter which type of published standard is involved, the
first step is to generate a spike\-trum from the table.

This requires a template spectrum to give the wavelength scale.  The
one to use is the observed standard spectrum, preferably already scrunched
to a linear wavelength scale.  (You can use unscrunched data, but it is not
advised.)  For example, suppose STANDOBS is such a spectrum of the standard
star HD84937.  There is a table of flux densities for this star (one of
th Oke-Gunn standards) in the main Figaro directory, in the file
HD84937.TAB.  The command

\begin{verbatim}
   % gspike spectrum=standobs \
       table=hd84937 spiketrum=hdspike
\end{verbatim}

will generate a spike\-trum called HDSPIKE that can be used by the subsequent
steps.  If you want a look at it,

\begin{verbatim}
   % splot hdspike reset accept
\end{verbatim}

will show it as a series of vertical spikes, giving the flux density at
each of the tabulated points in the wavelength range of the observation.

GSPIKE will search directories for the table file in the usual Figaro
order: first the default directory, then the user's Figaro directory
(logical name FIGARO\-\_PROG\-\_U), then the local and national Figaro
directories (FIGARO\-\_PROG\-\_L and FIGARO\-\_PROG\-\_N), and finally
the main Figaro directory (FIGARO\-\_PROG\-\_S).  So a table file in any
of these will be found, should you need to create your own.


\goodbreak
\vspace{12pt}
{\it 6) Second Step --- For Oke \& Gunn Data}

The command CSPIKE takes this generated spike\-trum (HDSPIKE) and
combines it with the observation of the standard (STANDOBS) to generate
a new spike\-trum, whose values are now the instrumental response sampled
at the points of the original spike\-trum.

\begin{verbatim}
   % cspike spiketrum=hdspike \
      spectrum=standobs output=calspike
\end{verbatim}

generates this new spike\-trum and calls it CAL\-SPIKE.  The values in CALSPIKE
are calculated for each point by summing the counts in the observed
spectrum over the appropriate wavelength range, dividing them by the
wavelength range and the exposure time, and dividing the result into
the flux density value given in HDSPIKE.  The units of CALSPIKE are
therefore ``units per (count per Angstrom per second)''.  

CALSPIKE can be turned into the calibration spectrum required by 
SPFLUX by interpolation.  The most direct way to do this is just to
use the command INTERP.

\begin{verbatim}
   % interp spiketrum=calspike \
       spectrum=calib
\end{verbatim}

will produce a new spectrum, CALIB, which will be the required calibration
spectrum.  However, you may find that you are not happy with CALIB.  It
may not be smooth enough.  It may be that if there are regions of absorption
in the spectrum, poor alignment of the wavelength ranges will result in
some spurious values.  It may seem to be cheating, but the most direct
way to get round this is to edit the spike\-trum CALSPIKE using the SPIED
command.

\begin{verbatim}
   % spied spiketrum=calspike \
       output=modspike
\end{verbatim}

will display the spike\-trum and allow you to delete points, insert new points,
and see the results of spline interpolation or global polynomial fitting to
the modified points.  When you are happy, you can run INTERP on the
modified spike\-trum to produce a less honest, but more satisfactory, result.

Global polynomial fitting may seem a trifle crude as a way of interpolating
between spike\-trum points, but it may be that this gives a better result in
some circumstances.

\begin{verbatim}
   % spifit spiketrum=calspike order=5 \
       spectrum=calib
\end{verbatim}

would be an alternative to the use of INTERP to generate the calibration
spectrum.


\goodbreak
\vspace{12pt}
{\it 7) Second Step --- For Filippenko \& Greenstein Data}

The most direct way to deal with data where the published values
represent fitted continuum values is to fit a continuum to your observed
data, interpolate the tabulated data, and divide the two resulting spectra
to get a calibration spectrum.

Fitting a continuum is a task that is not easily automated.  The
simplest way in Figaro is to display the spectrum and then use CFIT.
For example:

\begin{verbatim}
   % splot standobs reset accept
   % cfit  output=standfit
\end{verbatim}

will enable you to use the graphics cursor to indicate continuum points
on the displayed spectrum and thus generate the continuum spectrum by
spline interpolation.  A messy job, but not one you have to do often.
Remember that the messy parts of the flux calibration are connected
with generating the calibration spectrum, and you only have to do that
once; applying it is simple.

The published points represent a smooth curve, so just
interpolating directly between them should be quite satisfactory.  That
is, there should be no need to edit the spike\-trum generated by GSPIKE
prior to using INTERP, although SPIED is always available if necessary.
This time, since HD84937 is an Oke-Gunn standard, let's use G158-100
instead.  So the first two steps will be

\begin{verbatim}
   % gspike spectrum=standobs \
       table=g158m100 spiketrum=gmspike
   % interp spiketrum=gmspike \
       spectrum=gmfit
\end{verbatim}

generates GMFIT as the interpolated spectrum from the published points.

Dividing GMFIT by STANDFIT will now generate the required calibration
spectrum.  Note that IDIV will not do, since the division has to allow for
the wavelength range of each element, and for the exposure time.  The
command CALDIV must be used instead.

\begin{verbatim}
   % caldiv standard=gmfit \
       spectrum=standfit output=calib
\end{verbatim}

will generate CALIB as the required calibration spectrum.


\goodbreak
\vspace{12pt}
{\it 8) An Alternative Second Step --- Filippenko-Green\-stein Data}

An alternative way of dealing with this type of data would actually
be to treat it as if it were Oke-Gunn data and use CSPIKE and INTERP as
described in section {\it 6)}.  This is probably satisfactory so long as the
data does not cover a range with significant absorption features.  (If it
does, you can always remove the bad points with SPIED before interpolating.)

If you do this, you have to supply a bandwidth, since the table file
will not have specified one.  You can put one into the spike\-trum generated
by GSPIKE (section {\it 5)})

\begin{verbatim}
  % setobj value=40 \
      object=gmspike.MORE.FIGARO.BANDWITH
\end{verbatim}

will set the bandwidth to 40 Angstroms.  The effect of this is going to
be similar to smoothing the observed spectrum with a filter 40 Angstroms
wide in order to get the continuum values at the points specified by the
table.  After that, you just go through the CSPIKE, (SPIED), INTERP
sequence as in section {\it 6)}, finally ending up with a calibration
spectrum called CALIB.


\goodbreak
\vspace{12pt}
{\it 9) The Final Step Revisited}

Given CALIB as the calibration spectrum, no matter how it was
generated, it is applied as follows:

\begin{verbatim}
   % spflux spectrum=obs calspect=calib \
       output=calobs
\end{verbatim}

which calibrates a spectrum called OBS, creating a resulting spectrum
called CALOBS.

At the risk of being obvious, you should be aware of what you have
created here.  Each element of CALOBS is a sample at a particular wavelength
of the continuous flux density function; it is not a measure of the total
flux within a wavelength bin of finite width, which the original spectra
were.  Apart from anything else, adding two such spectra has the magic
(and totally unreasonable) effect of doubling the magnitude of the object!
Generally, this is not something to worry too much about, but I personally
find it disconcerting to have a spectrum like this generated with a
non-linear wavelength scale. (I have a gut feeling that if the bin covers
half the wavelength it should have half the value, and that is not true
for these spectra.) That is the reason for the warning about not using
unscrunched data.


\goodbreak
\vspace{12pt}
{\it 10) AB Magnitudes, and a Test}

A calibrated spectrum generated in Jansky units (and others,
eventually) can be converted to AB magnitudes by ABCONV.  Our CALOBS
spectrum may be converted by

\begin{verbatim}
   % abconv spectrum=calobs output=abobs
\end{verbatim}

(ABCONV works out the units of the input spectrum by looking at
a .Z.UNITS data object in the input file.  In some cases, it may not
recognise the units - this can happen with spectra that have come from
some other system via one of the translation programs.  If that happens,
and you know that the units are, say, Janskys, you can set the units
by hand using

\begin{verbatim}
   % setobj value=Janskys \
       object=calobs.UNITS
\end{verbatim}

ABCONV will recognise ``mJy'', or anything that contains ``Jansky''
and the words ``milli'' or ``micro''.  The same remarks apply to FLCONV.)

An interesting test of the system is to calibrate an object
using itself, convert the result into AB magnitudes, and then compare
the result with a spike\-trum generated from the published AB magnitude
tables.  For example, remembering that our original spectrum of HD84937
was STANDOBS, and given that there is a table called HD84937A.TAB in the
main Figaro directory that has the values in AB magnitudes, we can try

\begin{verbatim}
   % spflux spectrum=standobs \
       calspect=calib output=hdcal
   % abconv spectrum=hdcal \
       output=abhdcal
   % gspike spectrum=abhdcal \
       table=hd84937a output=abhdspike
   % splot  spectrum=abhdcal reset \
       accept
   % splot  spectrum=abhdspike noaxes \
       noerase accept
\end{verbatim}

The result should be the spectrum of HD84937 in AB magnitudes, with
a series of spikes just touching the spectrum, indicating that (at least
at the tabulated wavelengths!) the spectrum has been calibrated to the correct
value.

A much tougher test would be to use one standard to calibrate another,
and then compare the result with the tabulated values.  This does of course
require that you really do have spectrophotometric data, with no filter
changes, with all of the object in the slit in both cases, and with no
clouds in the way.


\goodbreak
\vspace{12pt}
{\it 11) Summary}

For Oke-Gunn type data -

\begin{verbatim}
   % gspike spectrum=standobs \
       table=hd84937 spiketrum=hdspike
   % cspike spike=hdspike \
       spectrum=standobs output=calspike
   % spied  spiketrum=calspike \
       output=calspike       # (optional)
   % interp spiketrum=calspike \
       spectrum=calib
   % spflux spectrum=obs calspect=calib \
       output=calobs
\end{verbatim}

For Filippenko-Greenstein type data -

\begin{verbatim}
   % gspike spectrum=standobs \
       table=g158m100 spiketrum=gmspike
   % interp spiketrum=gmspike \
       spectrum=gmfit
   % splot  spectrum=standobs reset \
       accept
   % cfit   output=standfit
   % caldiv standard=gmfit \
       spectrum=standfit output=calib
   % spflux spectrum=obs calspect=calib \
       output=calobs
\end{verbatim}


\goodbreak
\vspace{12pt}
{\it References}

Filippenko, A.V., Greenstein, J.L.; 1984, PASP 96, 530

Oke, J.B., Gunn, J.E.; 1983, ApJ 266, 713

Walsh, J.R.; 1991, priv. comm.


\subsection{FFT}

A number of Figaro functions are available to manipulate complex
data, generally with a view to its being used for some process involving
Fourier transforms.  While there are packaged Figaro routines, such as SCROSS,
which make use of Fourier transforms internally, the functions covered in 
this section perform the more elementary operations, and can be put 
together to form a sequence of operations that duplicates the processing
performed by, say, SCROSS, but enabling a finer control to be exercised
over the details of the procedure.  The general design of this set of
routines is based on those provided as part of the SDRSYS system (Straede,
1985).  In these notes the term `Fourier transform' is used rather freely;
it should be realised that in all cases it is the discrete fourier transform
that is meant.

In Figaro 3.0 the structure of complex data files was changed slightly.
Prior to Figaro 3.0, such files contained the modulus of the data as the
main data array and held the real and imaginary parts separately.  From
Figaro 3.0 onwards, the modulus is no longer held as a distinct item, the 
main data array is the real part of the data, and the imaginary part is
still held separately. In practice this should make little difference to
the use of these routines, although it does mean that a complex file created
by Figaro 2.4 or earlier will not be handled properly by Figaro 3.0. The
reason this should make little difference is that complex files are normally
intermediate files and are not retained over a long period.


\goodbreak
\vspace{12pt}
{\it Complex Data Structures}

Figaro defines a `complex data structure' in a fairly precise way.
A complex data structure is a structure containing two arrays, one
holding the real part of the complex data, and one holding the imaginary
part. These arrays have to be the same size and shape, although they may 
have any number of
dimensions.  The dimensions of the arrays must be such that the Fast
Fourier Transform algorithm (Cooley and Tukey, 1965) may be applied to 
them. Different implementations of the FFT have different 
restrictions - many require that the number of elements be a power of 2.  
The NAG library used by the Figaro routines requires that, in each dimension,
the number of elements must have fewer than 20 prime factors, none
greater than 19.  The basic Figaro routines for constructing complex
data structures, R2CMPLX and I2CMPLX, will zero extend data to enforce
compliance with this requirement.

In a Figaro complex data structure the main data array is the real
part of the complex data. 
This means that most of the Figaro routines may be applied to a complex data
structure, and if this is done it will be the real part of the data that
they operate on.  For example, an easy way to look at complex data is
with SPLOT, which will happily plot the real part of the data, quite
ignorant of the fact that the data structure in question is complex.  To
plot the imaginary part, or the modulus of the data, these will first have 
to be extracted using CMPLX2M or CMPLX2I.  It is important to note that 
if an ordinary
Figaro function is used to change this data, for example by

\begin{verbatim}
   % icmult cmplxdata 2 cmplxdata
\end{verbatim}

which multiplies the real array by 2, the imaginary part will be unchanged.
Generally, to avoid unnecessary
conversions before using the NAG FFT routines, the real and imaginary
arrays are held in double precision. However, this is not a strict requirement.


\goodbreak
\vspace{12pt}
{\it Creating a Complex Structure}

In most cases, one starts with an `ordinary' (i.e.\ non-complex) 
file and wants to produce a complex structure in which this is the real
part of the complex data and the imaginary part is zero.  This is what the
command R2CMPLX does.  For example,

\begin{verbatim}
   % r2cmplx myspect cmplxspect
\end{verbatim}

will generate a complex structure with the real part taken from MYSPECT,
and the imaginary part will be set to zero.
If the dimensions of MYSPECT are not suitable for application
of the FFT algorithm, the complex data will be zero padded at high element
numbers to the nearest higher acceptable dimensions.  If this happens,
you are warned about it, since this may have undesirable consequences.
In general, you should not be attempting to transform data that does not
go smoothly to zero at each end (see COSBELL), but if you are then the
results obtained by transforming zero-extended data will show components
due to the sudden drop in value from the end of the data to the zero
values that have been appended as padding.  

If R2CMPLX has to zero pad the main data array, it will also attempt
to extend the data arrays in the axis sub-structures. If
these are linear (if the data has been scrunched, for example), it will be
able to do this.  It will delete any axis structure that contains
non-linear data.

You may prefer to exercise more precise control over the dimensions
of your data and the way they are extended.  For example, if your data
is a spectrum covering 1524 bins from 3000.0 to 4000.0 Angstroms, you
will find that 1524 is not an acceptable dimension.  The closest acceptable
values are 1521 and 1530 (and the easiest way to determine those values is
to run R2CMPLX on your data and let it tell you).  You could then decide
that rather than let your data be zero extended by R2CMPLX, you would
rather rebin it yourself to 1530 pixels.  Probably you would use SCRUNCH, e.g.

\begin{verbatim}
   % scrunch spectrum=myspect \
       wstart=3000 wend=4000 bins=1530 \
       output=scrap
   % r2cmplx scrap cmplxspect
\end{verbatim}

In some cases, you may want to set the imaginary part of a complex
structure as well as the real part.  In this case, you can use I2CMPLX,
which takes the data from a non-complex structure and uses it to set the
imaginary part of an existing complex structure.  This means that the
complex structure has to be created initially by R2CMPLX, and then the
imaginary part can be set by I2CMPLX (which will also modify the modulus
array appropriately).  So, if you have two spectra called RSPECT and
ISPECT which you want to be the real and imaginary parts of a complex
spectrum, the sequence is

\begin{verbatim}
   % r2cmplx rspect cmplxspect
   % i2cmplx ispect cmplxspect
\end{verbatim}

and the order of operations is important; doing the I2CMPLX step first
will produce a quite different result: the I2CMPLX will fail, unless
it so happens that CMPLXSPECT already exists, and even if it succeeds the
R2CMPLX will just produce a new version with a zero imaginary array.

I2CMPLX will zero extend its input data in the same way as R2CMPLX
does, should it be necessary, and the same comments about the desirability
-- or lack of it -- apply.

There is no procedure supplied to create a complex structure with
the imaginary data taken from a specified file, say ISPECT, but with the
real part set to zero.  It was thought that this would be an unusual
requirement.  If needed, the following sequence may be used:

\begin{verbatim}
   % icmult  ispect 0 scrap
   % r2cmplx scrap cmplxspect
   % i2cmplx ispect cmplxspect
\end{verbatim}


\goodbreak
\vspace{12pt}
{\it Going Smoothly to Zero at the Ends --- COSBELL}

The simplest way to ensure that your original data goes smoothly
to zero at the ends is to multiply it by a filter that is unity for most
of the spectral range but goes down to zero at each end in a smooth 
manner.  The most common form for such a filter is the `cosine bell',
and this is what is generated by the Figaro COSBELL function.  (For a
detailed discussion, see Brault \& White, 1971, and the references they
quote).

The only parameter needed by COSBELL is the percentage of the
data that is to be covered by the bell shapes at each end of the
data.  10\% is a common value to use.  COSBELL uses an input data
structure as a template and generates a structure that is the same as
the template except for the data itself.  Usually, you use the data
to which you intend to apply the filter as the template.  So, for example,
to apply a 10\% cosine bell to the data in MYSPECT,

\begin{verbatim}
   % cosbell myspect 10 bell
   % imult   myspect bell myspect
\end{verbatim}

At present, COSBELL cannot handle data with more than two
dimensions.


\goodbreak
\vspace{12pt}
{\it Taking the Fourier Transform}

Actually taking the Fourier transform of a complex data structure
is quite straightforward.  The forward transform is performed
by the Figaro function FFT and the reverse transform is performed by
BFFT.  The only parameters for FFT and BFFT are the names of the input
and output structures.  There are no restrictions on the number of
dimensions in the data.

For example, to calculate the Fourier transform of the spectrum
held in the non-complex structure MY\-SPECT, it should be enough to do:

\begin{verbatim}
   % r2cmplx myspect cmplxspect
   % fft  cmplxspect cmplxspect
\end{verbatim}

which results in CMPLXSPECT containing the FFT of MY\-SPECT.  If the
power spectrum of MY\-SPECT is required it can be obtained by CMPLX2M.  
For example, it may be plotted using:

\begin{verbatim}
   % cmplx2m cmplxspect modulus
   % splot   modulus reset accept
\end{verbatim}

Actually, the modulus is the square root of the power spectrum; the
statement that the power spectrum is available represents a slightly
cavalier attitude towards the proper usage of terms.  However, if the
power spectrum is only needed in order to get a feel for the frequency
distribution of the original data, then the modulus will generally do 
just as well.

Often, a power spectrum needs to be plotted logarithmically to
produce a sensible plot, so it is quite acceptable to try

\begin{verbatim}
   % ilog  modulus modulus
   % splot modulus reset accept
\end{verbatim}

The NAG FFT routines generate data with the low frequency terms in
the lowest numbered elements in each dimension of the data.  FFT (the Figaro
program) re-orders the transformed data so that the zero frequency term is
in the center of each dimension, and the resulting data now goes from $-$N to
+N (where N is the Nyquist frequency).  The plots of the modulus made as
described above will show a plot with an axis labeled from $-$1 to +1, (the
unit being the Nyquist frequency), that is symmetrical about the center and
which peaks (for most input data) in the center.  New axis structures are
created by FFT to reflect the new axis values.  The reason for this
re-ordering of the data is that it seems to be easier to visualise complex
filters, particularly in 2-dimensions, when the low frequency terms are 
collected in the middle of the data rather than scattered into the corners.

The reverse FFT is performed by BFFT.  Note that the precise definition 
of `forward transform' and `reverse transform' differ between FFT
implementations.  The NAG routines, for example, do not introduce any scaling
factors between data that is first forward and then reverse transformed and
the original data.  This means that the sequence

\begin{verbatim}
   % r2cmplx myspect cmplxspect
   % fft  cmplxspect cmplxspect
   % bfft cmplxspect cmplxspect
   % cmplx2r cmplxspect newspect
\end{verbatim}

should generate a NEWSPECT that is exactly the same as the original MYSPECT.
If MYSPECT was zero padded by R2CMPLX to increase its length for the FFT,
then NEWSPECT will have the padded length, but (in theory, at least) the
end elements should be zero.  That is, NEWSPECT should look exactly like the
padded spectrum before it was transformed by FFT.  The NAG routines used are
those in the C06 chapter, and the NAG manual should be consulted for more
details.


\goodbreak
\vspace{12pt}
{\it Extracting the Real and Imaginary Parts}

The previous section sneaked in a reference to the function CMPLX2R,
in the hope that what it did was obvious.  As the name is intended to imply,
CMPLX2R is the reverse of R2CMPLX, and generates a non-complex data structure
whose data array is taken from the real part of the data in a specified
complex data structure.

CMPLX2R does not make any changes to the dimensions of the data
array.  It does not attempt to clip it back to some value it had before
being padded by R2CMPLX.  So, for
example, if MYSPECT has a single dimension of 1524, then the sequence
in the previous section that transformed MYSPECT and then transformed it
back into NEWSPECT will produce a NEWSPECT with a length of 1530.
Getting back to the original in this case requires that the data be subsetted:

\begin{verbatim}
   % isubset newspect 1 1 1 1524 \
       newspect
\end{verbatim}

Analogous to CMPLX2R are CMPLX2I and CMPLX2M, which extract the
imaginary part of the data and the modulus of the data.  Usually, there
will be little point in explicitly extracting the real part, since it is
already available to most Figaro functions as described earlier.  However,
the file generated by CMPLX2R will be smaller than the original file, since
it will no longer contain the imaginary array, and this may be
a point in its favour.


\goodbreak
\vspace{12pt}
{\it Operations on Complex Data}

There isn't a great deal of point in just transforming data back and
forth, unless it's to get a feel for the errors introduced by such a 
process.  Usually one transforms into the fourier domain in order to do
something useful there.  The obvious fourier domain operations are
multiplication (equivalent to a convolution operation in normal space),
and multiplication by the complex conjugate (equivalent to a cross
correlation in normal space).

Figaro provides a number of functions that operate on complex data to
provide a complex result.  The operations performed by these should be
fairly obvious from their names: CMPLX\-MULT, CMPLX\-DIV, CMPLX\-ADD,
CMPLX\-SUB, and CMPLX\-CONJ.  The most useful will probably be
CMPLX\-CONJ, which produces the complex conjugate of a complex data
structure, and CMPLX\-MULT, which performs the complex multiplication of
two complex data structures.

For example, the cross-correlation function of two spectra may be
determined crudely by transforming each, taking the complex conjugate of
one, multiplying the two and transforming back.  This is crude, because it
omits any filtering and preparation of the data, but it will serve 
as a demonstration of the complex operations involved.  If the two spectra
in question are SPECT1 and SPECT2, then

\goodbreak
\begin{verbatim}
   % r2cmplx spect1 cspect1
   % r2cmplx spect2 cspect2
   % fft cspect1 cspect1
   % fft cspect2 cspect2
   % cmplxconj cspect2 cspect2
   % cmplxmult cspect1 cspect2 cspect1
   % bfft cspect1 cspect1
   % cmplx2r cspect1 corrln
\end{verbatim}
\goodbreak

will produce a cross correlation function in CORRLN.


\goodbreak
\vspace{12pt}
{\it Complex Filters and Data Smoothing}

Because the data in the fourier domain is in frequency order, it is
often simpler to create a filter in the fourier domain than to produce
the corresponding convolution function in normal space and then transform
it.  In fact, determining the optimum filter for data is often best done
by examining the power spectrum of the data to be filtered and then
designing the filter around it.  Brault and White (1971) spend some time
on this topic.

At present, Figaro provides only one function that generates complex
filters, and this is CMPLXFILT.  This produces filters of the type described
by Hunstead (1980) for use in cross-correlation measurements.  These are
mid-pass filters formed by taking a Gaussian that peaks at zero frequency
and drops to a half height at a specified high cut value, and subtracting
a narrower Gaussian that rises from zero reaching its half height at a
specified low cut value.  That is, the functional form of the data is
given by

$$f(x) = \exp{\left({{-x^2}\over{2 v^2}}\right)}
       - \exp{\left({{-x^2}\over{2 u^2}}\right)}$$
%f(x)=exp(-x**2/(2*v**2))-exp(-x**2/(2*u**2))

where u and v determine the low frequency and high frequency cutoff values.
From this definition it is clear that $u$ and $v$ are just the $\sigma$
values for
the two Gaussians, and that what has been rather loosely termed a `half height'
is in fact the point at which the Gaussian has a value of
$\exp(-1/2)\approx 0.6$.
They are specified for CMPLXFILT in terms of the Nyquist frequency, so a
filter might have, say, $u=0.1$ and $v=0.3$.
The best way to get a feel for these filters is to generate them and plot
them superimposed on the power spectrum of the data to be filtered.

If the low cutoff value is specified as zero, CMPLXFILT generates
a low pass filter whose functional form is just

$$f(x) = \exp{\left({{-x^2}\over{2 v^2}}\right)}$$
%f(x)=exp(-x**2/(2*v**2))

i.e.\ a Gaussian that drops from unity at zero frequency, having a half
width of $v$.  Note that the cyclic nature of the fourier domain data means
that the filter generated is actually symmetrical about the mid point of
the data -- this is something to be remembered if you want to try to
produce other, more specific, filters by generating them yourself.
The imaginary parts of the filters generated by CMPLXFILT are always
zero.  This means that CMPLXMULT will have the effect of just multiplying
both the real and imaginary parts of the data to be filtered by the real
part of the filter (obviously, since $[a+ib]*[c+id] = ac+ibc$ if $d=0$).

CMPLXFILT requires a template complex data structure, which it will
use as the basis for the filter it produces.  Normally, this will be the
data to be filtered, although any data of the same dimensions will do.
If the template data is n-dimensional, so will be the resulting filter.  The
low and high cutoff frequencies will be the same in all dimensions.  Since
they are specified in terms of the Nyquist frequency, this is usually
fairly satisfactory.

So, to take an easy example, an elaborate way of smoothing a spectrum,
MYSPECT say, by Gaussian convolution, would be\footnote{
   If {\tt ADAM\_USER} is not set, use {\tt \$HOME/adam} instead.  Also,
   ICL has simpler mechanisms to transfer parameter values from one
   application to the other.}

\begin{verbatim}
   % istat myspect reset accept
   % icsub myspect \
       @$ADAM_USER/GLOBAL.STAT_MEAN \
       myspect
   % cosbell myspect 10 bell
   % imult myspect bell myspect
   % r2cmplx myspect cmplxspect
   % fft cmplxspect cmplxspect
   % cmplxfilt cmplxspect 0 0.3 cfilt
   % cmplxmult cmplxspect cfilt \
       cmplxspect
   % bfft cmplxspect cmplxspect
   % cmplx2r cmplxspect myspect
   % idiv myspect bell myspect
   % icadd myspect \
       @$ADAM_USER/GLOBAL.STAT_MEAN \
       myspect
\end{verbatim}

where the 0,0.3 parameters for CMPLXFILT indicate that a low pass filter 
is to be created.  The 0.3 indicates a cut at around a third of the Nyquist
frequency for the data,
and the actual value is one best determined by comparison with the power
spectrum of MYSPECT, obtained by performing a SPLOT of CMPLXSPECT just after
the FFT is obtained.  Being able to do this is really the main justification
for indulging in such a complex procedure when IXSMOOTH could do the same
job far faster.

Note that the sequence given above is a pretty complete one.  The use
of ISTAT at the start is to determine the mean level of the data in the
spectrum and the result of the ICSUB is to reduce the spectrum to a zero
mean level.  This reduces the constant component of the power spectrum and
should produce a better result.  (Note that ISTAT sets the Figaro variable
STAT\_MEAN to the mean value it determines.)  The data is apodised by the
application of a 10\% cosine bell prior to the transform.  Both the effects
of the cosine bell and the subtraction of the mean level are reversed at
the end of the sequence.  Note that this assumes that the data did not have
to be zero padded by R2CMPLX; if it were padded, the resulting MYSPECT would
be longer than BELL, and the IDIV step would fail.


\goodbreak
\vspace{12pt}
{\it Cross Correlation, and a Sequence Something Like SCROSS}

As an additional example, consider the following sequence, which 
attempts to duplicate the functions performed by SCROSS.  It is not an
exact duplication, since the internals of SCROSS are rather different
to those of these FFT routines (SCROSS does not use NAG, for example),
and the filters used by SCROSS also differ from those generated by
CMPLXFILT.  

\goodbreak
\begin{verbatim}
   % sfit spect1 4 cont1 log
   % isub spect1 cont1 sub1
   % sfit spect2 4 cont2 log
   % isub spect2 cont2 sub2
   % cosbell spect1 10 bell
   % imult sub1 bell sub1
   % imult sub2 bell sub2
   % r2cmplx sub1 cspect1
   % r2cmplx sub2 cspect2
   % fft cspect1 cspect1
   % fft cspect2 cspect2
   % cmplxconj cspect2 cspect2
   % cmplxmult cspect1 cspect2 cspect1
   % cmplxfilt cspect1 0.1 0.3 cfilt
   % cmplxmult cspect1 cfilt cspect1
   % bfft cspect1 cspect1
   % cmplx2r cspect1 result
   % peak result
\end{verbatim}
\goodbreak

RESULT now contains the cross-correlation of the two spectra, SPECT1
and SPECT2.  PEAK is a utility produced especially for this sort of sequence,
listing the position of the highest peak in the spectrum it is given in terms
of a shift from the center of the first element.  This is in fact the relative
shift of the two spectra SPECT1 and SPECT2.

Note that the cross-correlation peak will in fact not be
central in RESULT, but will be at one end or the other.  It is often easier 
to handle data like this if the data is rotated so that the peak is roughly
central, and this can be done by the function ROTX.

\begin{verbatim}
   % rotx result 765 result
\end{verbatim}

where 765 is half the number of pixels in the data (here assumed to be
1530).  This has the effect of turning RESULT into the cross-correlation
spectrum that might have been produced by SCROSS.

Some of the other strange numbers in the sequence may need explanation.
The `4's in the SFIT indicate that the sequence fits a 4th order polynomial
to the spectra, and then subtracts away the polynomial fit.  This is in
fact what SCROSS does, unless the fitting is disabled.  The 0.1,0.3
parameters in CMPLXFILT are quite arbitrary, and the optimum values to use
are in fact best determined either by looking at the power spectrum of
the data to be filtered, or by consideration of the types of features one
wants to emphasise in the determination of the correlation (Hunstead,
1980).


\goodbreak
\vspace{12pt}
{\it Summary of Figaro FFT Routines}

\begin{itemize}
\item BFFT Takes the reverse FFT of complex data.
\item CMPLX2I Extracts the imaginary part of a complex data structure.
\item CMPLX2M Extracts the modulus of a complex data structure.
\item CMPLX2R Extracts the real part of a complex data structure.
\item CMPLXADD  CMPLXDIV  CMPLXMULT  CMPLXSUB    Perform arithmetic operations
   on two complex data structures giving a third.  Cf. IADD etc.
\item CMPLXCONJ Produces the complex conjugate of a
   complex data structure.
\item CMPLXFILT Creates a mid-pass filter for complex data.
\item COSBELL Creates data that goes to zero at the edges in a
   cosine bell.
\item FFT Takes the forward FFT of complex data.
\item I2CMPLX Copies a data array into the imaginary part of a complex 
   data structure.
\item PEAK Determines position of highest peak in a spectrum.
\item R2CMPLX Creates a complex data structure from a real data array.
\item ROTX Rotates a data array along the X axis an integer \# of pixels.
\end{itemize}


\goodbreak
\vspace{12pt}
{\it References}

Brault, J.W., and White, O.R. 1971, Astron. Astrophys., 13, 169

Cooley, J.W., and Tukey, J.W. 1965, Math. Comput. 19, 297

Hunstead, R.W. 1980, Proc. A.S.A. 4, 77

Straede, J.O. 1985, Scanner Data Reduction System User's Manual, 
Anglo - Australian Observatory.


\subsection{S-Distortion}

Instruments such as the 2D-Frutti and IPCS that use image intensifiers
suffer from various distortions, in particular S-distortion.  The effect --
and the reason for the name -- can be seen clearly by displaying any 2D-frutti
image of a point object, such as a star, on the display. Instead of being
a perfectly horizontal line, the spectrum snakes across the image in the shape
of a horizontal letter S.  

Note that this distortion is actually a two-dim\-en\-sional distortion --
a picture of cartwheel taken through an image tube will show all the spokes
bent into S shapes in a radially symmetric manner.  However, the difference in
pixel scale in the two dimensions for spectral detectors means that -- to a
first approximation -- the distortion can be treated as though it were simply
a vertical displacement in the data whose magnitude varies along the spectrum
(and to a lesser extent, with position along the slit).
In any case, the two dimensional distortion can be corrected by two orthogonal
one dimensional corrections: the S-distortion correction described here is one,
and the other can be 
performed as a side effect of a two-dimensional rebinning to
a linear wavelength scale.  It may be that such full 2-D `scrunching' is 
regarded as overkill -- see the section on wavelengths for more details.

The process described here is a one-dimensional correction, in which
data is rebinned in the direction perpendicular to the dispersion, in such
a way as to straighten the spectra.


\goodbreak
\vspace{12pt}
{\it Usual Sequence}

The tricky part is to determine the exact shape of the distortion, and
in Figaro this is done using the command SDIST.  SDIST requires an image with
one or more spectra in it.  The usual thing to do is to take an observation
of a flat field through a multi-hole mask, in which case the resulting image
will show one spectrum for each hole in the mask, each distorted into an S.
The spectra will be roughly parallel, but the two-dimensional nature of the
distortion will mean that they are not exactly so; this is the reason for
using multiple spectra: it allows the change in distortion with slit
position to be determined.  The CDIST command can then be used to apply the
results of the distortion analysis performed by SDIST in order to correct
an image.

The usual sequence of operations is as follows: assume HOLES is a
multi-hole calibration image, and OBJECT is an image to be corrected.

\begin{enumerate}
\item Use IMAGE to display the calibration image (HOLES) on the image 
   display.
\item Use ICUR to indicate
   a starting point for each one of 
   the spectra to be used for the analysis.  The starting point should simply
   be a point, usually near the center of the spectrum, where the spectrum is
   strongest.
   SDIST will start from these positions as it traces out the
   spectra.
\item Run SDIST.  This will trace out the indicated spectra and indicate
   on the display how much of each spectrum it was able to follow.  It then
   fits a polynomial (10th order usually works best) to each and writes the
   resulting coefficients into a disk file.
\item Run CDIST on the image to be corrected.  This will read in the
   results from the previous step and rebin the image data as indicated
   by the SDIST analysis.  To determine the distortion values at points
   between the analysed spectra, for each column in the image 
   CDIST fits a low order polynomial to positions of the fitted spectra in
   that column.  Obviously, if only one spectrum were used for the analysis
   CDIST will have to assume that the distortion is constant along the slit.
   A good thing to try is to attempt to correct the calibration image.  If it
   doesn't end up with perfectly horizontal spectra then something has gone 
   wrong.
\end{enumerate}

\begin{verbatim}
   % image holes reset high=xxxx \
       low=xxxx accept
   % icur # using space bar to indicate \
          # spectra to be used
   % sdist image=holes columns=9 width=2 \
       maxdeg=10
   % cdist image=holes ystart=-1e30 \
       yend=1e30 output=test maxdegy=5
   % image test accept
   % cdist image=object ystart=-1e30 \
       yend=1e30 output=objc maxdegy=5
\end{verbatim}

There are a couple of decisions that have to be made in this sequence.
Firstly, there is the question of just how many of the hole spectra should
be used.  Generally the answer is that all should be used, unless they are
going to cause problems -- a spectrum that gets too close to the edge of the
image will probably not be traced properly, for example.  Then there is the
use of the COLUMNS parameter in SDIST to help control the tracing of the data.
In most cases, the main point about the COLUMNS parameter is that making it
larger reduces the number of points to which the polynomials are fitted, and 
so speeds up the process considerably.  However, if the spectra are
particularly weak, increasing COLUMNS will also improve the signal to noise
of the cross-section profile of the spectra.  (SDIST sums 
the data over the
specified number of columns, then tries to locate the center of the resulting
profile, assuming that the best starting guess for the center is the value
obtained from the previous fit.)  

SDIST can use a number of different 
algorithms for tracing the spectra, selected by the TRACE parameter.  The
original algorithm used by SDIST is the G(aussian) option, and this is
most suitable if the profile of the summed data is roughly Gaussian (as it
usually is for stars observed using a slit).  If the profile is roughly
`top hat' in profile (if a dekker has been used, for example), then either 
E(dge) or C(enter of gravity) will probably be better.  Edge fitting tends 
to produce rather jumpy fits, particularly if the edges are very sharp and
therefore cannot be located to better than a pixel.  The center of gravity
fit (which takes the COG of the data within the edges) is usually smoother,
but can be inaccurate if the top of the data is not flat.  A YSTRACT through
the data, followed by a SPLOT can give a good feel for the data profiles,
and the diagnostic display produced by selecting the SOFT option can be
very helpful here.  The information produced by specifying the DIAGNOSTIC
keyword is really for debugging new trace algorithms and is unlikely to be
of general use.

In the end, there is no substitute for watching the results on the
display.
Remember that the polynomial
will be unconstrained outside the range where it could trace the spectrum,
and this may well show up in the
final results from CDIST.  So see if the spectra seem to have been fitted
acceptably, and if not, either ignore the bad ones or try to fine tune the
fit using the SDIST parameters.  The display that SDIST can produce on a 
line graphics device is a useful diagnostic.
(In Figaro 3.0 SDIST could overlay the fit on a previous display done with
IMAGE. Sadly this option is not available in the current version.)

OFFDIST may be useful if there is a slight
linear offset in Y between the calibration data and the data to be
corrected.  This is not usually important if the data is to be corrected
by CDIST, but other routines such as MASKEXT can make use of SDIST results
in a way that makes such offsets (often the result of guiding changes)
important.  OFFDIST adds an offset in Y to the results produced by SDIST.

The final step, the correction of the object data, is an automatic
process.  So if a number of images are to be corrected there is a lot to be
said for doing this as a batch job.


\goodbreak
\vspace{12pt}
{\it Self-Correcting Objects}

Image tubes are not always tremendously stable; it may not be reasonable
to assume that the details of the distortion do not change with time.  This
means that if there is only one multi-hole calibration image, probably taken at
the start of the night, a distortion analysis based on this single early image
may become progressively less correct as images from later and later in the
night are processed.  And, of course, there is always the possibility that
no such image was taken at all.

In these cases, it is always possible to produce a distortion analysis
from a spectrum of a single point object.  An image of such an object can
be used to calibrate itself, from a distortion point of view.  The disadvantage
of this over the use of a multi-hole exposure is that the correction based
on a single object will be exactly correct only at the position of that object
in the slit -- the sky nearby will not be quite so well corrected.

So the choice of using objects to calibrate themselves, against the use
of a multi-hole calibration image, is a trade-off of one source of error
against another.  The choice has to be yours.


\goodbreak
\vspace{12pt}
{\it Use of the CDIST parameter ROTATE}

The operation performed by CDIST involves working on the image
column by column.  On a virtual memory machine such as the VAX, this
is to commit the cardinal sin of accessing an array in the wrong 
order -- against the grain of the virtual memory system.  When large
images are processed, the result will be excessive page faulting by
the process, together with a dramatic increase in the time taken
(both CPU time and elapsed time go up), and an even more dramatic
increase in the anger levels of the other users of the VAX, since the
excessive paging will begin to saturate the disk I/O system.  This
condition is described as `thrashing'.

To reduce this faulting, it is possible to rotate the image before
it is processed and rotate it back afterwards.  CDIST has a hidden 
parameter `ROTATE' which makes it do this automatically.  The final
results obtained are the same whether or not ROTATE is specified, but
the efficiency of the operation can be quite different.
Specifying ROTATE adds the overheads
of the two rotations, but makes the correction work properly with the
virtual memory system.  The rotation algorithm used understands about
virtual memory, and attempts to optimise itself to your working
set (the amount of real memory you have available) in order to operate 
efficiently, so for large images the improvement
can be quite considerable.  For small images, where the correction 
algorithm probably does not induce thrashing anyway, use of pre and post
rotation will be inefficient since it adds the overheads of the two
rotations.  However, these overheads are small for small images, and
it is probably acceptable to always specify ROTATE.  This is why ROTATE 
is a hidden parameter, true by default.  

Just what constitutes a `small' or a `large' image -- i.e.\ the size
of image at which one starts to gain by specifying ROTATE -- is hard to
say.  It depends on the working set quotas of the user, and on the load
on the system at the time.  In principle, if your working set is W pages, 
(ask your system manager) and your image is NX by NY, then CDIST will
thrash if ROTATE is not specified and $NX*NY*2 > W*128$.
(The 2 is because there is an input
and an output image for the correction, the 128 is the number of real
array elements held in a page.)  The best guide, however, is trial and
error.  As it usually is.


\goodbreak
\vspace{12pt}
{\it Making Do without an Image Display}

Firstly, if you cannot display greyscale or colour, you still can use ICONT
on any line graphics terminal. Its display is equivalent to that of IGREY
and can be used by IGCUR {\it (not ICUR)}.

It is -- just -- possible to make do without any display at all,
should one not be
available. SDIST picks up the number of spectra and the pixel coordinates from
global parameters NPIXELS, XPIXELS and YPIXELS. You can create these with
CREOBJ, or delete them with DELOBJ if necessary. XPIXELS and YPIXELS are
vectors, if these are too short, delete and re-create them. Once created you
can assign values to these parameters using SETOBJ:\footnote{
   If {\tt ADAM\_USER} is not set, use {\tt \$HOME/adam} instead.}

\begin{verbatim}
   % delobj \
       object=$ADAM_USER/GLOBAL.NPIXELS
   % delobj \
       object=$ADAM_USER/GLOBAL.XPIXELS
   % delobj \
       object=$ADAM_USER/GLOBAL.YPIXELS
   % creobj type=_REAL dims=0 \
       object=$ADAM_USER/GLOBAL.NPIXELS
   % creobj type=_REAL dims=8 \
       object=$ADAM_USER/GLOBAL.XPIXELS
   % creobj type=_REAL dims=8 \
       object=$ADAM_USER/GLOBAL.YPIXELS
   % setobj value=2 \
       object=$ADAM_USER/GLOBAL.NPIXELS
   % setobj value=15 \
       object=$ADAM_USER/GLOBAL.XPIXELS(1)
   % setobj value=21 \
       object=$ADAM_USER/GLOBAL.YPIXELS(1)
   % setobj value=25 \
       object=$ADAM_USER/GLOBAL.XPIXELS(2)
   % setobj value=35 \
       object=$ADAM_USER/GLOBAL.YPIXELS(2)
\end{verbatim}


\subsection{Wavelengths}

Simple statements such as `the wavelength of channel n
is so many Angstroms' are not as unambiguous as they
appear.  This statement is really a slightly simplified
version of `the wavelength range covered by channel n is
from so many Angstroms to so many Angstroms' but some
precision has been lost in the simplification.  It presumably 
means `the wavelength of the center of the range
covered by channel n is so many Angstroms' -- {\it or} does it
mean that `the wavelength of the start of the range
covered by channel n is so many Angstroms'?

These notes are intended to explain the conventions used
by Figaro. Please note that they are all somewhat arbitrary
and in some cases are historical rather than ideal.
However, they are consistent and they are precisely defined.
Hopefully, these notes will also serve as a brief guide to
the use of Figaro for wavelength calibrations.  If they
seem a trifle pedantic, this is because experience has
shown that the whole subject is a real can of worms.


\goodbreak
\vspace{12pt}
{\it Discrete and Continuous Channel Numbers}

Most of the problems arise because

\begin{enumerate}
\item It is impossible to get away from the fact that data is held in a
   number of discrete `bins' or `channels', each of which has to be
   given an integer channel number (to be called `n' in these notes),
   and each of which originally covered a range on the detector.
\item A center of gravity analysis of an arc line, say, will not in
   general produce an integer channel number as a result.  So there also
   has to be a `continuous channel number' (called `x' here).
\item The channels also map onto a wavelength scale, and wavelength
   scales are most naturally thought of as continuous.  It is therefore
   quite natural to think of a wavelength/channel number relationship of
   the form

   $$\lambda(x) = f(x)$$

   where $f(x)$ might be a polynomial in $x$.  Note that Figaro does not
   actually store wavelength information using polynomial coefficients,
   unlike many systems.
\end{enumerate}

The sad fact is that there is no obviously correct
method of mapping the continuous channel numbers (`x')
onto the integer channel numbers (`n').  Mainly it's a
question of selecting the zero point.  The Figaro
convention is that

\begin{enumerate}
\item The center of channel $n=200$ is at $x=200.00$, and the channel
   therefore spans in $x$ from $x=199.5$ to $x=200.5$
\item The `wavelength of a channel' means the wavelength of the center
   of that channel, i.e.\ $\lambda(n)=f(x)$ where $x=n$.
\end{enumerate}

This all sounds mind-blowingly trivial, BUT it is
possible to produce other conventions.  (In particular,
if the left edge of channel 1 were taken as the $x=0.0$
point, the center of channel 200 would be at $x=199.5$)


\goodbreak
\vspace{12pt}
{\it Wavelength Arrays}

Figaro tries to hold wavelength information in as general a way 
as possible.  If a file called, say, SPECT contains a wavelength calibrated
spectrum, then there will be two data arrays in the structure held in the
file.  The main data array will be an array
containing the actual spectrum, and 
the X-axis array will be an array of related size containing the wavelength
information.

Each element of the X-axis array contains the wavelength value for
the corresponding element of the spectrum -- that is, for the CENTER of the
corresponding data bin.  For uncalibrated data, the X-axis array will
either not exist, or will simply contain the pixel numbers.

The EXAM command may be used to examine elements of either the data
or wavelength arrays, and the ILIST command can be used to tabulate them.


\goodbreak
\vspace{12pt}
{\it The ARC Command}

A single arc spectrum can be generated by summing successive rows of
an arc image, using the EXTRACT command.  The wavelength to channel number
relationship can then be determined using the ARC command.  This invokes
an interactive line identification program which gets the user to select
lines in the arc and to specify their wavelengths.  ARC makes full use of
interactive graphics and a running fit to make the manual identification
process as easy as possible, and it also has an automatic line identification
feature that can be used to add lines to a fit.  ARC is
described in more detail in the section on techniques headed `ARC'.

Once a satisfactory fit has been obtained, ARC can be used to set the
wavelength scale in the arc spectrum being identified.  This is the import of
the question ``make use of this fit?'' that the user is asked at the end of 
ARC.  One the wavelength scale has been set, subsequent plots of that arc
spectrum made by SPLOT will be made with a scale in wavelength units.

Note that Figaro programs are all designed to work with data whose
X scale either increases or decreases.  However, as a practical point, it is
regarded as more usual to arrange data so that wavelength increases with 
pixel number, and there may be bugs still lurking in some routines that
will only turn up when used with reversed data.  
You are therefore strongly recommended
to use increasing X scales, if necessary using the IREVX command to
reverse spectra and images.

If ARC is used to fit a spectrum very similar to the one previously
fitted, it is enough to make use of the previous fit (by answering YES to
the PREVIOUS prompt) and then re-identify a line used in the previous
fit.  ARC will ask if the previous identification is to be ignored, or the
new identification, and as a final option will assume that there is a shift
in the data relative to the identifications and will re-analyse the
line centers accordingly.  This makes the analysis of a sequence of arcs
quite simple.


\goodbreak
\vspace{12pt}
{\it Applying an Arc Fit to Other Spectra}

Of course, the point of getting such a fit is to apply it to other
spectra.  The command XCOPY copies the wavelength scale from one spectrum
to another.  Normally the spectrum whose wavelength scale is copied is
a fitted arc and the spectrum to which it is copied is that of an 
observed object.

Typically, an observation of an object is bracketed by arc observations.
These different arcs will inevitably give slightly different fits, either
just because of random variations, or because of some instrumental drift.  
Particularly if there is drift involved, it may be better to use XCOPI rather
than XCOPY.  XCOPI takes as input two spectra with wavelength scales and
generates a new scale that is an interpolation between them.  It then applies
that interpolated scale to the object spectrum.


\goodbreak
\vspace{12pt}
{\it Linear Wavelength Scales --- Scrunching}

It is possible to rebin data whose wavelength/channel number relation
is known in such a way that the relation is linear.  Such an operation is
known as `scrunching', and is performed by the SCRUNCH command.  
Scrunched data is generally easier to handle than is unscrunched.  For
example, if spectra
with slightly differing wavelength scales must be added together they should
be scrunched to a common wavelength scale first.  

Similarly, there are cases where one needs data rebinned on a 
logarithmic wavelength scale -- cross-correlations to determine redshift need
this sort of scale, for example -- and this can also be performed using
SCRUNCH.

SCRUNCH has parameters that specify the start and end wavelengths for
the resulting scrunched spectrum, and the number of elements (bins) it should
have.  Note that the wavelengths are those of the CENTERS of the end bins;
it is easy to miscalculate the number of bins by 1 when aiming to get a
linear dispersion of exactly so many Angstroms per bin.  The target wavelength
range may also be specified in terms of the start wavelength and the increment,
by means of the rather crude convention that if the end wavelength specified
is less than the starting wavelength, then that `end wavelength' value will
be taken as the wavelength increment.  (This behaviour may be controlled
explicitly by two keywords FINAL and INCREMENT, which are used to specify the
interpretation to be placed on the `end wavelength' value.)

SCRUNCH has one keyword that can confuse users.  If the spectrum being
scrunched is in flux units, then the total flux in the spectrum should be
conserved by the scrunching process.  Note that the distinction is between
flux units and flux density units, so `raw counts' {\it are} a flux unit.
If a spectrum is in flux density units, then scrunching should not change
the absolute value of the data.  If the scrunching is simply such that it
doubles the number of bins in the spectrum, then an element that has a value
of, say, 10mJy should become two elements each with a value of 10mJy, since
Janskys are a flux density unit.  Conversely, if the spectrum were still in
raw counts, then an element with 10 counts should become two elements each
with 5 counts.  The default for the FLUX keyword in SCRUNCH -- whose prompt
is the perhaps confusing ``conserve flux (as opposed to mean counts)?'' is
YES, and this is correct for raw counts.


\goodbreak
\vspace{12pt}
{\it 2-Dimensional Scrunching}

The previous sections have all been concerned with single spectra,
and this is usually satisfactory where images can easily be reduced
to spectra by use of EXTRACT -- i.e.\ where there is no significant change in
wavelength/channel relationship across the subset of the image that is
being collapsed into a single spectrum.
However, there is an alternative approach, which is to scrunch each cross
section of an image separately.  This may be needed for data on extended
objects, or it may simply be regarded as a better (more strictly correct)
means of proceeding.

To do this, one needs a good fit to {\it every} cross-section of an arc
image.  This is performed automatically by IARC, starting off from a manual
fit produced by ARC.  The usual technique is to extract a spectrum from
the center of the image, probably adding together a few cross-sections to get
good signal to noise, and then fit that using ARC.  It is important to have
as many lines as possible in the fit; it is even worth adding lines in any
sparse areas to help lock down the fit, even if their identifications are
uncertain -- just treat the fitted values as exact.  IARC does have an
automatic search for such `lock' lines, but it is usually better to
pick good ones manually.

By default, IARC performs its analysis using the same line width
parameter, SIGMA, as was used in the original fit performed by ARC.
However, the algorithm used by IARC is quite sensitive to the value
of this parameter, and better fits may sometimes be obtained by varying
it.  For this reason, IARC has a hidden parameter RSIGMA which may be
used to over-ride the ARC value.  If you have problems with IARC, there
is a hidden keyword, DETAIL, which causes the details of the fit for
each cross-section to be output.  This can be used to see which lines
the program is having trouble with.  IARC will drop a line from its
search list if it fails to find it in a a given number of successive
cross-sections -- this is to prevent it accidentally picking up the wrong
line due to distortion moving another line towards the position where it
lost the original line.  The number of cross-sections is given by the
hidden parameter GAP -- normally, GAP is set to 1, meaning that the 
second time in succession a line is missed, it will be dropped.  Setting
GAP sufficiently high will effectively disable this feature -- this may be
the best way to deal with data where there is little distortion, but where a
number of cross-sections may have poor data; fibre data can be of this type.
If GAP is set to zero, a line will be dropped immediately if it cannot be
found in a spectrum.
In order to pick up as many lines as possible in each cross-section, IARC
uses a technique known as `spreading' -- it first looks for the lines using 
a larger sigma value than that specified, and then refines the fits using
the specified sigma value.  In some cases this is undesirable, and may be 
bypassed by the use of the hidden keyword `NOSPREAD'.

Sets of spectra which have been taken in some long slit mode with a
distorting detector, such as an image tube, will differ from spectrum to 
spectrum but will do so smoothly.  Sets of spectra taken with a fibre system
may vary discontinuously, with sudden linear shifts between spectra.  The
XCORR keyword in IARC is designed to be used with such discontinuous
spectra.  It causes IARC to attempt to determine a linear shift between
successive spectra by cross-correlation and to apply that shift to its
expected arc line positions before searching for them in the new spectrum.
XCORR should be used for fibre and similar data, but is probably inappropriate
for other data.

Given a fit from IARC, ISCRUNCH can be used to scrunch an 
image of an object.
It is actually a good idea to scrunch the original arc and see what the
results look like -- lines waving about in some regions show that the fit
is badly constrained in those regions.  IARC only performs a sequence of
individual 1D arc fits, rather than a 2D fit to the whole image at once,
so there is little constraint that the fits be continuous from one cross
section to the next, other than that imposed by the lines themselves.

Note that the IARC results can only be used by ISCRUNCH; they are not
written into the data structure in a way that can be used by other routines
such as SPLOT.

For object images bracketed by arc images, ISCRUNI can be used 
instead of ISCRUNCH.  This uses wavelength values obtained by interpolation
between two different IARC fits, in an similar way to XCOPI as described
earlier.


\goodbreak
\vspace{12pt}
{\it Sequence Summary}

For individual spectra:

\begin{itemize}
\item EXTRACT       get spectra from image.  Both arcs and objects.
\item ARC           set wavelength scale on arc spectra.
\item XCOPY/XCOPI   copy wavelength scales to objects from arc(s).
\item SCRUNCH       reduce spectra to linear/log wavelength scales.
\end{itemize}

For images:

\begin{itemize}
\item EXTRACT       get spectrum from center of arc image.
\item ARC           set wavelength scale on arc spectra.
\item IARC          automatic fit to all cross-sections of arc image.
\item ISCRUNCH      reduce images to linear/log wavelength scale.
\item EXTRACT       get spectra from scrunched images.
\end{itemize}


\subsection{Extinction}

The EXTIN command is used to correct a spectrum
for atmospheric extinction.  It requires a `coefficient' spectrum --
a spectrum giving the atmospheric extinction coefficient for each
element of the spectrum.  This coefficient spectrum is normally
generated by interpolating a spike\-trum formed from a suitable table
of extinction coefficients.

One such table is EXTIN, (actually the file EXTIN.TAB in the
FIGARO\_PROG\_S directory).  This contains the coefficients given for
Palomar by Hayes and Latham (1975).  So for Palomar data, the following
sequence will perform the extinction correction:

\begin{verbatim}
   % gspike spectrum=object table=extin \
       spiketrum=extspike
   % linterp spiketrum=extspike \
       spectrum=extcal
   % extin spectrum=object coeff=extcal \
       output=cobject
\end{verbatim}

generating a corrected spectrum, COBJECT, from the original uncorrected
spectrum, OBJECT.  The first two steps do not need to be repeated for
subsequent spectra, so long as the wavelength range covered remains
unchanged.

(An alternative extinction table is PALOMAR, which contains the
coefficients used by TYB's Forth system.  This differs slightly from
EXTIN, particularly around 9000 Angstroms, where it attempts to
correct for some atmospheric features ignored by Hayes and Latham.
The differences can be seen by generating and then plotting the spiketra
produced by the two tables.)   

The AAOEXT table gives the standard AAO
extinction coefficients -- those used by the SDRSYS data reduction system.


\goodbreak
\vspace{12pt}
{\it References}

Hayes, D.S. and Latham, D.W.; 1975, Ap.J. 197, 593


\subsection{Arc}

This section describes the use of the ARC program in Figaro.  ARC
is an interactive arc fitter, which displays an arc spectrum on the
current soft graphics device and gets the user to indicate arc lines and
enter their wavelengths.  Given sufficient lines, a polynomial fit of
wavelength against channel number can be performed, and the results of
this fit used to fill the wavelength array (the .AXIS(1).DATA\_ARRAY).  The
section on techniques headed `WAVELENGTHS' describes the way ARC fits
can be used.

ARC is primarily designed as an interactive arc fitter, but it
does have an an automatic line finding capability.  This description will
emphasise the interactive aspects of ARC, however, since this 
automatic capability is intended to help add lines to an already good fit,
and so the first and most important thing is to get a good fit manually.

ARC has a number
of features that are intended to make it particularly simple to use.
These are not always obvious to the user who simply types the command
`ARC' and waits to see what happens next; hence this description.


\goodbreak
\vspace{12pt}
{\it Arc line lists}

To get the best out of ARC you need a comprehensive line list for
the arc you are trying to identify.  Arc line lists are text files containing
lists of arc line wavelengths.  They have the extension .ARC and are
usually held in the main Figaro directory (FIGARO\_PROG\_S).  There may
also be files in the national or local Figaro directory (FIGARO\_PROG\_N or
FIGARO\_PROG\_L), or
you, the user, may have your own in your user Figaro directory 
(FIGARO\_PROG\_U), or in the default directory.  As an example, the
file ARGON.ARC in the main Figaro directory, begins -

\goodbreak
\begin{verbatim}
   *  Argon lines

   3243.6887
   3249.8003
   3281.7016
   3307.2283
   3350.9243
   3376.4359
   3388.5309
   3454.0952
   3476.7474
   3478.2324
   3480.5055
\end{verbatim}

and carries on in the same vein for quite some time.  The format is very
simple -- each line of the file has a wavelength, in free-format, and blank
lines and lines beginning `*' are ignored.  Note that no line strength
information is used. The importance of this line list
will become clearer as we go on.

When the `ARC' command is given, one of the first prompts asks
`Type of arc?'.  ARC wants to know which arc line lists to read in.
You can specify up to three arc types, although it is unusual to use
more than one.  If your reply is 'ARC1,ARC2,ARC3', then ARC will look
in the various Figaro directories for the files ARC1.ARC, ARC2.ARC and 
ARC3.ARC.  It will then read in the arc lines from all three.  The idea
here is that if you have, say, a copper-argon arc, you might give the
type as 'COPPER,ARGON', and ARC can make use of two separate files, one
for each element.  In practice, it would be better to have a single file
called CUAR.ARC, set up specially for the arc you are using, and to
reply 'CUAR'.  If you have no suitable arc file, reply 'NONE'.


\goodbreak
\vspace{12pt}
{\it Other initial prompts}

ARC will also prompt for the arc line half width in pixels (the
parameter called SIGMA), which it uses as a guide when finding the
centers of the arc lines, and for the initial order of polynomial to
use for the fit.  
ARC performs running fits, and unless you are using a previous
line list (see below), the first fit will have to be made to a low order,
simply because there will not be enough lines for a higher order.  Once enough
lines have been identified, ARC will start to use the order you specified
initially.  You will be able to change the values for both Sigma and 
Order interactively during the fitting process.

ARC will also ask if lines from the previous fit are to be used.
During a fit, ARC keeps writing out to disk the positions and wavelengths
of the lines identified so far.  If you reply YES to this prompt, ARC will
read in the file giving the previous list of identified lines.  This
allows you to start again where you left off the previous time -- either
because ARC or the computer crashed (perish the thought!) during the
previous fit, or simply because, on reflection, you are unhappy with the
fit you obtained and want to experiment with other orders, a different
selection, etc.  The default for this previous list file is always
ARLINES.LIS, since that is the name of the file that ARC writes.  However,
you may specify a different file.  You may, for example, have one file
that you want to use as the basis for fits to a number of arcs, which you
have renamed from ARLINES.LIS (the name ARC will have given it originally
when it was produced) to, say, BASEFIT.LIS.

You can also use a previous fit if the arc you are
identifying is very similar to the previous arc, but is shifted slightly.
This is often the case with arcs taken at intervals throughout a night.
ARC will notice if the file that was used to create the previous fit is
different to the file you are fitting.  If this is the case, you are prompted
for the XCORR keyword, which allows you to request that ARC locate the
previous spectrum used and attempt to determine a linear shift by 
cross-correlating the two arcs and applying the determined shift to the 
arc line list.  (If you are going to use this option, specifying a slightly
larger SIGMA than that used for the previous arc will give the line
finding algorithm a little more flexibility when it looks for the lines
at their shifted positions.)
This is a simple operation, but the shift is determined
over the whole of the arc.  There is an alternative, messier, way to
indicate a shift to ARC on the basis of a single selected line, but this
is described later.


\goodbreak
\vspace{12pt}
{\it The line selection process}

ARC will read in the specified spectrum and display a portion
of it on the graphics device.  Initially, the portion will be 200 channels
long; you can change this should you want to.  You will be invited to
use the cursor to indicate an arc line.

Normally, you move the cursor until it is close to a line whose
wavelength you know (you will often find it useful to have a hard plot of
the whole arc in front of you as you perform the fit) and select it by
hitting the space bar.  (Strictly, you can use any key that does not have
some specific function, but there are rather a lot that do and the
space bar is a safe one to use.)  ARC will then try to find a line close
to the point you have indicated, and if it finds one will show you its
center on the display with an arrow.  The algorithm used to find a line
center is one described as `convolution with the derivative of a
Gaussian', and it incorporates some requirements as to just what constitutes
a line, which can lead on rare occasions to its being unable to find a 
line centered near the position indicated.  You can think of the algorithm
as a fit to a Gaussian of fixed sigma.

You will be told the channel number of the line, and asked to
enter a wavelength.  Strictly, you enter a wavelength followed by a type,
e.g.

\begin{verbatim}
   3850 ARGON
\end{verbatim}

The type should be one of the names specified in response to the
`What type of Arc' prompt.  ARC then looks in the table for that type
and selects the line whose wavelength is nearest to the one you specified.
If you do not specify a type (which is by far the most usual case), ARC
uses the first of the types.  Since in most cases only one type was
specified, this means that ARC will normally just look in the one table
that it read in.  Which is what you would expect.  So the response is
usually just a number, e.g.

\begin{verbatim}
   3850
\end{verbatim}

ARC will then tell you what line it assumes you mean -

\begin{verbatim}
   Wavelength is 3350.924 OK? [YES]
\end{verbatim}

If you reply Y, YES, or just hit the return key, ARC will use that
wavelength.  If you reply NO, or N, it will ask you for the wavelength
again.  If you just hit return in response to the wavelength prompt, the
line will be deleted and you will back with the cursor looking for another
line.


\goodbreak
\vspace{12pt}
{\it Subtleties --- this bit is worth reading!}

There are some important alternative ways of specifying the
wavelength of the line.

\begin{enumerate}
\item If you know the wavelength exactly, but it is not in the line
   list, you can specify the type as `EXACT'.  So if your response to
   the wavelength prompt is -

   \begin{verbatim}
   3456.789 e
   \end{verbatim}

   `e' being short for `EXACT' -- ARC will use that value as the wavelength.
   If you have specified 'NONE' as the arc type, ARC will assume that 
   all wavelengths given are exact values.
\item Once two lines have been identified, ARC is able to estimate the
   wavelength of a new line by linear interpolation.  It will then tell
   you this interpolated wavelength as well as the channel number.  If
   more than two lines have been identified, the interpolation is based
   on the two nearest to the new line.  You can now use `I' in your
   response instead of specifying a wavelength -- the result is the same
   as if you had typed in the interpolated wavelength.  ARC will then
   look for the listed wavelength nearest to the interpolated value.
\item Similarly, once more than two lines have been identified, ARC is
   able to start performing a running fit to the identified lines.  The
   fit is recalculated each time a new line is identified and the RMS of
   the fit is displayed.  So a bad identification will usually show up
   immediately as a large rise in the RMS figure.  ARC can now display
   the fitted wavelength as well as the interpolated wavelength when a
   new line is selected, and you can use `F' for the fitted wavelength
   in the same way as you can use `I'.
\end{enumerate}

What this means in practice, is that with a good line list, you can
identify enough lines to tie down the fit fairly well (it helps to do some
lines at one end, then at the other, then work in), and then just respond
`F' to each wavelength prompt.  ARC will then use the line in the line list
closest to the fitted wavelength.  This simplifies the process considerably,
but it is still under your control, and you can intervene if a new fit
shows a considerable increase in RMS.  


\goodbreak
\vspace{12pt}
{\it Moving about the spectrum and other commands.}

When you are asked to indicate a line using the cursor, you can
hit -- instead of the space bar, which just selects a line -- any one of
a number of keys that have specific functions.  

For example, `N' moves you on to the Next portion of the arc.
`B' moves you Back to the previous section.  `M' Moves you to a 
specific X value.  (Note that the value is not always in channels -- if
the spectrum already has wavelength data associated with it, the X
values will be in Angstroms.  This is sometimes a useful feature, sometimes 
an irritating one.)

This is how you move around from one section to another.  `L' lets
you change the Length of the portion displayed at any one time.

`D' deletes the identified line nearest the cursor.  If you see
the RMS for the fit shoot up, hitting `D' will delete the last line
identified -- so long as you haven't moved the cursor since you selected
it.

An important command is `Q', which Quits the identification process
and moves on to the fitting and line editing stage. Note that this
does not commit you to anything -- you can always come back to the
interactive selection stage.

If the normal line centering algorithm cannot find a line, you can
use `C' to force a line identification where the line center is determined
by a center-of-gravity analysis.  This is a time-consuming operation, and
is not really recommended for normal use.

Strongly {\it not} recommended is the use of `E', which allows you to
indicate the line center yourself on an Expanded plot.  This command
does nothing clever at all -- it just takes the position you indicate.

Like most Figaro programs that involve key-selected options, ARC
will respond to `H' or `?' by printing a list of the options it
accepts.


\goodbreak
\vspace{12pt}
{\it Fitting and Editing --- The Menu Stage.}

The `Q' command takes you to a more conventional stage where the
fit is repeated and the results displayed.  As soon as you hit `Q' and
confirm that you indeed want to move to this next stage, a fit is performed
to the lines identified so far and the results are displayed.  For each line,
the calculated and actual wavelengths are given, together with the discrepancy
in Angstroms, and the RMS for the fit that would be obtained if that line
were deleted from the list of lines.  Finally, the RMS for the current fit
is displayed.  Remember that the RMS is in terms of Angstroms.

If the fit is bad, a look at
the table of residuals output may help to indicate which identifications
are at fault.   The `RMS without this line' is a particularly useful figure
when there are relatively few lines in the fit; it saves you the effort of
making a tentative deletion and a refit to see if the result is really an
improvement.  With a large number of lines, the RMS is less dependent on any
one line, and this figure becomes less useful.

You are given a number of options at this point, all of which are
listed in a single prompt.  This is described as the `menu' prompt.  The
options are as follows -

\begin{itemize}
\item Fit -- Repeat the fit.  This will show you the effects of any
   changes you have made, either by editing, or by use of the automatic
   search facility.
\item Disp -- Display the deviation of the fit from a linear fit.  This
   shows the trend of the fit, and the errors in each line, and is a
   particularly valuable diagnostic.  If your graphics device supports
   colour, those lines fitted automatically are shown in a different
   colour to those selected manually, and this can help to show if the
   automatic lines are really helping.
\item Order -- Change the order of the fit.  A new fit is performed
   immediately.
\item Edit -- Delete or change the wavelength of one or more of the
   selected lines, without returning to the cursor selection.  You have
   to specify the line(s) by line number, and are then prompted for the
   wavelength to use.  A null wavelength will delete the line (and to
   get it back you will have to return to cursor selection).
\item Reselect -- Return to selection using the cursor.
\item Print -- Prints a copy of the fit (what ARLINES.LIS would look
   like if you were to exit now).  Note that this is not quite the same
   as the display you get by doing a fit -- it does not include the `RMS
   if omitted' figures, for example.  Eventually, it will be, but it was
   easier the way it is.
\item Auto -- Starting from your current fit and arc line list, ARC
   looks for additional lines in the arc at wavelengths given in the
   line list and adds any it finds to the identified line tables.
\item Xauto -- Deletes all the lines found by `Auto'
\item Modify -- Explains the function of the Autofit parameters and
   allows you to change them.
\item Quit -- Start to exit ARC.  See the section called `On the way out
   of ARC' for a description of what happens next, which is not very
   much.
\item Help -- (or ?) Display a summary of this information.
\end{itemize}

The first letter of each command is sufficient.


\goodbreak
\vspace{12pt}
{\it The Automatic Line-Finding Facility}

When you select the `Auto' option in the menu, ARC tries to use the
fit you have obtained already as a starting point and
tries to find lines in the arc that match lines in the tables.
The algorithm used is very simple, and is based on the principle that the
automatic fit should not add lines that will make the fit significantly 
worse than do the lines you have already got -- and are presumably happy
with.  There are only two parameters (at present) involved and, really,
only one is important.

ARC takes each pixel in the spectrum in turn.  If that pixel is 
more than CHFACT times the current sigma value from any line already found, 
it uses  that pixel as the starting point for a line search.  This is exactly
as if you had selected that pixel with the cursor during the interactive
part of the process.  If anything resembling a line can be found, it 
calculates its wavelength and looks in the line tables for a line 
close to that wavelength.

A line is accepted if the discrepancy between its calculated and
tabulated wavelength is less than SIGFACT times the current RMS
value.  It is this that means that the criterion for accepting new lines
is based on how their wavelength discrepancies compare with those for
the lines that have already been accepted.  SIGFACT is the more important 
parameter of the two.  The default value of 3.0 means that the automatic
search can make the overall RMS of the fit somewhat worse, but it will
give the program a fair go at finding some lines.  Setting SIGFACT to 1 or
less, which you may do with the `Modify' menu option, ensures that the
automatic search will not make the fit worse, but it will probably not find
many lines either.

Just how best to use the automatic line finder is a matter of
experience and, probably, opinion.  At the moment, it is a relatively new
feature and so the experience is lacking, even if the opinion is not.
It does, however, seem fair to say that the better the original fit, the
more likely the automatic fit is to make correct rather than misleading
identifications.  The `Xauto' menu option, which causes all automatic line
identifications to be deleted, at least means that you can experiment a little
without doing irreparable damage to your fit.  

One approach is to let the
automatic fit loose, and then tidy up after it, deleting those lines that it
found but that you don't like the look of.  You can do this from the
line list produced by the `Fit' menu option, using the `Edit' option to 
delete the lines affecting the fit the most.  Lines found automatically
are flagged in the list by a plus sign (and are shown in ARLINES.LIS with
a (A) symbol).

An alternative, and quite a
simple operation, is to return to the interactive selection (the `Reselect'
menu option) and examine the lines found.  The lines found by the Auto
option are displayed with their wavelengths in parentheses, so it is fairly
straightforward to run through the spectrum with the cursor, hitting `D'
at every bracketed line that looks like nothing more than a noise bump.


\goodbreak
\vspace{12pt}
{\it On the way out of ARC}

If you do not return to the interactive stage, you will be asked if
you want to make use of this fit.  If you reply `YES', ARC will generate
a set of wavelength values for the spectrum and either create a new output
spectrum with these values in the X data array, or simply put these values
in the X array of the current arc spectrum.  See the section on Wavelengths
(under Techniques) for details of how you use these values.  If you are going
on to IARC, you do not need to do this.  If you are going on to SCRUNCH,
you do need a spectrum with an array of X wavelengths.

Finally, you have the option of producing a hard plot showing the
identifications you have made.  Generally, the resolution of a Printronix
is such that this plot is not much use -- it comes into its own with a
higher resolution device such as a Versatek.  You can also produce a hard
copy of the dispersion curve -- the plot produced by the `Disp' menu option.
Note that if you produce both plots, they will be produced as separate
files, both of which will have to be output to the hard copy device.


\goodbreak
\vspace{12pt}
{\it Working with shifted spectra}

If you identify one arc, and then move on to another arc which is
essentially the same except for a linear shift, you can make use of
the cross-correlation option (the XCORR keyword described earlier), or
you can try the following sequence, which gives you a little more
control over the way the shift is determined:

\begin{enumerate}
\item Reply YES to the `Use results of previous fit?' prompt.
\item Reply NO to the `Determine shift by cross-correlation?' prompt.
\item The displays will now show the new arc, but with identifications
   that are offset by the amount of the shift.  The wavelengths will not
   match the lines properly.
\item Select a line that was identified in the previous fit.  When
   prompted for the wavelength, respond with the correct wavelength for
   the line.
\item Since this wavelength is that of a line already in the list of
   identified lines, ARC will not accept it.  It will ask if you want to
   delete the previous identification.  You do not, since that
   identification was correct.  It will ask if you want to delete your
   new identification.  You do not, since it is correct.  Answer NO to
   both questions.
\item This leaves ARC with only one alternative.  If both are correct,
   the arc must be shifted.  So it asks if it is to re-identify the
   lines, assuming the apparent shift.  If you reply YES, it effectively
   takes all the lines in its list, shifts them over and repeats the
   centering analysis using the shifted position as the starting point.
   End of arc identification, in theory.
\item In practice, especially if the shift is not perfectly linear, some
   lines may be missed.  These are logged, so you can see if this is
   happening.  One trick is to increase the sigma (use the `S' command)
   before forcing the re-identification.  This gives the algorithm a
   little more scope.  You can then reduce the sigma and force another
   re-fit, by again reselecting a known line.  (This time the
   re-analysis will be assuming a shift of zero.)  This trick is
   effectively that used by IARC when working on a set of spectra.
\end{enumerate}


\subsection{Abline --- A Figaro Program for Absorption Line Analysis}

{\it This Section is the documentation by J.G.\ Robertson, dated 2 February
1987 as included as on-line document in Figaro 3.0.}


\vspace{12pt}
{\it 1. Introduction}

ABLINE is an interactive program which runs within the FIGARO data 
reduction system, and whose main purpose is to find wavelengths and 
equivalent widths of absorption lines. The facility for fitting a 
polynomial to the continuum may also be of use in other situations. As well 
as wavelengths and equivalent widths the program estimates width 
and asymmetry parameters for each line. 

The program does {\it not} fit Gaussian profiles to lines; in fact this 
approach is specifically avoided, since it results in model-dependent 
wavelength and equivalent width values. In practice one has no reason to 
think lines are even approximately Gaussian, e.g.\ the Voigt profiles of 
saturated or damped lines, or the multiple component lines typical of QSOs.

The wavelength estimator used is the median (i.e.\ the value that equally 
splits the area of the line). Not only is this more resistant to the 
effects of low level wings on the line than is the centroid (mean; centre of 
gravity) but also it can be shown that the median is actually a 
statistically better (less noisy) estimator than the centroid for locating 
absorption features.  For further discussion of the estimators used for all
the parameters see Publ. Astron. Soc. Pacific vol 98, 1220, (1986).

The program can also be used on emission lines, and will find median 
wavelength, equivalent width, line width and asymmetry.

This version of the program does not calculate uncertainties of the 
wavelength or equivalent width. I have another version which does, but one 
has to go through a long preparatory reduction to obtain a spectral scan 
giving the variance as a function of wavelength before using that version.


\goodbreak
\vspace{12pt}
{\it 2. Running ABLINE}

Before starting ABLINE, make sure the soft and hard plot devices are 
appropriately specified, using the Figaro commands SOFT and HARD.

When you are in FIGARO, just type ABLINE to run the program. Only
interactive use (not batch) is feasible.
ABLINE reads data from one dimensional spectra only (in 
FIGARO format). There are 3 options for handling the 
continuum fit in the vicinity of the required line:

\begin{enumerate}
\item Make a continuum fit, use it for analysis of one or more lines,
   but do not save the continuum fit.
\item Make a continuum fit, use it as above and save the fit to the
   continuum as a separate spectrum.
\item Read in continuum fit as written in (2) and use it instead of
   fitting it again. This mode also allows you to use a continuum fit
   you have constructed in some other way (check the .X structures
   match!).
\end{enumerate}

ABLINE needs a graphics terminal for soft plots and
cursor input (as specified by FIGARO parameter SOFT). It will be useful to have
a hard copy plot of the spectrum on hand before entering ABLINE, so that you
can see the approximate wavelengths of lines to be analysed, and reasonable
values for the wavelength range over which to fit the continuum. ABLINE assumes
that the .AXIS(1).DATA\_ARRAY
structure of the input spectrum gives the wavelength in
Angstroms, although other units (e.g.\ velocity) will work if one suitably
reinterprets the wavelength, equivalent width and line width outputs. 
The .AXIS(1).DATA\_ARRAY
structure must be linear (i.e.\ scrunched) because this is 
assumed in
the calculations. Conversion of the .X structure to vacuum heliocentric or LSR
can be made using VACHEL (and SCRUNCH) if necessary, before using ABLINE. For a
noisy spectrum it may be useful to lightly smooth the data used as input to
ABLINE (this does not bias the wavelength or equivalent width; it obviously
does affect the width parameter and to a lesser extent the asymmetry). 

ABLINE has a number of parameters.  These may be set in the command line,
since they are ordinary Figaro parameters and keywords, but since ABLINE is
inherently an interactive program it is expected that most users will just
type `ABLINE' and allow themselves to be prompted by the program.  The prompts
in this case are as follows -

`(SPectrum) Spectrum with lines to be fitted'. FIGARO spectrum to be 
analysed. e.g.\ QSPEC

`(OLDcont) Use precomputed continuum?' Answer `Y' if you
already have a continuum fit covering the wavelength region of the 
(next) line to be analysed, i.e.\ case (3) above. If you answer `N' a 
continuum will be fitted, i.e.\ case (1) or (2) above.

If the continuum is to be fitted in this run, the system next asks:

`(SIG) Multiple of sigma for continuum point rejection?'  Points in the 
designated
wavelength range(s) for continuum will be iteratively rejected if they are
further from the fitted curve than this number of standard deviations. 2.25
seems to work quite well. If the value is too low it may reject valid noise
features. 

`(ITN) Number of iterations for continuum point rejection'. About 4 seems 
adequate.

`(DEG) Degree of polynomial for continuum fit'. The acceptable values 
are 0 - 7. Use
the lowest degree which will be able to fit the believable trend of the
continuum over the range used. Usually 1-3 is plenty. The higher degrees are
intended for cases such as trying to fit a ``continuum'' which includes a broad
emission line. 

Alternatively, if you elect to read in a pre-computed continuum, the program
prompts 

`(COntin) File containing precomputed continuum'.  This should be a normal
Figaro 1-D spectrum, of the same dimensions as the spectrum to be analysed.

At this stage the program examines the .X structure of the input 
spectrum, and displays its label and units for the user to check (e.g.\ 
they will usually be `Wavelength' and `Angstroms'). The first and last 
values of the .X structure are also displayed for checking.

The next parameter requested from the user controls how the wavelength 
bounds of the absorption line will be found:

`(LImit) Set line limits at indicated points?'

YES is the more usual answer.  It means that you will define the 
bounds of each absorption line using the cursor to indicate the last 
channel (i.e.\ furthest from the line centre) on both sides which you want 
to be included in the line. It is not possible to split channels; the 
entire channel indicated by the cursor will be included. If you opt for `N' 
then the last channel included on each side will be the first whose data 
value equals or exceeds the continuum value. You still have to use the 
cursor to indicate approximate lower and upper bounds of the line: be 
generous, i.e.\ go beyond the channels where you can see the data exceeds 
the continuum. If there is no channel whose data exceeds the continuum 
before the cursor specified position is reached, the latter will be used. 
For emission lines only the `Y' option will work here.

The next parameter requested is 

`(WIdth) Wavelength range to display at one time'.

This is the wavelength range which will be plotted on the soft plot.  If you
are going to do a continuum fit to this region, a fairly large range (i.e.\
going well beyond the desired line edges on both sides) is desirable, so that
the continuum can be well tied down.  (The continuum fit cannot extend beyond
the range selected when the fit is done.)  A range of about 200 - 500 channels
(specified as the appropriate value in Angstroms) is usually satisfactory. 
When you come
to delimit the line itself with the cursor (see below), you can elect to
`zoom' this plot by specifying a smaller wavelength range (Width). It is not
acceptable for this width to be larger than the entire spectrum length, nor
can it exceed an inbuilt limit of 2048 channels. 

You are now asked whether you want to save the continuum fit(s) to a separate
spectrum, and if so, what the name of this spectrum should be. Normally one
does not want to use the continuum after the present ABLINE run; in this case
answer NO.

You now reach the loop in which individual lines (all from the same input 
spectrum) are analysed. A HELP facility explains the available commands.
The program prompts with

\begin{verbatim}
   123.4,Sig,Deg,Itn,(No)Limit,Width,Cont,
      Fit,Recont,Quit,Help
\end{verbatim}

The most usual response is

Floating point number: This is the approximate wavelength of the next 
line to be analysed. After a further question to find the name of this line 
(which will label the hard and soft plots) a soft plot will be displayed. 
Its wavelength range will be as specified above for `wavelength range...'
and it will be centred at the wavelength specified here, unless this is 
too close to one end of the spectrum, in which case the plot will extend 
just to the end of the data. Further progress from this point is discussed 
below.

Other responses enable you to change the way the program fits the 
continuum or analyses the line. It is useful to be able to change these 
parameters here because different lines may require different treatment, 
and we do not want to have to reenter the program to specify these. The 
other possible responses are (where xxx is a floating point number and nnn 
is an integer):

\begin{itemize}
\item Sig xxx: Set `multiple of sigma for continuum point rejection' to
   xxx.
\item Deg nnn: Set `degree of polynomial fit to continuum' to nnn.
\item Itn nnn: Set `number of iterations for continuum point rejection'
   to nnn.
\item Wid xxx: Set `wavelength range to display at one time' to xxx.
\item Lim: To cutoff integrations for equivalent width and wavelength at
   positions designated by cursor.
\item Nolim: To cutoff at first channel on each side which equals or
   exceeds continuum
\item Cont: Initiate continuum subsegment selection and fitting (see
   below)
\item Fit: Initiate cursor selection of line range and parameter
   calculation
\item Recont: Recompute continuum from same selection of subsegments as
   previous fit; e.g.\ after alteration of degree of polynomial
\item Quit: Exit ABLINE
\item Help: Display explanation of the available commands
\end{itemize}

The changes made will remain 
in force for all subsequent lines analysed unless they are changed again.

At this stage in analysing a line you have a soft plot of the selected 
region. You now have to use the cursor to specify the exact channel ranges 
to be used for fitting the continuum (unless a precomputed continuum fit is 
being used). It is clearly necessary that the continuum fit does not 
include the absorption line itself; there may also be other absorption 
lines or defects nearby which you want to explicitly exclude from 
influencing the continuum fit. This is handled by allowing you to specify 
up to 10 wavelength subsegments (within the plotted range) which will be 
used as input to the continuum fit. 
Type CONT and in answer to the rather extensive 
prompt `Selection of subsegments for continuum fitting ....' Do the 
following:

\begin{enumerate}
\item Move the cursor to the lowest wavelength to be used in the
   continuum fit, and press any key except Q, to read the cursor
   position.
\item Move the cursor up in wavelength to the end of the first
   subsegment you want to include (e.g.\ just before an absorption line)
   and press any key except Q [unless this is the only subsegment, in
   which case press Q].
\item Repeat for up to 10 subsegments, specifying both lower and upper
   boundaries. Use Q to terminate subsegment input.  Naturally you
   should choose subsegments to span both sides of the line to be
   fitted, so that the continuum is well tied down.
\item Notes: If you specify a start position for a subsegment lower in
   wavelength than the end position of the previous one, it will be
   simply truncated (and a message output). However if any subsegment
   has an end wavelength less than its start wavelength, or if any
   subsegment totally encloses another, you will have to start the whole
   process again.
\end{enumerate}

Having finished subsegment selection, a polynomial fit is made to the 
specified continuum channels. Channels whose data values deviate from the 
fitted curve by more than the `multiple of sigma for continuum point 
rejection' are rejected and the fit recomputed. This process is repeated as 
specified by `number of iterations for continuum point rejection'. For the 
initial fit and each subsequent iteration the terminal screen displays 
information about the fit. The right hand two columns give the number of 
channels included in the fit on each iteration, and the number rejected as 
being too far from the fitted curve. This number usually stabilises after a 
few iterations, i.e.\ the process converges to a limit and further 
iterations result in no further change. The first 8 columns of the display 
are the rms residuals of the (remaining) data points relative to 
polynomial fits of degrees 0 to 7. This full table is produced no matter what 
degree of polynomial fit you have selected. It enables you to see how low 
the degree of the polynomial can be without obtaining a noticeably worse 
fit to the data.

The continuum fit is drawn on the soft plot; if you have elected to write 
the continuum to an output FIGARO file, the same data is written into the 
appropriate wavelength range of that file. It will overwrite any previous 
fit in this same ABLINE run which included the same wavelength range; i.e.\ 
all continuum fits in the one run go into the same output continuum file.

The next step is to use the cursor to specify the wavelength (channel) 
limits of the line. How this is done depends on the response you gave to 
the Limit parameter.
The usual case is LIMIT = YES: type FIT and centre the 
cursor on the last channel you want included, on each side of the line. Do 
the lower wavelength edge first.

The limits of the line are then drawn on the soft plot as vertical lines; 
for clarity they are drawn at the outer edges of the last channels 
included.

If the net area of the selected line is above the continuum rather than 
below it, the program reports that this is an emission line.


\goodbreak
\vspace{12pt}
{\it 3. Line Parameter Estimation}

The program now has both the continuum and line specified, so it 
calculates and displays the quantities describing the line (for further
details see Publ. Astron. Soc. Pacific Vol 98, 1220 (1986)):

\begin{enumerate}
\item The median wavelength, as location parameter for the line in the
   same units as the .X structure of the input spectrum.
\item Equivalent width, in the same units (usually Angstroms). This is
   calculated using the data as normalised by the continuum, which is
   the best if the continuum is sloping significantly over the line
   width: i.e.

   $$EW = \sum_k \left( 1 - {F(k)\over A(k)} \right) {{\rm \AA}/ch}$$
   %.i+5;EW = Sum over K of (1 - F(K)/A(K)) * A/ch

   where $F(k)$ is the data value in channel $k$ and $A(k)$ is the value
   of the fitted continuum there. The sum is over the specified channel
   range. ${\rm \AA}/ch$ indicates the dispersion in Angstroms per
   channel.
\item The line width parameter is a measure of the width of the line (in
   Angstroms, or whatever is in the input .X) and is calculated as
   follows.  Let X(M) be the median wavelength, i.e.\

   $$\sum_k^{x(m)} \left( 1 - {F(k)\over A(k)} \right) = {0.5 EW \over {\rm \AA}/ch}$$
   %.i +5;Sum over K [to X(M)] of (1 - F(K)/A(K)) = 0.5 EW / A/ch

   Define $x(l)$ and $x(h)$ such that 
   $$\sum_k^{x(l)} \left( 1 - {F(k)\over A(k)} \right)
      = {0.1587 EW \over {\rm \AA}/ch}$$
   %.i +5;Sum over K [to X(L)] of (1 - F(K)/A(K)) = 0.1587 EW / A/ch

   $$\sum_k^{x(h)} \left( 1 - {F(k)\over A(k)} \right)
      = {0.8413 EW \over {\rm \AA}/ch}$$
   %.i +5;Sum over K [to X(H)] of (1 - F(K)/A(K)) = 0.8413 EW / A/ch

   (For $x(m), x(l), x(h)$ linear interpolation is used to find the
   fractional part of the ``channel'' satisfying the desired equation.)

   The width parameter is

   $$Line\_width\_parameter = 1.1775 (x(h)-x(l))$$

   For a line of Gaussian profile this gives exactly the FWHM, since
   $x(l)$ and $x(h)$ are at $\pm 1$ standard deviation. For any
   reasonable centrally concentrated profile it gives a result very
   close to the FWHM. This definition is an attempt to obtain a
   parameter that is useful for a variety of line profiles. Note that
   the width parameter is considerably influenced by the cursor
   placement of the line boundaries, if these are at places where $F(k)$
   is not close to $A(k)$.  This is expected, and means that the width
   of the line as designated is partially under manual control. The line
   width parameter as defined above is less affected by low level wings
   far from the line centre than is the calculation of the width from
   the variance of the data about the mean, i.e.\ it refers more to the
   width of the core of the line.
\item The relative displacements of $x(l)$ and $x(h)$ from the median
   $x(m)$ carry some information about the asymmetry of the line
   profile, so an asymmetry parameter is calculated as follows:

   $$asymmetry = 100 {(x(h)+x(l)) 0.5 - x(m)\over
      Line\_width\_parameter}$$
   %asymmetry = 100 ((X(H) + X(L))0.5 - X(M)) / Line width parameter

   The result is a dimensionless parameter (in percent), normalised by
   the line width. As an extreme case, a triangular line with a vertical
   low wavelength side has asymmetry = 8.1; if the high wavelength side
   is vertical, asymmetry = $-8.1$. Higher values may result from deep
   narrow lines with prominent (noise) wings on one side. In general one
   expects the asymmetry parameter to be strongly affected by noise, and
   for many lines of low to moderate signal to noise it will be
   meaningless. Even the width parameter is rather noisy and should be
   interpreted carefully.
\end{enumerate}


\goodbreak
\vspace{12pt}
{\it 4. Conclusion}

At this stage there is a short delay while the hardcopy plot is written to 
the designated file (see above for information about how to plot this after 
the run). The hardcopy gives a plot of the same wavelength range as 
selected and plotted on the soft plot; it shows the input data spectrum, 
the continuum fit, and two vertical lines bracketing the channel range 
selected for the line itself. A printed description on the same page as the 
plot lists all 4 parameters from analysis of the line (wavelength, 
equivalent width etc) and also some of the details regarding the origin of 
the data and the type of continuum fit.

The program then loops back and asks for another centre wavelength for 
analysis. Give QUIT to terminate if there are no other lines to analyse.


\subsection{Gauss --- A Figaro Program for Interactive Gaussian Fitting}

{\it This Section is the documentation by Jeremy Walsh as included as on-line
document in Figaro 3.0.}

\vspace{12pt}
{\it Introduction}

GAUSS is a Figaro program for interactive fitting of Gaussians to an emission
or absorption line spectrum. The program is suitable for handling both low 
resolution data, where line fluxes of many lines are required, or high 
resolution data where a multiple Gaussian fit to a single line profile is 
obtained. The line-free continuum is fitted by a polynomial of desired order
with iterative rejection of points greater than a given distance from the fit. 
The interactive line fit is optimized by minimizing the sum of squared
residuals
between the observed and fitted profiles. The results of the fit are written
to a data file. The fitting profile can be saved as a spectrum.

\goodbreak
\vspace{12pt}
{\it Running the program}

Before running GAUSS make sure that a soft and a hard plot device have been
allocated using the FIGARO commands SOFT and HARD. These are required for 
graphics (although use of the HARD device is not mandatory). The program 
will fail if SOFT has not been specified.

The program will prompt for the following:

\begin{itemize}
\item NAME OF SPECTRUM FILE\\
   Name of spectrum file to be fitted. Must be 1-d. If 
   no errors are available
   (.VARIANCE) then a message is written to the terminal to this effect and
   the goodness of fit will be in terms of rms only.
   If SOFT and HARD have not been set, then the program will terminate at this 
   point.
\item LABEL FOR PLOT\\
   Label written to top of soft and hard plot, and recorded in the 
   results data file.
\item ARE GAUSSIAN FIT RESULTS TO BE RECORDED ON A FILE\\
   If YES then name of file is prompted for. If NO results will be 
   written to terminal only.
\item NAME OF DATA FILE FOR RESULTS\\
   This is the name of the data file (.DAT) to 
   which results are written. If a file of this name already exists then 
   the results are simply appended to 
   this file. If not, then a new file of this name is created.
\item USE WHOLE OF SPECTRUM FOR LINE ANALYSIS\\
   If YES then all the spectrum is displayed. If NO the 
   XSTART and XEND of the
   region to be displayed is prompted for (cf. SPLOT).
\item SCALE SO ALL OF SPECTRUM FITS\\
   If YES then whole Y extent plotted. If not HIGH, LOW 
   and BIAS are prompted for (cf. SPLOT).
\end{itemize}

The spectrum is then plotted on the soft device in the lower box. If
errors are available (either through use of SQRTERR or for FIGS data),
then these are displayed in the upper box. The upper box is used for
residuals on line and continuum fit. The scalar for the residuals
indicates the ratio between the range of spectrum data to the range of
residuals data, i.e.\ smaller errors and residuals have a larger scalar
on the residuals box.

The continuum fitting menu next appears. The line or lines are demarcated from
the continuum using the cursor, the parameters of the polynomial fitting set,
and the fit performed. When satisfactory the fit and residuals are plotted
ready for Gaussian fitting.

The options are as follows:

\begin{itemize}
\item CUR: use the cursor to indicate the left and right edges of lines.
   There is no limit to the number of lines that can be so demarcated.
\item ORD: order of polynomial fit to continuum points.
\item SIG: factor times sigma on last continuum fit such that points
   whose deviation from the last fit exceed this value are not used for
   the subsequent fits.
\item ERR: factor times the individual error on a point such that a
   point whose deviation from the last fit exceeds this value is not
   used in subsequent fits (only usable if errors are available)
\item ITN: number of iterations performed for rejecting points with
   deviations exceeding the rejection value
\item FIT: apply the polynomial fit specified by ORD,ITN and SIG/ERR to
   the continuum.  (CUR must have been set for this to occur.) If ORD,
   SIG, ERR, or ITN have not been set then the default values are 3,
   2.0, 1.0, and 1 respectively.  For each iteration, the iteration
   number, the rms on the fit for each order (up to a maximum of 7), the
   number of rejected points and the number of fitted points are printed
   to the terminal (cf. ABLINE). In the case when errors are available,
   the mean value of the deviation as a factor times the actual error
   bar is printed out as a function of the order. The aim is to take the
   lowest order and minimum number of iterations so that the rms (or
   mean fractional error) and number of rejected points level out. The
   values of SIG/ERR, ORD and ITN can be altered until this condition is
   achieved. The continuum fit highest order, highest iteration number
   fit is plotted on the SOFT device from the left edge of the left
   section to the right edge of the right section. The residuals on the
   continuum fit appear in the residuals box over the extent of the
   fitted continuum (excluding the line(s)).
\item GAU: continuum fitting complete; move to GAUssian fitting.
\end{itemize}

The Gaussian fitting menu now appears. The line or lines are simulated by one
or more Gaussians.

The options are as follows:

\begin{itemize}
\item LIM: left and right edges of observed profile to be fitted are
   indicated by the cursor. If this option is omitted the profile edges
   are taken as the nearest edges of the continuum sections, and a
   warning to this effect is issued.
\item SIN: fit a single Gaussian to a profile. This is a quick way to
   fit a line by a single Gaussian involving minimum user interaction.
   With the cursor the peak position of the line is indicated. The peak
   height of this line is found and the half height points taken to get
   the line width. These initial values are then used to optimize the
   fit. If the fit is successful then the results - peak position, peak
   height, sigma (= FWHM/2.3540), flux in the line, equivalent width of
   the line and the rms deviation on the fit (mean deviation in terms of
   the error bars if errors are available) are written to the screen and
   to the data file if appropriate. See OPT below for details of what
   happens if the optimization fails.
\item NEW: introduce a Gaussian at a cursor defined position, of height
   the same as the data value and width 4 X channels.
\item NEX: introduce another Gaussian to a profile where one or more
   Gaussians have been fitted. A new Gaussian is introduced at the peak
   of the residuals (observed - fitted). This can have negative height
   (absorption line) in an emission profile, or vice versa, if residuals
   are small, so watch out.
\goodbreak
\item INCH: interactively alter the position, height or width of a
   Gaussian. The latest Gaussian introduced, or the one SELected (see
   below) is changed thus:
   \begin{itemize}
   \item[P] position
   \item[H] height; followed by a number to give additive modification 
   \item[W] width
   \item[S] stop the alteration
   \end{itemize}
   For example: P100 shifts a Gaussian right by the profile extent; H-20
   decreases the Gaussian height by 20\% of the peak height of the
   original line; W10 widens the line by 10\% of the total profile
   extent. At each change the old fit is erased and the new fit is
   redrawn.

   S stops the alteration and replots the whole display, including
   residuals, on the latest fit. The rms (or mean fractional deviation
   in terms of the error bars) on the fit is written to the terminal.
\item LIS: lists the fitting Gaussians in order of introduction, giving
   position, height, sigma and flux (proportional to height x FWHM).
\item SEL: set index number to that of the Gaussian whose parameters are
   to be modified. The list of fitting Gaussians is printed and
   selection made.
\item DEL: set index number to that of the Gaussian which is to be
   deleted.  The list of fitting Gaussians is printed and selection
   made. The display is redrawn and the new sum of fitting Gaussians and
   their residuals are plotted.
\item OPT: optimizes the fit. Any of the Gaussian parameters - position,
   height or width - can be constrained so that the value does not
   change during the optimization process. If such single constraints
   are required then the index number of the Gaussian and the parameter
   to be constrained are entered in response to the NPCON prompt. Sets
   of Gaussians can also be `chained' so that their values vary
   together: for example the separation of two Gaussians can be kept
   fixed when fitting a doublet with known separation. The index number
   of the Gaussians, the parameter to be chained and relation between
   the parameters is entered in reply to the ICHAIN, CHAIN and RCHAIN
   prompts. Different sets of parameters can be chained, e.g.\ in a
   three Gaussian fit the positions of Gaussians 1 and 2 could be
   chained and the relative heights of Gaussians 2 and 3.  However if a
   parameter of a Gaussian is already constrained it cannot be included
   in a chain.

   The form of weighting for the residuals are prompted. Three options are 
   available:
   \begin{itemize}
   \item[a)] no weighting (i.e.\ unit weights);
   \item[b)] weighting by signal value (without continuum subtraction);
   \item[c)] weighting by inverse square of errors (when available).
   \end{itemize}

   By employing weights more emphasis on points with either higher value
   or lower errors can help to constrain the fit. Method c is analogous
   to minimizing chi-squared. NAG routine E04JBF (comprehensive
   quasi-Newton algorithm for minimization) is used to minimize the
   weighted rms deviation on the observed - fitted profiles over the
   extent of the line.  If constraints or chaining are used then the
   iteration number and the weighted mean deviation (not the same as the
   rms) on the Gaussian fit are written to the terminal every third
   iteration. Success cannot be guaranteed with this routine. In
   particular if constraints or chaining are employed then the
   optimization can be more troublesome. Two sort of failures can occur:
   No minimum found. More evaluations required. This occurs either if
   the starting point was a long way from a minimum, or the fit is poor
   and a minimum with the given number of Gaussians could not be found.
   No minimum found. XTOL may be set too small. XTOL is the required
   accuracy on the value of the function. The default value is
   $3$~$10^{-3}$ (i.e.\ an accuracy of $\approx 0.3\%$), but this may be
   too low for a very high signal-to-noise profile or too high for noisy
   data. If failure occurs on the default value of XTOL, then the value
   is increased by a factor of two and a minimum searched for at each
   step; this process is repeated two more times. If no success is
   achieved after three attempts, the option is given to accept the
   manual fit or alter the interactive fit (back to Gaussian fitting
   menu). If after several attempts this failure recurs it is suggested
   to rerun the program setting TOL in the command line (hidden
   parameter).

   Usually a small change to the interactive fit will enable a minimum
   to be found if either of these failures occur. However don't expect
   miracles.  The algorithm only finds a local minimum. If you don't
   believe the fit, try another interactive fit and optimize again. The
   general rule is to try for a fit by a minimum number of Gaussians and
   to check that the deviations on the line fit are greater than or
   equal to the error bars, or the deviations on the continuum fit if
   errors not available.

   If the optimization is successful or the interactive fit is adopted,
   then position of peak, peak height, sigma and flux in each fitting
   Gaussian as well as the equivalent width on the whole profile and the
   simple (i.e.\ unweighted) rms deviation on the fit (mean deviation in
   terms of the error bars if errors are available) are written to the
   terminal and recorded on the data file (if applicable). On the SOFT
   display, the fitting Gaussians, their sum and the residuals on the
   fit are displayed

\item REC: recall a previous Gaussian fit to the profile. The name of
   the data file on which the fit was recorded is prompted for. The
   position, height and width of the recalled Gaussians, equivalent
   width and rms/mean error on the fit are written to the terminal and
   the individual Gaussians and their sum and residuals displayed. Any
   of the individual lines can then be altered by SEL and INCH. If any
   recalled line falls outside the line extent (which must have been set
   previously) a warning is issued and the line ignored.
\item HARD plot final fit on the specified HARD device. Produces a file
   which needs to be PLT'ed (for line printer) or PLF'ed (for the Laser
   printer).
\item SAV: save the continuum and Gaussian fit spectrum. The sum of the
   Gaussian fit on the continuum is saved as a spectrum file. The name
   of this file will be prompted for on quitting from the Gaussian
   fitting menu.
\item CON: move on to next section of the spectrum for more line and
   continuum fitting (returns to point where XSTART and XEND are
   specified)
\item QUIT: quit from program (fit finished).  If the fit is to be saved
   then the name of the output spectrum is prompted for:
   \begin{itemize}
   \item FITTED SPECTRUM TO BE SAVED\\
      Name of 1-d spectrum file to be produced. Note that the continuum
      on this spectrum only extends from the minimum to the maximum X values 
      delimited in the continuum fitting (CUR), but the range of the X values
      is the same as that of the input spectrum. 
   \end{itemize}
\end{itemize}


\goodbreak
\vspace{12pt}
{\it Useful Recipes}

a) Low spectral resolution spectra (e.g.\ line strengths for single 
emission lines)
%\small %1-col
\begin{verbatim}
  CUR   ORD   SIG/ERR  ITN   FIT   GAU
  LIM   SIN   CON
\end{verbatim}
%\normalsize %1-col
b) High spectral resolution spectra (e.g.\ radial velocity structure of 
a single profile)
%\small %1-col
\begin{verbatim}
  CUR   ORD   SIG/ERR  ITN   FIT   GAU
  LIM   NEW   INCH     NEX   INCH  NEX
  INCH   SEL   INCH   OPT   HARD   QUIT
\end{verbatim}
%\normalsize %1-col


\subsection{\'Echelle Reduction}

{\it This Section is the documentation ``UCL Echelle Spectrograph Data
Reduction Software'' by William Lupton, dated 19 September 1988, as included as
on-line document in Figaro 3.0. The appendices are not included here. The
command ECHTRACT announced in the 3.0 document has been available under the
disguise of ECHSELECT for a while.}


\goodbreak
\vspace{12pt}
{\it 1 Introduction}

This note consists of a simple description of the current state of the
\'echelle reduction software that I have been working on. Wherever
possible I have used the \'echelle reduction package kindly provided by
Jim McCarthy of Caltech but in several cases it has been necessary to
modify the programs or else to use different approaches.

The software is now nearly in a state where it, together with standard Figaro
applications, contains all the facilities necessary for the reduction of
\'echelle data. It is expected that all further development will be performed
by the \'echelle data reduction programmer who is to be employed by UCL to
produce a general purpose \'echelle data reduction package.

All of the programs described here are Figaro programs and are available
as part of Figaro 2.4, due for release at the end of September 1988.


\goodbreak
\vspace{12pt}
{\it 2 Overview}

The diagram illustrates the recommended processing steps starting
from raw CCD or IPCS data. It is followed by some explanatory notes.

% The big picture should normally appear here.
% Since it is very long, it must be moved somewhere where it does not
% affect vertical filling of pages too much.

Note that no special facilities are provided for bias subtraction, background
estimation, sky subtraction or flat-fielding. It is assumed that you will use
standard Figaro facilities for these operations.


\goodbreak
\vspace{12pt}
{\it 3 Worked Example}

Most of the rest of this note consists of an annotated example of how to
proceed from a raw \'echelle image to a wavelength calibrated single merged
spectrum.

Assume that you have just moved the spectrograph to a new configuration and
that you have taken images of a continuum source such as a bright, smooth
spectrum star (for tracking the \'echelle orders), of the Th-Ar lamp (for
wavelength calibration) and of your object. In many cases the object spectra
are sufficiently well exposed and free of discontinuities to allow direct order
tracking. In such cases there will be no need for a separate continuum
spectrum.

These files are in your current directory and are called:

\begin{verbatim}
    contraw.dst  -       continuum
    arcraw.dst   -       arc
    objraw.dst   -       object
\end{verbatim}

In what follows it will be useful to refer to the diagram in the overview
section.

% This picture takes up almost the whole length of a column.
% Its position in the text must be adjusted accordingly.
{
\setlength{\unitlength}{75mm}
\begin{picture}(1,2.84375)
\tiny

\put(0.25,2.78125){\oval(.25,.0625)}
\put(0.25,2.78125){\makebox(0,0){\shortstack{raw CCD}}}
\put(0.25,2.75){\line(0,-1){.03125}}
\put(0.125,2.65625){\framebox(.25,.0625){\shortstack{irot90}}}
\put(0.25,2.65625){\line(0,-1){.03125}}
\put(0.125,2.5625){\framebox(.25,.0625){\shortstack{irevy}}}
\put(0.25,2.5625){\vector(0,-1){.0625}}

\put(0.75,2.78125){\oval(.25,.0625)}
\put(0.75,2.78125){\makebox(0,0){\shortstack{raw IPCS}}}
\put(0.75,2.75){\vector(0,-1){.25}}

\put(0.25,2.5){\vector(1,0){.25}}
\put(0.75,2.5){\vector(-1,0){.25}}
\put(0.50,2.5){\vector(0,-1){.0625}}
\put(0.5,2.40625){\oval(.25,.0625)}
\put(0.5,2.40625){\makebox(0,0){\shortstack{oriented\\image}}}
\put(0.5,2.375){\vector(0,-1){.0625}}

\put(0.5,2.3125){\vector(-1,0){.4375}}
\put(0.0625,2.3125){\line(0,-1){.0625}}
\put(0.5,2.3125){\vector(1,0){.4375}}
\put(0.9375,2.3125){\line(0,-1){.0625}}
\put(0.5,2.3125){\line(0,-1){.0625}}
\put(0.0625,2.21875){\makebox(0,0){\shortstack{arcs and\\objects}}}
\put(0.5000,2.21875){\makebox(0,0){\shortstack{continua or\\bright stars}}}
\put(0.9375,2.21875){\makebox(0,0){\shortstack{arcs and\\objects}}}
\put(0.5,2.1875){\vector(0,-1){.0625}}
\put(0.5,2.125){\vector(-1,0){.25}}
\put(0.5,2.125){\vector(1,0){.25}}

\put(0.250,2.12500){\line(0,-1){.0625}}
\put(0.125,2.00000){\framebox(.25,.0625){\shortstack{image}}}
\put(0.250,2.00000){\line(0,-1){.03125}}
\put(0.125,1.90625){\framebox(.25,.0625){\shortstack{icur}}}
\put(0.250,1.90625){\line(0,-1){.03125}}
\put(0.250,1.84375){\oval(.25,.0625)}
\put(0.250,1.84375){\makebox(0,0){\shortstack{list of X,Y\\positions}}}
\put(0.250,1.81250){\line(0,-1){.03125}}
\put(0.125,1.71875){\framebox(.25,.0625){\shortstack{sdist}}}
\put(0.250,1.71875){\vector(0,-1){.0625}}

\put(0.750,2.12500){\vector(0,-1){.15625}}
\put(0.625,1.90625){\framebox(.25,.0625){\shortstack{echfind}}}
\put(0.750,1.90625){\vector(0,-1){.25}}

\put(0.25,1.65625){\vector(1,0){.25}}
\put(0.75,1.65625){\vector(-1,0){.25}}
\put(0.50,1.65625){\vector(0,-1){.0625}}
\put(0.50,1.56250){\oval(.25,.0625)}
\put(0.50,1.56250){\makebox(0,0){\shortstack{sdist.dat\\file}}}
\put(0.50,1.53125){\vector(0,-1){.0625}}
\put(0.50,1.46875){\vector(-1,0){.25}}
\put(0.50,1.46875){\vector(1,0){.25}}

\put(0.250,1.46875){\line(0,-1){.0625}}
\put(0.125,1.34375){\framebox(.25,.0625){\shortstack{cdist}}}
\put(0.0625,2.1875){\vector(0,-1){.8125}}
\put(0.0625,1.3750){\vector(1,0){.0625}}

\put(0.25,1.34375){\line(0,-1){.03125}}
\put(0.25,1.28125){\oval(.25,.0625)}
\put(0.25,1.28125){\makebox(0,0){\shortstack{straightened\\image}}}
\put(0.25,1.25){\line(0,-1){.03125}}
\put(0.125,1.15625){\framebox(.25,.0625){\shortstack{echselect}}}
\put(0.250,1.15625){\vector(0,-1){.0625}}

\put(0.750,1.46875){\line(0,-1){.0625}}
\put(0.625,1.34375){\framebox(.25,.0625){\shortstack{echmask}}}
\put(0.75,1.34375){\line(0,-1){.03125}}
\put(0.75,1.28125){\oval(.25,.0625)}
\put(0.75,1.28125){\makebox(0,0){\shortstack{mask\\image}}}

\put(0.75,1.25){\line(0,-1){.03125}}
\put(0.625,1.15625){\framebox(.25,.0625){\shortstack{maskext}}}
\put(0.9375,2.1875){\vector(0,-1){1.0}}
\put(0.9375,1.1875){\vector(-1,0){.0625}}
\put(0.750,1.15625){\vector(0,-1){.0625}}

\put(0.25,1.09375){\vector(1,0){.25}}
\put(0.75,1.09375){\vector(-1,0){.25}}
\put(0.50,1.09375){\vector(0,-1){.0625}}
\put(0.50,1.0){\oval(.25,.0625)}
\put(0.50,1.0){\makebox(0,0){\shortstack{collapsed\\echellogram}}}
\put(0.50,.96875){\vector(0,-1){.0625}}

\put(0.50,.90625){\vector(-1,0){.25}}
\put(0.50,.90625){\vector(1,0){.25}}
\put(0.25,.90625){\line(0,-1){.0625}}
\put(0.75,.90625){\line(0,-1){.0625}}
\put(0.25,.8125){\makebox(0,0){\shortstack{arcs}}}
\put(0.75,.8125){\makebox(0,0){\shortstack{objects}}}

\put(0.250,.78125){\line(0,-1){.03125}}
\put(0.125,.68750){\framebox(.25,.0625){\shortstack{echarc}}}
\put(0.250,.68750){\line(0,-1){.03125}}
\put(0.250,.62500){\oval(.25,.0625)}
\put(0.250,.62500){\makebox(0,0){\shortstack{wavelength\\calib'd arc}}}
\put(0.375,.62500){\vector(1,0){.25}}
\put(0.250,.59375){\vector(0,-1){.15625}}

\put(0.750,.78125){\vector(0,-1){.125}}
\put(0.625,.59375){\framebox(.25,.0625){\shortstack{xcopy}}}
\put(0.750,.59375){\line(0,-1){.03125}}
\put(0.750,.53125){\oval(.25,.0625)}
\put(0.750,.53125){\makebox(0,0){\shortstack{wavelength\\calib'd object}}}
\put(0.750,.50000){\vector(0,-1){.0625}}

\put(0.250,.4375){\vector(1,0){.25}}
\put(0.750,.4375){\vector(-1,0){.25}}
\put(0.500,.4375){\vector(0,-1){.0625}}
\put(0.375,.3125){\framebox(.25,.0625){\shortstack{scrunch}}}
\put(0.500,.3125){\line(0,-1){.03125}}
\put(0.500,.2500){\oval(.25,.0625)}
\put(0.500,.2500){\makebox(0,0){\shortstack{scrunched\\echellogram}}}
\put(0.5,.21875){\line(0,-1){.03125}}
\put(0.375,.125){\framebox(.25,.0625){\shortstack{echmerge}}}
\put(0.5,.125){\line(0,-1){.03125}}
\put(0.5,.0625){\oval(.25,.0625)}
\put(0.5,.0625){\makebox(0,0){\shortstack{scrunched\\spectrum}}}

%                              |                 |
%                           raw CCD           raw IPCS 
%                              |                 |
%                            IROT90              |
%                              |                 |
%                            IREVY               |
%                              |                 |
%                              +--------+--------+
%                                       |
%                +------<------ orientated image ----->------+
%                |                      |                    |
%      arcs and  |                      | continua or        | arcs and
%      objects   |                      | bright stars       | objects
%                |                      |                    |
%                |             +--------+--------+           |
%                |             |                 |           |
%                |           IMAGE               |           |
%                |             |                 |           |
%                |           ICUR             ECHFIND        |
%                |             |                 |           |
%                |   list of X,Y positions       |           |
%                |             |                 |           |
%                |           SDIST               |           |
%                |             |                 |           |
%                |             +-------+---------+           |
%                |                     |                     |
%                |               sdist.dat file              |
%                |                     |                     |
%                |             +-------+---------+           |
%       arcs and |             |                 |           |
%       objects  +-------->  CDIST            ECHMASK        |
%                              |                 |           |
%                      straightened image    mask image      |
%                              |                 |           | arcs and
%                          ECHSELECT          MASKEXT <------+ objects
%                              |                 |
%                              +--------+--------+
%                                       |
%                             collapsed echellogram 
%                                       |
%                               +-------+---------+
%                          arcs |                 | objects
%                               |                 |
%       arc line list -----> ECHARC               |
%                               |                 |
%                      waveleng calib'd arc --> XCOPY 
%                               |                 |
%                               |       waveleng calib'd object 
%                               |                 |
%                               +--------+--------+
%                                        |
%                                     SCRUNCH 
%                                        |
%                              scrunched echellogram 
%                                        |
%                                    ECHMERGE 
%                                        |
%                                scrunched spectrum 
%                                        |

\end{picture}
}


\goodbreak
\vspace{12pt}
{\it 3.1 Conventional Orientation}

The first thing to do is to ensure that your data is in the conventional
orientation with wavelength increasing from left to right and from bottom to
top of the image and thus with order number decreasing from bottom to top of
the image.

{
\setlength{\unitlength}{75mm}
\begin{picture}(1,0.8)
\tiny
\put(.10,.10){\framebox(.89,.59)}
\put(.10,.08){\makebox(0,0){1}}
\put(.98,.08){\makebox(0,0){NX}}
\put(.05,.68){\makebox(0,0){NY}}
\put(.07,.10){\makebox(0,0){1}}
\put(.25,.60){\vector(1,0){.25}}
\put(.25,.60){\vector(0,-1){.25}}
\put(.52,.60){\makebox(0,0)[l]{\shortstack{wavelength}}}
\put(.23,.33){\makebox(0,0)[tl]{\shortstack{order number}}}
%NY +--------------------------------------+
%   |                                      |
%   |                                      |
%   |          ^                           |
%   |          |  wavelength               |
%   |          |                           |
%   |          +---->                      |
%   |                                      |
%   |          |                           |
%   |          |  order number             |
%   |          v                           |
%   |                                      |
% 1 +--------------------------------------+
%   1                                     NX
\end{picture}
}

IPCS data will already be in this orientation but unfortunately CCD data
must be rotated and flipped. This is quite time-consuming but is at present
necessary for all CCD images. The following commands achieve this for the
continuum:

\begin{verbatim}
   % irot90 contraw contrawrot
   % irevy  contrawrot cont
\end{verbatim}

with analogous commands for the arc and the object. The {\it raw} files can
now be deleted if desired.

In what follows, assume that the IPCS raw data files CONTRAW.DST have been
renamed to CONT.DST etc.


\goodbreak
\vspace{12pt}
{\it 3.2 Order Location}

At present locating the orders is rather an interactive process (there is
a program ECHFIND that does it automatically but it does not yet work very
well in all cases and its use is not recommended).

First decide whether you are going to use the continuum source or the object to
locate the orders. It is a good idea first to do a YSTRACT / SPLOT of the data
to get a feel for the width, intensity and profiles of the orders. Sensible
commands to use for the GEC chip and the continuum are:

\begin{verbatim}
   % ystract cont 185 194 s
   % splot s reset accept
\end{verbatim}

Now display the image using IMAGE:

\begin{verbatim}
   % image cont high=hhhh reset accept
\end{verbatim}

The next stage is to use ICUR
to define a point somewhere near the peak of each
order that you wish to track and extract. You can track orders that are only
partially on the image if you wish to but this is not recommended, since it
could well affect the wavelength calibration, especially if only a small part
of the free spectral range is being covered. It is quite important to choose
points close to the peak intensity.

\begin{verbatim}
   % icur
\end{verbatim}

Now run SDIST to track the orders and fit polynomials to them:

\begin{verbatim}
   % sdist image=cont columns=8 trace=G \
      width=3 maxdeg=10 softd=no
\end{verbatim}

The two non-obvious parameters are TRACE and WIDTH. Specify ``Gaussian'' for
TRACE if the profiles across the orders are roughly Gaussian and are not cut
off by the dekker. Normally specify ``COG'' otherwise but if there is a
noticeable gradient along the profile you can try ``Edge'' (they are identical
in that both locate the rising and falling edges of the orders, but ``COG''
estimates
the centre by calculating the centre of gravity and ``Edge'' estimates
it simply by taking the mean of the edge positions). For WIDTH, specify an
estimate of the FWHM for ``Gaussian''
and an estimate of half the order width for
``COG'' and ``Edge''. If anything, underestimate
it for ``Gaussian'' and overestimate
it for ``COG'' and ``Edge''.

Beware that sky data can confuse SDIST because it gives rise to profiles that
don't fit any of the trace modes. If this appears to be a problem, use CLIP
(which sets all data values below a given low value to that low value and sets
all values above a given high value to that high value) to get rid of the
sky data values that are causing the problem.

There is another program that may be useful here if you have moved to a new
object that is not quite in the same place on the slit. OFFDIST operates on an
SDIST.DAT file and adjusts the constant terms so as to shift the tracked orders
up or down by a specified amount.


\goodbreak
\vspace{12pt}
{\it 3.3 Order Extraction}

You now have a choice. For quicklook extraction, you can create a mask image
whose data values indicate which order (if any) each pixel belongs to. This
mask is created by ECHMASK and applied by MASKEXT. ECHMASK allows separate
extraction of object and sky but requires the number of rows of object and sky
data to be independent of order number. This is not acceptable when (as is
usually the case) it is important to minimise the sky noise and to maximise the
signal.

For final data reduction or where the use of a mask is not acceptable the
orders can be straightened using CDIST (UCLES is extremely stable and
preliminary results indicate that many images can be co-added prior to
application of CDIST, so the cost in processing time should be acceptable).
Having straightened the orders,
ECHSELECT can be run to identify, for each
order, the rows to be used for the object and those to be used for sky.

The result in both cases is a ``collapsed echellogram'' where X is wavelength
and Y is order number. Your object image will of course give rise to both an
object and a sky echellogram.


\goodbreak
\vspace{12pt}
{\it 3.3.1 Quicklook Extraction Using a Mask}

As explained above, this method will not maximise signal to noise ratio, but it
will do a reasonably good job, especially where the orders in question are not
too far from being horizontally aligned on the detector.


\goodbreak
\vspace{12pt}
{\it 3.3.1.1 Mask Creation}

SDIST outputs an SDIST.DAT file that contains details of the orders that it has
tracked. This file is read by ECHMASK, which produces a mask image that can be
used for fast extraction of orders directly from (in the case of the CCD
IROT90'ed and IREVY'd) raw images.  ECHMASK can cope with the case where the
star / sky periscope is fitted and also allows you to specify the position of
object and sky data relative to the centre of the order as determined by the
tracking algorithm, but these details will be ignored here.

The normal straightforward behaviour is achieved by specifying PERISCOPE false
and giving zeroes for all widths and offsets. This causes a width derived from
the SDIST fit to be used; if the fit was Gaussian the derived width is just
twice the estimate that you gave to SDIST and if the fit was Edges it is the
actual calculated width (the same width is used for each order and the third
largest width of all the order widths is used so as to exclude atypical
values). If you know better, you can specify your own value for OBJWIDTH
-- overestimate rather than underestimate so as to prevent noticeable jumps
in the extracted data due to the slope and curvature of the orders.

The other thing that ECHMASK needs to know is the order numbers corresponding
to the orders that it has tracked. The value that you give for MSTART is
the order number corresponding to the first point that you selected with
ICUR. There should always be an order near to the image centre and this
order should always be that listed by:\footnote{
   This structure should not normally exist in a DST-format file, though
   it might.  There is no equivalent place in NDF format. (hme).}

\begin{verbatim}
   % exam cont.FITS.UCLES.ORDER_CEN
\end{verbatim}

but there is always the possibility of a $\pm 1$ error (although if the
setup routine that calculates \'echelle angle offsets was run, there
should be no error and the central order should be centred exactly in
the centre of the image).  Don't worry if you get it wrong - you can
always adjust the order number by using ICADD to add or subtract the
error from the mask structure. You have to add or subtract ten counts to
adjust by one order, e.g.\ if the MASK contains a data value of 420, this
refers to order 42 and adding ten to it to make it 430 causes it to
refer to order 43.

This is a typical run of ECHMASK. I hope to improve the help given to the
user with regard to choosing the correct order numbers since this is at present
rather tricky.

\begin{verbatim}
   % echmask cofile=sdist periscope=no \
      objwidth=0 objoffset=0 s1width=10 \
      s1offset=9 s2width=0 mstart=82 \
      mdelta=-1 mask=mask
    *** Will use an OBJWIDTH of  6 pixels
\end{verbatim}

When running with the periscope, each order is split up into two parts,
each of which looks rather like an order in its own right. If you are unsure
how they are grouped, display an arc and all will be revealed. When using
the periscope it is your responsibility to ensure that the first and second
points selected with ICUR correspond to the two parts of the same order
and similarly for the third and fourth points etc.


\goodbreak
\vspace{12pt}
{\it 3.3.1.2 Mask Extraction}

The resulting mask image can be used for fast extraction of orders from (in the
case of the CCD IROT90'ed and IREVY'd) raw images taken at the same
spectrograph configuration as it. This is done using the MASKEXT program.
MASKEXT needs to be told the range of order numbers that you want to extract
and this determines the Y size of the extracted file (referred to as a
collapsed echellogram) and its Y units.

The ``sub-order'' controls which bits of the
order are extracted into the output
image. A sub-order of 0 always extracts object and sky and sub-orders of 1 and
2 can be used to extract object and sky separately. If the periscope is fitted,
sub-order 2 corresponds to the first encountered part of the order and
sub-order 1 corresponds to the second encountered part of the order (so if the
tracked orders went from the bottom upwards the data values in the mask are
monotonically decreasing as you go from bottom to top). If the periscope is not
fitted, sub-order 1 always refers to object and sub-order 2 always refers to
sky. This is confusing and will probably change!

\begin{verbatim}
   % maskext image=arc mask=mask mlow=68 \
      mhigh=82 subord=0 output=arce
\end{verbatim}


\goodbreak
\vspace{12pt}
{\it 3.3.2 Accurate Extraction From Straightened Orders}

The trouble with curved orders is that when the object projects to only one or
two pixels on the detector, the bulk of the signal will sometimes fall into one
pixel and sometimes it will be split between several pixels. This means that
there is no single correct number of rows of data to extract, forcing the
extraction of unwanted sky as well as wanted signal.

Obviously it would be possible to provide a program that would make a sensible
decision about how many rows of data to extract at each point along the order,
and we intend investigating the use of some optimal weighted extraction scheme.
However, any such scheme needs accurate knowledge of the noise characteristics
of the data and it would take considerable effort to implement a reliable
automatic extraction algorithm.

Accordingly, the recommended approach for accurate extraction is first to use
the CDIST program to resample in the Y direction so as to straighten the
orders. Experience shows that this program does an excellent job, with little
discernable loss of resolution or variation of profile along the order. Once
the orders are straightened, the number of rows to extract for object and for
sky is merely a function of order number and not of wavelength.


\goodbreak
\vspace{12pt}
{\it 3.3.2.1 Order Straightening}

The CDIST program uses the SDIST.DAT file that was written by SDIST. It uses
the polynomial coefficients to resample in Y so as to straighten the
corresponding orders.

\begin{verbatim}
   % cdist image=arc ystart=1 yend=250 \
      output=arcc maxdegy=5
\end{verbatim}


\goodbreak
\vspace{12pt}
{\it 3.3.2.2 Order Extraction}

Having got an image with straight orders, conceptually one wants to take
a YSTRACT through somewhere near the centre of the orders, display with
SPLOT and then, for each order, somehow identify which rows are to be used
for object and for sky. Having done this, the relevant rows can be extracted
into a collapsed echellogram of the same format as that produced by MASKEXT.

ECHSELECT allows the user to indicate interactively the cross-sections of a
corrected echellogram to be used as object and sky for the various orders. It
then creates a collapsed echellogram for the object orders, and -- optionally
-- one for the sky orders.


\goodbreak
\vspace{12pt}
{\it 3.4 Wavelength Calibration}

We are now at the stage where arc and object have both had their orders
extracted into collapsed echellograms. These are 2D images with X still being
pixel number and Y being order number. The ECHARC program is used for
identifying lines in the arc. This writes wavelength calibration data as a 2D
ARCE.X.DATA array and the ARCE.X structure is then copied to the OBJE.X
structure using the XCOPY program.

ECHARC works by first of all doing the equivalent of the 1D ARC program
on a set of three or more orders that you nominate to be fitted interactively.
Then it enters an automatic mode where lines are identified in all the other
orders. It estimates wavelength in the other orders by fitting lines of
constant [(order number) * (wavelength)] between the interactively fitted
orders.

Experience with ECHARC has shown that it is vital to include the extreme
orders among those that are interactively fitted, that it may well be worth
using four rather than three interactively fitted orders, that a good line
list is absolutely vital and that if things start going wrong then they
will probably stay wrong (use of Ctrl-C is recommended in this case). Care with
the interactively fitted orders is usually rewarded.

The file ARLINES.ECH always contains details of identified lines and if
a fit fails a good policy is to restart using the results from the previous
fit and perhaps selecting slightly different interactively fitted orders,
e.g.\ add the orders that were worst in the automatic phase from the previous
run. All the lines previously identified in the orders that are to be
interactively fitted are still available so a re-run is not too time-consuming.

The DOWAVES keyword should always be false (if it is true then the wavelength
information is written to a separate file and we want it in the input file
for the purpose of scrunching). Also note that if the MONITOR keyword is
true then a graphical record of how the automatic mode is proceeding will
be output to the soft device.

Here is a typical example of ECHARC.

\begin{verbatim}
   % echarc image=arce arctype=thar \
      previous=no interactive=3 \
      orders=\[82,75,68\] orderfit=5 \
      sigma=3 dowaves=no
\end{verbatim}

After a successful arc fit, use XCOPY to copy the ARCE.X structure to the
OBJE.X structure.

\begin{verbatim}
   % xcopy spectrum=obje arc=arce \
      output=obje
\end{verbatim}

I plan investigating whether the arc line identification can be made a lot
easier by making use of all the instrument setup details that are held in
the data files. At least for the CCD it really should be possible to do
an almost completely automatic arc identification. For the IPCS it may be
harder.


\goodbreak
\vspace{12pt}
{\it 3.5 Scrunching}

The next step is to scrunch the wavelength calibrated OBJE and ARCE files.
Contrary to possible expectation the program used is SCRUNCH rather than
ISCRUNCH. Rather than using an ARLINES.IAR file, SCRUNCH uses the 2D .X.DATA
array produced by ECHARC as its source of wavelength information. You tell it
the wavelength range and the number of bins that you want to scrunch into and
whether to use a linear or logarithmic wavelength scale and it does the rest.

Since the \'echelle spectra are such that a pixel corresponds to a fixed
velocity interval irrespective of wavelength (e.g., 22microns =
2.73km/sec), a logarithmic wavelength scale will give bins that each
correspond to the same number of pixels, whereas a linear wavelength
scale will give bins at the red end that significantly oversample the
resolution element relative to bins at the blue end. However, a linear
scrunch will still probably be the favoured option in many cases.

It's probably worth using EXAM to look at the .X structure to find out the
minimum and maximum wavelengths before running SCRUNCH.

\begin{verbatim}
   % scrunch spectrum=obje log=no \
      wstart=4000 wend=0.02 bins=5000 \
      mean=no quad=yes output=objs
\end{verbatim}

At present every order will be scrunched into the full wavelength range
(4550 to 5000 Angstroms in the above example) and consequently most pixels
will be zero. This doesn't really matter but improvements are planned.


\goodbreak
\vspace{12pt}
{\it 3.6 Merging Orders}

The final stage is to merge the scrunched orders from possibly several images
into a single long spectrum. This is done using the ECHMERGE program. If the
data is from a region of the field where the full free spectral range is
obtained then adjacent orders will overlap and the contributions from the
overlapping orders are weighted with estimates of their inverse variances on
the assumption of purely Poisson noise. Where one contribution is below a given
fraction of the other then it is ignored completely. The variance estimates are
based on median filtered versions of the input orders and the median filter box
size is controlled by the BOX parameter.

The input images must all have the same X size, units and values and can be 1D
or 2D. The output image is always 1D and can be the same as either of the input
files, in which case no new image is created. The second input image can be
specified as blank in which case it is not required or used.

\begin{verbatim}
   % echmerge image=objs image1=\"\" \
      box=7 cutoff=4 output=objl
\end{verbatim}


\goodbreak
\vspace{12pt}
{\it 4 Summary of the Example}

Here there is just a reference list of the programs that were run in the
previous section.

\begin{verbatim}
 # conventional orientation (CCD)
   % irot90 contraw contrawrot
   % irevy  contrawrot cont
   % irot90 arcraw arcrawrot
   % irevy  arcrawrot arc
   % irot90 objraw objrawrot
   % irevy  objrawrot obj

 # review of counts, widths and shapes
   % ystract cont 185 194 s
   % splot s reset accept
   % image cont high=hhhh reset accept

 # select points on orders
 # track and fit orders
   % icur
   % sdist cont .

 # Either ...

    # create mask
      % echmask sdist . mask
    # extract orders using mask
      % maskext arc mask . arce
      % maskext obj mask . obje

 # Or ...

    # straighten orders
      % cdist arc . arcc
      % cdist obj . objc
    # extract orders
      % echselect arcc arce
      % echselect objc obje

 # End either.

 # identify arc lines
   % echarc arce .
 # copy wavelength info to object
   % xcopy obje arce obje

 # scrunch arc and object
   % scrunch arce . arcs
   % scrunch obje . objs

 # merge orders
   % echmerge arcs . arcl
   % echmerge objs . objl
\end{verbatim}


\section{Flavours of Figaro}
\label{flavours}

Figaro has quite some history behind it.  As a result there is a variety
of versions that you might come across or might have used before.  The
following list -- which is not complete --, indicates which versions exist
or existed on which platforms.  Important are the remarks on data
formats.

Old versions of Figaro did use a different data format -- referred to
here as DTA format -- which also was machine-dependent.

\begin{itemize}
\item {\bf Portable Figaro 3.2.}  This is the present version released
   for certain Unix platforms by Starlink.  It is the successor of
   Portable Figaro 3.1, which in turn is based on VAX Figaro 3.0-5 and
   (Starlink) National Figaro 3.0-6.

   The data formats supported are DST and NDF.  A conversion between
   these two is possible with applications in Starlink's Convert package
   (SUN/55).  Both are based on HDS version 4.  HDS version 3 can be
   read, but will be converted to version 4 automatically.  Conversion
   from HDS version 4 to version 3 is not possible.  Conversion from the
   older DTA formats to DST or NDF is not possible within Portable
   Figaro 3.2.

\item {\bf Standalone Portable Figaro 4.0.} This is being developed for
   Sun4 and DECstation by the Australian side of the Figaro Port Group
   (see Section \ref{intro} for names).  It is a development parallel to
   and in close connection with Portable Figaro 3.1/3.2.  The data
   formats -- in fact most of the release -- are identical.

\item {\bf Figaro 3.0.}  This is the predecessor of Portable Figaro 3.1
   and 3.2.  It is available only for VAX/VMS and is still the
   up-to-date release for VAX from Starlink.  The current version is
   Figaro 3.0-9 / National Figaro 3.0-9.

   Figaro 3.0 itself was released in 1991 by AAO.  Starlink used the
   concept of a ``national'' Figaro directory to release bug fixes etc.
   Thus came about National Figaro 3.0-1 through 3.0-9.  In 3.0-3
   Standard Figaro was modified as well, in order to migrate from HDS
   version 3 to version 4.  Versions 3.0-8 and 3.0-9 also modified Standard
   Figaro: The data access libraries, DSA and DTA, and the GKD library
   were adapted from Portable Figaro 3.1.

   Figaro 3.0 on VAX and Portable Figaro 3.2 are fully compatible.

   The data formats supported are DST and NDF.  A conversion between
   these two is possible with applications in Starlink's Convert package
   (SUN/55).  Both are based on HDS version 4.  HDS version 3 can be
   read, but will be converted to version 4 automatically.  Conversion
   from HDS version 4 to version 3 is not possible.  Conversion from the
   older (VAX)DTA format to DST or NDF is possible with the program
   {\tt dta2hds}, which is part of Figaro 3.0.

\item{\bf Sun Figaro 2.4.5.}  This version evolved at Caltech from VAX
   and Convex versions of Figaro 2.4.  Before ``patch 5'' this version
   used (Sun)DTA format.  In patch 5 it changed over to DST format based
   on HDS version 4.  Conversion from (Sun)DTA to DST is possible with
   the program {\tt dta2hds}, which is part of Sun Figaro 2.4.5 from
   patch 5 onwards.

   Patch 8 was the last before Figaro 2.4.5 left the Public Domain.
\end{itemize}

One point in favour of Portable Figaro 3.2 is that Starlink is
committed to supporting it.  Another is that with the NDF format you can
use other Starlink packages alongside it.  Kappa (SUN/95) is the Kernel
Applications Package and as such the main data reduction software in the
Starlink collection.  Several Kappa-like packages have recently become
available, namely CCDPack, Specdre, and Pongo.  Those packages support
bad values and error propagation to a higher degree than Figaro does.
They use the same data format (NDF), the same data access routines
(NDF-routines with a standard way of taking sections of data sets), and
the same parameter system.  Figaro 3.0 made the step to support the NDF
data format, though with its own data access routines. Portable Figaro
3.1 took the next step in using the same parameter system.

Users may encounter problems with NDF-format data that have been
created by Figaro 3.0 before 3.0-8. To Figaro 3.0-8 and to Portable
Figaro 3.1 they may seem unacceptable if they have a quality array. Such
NDFs can be ``repaired'' by merging the quality information into magic
values in the data array itself. For this purpose there is an
application called {\tt ndfbad} (version 3.0) or {\tt q2bad} (version
3.2).  The background is that Figaro 3.0 before 3.0-8 had a wrong model
of what and NDF is, so that it could flag an NDF as free of magic
values. With the correct model of an NDF a Figaro-created NDF can not
have such a flag. Thus it is assumed that there might be magic values
present.  But Figaro does not allow magic values and quality to be
present in the same data set.

This problem is slightly different from what might occur with NDF-format
data from non-Figaro software. Such software does allow magic values and
quality to be present simultaneously, while Figaro doesn't. Again it may
be necessary to apply {\tt ndfbad/q2bad}.


\section{Hints for Programmers}
\label{proghint}

{\it As opposed to previous versions of Figaro,
Portable Figaro 3.2 is not intended as a
programming environment. It is not recommended to write software that calls
the Figaro 3.2 object libraries. You should use the Starlink and ADAM
facilities instead, namely the NDF data access subroutine library and the ADAM
parameter system library.}

However, there exists a large body of code -- in a handful of released
software items as well as user-written private applications -- that relies on
calling Figaro subroutines. For this reason the compiled object libraries are
included in the release of Portable Figaro 3.2. They can be found in
{\tt /star/figaro/lib}. There
are no shareable images. There is also no link script {\tt fig.com} like in
Figaro 3.0. Finally, existing ``connection files'' have to be converted into
``interface files''. A dumb conversion is possible with the {\tt par2ifl}
utility to be run on VAXs.

The following description is not very exhaustive. You may find it helpful to
look at the {\tt makefile} files in
{\tt /star/figaro} and {\tt /star/figaro/mono}.

First the good news; your application code needs no change at all, except for
include statements. Portable Figaro 3.2 itself does it as follows. The include
statements use as file specifications upper case names without paths and
without extensions. The most common include statement then becomes
\begin{verbatim}
   INCLUDE 'DYNAMIC_MEMORY'
\end{verbatim}
This is about all one can hope to be portable and the rest must be done in the
operating system before compilation. One will create a
symbolic link to the actual file (including path and extension).
After compilation the
link can be removed:
\begin{verbatim}
   % ln -s \
      /star/figaro/inc/dynamic_memory.inc\
      DYNAMIC_MEMORY
   % f77 -c myapplic.f
   % rm DYNAMIC_MEMORY
\end{verbatim}

It is recommended to compile all your applications into a single executable
just like it is done for Portable
Figaro 3.2 itself. This will save considerably on
size of executables and does not impose a significant executable startup
penalty on Unix systems.

You would compile your applications and archive them into an object library
equivalent to {\tt lib\-app\-lic.a}. You would then adapt a
monolith routine from {\tt /star/\-figaro/\-mono/\-figaro\_\-unix.for}
At this point you
should choose a name for your package, say ``mypack''.

In the actual Figaro 3.2 files, the list
of applications is not in {\tt figaro\_unix.for}, but in an include
file {\tt ifblock\_unix.for}. You can of course include this with the editor
rather than the compiler. You should also consider using a file name extension
{\tt .f} as is common on Unix systems.
Portable Figaro 3.2 uses {\tt .for}, which
really is alien to Unix.

When {\tt mypack.f} is ready, compile it into an
object file {\tt mypack.o}. Then use {\tt alink}
to link {\tt mypack.o} with your applications object library
and the Figaro libraries. For the proper
order of the libraries consult the {\tt /star/figaro/mono/makefile}. There you
will also find references to Starlink libraries, some of which you may have
to include in your link command as well. {\it Note} that
{\tt `pgp\_link\_adam`} etc.\ are enclosed in {\it back} quotes.

After linking you will have an executable {\tt mypack}. In order to execute an
application, say {\tt applic} within the package, you need a symbolic link from
the application name to the executable name.
As part of the package building:
\begin{verbatim}
   % ln -s mypack applic
\end{verbatim}
As part of package startup (in {\tt mypack.csh}):
\begin{verbatim}
   % alias applic <path>/applic
\end{verbatim}
This startup file is invoked by
\begin{verbatim}
   % source <path>/mypack.csh
\end{verbatim}
for which you can set up abbreviations in your {\tt \$HOME/.cshrc} file.

However, each application needs an interface file, either ASCII or compiled.
The {\tt applic.ifl} file corresponds to the connection file in Figaro 3.0.
You can avoid completely re-writing your connection files as interface
files, if you have the binary parameter files for DCL Figaro 3.0 on the VAX.
For this you need the utility {\tt par2ifl}. It will produce a dumb first
version of the interface file. If necessary you can learn about these files
and improve them with a text editor.

Suppose you have a connection file {\tt applic.con} on a VAX. This
is a text file. You will convert this on the VAX into an interface file as
follows:

\begin{verbatim}
 !  Startup for Figaro software
 !  development
   $ figaro
   $ figdev

 !  Convert .con into .par
 !  Convert .par into .ifl
   $ crepar applic
   $ par2ifl applic
\end{verbatim}

{\tt applic.ifl} is again a text file. In many cases it will work right
away.  If the application calls {\tt par\_q*} routines you should use
{\tt par2ifl applic parq=yes}. Still, if your application makes calls to
{\tt var\_get*} or {\tt var\_set*} routines, then you will have to add
extra parameters for these ``user variables''. The exact entries in {\tt
applic.ifl} are described in Section \ref{changes}. You can also improve
the interface file considerably once you have learnt about its workings.
See SUN/115 for the details.

The final {\tt applic.ifl} can be copied to the Unix machine where you want
to build {\tt mypack}. It can be used as is, or it can be compiled to speed up
the applications. It would be compiled into {\tt applic.ifc} with
\begin{verbatim}
   % compifl applic
\end{verbatim}
The interface file should be in the same directory as
the symbolic link {\tt applic->mypack}.


\section{Changes since Version 3.0}
\label{changes}

{\it Not all small changes are listed here.  Assuming that most users
will migrate from VAX Figaro 3.0 and have hardly used Portable Figaro
3.1, this section mainly describes the changes relative to 3.0 and not
relative to 3.1-0 or 3.1-1.  But it also mentions bugs in 3.1-0 or 3.1-1
fixed in 3.2.}

The motivation for Portable Figaro 3.2 is to have a Unix version -- or
rather a portable version -- of Figaro. It is available on all
Starlink-supported Unix hardware, currently Sun4 (SunOS 4.x), Sun4
(Solaris), DECstation (Ultrix) and Alpha AXP (OSF/1).  It could also run
on VAX (ICL) if required.

The main changes are
   \begin{itemize}
      \item The interface files must be ADAM interfaces ({\tt .ifl}
         files).
      \item The executable is a single ``pseudo-monolith'', though
         triggered from the Unix shell by typing the application name.
      \item {\tt par\_batch} always returned {\tt .FALSE.} in version
         3.1.  In version 3.2 the monolith routine was changed to find
         out the batch mode from the environment variable {\tt FIGARO\_MODE}.
         If and only if it exists and its value converted to upper case
         equals ``BATCH'', then batch mode is assumed.  Batch mode or
         not does not affect how (F)PAR works, but applications can
         take different action depending on the returned value of {\tt
         par\_batch}.
      \item There are no ready-to-use utilities for Figaro software
         development.
      \item The object libraries do not contain all routines from Figaro
         3.0.
      \item {\tt image} was not available in version 3.1-0, but is in
         3.2.  For {\tt icur} an equivalent {\tt igcur}, which works on
         a previous display by {\tt igrey} or {\tt icont}, exists.  In
         version 3.1-1 {\tt image, icur, colour, clean} were added.
         They use a device chosen by {\tt idev}.
      \item Image pairs cannot be handled, blinking is not possible.
         This is primarily because modern displays are less than 8 bit
         deep. Blinking does not work properly on VWS/TVP and
         Xwindows/TVP in Figaro 3.0 either.
      \item FITS input/output (disk or tape) was not available in
         version 3.1.  In version 3.2 disk-FITS can be read and written
         with {\tt rdfits} and {\tt wdfits}.  In view of forthcoming
         major changes to the FITS standard, Figaro will not contain
         FITS readers/writers other than these two.  Even these will not
         be upgraded further.  Up-to-date FITS support will be a matter
         for the Kappa or Convert packages.
   \end{itemize}


\subsection{Documentation}

The on-line help is mainly what was the {\tt commands.hlb} library. It
includes all the new prologues with updated history information. Also
the hierarchy was changed so that it is in line with the standard as
produced by SST {\tt prohlp} and binds easily into the (A)PAR parameter
system run-time help.

The general help {\tt figaro.hlp} as in Figaro 3.0 was not ported. Much
of the information is not exactly applicable to Portable Figaro 3.2. The
user guide was identical to that help library. The relevant parts were
updated and included in this document (Section \ref{techno}). That user
guide also made references to further documents that existed only
on-line. Those were also reviewed and included in this document.

The Figaro Programmers' Guide of January 1991 is still valid, apart from
Section 12. A recipe for building VAX/ICL monoliths from Figaro-ish
applications was outlined in Section 3.3 of SSN/40. However that is not
exactly applicable under Unix; refer also to Section \ref{proghint} in
this document.  And note that Figaro is no longer recommended as a
programming environment. New software should use Starlink libraries.


\subsection{General}

The port started out from Standard Figaro version 3.0 as modified and
extended by Starlink/UK National Figaro version 3.0-6. These are
releases for VAX machines. The DTA library had been ported by Keith
Shortridge in early 1992 and had been released in Sun Figaro version
2.4.5 patch 5. The DSA library has been ported in August 1992 by Keith
Shortridge with contributions from Horst Meyerdierks. A small number of
routines that were in VAX Macro or in VAX Pascal have not been taken
from Figaro 3.0. Instead their C or Fortran counterparts in Sun Figaro
2.4.5 were used and reviewed. The C code is ANSI-compliant and most of
it is unacceptable to {\tt cc} under SunOS 4.x.  The C compilers used
depend on the machine.

Portable Figaro 3.1-0 concentrated on providing as many Figaro 3.0
applications as possible in a portable release and on a reasonable time
scale.  Version 3.1-1 added the most important missing applications,
those for image display etc.  Version 3.2 is a complete port of Figaro
3.0 in that is also contains disk-FITS readers and writers.  Fringe
items like {\tt dta2hds} or {\tt cabujy} are not available. Nor is there
a software development environment; programmers have to adapt their
software to {\tt alink} and {\tt compifl} themselves (see (F)PAR below).

There is only one executable system similar to the monolithic ICL Figaro
3.0. DCL and Callable Figaro are not available in version 3.1.  However,
Portable Figaro 3.2 uses the ADAM parameter system in a much improved
way; it is superior to ICL Figaro 3.0. The Figaro libraries are
available only as object libraries, no shareable image exists.

Portable Figaro 3.2 does not include TVP or MTP.  It also does not
include DSK which affects one mode of operation of a small number of
line graphics applications. CNV is discontinued, the small number of
calls to it could be redirected to one new routine {\tt dsa\_fmtcon}.
DYN was merged into DSA, VARS into PAR. With TVP gone, DUT and MEM are
not needed either.  Since Portable Figaro 3.2 does not support tape
handling, there is only a dummy TIO library. The FIT library is
complete, in order to support disk-FITS.  NAG\_FIX was eliminated by
changing the out-of-date calls to NAG. FIG and GEN were ported only to
the extent that ported applications called these routines. The JTY
(formerly JT) library was brought into line with the other libraries:
the routine names and file names now have prefixes JTY and the files
contain in general one routine only.

What remains of the Figaro environment -- apart from the object
libraries -- are some environment variables. These are set in the
Starlink startup scripts:

\begin{verbatim}
   FIG_DIR        /star/bin/figaro
   FIG_HELP       /star/help/figaro/figaro
   FIGARO_PROG_S  /star/etc/figaro
   FIGARO_PROG_N  /dev/null
   FIGARO_FORMATS ndf,dst
\end{verbatim}

The usual file search path is the same, only that the directory of the
executable is not available: {\tt ./file, \$FIG\-ARO\-\_PROG\-\_U/\-file,
\$FIG\-ARO\-\_PROG\-\_L/\-file, \$FIG\-ARO\-\_PROG\-\_N/\-file} and {\tt
\$FIG\-ARO\-\_\-PROG\-\_S/\-file}.  The {\tt \_U} variable is free for the user
to set, {\tt \_L} is for the site manager to set. They can of course be
left un-set.  The concept of a national directory is given up, since it
did not work out quite as well as one might have thought. With Portable
Figaro 3.2 being a release from Starlink, there is no longer the need
for a separate directory for Starlink's modifications.

Portable Figaro 3.2 relies on Starlink infrastructure software to a
larger extent than Figaro 3.0 did.  In general more reliance on Starlink
software results in easier maintenance of the resulting Figaro package.
Figaro 3.0 did already use

\begin{itemize}
\item GKS: Graphics Kernel System
   (not a Starlink item, but available from RAL),
\item SGS: Simple Graphics System,
\item GNS: Graphics workstation Name Service,
\item GWM: Graphics Window Manager,
\item PGPLOT: Graphics Subroutine Library,
\item HDS: Hierarchical Data System.
\end{itemize}

In Portable Figaro 3.2 the following were added:

\begin{itemize}
\item (A)PAR: The ADAM parameter system, and the {\tt alink, compifl}
   commands to link and to compile interface files.
\item PRIMDAT: Primitive Numerical Data processing. These are used for type
   conversions, while trapping errors and using bad values.
\item PSX: Posix Fortran Interface.
\item HELP: Interactive Help System.
\item NDF: Extensible N-dimensional Data Format access routines. These
   are used by two or three applications to rectify NDF-format files
   that may be unacceptable to Figaro. These are not necessary for a
   Figaro-only data reduction system.
\item FIO: Fortran Input/Output routines. These are used to replace the
   VAX run-time library routines to get and release Fortran I/O units.
\item CHR: Character Handling Routines.  These are used only in a few
   A-task applications in {\tt appstl} (see below).
\item EMS: Starlink error reporting.
\item ERR: Dto.  These are used only in a few A-task applications in
   {\tt appstl} (see below).
\end{itemize}

Colour tables are no longer unformatted 2-byte integer arrays in the
range 0 to 255 with 3 records of 256 integers.  Instead they are 3-by-N
images in NDF format.  The values range from 0 to 1 now.  All existing
colour tables happen to be 3 by 256.  Colour tables are still stored in
the directory {\tt \$FIGARO\_PROG\_S}, they have names {\tt
<table>\_lut.sdf}.  Not only the traditional tables from Figaro 3.0 were
ported, but also some tables from NDPROGS.  In addition, Kappa's colour
tables have the same format and can be used.

On Sun4, version 3.1-0 was linked dynamically with Starlink and system
libraries.  This implied a significant startup time penalty.  Version
3.1-1 reduced this by being linked statically with Starlink though
dynamically with system.  Version 3.2 is linked statically on all
platforms (and separately for SunOS 4.x and Solaris).

On DECstations version 3.1-0 would not allow ordinary users to read
files in {\tt \$FIGARO\_PROG\_S}, e.g.\ menu help texts or standard flux
tables.  This has been fixed in version 3.1-1.


\subsection{Ported Applications}

There are two directories with application source code, {\tt applic} for
the traditional sources and {\tt appstl} for a small set of ``pure
Starlink'' applications.  The latter are written in quite different
style; they use only Starlink libraries and not Figaro libraries.  In
addition, {\tt appsub} is used to absorb any subroutines that were split
off the otherwise biggest application source files in {\tt applic} (the
sources that exceeded 50 kByte).

Most applications were ported straight from VAX Figaro 3.0, either from
the Standard release or the National release.
The changes made to these applications are mainly hard-wired file names or
parts thereof, which must be lower case now. Also {\tt OPEN, CLOSE,
ENQUIRE} statements sometimes used VAX extensions to Fortran 77. Some
bugs were fixed (in addition to fixes already in National Figaro 3.0-6).

Some applications use text files with fixed names for output.  This may
prevent running them again before the output file is deleted or renamed.
The problem was serious for {\tt echselect}, which uses such a file for
input and subsequently for output.  {\tt echselect} will now delete any
existing file before writing.  This is in effect the same as on VAX,
only that VAX/VMS would keep old versions of the file.

A small number of applications have been renamed: {\tt fitslist} becomes
{\tt fitskeys}, {\tt rotate} becomes {\tt irot90}, {\tt ndfbad} becomes
{\tt q2bad}.

{\tt soft, hard, idev} use GNS instead of SGS to check the device name.
This allows device names to be abbreviated.
The new imaging routines replace calls to TVP with calls to PGPLOT. Thus
all graphics except {\tt findsp} and {\tt overpf} is done with PGPLOT.
These two GKS-calling applications had to be
adapted to GKS 7.4 in the Unix version. The VAXs are still at GKS 7.2.

{\tt ab\-line}, {\tt ech\-arc}, {\tt ech\-find}, {\tt ech\-mask}, {\tt ech\-merge}, {\tt fet\-321} and
{\tt figs424} have
been converted to use the DSA library rather than DTA.  Now only {\tt
exam} remains a non-DSA routine; all of Figaro supports DST and NDF
formats.

{\tt ab\-line}, {\tt arc}, {\tt cen\-ters}, {\tt cfit}, {\tt clean},
{\tt cset}, {\tt ech\-arc, ech\-sel\-ect}, {\tt findsp}, {\tt foto},
{\tt gauss}, {\tt iplots}, {\tt is\-edit}, {\tt ms\-plot}, {\tt sdist,
spied} and {\tt tip\-pex} have been modified to avoid calls to {\tt
par\_q*} or {\tt par\_rd\-user}.  Instead of asking with a generic
parameter name and a dynamic parameter prompt string, these routines
now have additional parameters with static prompt strings.  This should
not affect the use of these applications, the new parameters are not to
be given on the command line anyway, and users are used to the questions
that are being asked.  However, it is now possible to abort the
application when it was not possible before.

The ``straight port'' traditional Figaro routines are:

%\small %1-col
\begin{verbatim}
   abconv        abline        adjoin   
   alasin        alasout       arc      
   bclean        bfft          bsmult   
   ccdlin        ccur          caldiv   
   cdist         centers       cfit     
   clean         clip          cmplx2i  
   cmplx2m       cmplx2r       cmplxadd 
   cmplxconj     cmplxdiv      cmplxfilt
   cmplxmult     cmplxsub      coadd    
   combine       cosbell       cosrej   
   cset          cspike        dvdplot  
   echarc        echfind       echmask  
   echmerge      echselect     exam     
   emlt          elsplot       errcon   
   esplot        extin         extract  
   fet321        ff            ffcross  
   fft           figs321       figs322  
   figs422       figs423       figs424  
   figsee        figsflux      findsp   
   fitskeys      flconv        foto          
   fscrunch      fwconv        gauss    
   growx         growxt        growxy   
   growy         growyt        gspike   
   hard          hist          hopt     
   i2cmplx       iadd          iarc     
   icadd         icdiv         icmult   
   iconv3        icont         icor16   
   icset         icsub         idiff    
   idiv          igrey         ilist    
   ilog          imult         interp   
   iplots        ipower        irconv   
   irevx         irevy         irflat   
   irflux        irot90        iscrunch 
   iscruni       isedit        ishift   
   ismooth       isplot        istat    
   istretch      isub          isubset  
   isuper        isxadd        isxdiv   
   isxmul        isxsub        isyadd   
   isydiv        isymul        isysub   
   ixsmooth      linterp       lsplot        
   lxset         lyset         mask     
   maskext       mcfit         medfilt  
   medsky        msplot        ncset    
   offdist       optextract    overpf   
   peak          polext        polysky  
   profile       r2cmplx       rcgs2    
   rdfits        rembad        rescale  
   retype        rotx          scnsky   
   scross        scrunch       sdist    
   sfit          slice         soft     
   spflux        spied         spifit   
   splot         sqrterr       tippex   
   vachel        wdfits        xcadd    
   xcdiv         xcmult        xcopi    
   xcopy         xcsub         xcur     
   xtplane       xyplane       ystract  
   ytplane  
\end{verbatim}
%\normalsize %1-col

Apart from {\tt clean}, which could be modified to use PGPLOT instead of
TVP, the imaging routines have been re-written.  {\tt idev} has been
added to select the imaging device.  {\tt image} and {\tt clean} now
support bad values (or quality).  {\tt igcur} is an equivalent to {\tt
icur}, but uses a previous display on the ``soft'' device made with {\tt
igrey/icont}.  The routines re-written in traditional Figaro style are:

%\small %1-col
\begin{verbatim}
   colour        icur          idev     
   igcur         image
\end{verbatim}
%\normalsize %1-col

Most of the HDS file structure manipulation routines have been
re-written as ``Starlink-style'' A-tasks and are stored in the {\tt
appstl} directory. {\tt crobj} is replaced by {\tt creobj}, {\tt let} by
{\tt copobj/setobj}.  This group also contains the on-line
help program {\tt fighelp}, and the two NDF-fixing routines {\tt q2bad}
to merge quality into bad values and {\tt goodvar} to replace bad and
negative variances.

These applications not being in the traditional Figaro style and not
using (F)PAR, the syntax to specify HDS structures is slightly
different.  Where traditionally (and still in {\tt exam}) array elements
were specified in square brackets {\tt []}, you must now use parentheses
{\tt ()}.
Array shapes and sizes are no longer part of a structure specification,
but are passed as separate parameters.  Furthermore, data types must be
HDS types, e.g.\ {\tt Float} becomes {\tt \_REAL}.

The A-task applications in {\tt appstl} are:

%\small %1-col
\begin{verbatim}
   creobj        copobj        delobj   
   fighelp       goodvar       q2bad    
   renobj        setobj        trimfile
\end{verbatim}
%\normalsize %1-col

{\tt appstl} contains not only the A-task routines, but also the
subroutines called by these A-tasks. Those have names with the FIG
prefix; they are kept here to isolate Starlink-ish routines from
Figaro-ish ones.

The following fixes to applications
were made in 3.1-1 or 3.2.
In version 3.1-0 {\tt renobj} had a bug so that the file would be
corrupted if the destination was just below the top level within the file.
{\tt igrey} was changed to draw the box (and the ticks) after doing the
grey plot. Thus the ticks are now visible.
{\tt arc} no longer allows you to plot garbage in a hardcopy if no output
file is specified.
{\tt arc, echarc} and some other applications that used the
{\tt par\_q*} or {\tt par\_rduser} routines could enter an endless loop if the
user tried to abort.
{\tt medsky} would run out of workspace slot due to a bug in {\tt dsa\_unmap}.
{\tt echarc} no longer crashes because the string for an internal
{\tt WRITE} being too short.  It also uses its own help text file,
instead of one written
for {\tt arc}.
{\tt echselect} now can read a previous selection and write an updated
selection to the same file.  It deletes the original file and writes a
new one.
The interface of {\tt extin} declares the {\tt coeff} spectrum as of
type {\tt LITERAL}, the former type {\tt \_REAL} was not suitable for a
data structure name.
{\tt foto} no longer crashes because of machine
precision, before a variance could be negative and taking the square root
crashed the routine.
{\tt irflat}'s {\tt more} parameter could not be given on the command
line.
{\tt cdist} no longer crashes when you have between 21 and 50 spectra.


\subsection{Un-ported Applications}

Some routines are not compiled in the Unix release, but would be
included in a release of Portable Figaro 3.2 for VAX (VMS):

%\small %1-col
\begin{verbatim}
   rdipso        rspica        table    
   wdipso        wspica
\end{verbatim}
%\normalsize %1-col

A number of applications could not be ported because tape handling is
not supported or DSK is not available.  Other un-ported applications
have to do with image pairs, since they rely on the display device being
8 bit deep.  In addition most format conversions were not ported.
Furthermore some applications are meaningless in an environment other
than DCL- or Callable Figaro.  Finally {\tt extlist} never worked.
The un-ported applications are:

%\small %1-col
\begin{verbatim}
   args          blink         bplot    
   contract      cpair         cpos     
   expand        extlist       figset   
   find          fits          ierase   
   ikon          image2        imageps  
   impair        odist         pair     
   r4s           rbaz          rcshec   
   rctio         recoff        recon    
   rewind        rjab          rjkm     
   rjpl          rjt           rlol     
   rntyb         rpdm          rpfuei   
   rshec         rsit          rspdm    
   rtyb          rxmic         sfind    
   skip          starin        starout  
   tape          tapeo         vshow
   wais          wifits        wjab     
   wlol          wpdm          wjt      
   wvista        zoom     
\end{verbatim}
%\normalsize %1-col


\subsection{Parameters / Variables: (F)PAR}

The Figaro PAR and VAR routines have been merged into one library (F)PAR
(retaining the original routine prefixes). The routines translate more
or less directly into calls to the ADAM parameter system (A)PAR. They
emulate the behaviour of the Figaro 3.0 parameter and variable system as
best as possible under these circumstances. The MAX/MIN-feature is
currently not available, the @-feature is no longer supported.

Variables are still supported, but formally become global parameters and
must be declared as such in the interface files. The exact entry is a
bit tricky, for a read-only variable use:

\goodbreak
%\small %1-col
\begin{verbatim}
   parameter XXXX
      type    '_CHAR' or '_REAL'
      access  'READ'
      vpath   'GLOBAL'
      ppath   'GLOBAL'
      default ' ' or 0.
      association '<-GLOBAL.XXXX'
   endparameter
\end{verbatim}
%\normalsize %1-col
\goodbreak

For a write-only variable use:

\goodbreak
%\small %1-col
\begin{verbatim}
   parameter XXXX
      type    '_CHAR' or '_REAL'
      access  'WRITE'
      vpath   'DEFAULT'
      default ' ' or 0.
      association '->GLOBAL.XXXX'
   endparameter
\end{verbatim}
%\normalsize %1-col
\goodbreak

It is possible within one application to read through the prompt path
with e.g.\ {\tt par\_rdary} and later write into the global association
with {\tt var\_setary}.

%\small %1-col
\begin{verbatim}
   parameter XXXX
      type    '_CHAR' or '_REAL'
      vpath   'PROMPT'                        
      ppath   'GLOBAL,aaaaa,bbbbb,....'
      association '<->GLOBAL.XXXX'
      prompt  'yyyyyy'
   endparameter
\end{verbatim}
%\normalsize %1-col

More elaborate schemes may prove difficult.

In some circumstances VAR routines may prompt for global parameters.
Say, the {\tt soft} application has never been used to select a plot device.
When an application tries to get the value of the global parameter {\tt soft}
it will prompt the user. Also if the prompt keyword is specified by the
user, variables are just normal parameters and will be prompted for. The
inconvenience this causes amounts only to hitting the {\tt Return} key
in order to accept the default value.

The application interface files {\tt <applic>.ifl} derive from the
Figaro 3.0 binary parameter files {\tt <applic>\-.par}. For Figaro 3.0
this conversion had been done automatically with the utility {\tt
creifl.exe}.  Figaro 3.0 does contain the utility {\tt par2ifl} for this
conversion.  See also Section \ref{proghint}.

No keywords are used in
the interfaces.  Thus the full parameter name has to be used to specify
the value on the command line; in Figaro 3.0 a shortest abbreviation had
to be used.

In National Figaro 3.0-6 a small number of parameters in a moderate
number of ICL Figaro applications (mostly parameters {\tt xstart/xend}) had
been changed to become global parameters. In Portable Figaro 3.1
{\tt ystart/yend, spectrum, image} were global too.  In Figaro 3.2 many
instances of these parameters are non-global again.  The only global
parameters are {\tt image/spectrum}
in all applications, and {\tt xstart/xend} in {\tt xcur} and
{\tt splot/esplot}.

While in Figaro 3.0 all non-logical parameters were of type {\tt LITERAL},
numeric parameters are now of type {\tt \_REAL}. Portable Figaro 3.2 itself
does not use any double precision parameters, those would be of type
{\tt \_DOUBLE}.

All non-global prompt paths were changed in version 3.1 from
{\tt "CURRENT,DYNAMIC"} to {\tt "DYNAMIC, CURRENT,DEFAULT"},
so that the applications'
dynamic defaults were always used.  In version 3.2 this is changed back
again.  So normally the current value is used, but with the {\tt reset}
keyword the dynamic value is used.

All interface files connect to the help library {\tt 'FIG\_HELP:'}. For all
parameters the help key {\tt '*'} is used, equivalent to {\tt '<applic>
Parameters <param>'}.

Since (A)PAR cannot distinguish between batch and interactive,
(F)PAR will not re-prompt if a given value is outside the range
specified in the call to (F)PAR. (It used to do so in version 3.0 when
not in batch.) Instead the appropriate extreme acceptable value is
returned to the application and put back into (A)PAR. The effect of {\tt
MIN} or {\tt MAX} can be achieved by entering {\tt -1e30} or {\tt 1e30}
or some number outside the allowed range.

{\tt par\_wruser} will now strip off a leading {\tt \$} or {\tt +} thus
disabling the line feed control. The status argument of {\tt
par\_wruser} is a returned argument; {\tt par\_wruser} works even when
called with bad status, but it turns the status into OK.

Null and abort responses given by the user to (A)PAR are detected by as
non-OK status by (F)PAR. (F)PAR will set the abort flag in its common
block. The ``normal'' (F)PAR routines {\tt par\_rdchar}, {\tt par\_rdkey,
par\_rdval}, {\tt par\_rdvald} and {\tt par\_rdary} will return without
action if the abort flag has been set by a previous call to (F)PAR.
Applications should test the abort flag and take appropriate action.
Most do.

The ``abnormal'' (F)PAR routines {\tt par\_qnum}, {\tt par\_qstr}, {\tt
par\_quest} and {\tt par\_rduser} do not handle abort requests.  Also
these routines use dynamic prompt strings.  Both these features make
them difficult to use on top of (A)PAR.  Use of these routines should be
avoided.  Portable Figaro 3.2 does avoid them, though they are still
included in the object library.

In the ADAM environment PGPLOT is not allowed to communicate with the
user when {\tt pgadvance} is called.  Thus ideally any call to {\tt
pgbegin} should be followed by a {\tt CALL PGASK(.FALSE.)}, at least if
the application also calls {\tt pgadvance}.  Where it is necessary for
the application to hold before clearing the previous plot, this should
be achieved through the parameter system.

In version 3.1, {\tt par\_rduser} had a bug involving the abort flag.
It now behaves correctly, but does not allow the user to abort the
application.


\subsection{Data Access Routines: DSA and DTA}

The need for the VAX error condition handler has been eliminated by more
robust coding of some routines and by using PRIMDAT routines.

A problem arose with the use of the string {$\tt\backslash$\tt n} in the
buffered output by calls to {\tt dsa\_wruser}. The backslash is not in
the Fortran character set. In Sun Fortran it is interpreted in a similar
way as in Unix or C.  {\tt dsa\_wruser} should no longer be called with
the new-line escape sequence {$\tt\backslash$\tt n}. Instead an extra
call to {\tt dsa\_wrflush} should be made. In general, applications
should call {\tt par\_wruser} instead of {\tt dsa\_wruser}. Where a
backslash is needed in source code -- e.g.\ in text strings for PGPLOT
--, {\tt CHAR(92)} should be used.

{\tt dsa\_map\_data} would un-flag data even when access was {\tt
WRITE}. In that case the mapped array contains garbage. Comparing it
against the bad value results in the occasional invalid operand, and
will often encounter {\tt NaN}s. The routine now does not un-flag
write-accessed data.  {\tt dsa\_map\_errors} and {\tt
dsa\_map\_variance} did not zero-initialise an existing write-accessed
array; they do now.

Two features of DST-format axes cannot be accommodated in NDF-format
data: (i) each axis exists independent of the others, (ii) any axis can
be (n-m)-dimensional. In an NDF either the whole axis structure array is
absent, or each axis up to the dimensionality of the main data exists
and has a {\tt DATA\_ARRAY}. (By default that data array contains pixel
numbers minus 0.5, Figaro uses by default pixel numbers ignoring origin,
i.e. numbers 1,2,3,...)
Both features are still available for DST-format data.

As for feature (i), a check and rectification is performed now when a
structure or DSA itself closes down: {\tt dsa\_check\_structure} now at the
very end makes a check of the {\tt .AXIS} structure if the structure to
be checked is an NDF. This is done by a call to {\tt dsa\_check\_ndf\_axis},
which is new. Note that this check is made for all DSA structures of NDF
type even if they were opened for input and are write-protected. If the
rectification fails a message is issued telling whether the whole axis
structure was deleted or the structure remained in contravention of the
NDF specification.

Feature (ii) is virtually unused. This feature has been disabled for
NDFs in {\tt dsa\_coerce\_axis\-\_data} and {\tt dsa\_reshape\_axis}.
These will now issue a message and return an error status whenever an
NDF is at hand and an axis dimensionality higher than 1 is requested.
There is, however, at least one instance where N-dimensional axis arrays
are needed and where DST format must be used: This is in \'echelle
reduction between {\tt echarc} and {\tt scrunch}.

The present DTA library derives from the one released in Sun Figaro
2.4.5, patch 5.
The folding of filenames and appending of version numbers on Unix
systems via {\tt dta\_filefold} and {\tt dta\_versname} has been discarded.

There was a bug in {\tt dta\_dlvar} and {\tt dta\_rnvar} regarding their
use of {\tt dta\_splitn} and {\tt dta\_loc\-ate} when an upper level
structure was a cell in a structure array.  This has been fixed.  {\tt
dta\_cyvar}'s job is not quite as easy as it was before structure arrays
could be encountered.  The routine was completely re-written.  The
re-write of {\tt dta\_cyvar} had a bug regarding its use of Starlink
error handling, which was fixed in version 3.2.

An obscure problem occurred whenever a DSA structure reference name was
the same as the corresponding parameter name and the parameter also had
a global association. This was tracked down to be a problem of
conflicting names of HDS locator groups as used by DTA and (A)PAR. DTA
now constructs group names by appending and extra {\tt G}. Group names
must be no longer than 15 characters. This now implies that DTA top
level names and DSA reference names must be 14 characters or less.

In version 3.1-1 DTA was converted to use {\tt dat\_\_szloc} instead of the
constant 15.  On an Alpha AXP, HDS locators are of length 16.

In version 3.1-0 and 3.1-1 {\tt dsa\_fmtcon} needed to be given two pointers
to arrays, in version 3.2 the arrays must be given.
{\tt dsa\_get\_flag\_value} was fixed to allow the type to be specified in
lower or mixed case.  {\tt dsa\_specific\_structure} was changed to handle
structure names longer than 16 characters.  {\tt dsa\_unmap} was fixed to
release all workspaces associated with the mapped array.


\subsection{High-level Subroutines: FIG}

Virtually all source files have been split into single-routine files and
all externally called routines have now the FIG prefix in their names.
Thus {\tt wxyfit, seterr, figx\_shift} have changed name.  Some file names are
abbreviated routine names in order to be only 12 characters long
(excluding the extension {\tt .for}).  Only {\tt fig\_eterp.for}
contains subsidiary routines without FIG prefix and all in one file.

While FIG still calls DSA, DSA no longer calls FIG; FIG need not appear
twice in the link command.

{\tt fig\_rebin2d} uses DSA to get a work space needed to call {\tt
gen\_poly2d}.  Thus {\tt dsa\_open} must have been called before {\tt
fig\_rebin2d}, and {\tt dsa\_close} must be called some time after {\tt
fig\_rebin2d}.


\subsection{The JT-library: JTY}

The files that contained the JT routines have been split into
single-routine files. Both routine names and file names have the JTY
prefix now. In some cases file names are shorter (12 character plus {\tt
.for}) than routine names.

Some care had to be taken when changing routine names to {\tt jty\_*}, because
some were functions and implicitly typed {\tt REAL}. The prefix would
imply them to be {\tt INTEGER}. Care must be taken wherever the
functions {\tt evaleg, getrms, poly2d, rmsfilter} are used. They must be
declared properly when changed to {\tt jty\_*}.

Many routines used {\tt PARAMETER} statements without brackets around
the assignment.  This is a VAX extension to Fortran 77; standard Fortran
statements are used now.  Some routines might have written to Fortran
unit 6; these statements have been replaced with calls to {\tt par\_wruser}.


\subsection{Low-level Subroutines: GEN}

There are some C routines. In general an ANSI-compliant
C compiler must be used, the makefile uses {\tt \$(CC)}. There is
however one exception, {\tt gen\_qdisort.c} which on Sun4 (SunOS 4.x,
possibly also Solaris) must be compiled by {\tt cc} with the {\tt
-misalign} flag. {\tt gcc} has no such flag. It must be set so that this
C routine allows for double precision arguments to be aligned on 4-byte
boundaries. Usually it would require alignment on an 8-byte boundary,
which contradicts the Fortran standard and may cause bus errors at
run-time when the C routine is called by a Fortran routine. The makefile
contains a system-dependent {\tt case} statement to use {\tt cc
-misalign} on {\tt sun4}, but {\tt \$(CC)} on {\tt mips}.

Some C routines come in machine-dependent versions.  The problem
is not the non-portability of the interface to Fortran code -- the
interface to Fortran is portable by using F77/CNF --, but the
machine-dependency of binary data.  Disk-FITS support requires
byte-swapping and conversion from IEEE floating point format.

The bug in {\tt gen\_gconv} causing problems with an un-initialised work
array element in the case of even {\tt WIDTH} argument was fixed.  {\tt
gen\_poly2d} was translated to Fortran.  It must now be given an extra
work space by the calling routine.  The most likely such routine is {\tt
fig\_rebin2d}.  {\tt gen\_errmsg} is no longer available.  A new routine
{\tt gen\_astatb} is like {\tt gen\_astat}, but recognises bad values.
Another routine {\tt gen\_getcwd} can be used on Unix platforms to find
out the current working directory.  In version 3.1 this routine was in
the RTL library as {\tt psx\_getcwd}.


\subsection{Other Libraries}

All GKD routines have been ported. {\tt gkd\-\_clear\-\_alpha, gkd\_close,
gkd\_init} are dummy routines. The rest are simple calls to (F)PAR.
Calls to GKD should be avoided and (F)PAR be called directly.

{\tt ich\_cf} and {\tt ich\_cd} were modified to avoid copying
substrings within one string. {\tt ich\_dfold} was included to provide
for folding to lower case.

Version 3.1 included a library RTL, which was only an interim measure to
satisfy numerous calls to VAX/VMS system routines on
other platforms.  Such calls have been eliminated from all code, the
RTL library is not part of Portable Figaro 3.2.

 \onecolumn %2-col
\appendix


\section{Classified List of Commands}
\label{classif}

%\small %1-col
{\bf Input}
\begin{verbatim}
  ALASIN     Read a spectrum in ALAS (Abs. Line Analysis System) format
  FITSKEYS   List the FITS keywords in a data file
  ICOR16     Corrects 16 bit data from signed to unsigned range
  RCGS2      Reads UKIRT CGS2 spectrum (also UKT9 and UKT6 CVF)
  RDFITS     Read file in AAO de facto 'Disk FITS' format
  RDIPSO     Read file in DIPSO/IUEDR/SPECTRUM format
  RSPICA     Reads a spectrum or image from a Spica Memory file
  TABLE      List contents of a SPICA memory file
\end{verbatim}
\goodbreak
{\bf Output}
\begin{verbatim}
  ALASOUT    Output a spectrum in ALAS (Abs. Line Analysis System) format
  WDFITS     Writes an image out in the AAO de facto 'Disk FITS' format
  WDIPSO     Writes a file in DIPSO/IUEDR/SPECTRUM format
  WSPICA     Writes an image or spectrum into a Spica Memory file
\end{verbatim}
\goodbreak
{\bf Display}
\begin{verbatim}
  CCUR       After SPLOT, uses graphics cursor to indicate data values
  COLOUR     Set colour table for image display
  DVDPLOT    Plot the data in one file against the data in another
  ELSPLOT    Produces a long (<3m) error bar plot of a spectrum
  ESPLOT     Produces an error bar plot of a spectrum
  HARD       Sets the file name for hard copy output
  HOPT       Histogram optimization of an image
  ICONT      Produces a contour map of an image
  ICUR       Inspect an image with cursor
  IDEV       Set the device for image display
  IGCUR      Use cursor to show x, y and data values
  IGREY      Produces a grey-scale plot of an image
  IMAGE      Display an image
  IPLOTS     Plots successive cross-sections of an image, several to a page
  ISPLOT     Plots successive cross-sections through an image
  LSPLOT     Hardcopy spectrum plot of specified size (up to 3 metres)
  MSPLOT     Plots a long spectrum as a series of separate plots
  SOFT       Sets the device/type for terminal graphics
  SPLOT      Plots a spectrum
  XCUR       Uses cursor to delimit part of a spectrum
\end{verbatim}
\goodbreak
{\bf Wavelengths}
\begin{verbatim}
  ARC        Interactive manual arc line identification
  ECHARC     Fit an echelle arc
  EMLT       Fits gaussians to the strongest lines in a spectrum
  FSCRUNCH   Rebin data with a disjoint wavelength coverage to a linear one
  IARC       Given fit to single spectrum, fit all spectra in a 2D arc
  ISCRUNCH   Rebin an image to linear wavelength scale given IARC results
  ISCRUNI    Like ISCRUNCH, but interpolates between two IARC result sets
  LXSET      Set X array of spectrum/image to specified range
  SCRUNCH    Rebin a spectrum to a linear wavelength range
  VACHEL     Air to vacuum, and/or recession velocity wavelength conversion
  XCOPI      Like XCOPY but interpolates X-data from 2 files
  XCOPY      Copy X-info (eg wavelengths) into a spectrum
\end{verbatim}
\goodbreak
{\bf B-stars}
\begin{verbatim}
  BSMULT     Atmospheric band removal using a B star calibration spectrum
  CFIT       Generate a spectrum using the cursor
  CSET       Interactively set regions of a spectrum to a constant value
  MASK       Generate a mask spectrum given a spectrum and a mask table
  MCFIT      Fit a continuum to a spectrum, given a mask spectrum
  NCSET      Set a region of a spectrum to a constant
\end{verbatim}
\goodbreak
{\bf Arithmetic}
\begin{verbatim}
  CLIP       Clip data above and below a pair of threshold values
  IADD       Adds two images (or two spectra)
  ICADD      Adds a constant to an image
  ICDIV      Divides an image by a constant
  ICMULT     Multiplies an image by a constant
  ICONV3     Convolve an image with a 3x3 convolution kernel
  ICSUB      Subtracts a constant from an image
  IDIFF      Takes the 'differential' of an image
  IDIV       Divides two images (or two spectra)
  ILOG       Takes the logarithm of an image
  IMULT      Multiplies two images (or two spectra)
  IPOWER     Raises an image to a specified power
  IREVX      Reverse an image (or spectrum) in the X-direction
  IREVY      Reverse an image in the Y-direction
  ISHIFT     Applies a linear x and a linear y shift to an image
  ISMOOTH    2D smooth of image using 9-point smoothing algorithm
  ISTRETCH   Stretches and shifts an image in X and Y.
  ISUB       Subtracts two images (or two spectra)
  ISUBSET    Produces a subset of an image
  ISUPER     Produces a superset of an image
  ISXADD     Adds a spectrum to each X direction x-section of an image
  ISXDIV     Divides     "   into "        "      "    "  "   "
  ISXMUL     Multiplies each X direction image x-sect by a spectrum
  ISXSUB     Subtracts "    from "   "    "        "       "  "   "
  ISYADD     Adds a spectrum to each Y direction x-section of an image
  ISYDIV     Divides     "   into "        "      "    "  "   "
  ISYMUL      Multiplies each Y direction image x-sect by a spectrum
  ISYSUB     Subtracts "    from "   "    "        "       "  "   "
  IXSMOOTH   Smooth in x-direction by gaussian convolution
  RESCALE    Rescale using user-defined upper and lower limits
  ROTX       Rotate data along the X-axis
\end{verbatim}
\goodbreak
{\bf Flats}
\begin{verbatim}
  CFIT       Generate a spectrum using the cursor
  FF         Flat field an image (uses JT's algorithm)
  FFCROSS    Cross-correlate an image and a flat field (mainly IPCS data)
  MASK       Generate a mask spectrum given a spectrum and a mask table
  MCFIT      Fit a continuum to a spectrum, given a mask spectrum
  ISXDIV     Divides a spectrum into each X direction x-section of an image
\end{verbatim}
\goodbreak
{\bf Manipulations}
\begin{verbatim}
  ADJOIN     Append two spectra (strictly a merge by wavelength value)
  BCLEAN     Automatic removal of bad lines & cosmic rays from CCD data
  CFIT       Generate a spectrum using the cursor
  CLEAN      Interactive patching of bad lines, bad pixels in an image
  COADD      Form the spectrum which is the mean of the rows in an image
  COMBINE    Combine two spectra, adding with weights according to errors
  COSREJ     Reject cosmic rays from a set of supposedly identical spectra
  FSCRUNCH   Rebin data with a disjoint wavelength coverage to a linear one
  HIST       Produce histogram of data value distribution in an image
  HOPT       Histogram optimization of an image
  ICONV3     Convolve an image with a 3x3 convolution kernel
  ICOR16     Corrects 16 bit data from signed to unsigned range
  IDIFF      Takes the 'differential' of an image
  IREVX      Reverse an image (or spectrum) in the X-direction
  IREVY      Reverse an image in the Y-direction
  IROT90     Rotates an image through 90 degrees
  MEDFILT    Applies a median filter to an image
  MEDSKY     Take the median of a number of images
  POLYSKY    Fits and subtracts sky from a long slit spectrum
  SCNSKY     Calculates a sky spectrum for a scanned CCD image
  SCROSS     Cross-correlate two spectra & get relative shift
  SCRUNCH    Rebin a spectrum to a linear wavelength range
  SFIT       Fit a polynomial to a spectrum
\end{verbatim}
\goodbreak
{\bf Photometry}
\begin{verbatim}
  CENTERS    Generate file of object centroids from ICUR/IGCUR output
  FOTO       Perform aperture photometry given CENTERS output
  ICUR       Inspect an image with cursor
  IGCUR      Use cursor to show x, y and data values
\end{verbatim}
\goodbreak
{\bf Line fits}
\begin{verbatim}
  ABLINE     Interactive absorption line analysis
  EMLT       Fits gaussians to the strongest lines in a spectrum
  GAUSS      Interactive fit of Gaussians to emission or absorption lines
\end{verbatim}
\goodbreak
{\bf Distortion}
\begin{verbatim}
  CDIST      S-distortion correction using SDIST results
  FINDSP     Locate fibre spectra in an image
  ICUR       Inspect an image with cursor
  IGCUR      Use cursor to show x, y and data values
  OFFDIST    Applies an offset to an SDIST fit
  OVERPF     Overlays a FINDSP fit on another image
  POLEXT     Extract fibre spectra from an image after a FINDSP analysis
  SDIST      Analyse an image containing spectra for S-distortion
\end{verbatim}
\goodbreak
{\bf Fudging}
\begin{verbatim}
  COPOBJ     Copy an HDS object
  CREOBJ     Create a data object or file
  CSET       Interactively set regions of a spectrum to a constant value
  DELOBJ     Delete a data object or a file
  GOODVAR    Replace negative, zero and bad variance values
  ICSET      Set a selected region of an image to a constant value
  ISEDIT     Allows interactive editing of a 1D or 2D spectrum
  LXSET      Set X array of spectrum/image to specified range
  LYSET      Set Y array of spectrum/image to specified range
  NCSET      Set a region of a spectrum to a constant
  Q2BAD      Converts an NDF's quality into bad values
  REMBAD     Removes pixels that have been flagged as bad from data
  RENOBJ     Change the name or location of an object within an HDS file
  RETYPE     Changes the type of the main data array in a file
  SETOBJ     Assign value to an HDS primitive
  SPIED      Interactive spiketrum editor
  TIPPEX     Modify individual pixel values with cursor
  TRIMFILE   Creates a copy of an HDS file without unused space
  XCADD      Adds a constant to the X data in a file
  XCDIV      Divides the X data in a file by a constant
  XCMULT     Multiplies the X data in a file by a constant
  XCSUB      Subtracts a constant from the X data in a file
\end{verbatim}
\goodbreak
{\bf Examination}
\begin{verbatim}
  HIST       Produce histogram of data value distribution in an image
  EXAM       Examines the contents of a data object
  ICUR       Inspect an image with cursor
  IGCUR      Use cursor to show x, y and data values
  ILIST      List the data in an image (or spectrum)
  ISTAT      Provides some statistics about an image (max, min etc.)
\end{verbatim}
\goodbreak
{\bf Slices}
\begin{verbatim}
  EXTRACT    Adds contiguous lines of an image -> a spectrum
  GROWX      Performs reverse function to that of EXTRACT
  GROWXT     Copies an image into contiguous XT planes of a cube
  GROWXY     Copies an image into contiguous XY planes of a cube
  GROWY      Performs reverse function to that of YSTRACT
  GROWYT     Copies an image into contiguous YT planes of a cube
  OPTEXTRACT Extracts a long slit spectrum using Horne's optimal extraction
  PROFILE    Determines a long slit spectrum profile for use by OPTEXTRACT
  SLICE      Takes a slice with arbitrary end points through an image
  XTPLANE    Adds contiguous XT planes of a data cube -> an image
  XYPLANE    Adds contiguous XY planes of a data cube -> an image
  YSTRACT    Adds contiguous columns of an image -> a spectrum
  YTPLANE    Adds contiguous YT planes of a data cube -> an image
\end{verbatim}
\goodbreak
{\bf Fibres}
\begin{verbatim}
  FINDSP     Locate fibre spectra in an image
  OVERPF     Overlays a FINDSP fit on another image
  POLEXT     Extract fibre spectra from an image after a FINDSP analysis
\end{verbatim}
\goodbreak
{\bf Fluxing}
\begin{verbatim}
  ABCONV     Convert spectrum from Janskys into AB magnitudes
  CALDIV     Generate calibration spectrum from continuum standard spectra
  CFIT       Generate a spectrum using the cursor
  CSET       Interactively set regions of a spectrum to a constant value
  CSPIKE     Create calibration spiketrum given spiketrum & standard spectrum
  FIGSFLUX   Flux calibrates a FIGS spectrum
  FLCONV     Convert a spectrum in Janskys into one in Ergs/cm**2/s/A
  FWCONV     General unit conversion for spectra
  GSPIKE     Generates a 'spiketrum' from a table of values
  INTERP     Interpolates between the points of a 'spiketrum' -> a spectrum
  IRFLUX     Flux calibrates an IR spectrum using a black-body model
  LINTERP    Linear interpolation between spiketrum points -> spectrum
  NCSET      Set a region of a spectrum to a constant
  SFIT       Fit a polynomial to a spectrum
  SPFLUX     Applies a flux calibration spectrum to an observed spectrum
  SPIED      Interactive spiketrum editor
  SPIFIT     Fits a global polynomial to a spiketrum -> a spectrum
\end{verbatim}
\goodbreak
{\bf Extinction}
\begin{verbatim}
  EXTIN      Correct spectrum for atmospheric extinction
  GSPIKE     Generates a 'spiketrum' from a table of values
  LINTERP    Linear interpolation between spiketrum points -> spectrum
\end{verbatim}
\goodbreak
{\bf Complex and FFT}
\begin{verbatim}
  BFFT       Takes the reverse FFT of a complex data structure
  CMPLX2I    Extracts the imaginary part of a complex data structure
  CMPLX2M    Extracts the modulus of a complex data structure
  CMPLX2R    Extracts the real part of a complex data structure
  CMPLXADD   Add two complex structures
  CMPLXCONJ  Produce the complex conjugate of a complex structure
  CMPLXDIV   Divide two complex structures
  CMPLXFILT  Create a mid-pass filter for complex data
  CMPLXMULT  Multiply two complex structures
  CMPLXSUB   Subtract two complex structures
  COSBELL    Create data that goes to zero at the edges in a cosine bell
  FFT        Takes the forward FFT of a complex data structure
  I2CMPLX    Copies an array into the imaginary part of a complex structure
  PEAK       Determines position of highest peak in a spectrum
  R2CMPLX    Creates a complex data structure from a real data array
  ROTX       Rotate data along the X-axis
\end{verbatim}
\goodbreak
{\bf Infra-red}
\begin{verbatim}
  FET321     Extracts a spectrum from 1 detector from etalon mode FIGS data
  FIGS321    Processes a FIGS data cube down to a single spectrum
  FIGS322    Processes a FIGS data cube down to an image
  FIGS422    Process a FIGS image-mode hypercube down to an image
  FIGS423    Process a FIGS image-mode hypercube down to a cube
  FIGS424    Sort a FIGS image-mode hypercube into wavelength order
  FIGSEE     Generate a seeing ripple spectrum from a FIGS spectrum
  FIGSFLUX   Flux calibrates a FIGS spectrum
  IRCONV     Converts data in Janskys to W/m**2/um
  IRFLAT     Generates a ripple spectrum from an IR spectrum
  IRFLUX     Flux calibrates an IR spectrum using a black-body model
\end{verbatim}
\goodbreak
{\bf Echelle}
\begin{verbatim}
  CDIST      S-distortion correction using SDIST results
  ECHARC     Fit an echelle arc
  ECHFIND    Locate spectra in echelle data
  ECHMASK    Produce an extraction mask from an SDIST analysis
  ECHMERGE   Merge echelle spectra into a single long spectrum
  ECHSELECT  Interactive selection of sky and object spectra for an echelle
  ICUR       Inspect an image with cursor
  IGCUR      Use cursor to show x, y and data values
  MASKEXT    Extracts echelle orders using a mask created by ECHMASK
  OFFDIST    Applies an offset to an SDIST fit
  SDIST      Analyse an image containing spectra for S-distortion
\end{verbatim}
\goodbreak
{\bf Miscellany}
\begin{verbatim}
  CCDLIN     Applies a linearity correction to AAO CCD data
  ERRCON     Converts percentage error values to absolute values
  FIGHELP    Provide Figaro on-line help
  SQRTERR    Generates an error array as Error = Square Root of (Data/Const)
\end{verbatim}
%\normalsize %1-col


\section{Spectrophotometric Flux Standards Converted from SPICA Format}

{\it Figaro includes a large number of standard files supplied by
Roderick Johnstone, IoA Cambridge.  This appendix reproduces the document
he supplied with those files.}

The files listed contain stellar fluxes in milli-janskys.
AB magnitude files have the same name, but with an A before the ``.'' as in
standard FIGARO style. I have tried to conform to the systematic naming used
by FIGARO so files are called after the star with +ve and -ve signs replaced
by P and M respectively. Note that some files have been included more than
once under aliases of the star name.

Numerical values of wavelength, flux, bandwidth and order changeover
wavelengths have been checked and are believed to be correct. Some errors
in the SPICA tables were found. Where existing FIGARO files are correct these
have not been duplicated. 

Note that there are several different magnitude systems which determine 
the fluxes presented in these tables. These are defined through different
calibrations of the primary  standard Vega and use different ways of
transferring the magnitude system from Vega to the fainter secondary
standards. In general the stars listed here are not actual standards but are
just good calibrations of non-variable stars. The user is strongly advised to
read the paper from which the numbers come (listed at the top of each file) to
gain an idea of both the random and the systematic errors in the fluxes as
well as the magnitude system from which the fluxes derived. 

I am most grateful to Doreen Oliver at RGO who I believe typed in most of the
original SPICA files and to Carolin Crawford who gave invaluable help checking
the numbers.

Roderick Johnstone / IoA Cambridge / December 1987

\goodbreak
%\small %1-col
\begin{verbatim}
Star            File            Coverage        Comments
====            ====            ========        ========

40 Eri B        40ERIB.TAB      3320-9880
AM CVn          HZ29.TAB        3340-8720
BD+25 3941      BDP253941.TAB   3200-8370
BD+28 4211      BDP284211.TAB   3200-8370
BD+33 2642      BDP332642.TAB   3200-8370
BD+40 4032      BDP404032.TAB   3200-8370
BD+82 2015      BDP82015.TAB    3200-8370
EG  5           VMA2.TAB        3260-10640      Used 2nd order values at overlap
EG 11           L870M2.TAB      3210-7220
EG 28           LB1240.TAB      3210-7220
EG 29           LB227.TAB       3210-7220
EG 31           HZ2.TAB         3210-7220
EG 33           40ERIB.TAB      3320-9880
EG 39           HZ7.TAB         3210-7220
EG 42           HZ14.TAB        3230-7420
EG 50           HE3.TAB         3340-9440
EG 54           L745M46AA.TAB   3320-10520      Cosmetic change to lambda_double
EG 63           LDS235B.TAB     3320-8920       Cosmetic change to lambda_double
EG 67           SA29M130.TAB    3210-10200
EG 76           AL970M30.TAB    3180-7040       40A bins in 2nd order
EG 76           BL970M30.TAB    3320-9880       80A bins in 2nd order
EG 77           TON573.TAB      3340-8160
EG 79           R627.TAB        3160-10840
EG 86           HZ21.TAB        3340-9660
EG 91           HZ29.TAB        3340-8720
EG 98           HZ43.TAB        3320-10520
EG 99           W485A.TAB       3320-9880
EG 102          GRWP705824.TAB  3210-10200
EG 119          ROSS640.TAB     3320-10520
EG 129          GRWP708247.TAB  3340-9200
EG 133          L1573M31.TAB    3336-10073
EG 139          W1346.TAB       3210-10640
EG 144          GRWP738031.TAB  3340-9440
EG 148          L1363M3.TAB     3316-10520
EG 149          L930M80.TAB     3320-9240       Cosmetic change to lambda_double and 
                                                correct value at 4510
EG 162          L1512M34B.TAB   3210-10040
EG 182          G47M18.TAB      3340-9280
EG 184          GD140.TAB       3160-10560
EG 192          AGD185.TAB      3200-7240       80A bins 2nd order
EG 192          BGD185.TAB      5820-9780       360A bins
EG 247          G191B2B.TAB     3320-10520
Feige 110       F110.TAB        3200-8370       _S: file seems to have wrong
                                                bandwidth and lambda_double
Feige 15        F15.TAB         3200-8370
Feige 25        F25.TAB         3200-8370
Feige 34        F34.TAB         3200-8370
Feige 56        F56.TAB         3200-8370
Feige 92        F92.TAB         3200-8370
Feige 98        F98.TAB         3200-8370
G191B2B         G191B2B.TAB     3320-10520
G47-18          G47M18.TAB      3340-9280
GD 140          GD140.TAB       3160-10560
GD 185          AGD185.TAB      3200-7240       80A bins 2nd order
GD 185          BGD185.TAB      5820-9780       360A bins
GD-248          GDM248.TAB      3300-10000      Not from SPICA
Grw+70 5824     GRWP705824.TAB  3210-10200
Grw+70 8247     GRWP708247.TAB  3340-9200
Grw+73 8031     GRWP738031.TAB  3340-9440
He 3            HE3.TAB         3340-9440
HZ 14           HZ14.TAB        3230-7420
HZ 15           HZ15.TAB        3200-8370
HZ 2            HZ2.TAB         3210-7220
HZ 21           HZ21.TAB        3340-9660
HZ 29           HZ29.TAB        3340-8720
HZ 43           HZ43.TAB        3320-10520
HZ 44           HZ44.TAB        3316-10520      Ambiguity in bandwidth at changeover
                                                to first order
HZ 7            HZ7.TAB         3210-7220
Kopff 27        KOP27.TAB       3200-8370
L1363-3         L1363M3.TAB     3316-10520
L1403-49        TON573.TAB      3340-8160
L1512-34B       L1512M34B.TAB   3210-10040
L1573-31        L1573M31.TAB    3336-10073
L745-46A        L745M46AA.TAB   3320-10520      Cosmetic change to lambda_double
L870-2          L870M2.TAB      3210-7220
L930-80         L930M80.TAB     3320-9240       Cosmetic change to lambda_double and 
                                                correct value at 4510
L970-30         AL970M30.TAB    3180-7040       40A bins in 2nd order
L970-30         BL970M30.TAB    3320-9880       80A bins in 2nd order
LB1240          LB1240.TAB      3210-7220
LB227           LB227.TAB       3210-7220
LDS235B         LDS235B.TAB     3320-8920       Cosmetic change to lambda_double
R627            R627.TAB        3160-10840
Ross 640        ROSS640.TAB     3320-10520
SA29-130        SA29M130.TAB    3210-10200
Ton573          TON573.TAB      3340-8160
VMa 2           VMA2.TAB        3260-10640      Used 2nd order values at overlap
W1346           W1346.TAB       3210-10640
W485A           W485A.TAB       3320-9880
\end{verbatim}
%\normalsize %1-col


\section{HST and New Oke Spectrophotometric Standard Star Flux Tables}

Jeremy Walsh at ST-ECF provided in 1991 two new sets of
spectrophotometric flux tables in a format for use by Figaro.  These
tables are available at Starlink's central data base machine.  Currently
this is a VAX with DECnet name STADAT.  The HST calibration stars are in
{\tt SPECTRAL\-\_ATLAS\-ROOT:[HST]}, the Oke (1990) calibration stars are in
{\tt SPECTRAL\-\_ATLAS\-ROOT:[OKE]}.  The files available are:

\goodbreak
%\small %1-col
\begin{verbatim}
Directory SPECTRAL_ATLASROOT:[HST]

AGKP81266.TAB;2     AGKP81266A.TAB;2    ALPHALYR.TAB;2      ALPHALYRA.TAB;2
BDP284211.TAB;2     BDP284211A.TAB;2    BDP332642.TAB;2     BDP332642A.TAB;2
BDP75325.TAB;2      BDP75325A.TAB;2     BPM16274.TAB;2      BPM16274A.TAB;2
ETAUMA.TAB;1        ETAUMAA.TAB;1       FEIGE110.TAB;2      FEIGE110A.TAB;2
FEIGE34.TAB;2       FEIGE34A.TAB;2      G191B2B.TAB;2       G191B2BA.TAB;2
G93M48.TAB;2        G93M48A.TAB;2       GAMMAUMA.TAB;2      GAMMAUMAA.TAB;2
GD108.TAB;2         GD108A.TAB;2        GD50.TAB;2          GD50A.TAB;2
GRWP70D5824.TAB;2   GRWP70D5824A.TAB;1  HD49798.TAB;2       HD49798A.TAB;2
HD60753.TAB;2       HD60753A.TAB;2      HD93521.TAB;2       HD93521A.TAB;1
HZ2.TAB;2           HZ21.TAB;2          HZ21A.TAB;1         HZ2A.TAB;1
HZ4.TAB;2           HZ44.TAB;2          HZ44A.TAB;1         HZ4A.TAB;1
LB227.TAB;2         LB227A.TAB;1        LDS749B.TAB;2       LDS749BA.TAB;1
MUCOL.TAB;1         MUCOLA.TAB;1        NGC7293.TAB;2       NGC7293A.TAB;1
ZETA-CAS.TAB;2      ZETA-CASA.TAB;2

Total of 54 files.


Directory SPECTRAL_ATLASROOT:[OKE]

BDP254655.TAB;2     BDP254655A.TAB;2    BDP284211.TAB;2     BDP284211A.TAB;2
BDP332642.TAB;2     BDP332642A.TAB;2    BDP75325.TAB;2      BDP75325A.TAB;2
FEIGE110.TAB;2      FEIGE110A.TAB;2     FEIGE34.TAB;2       FEIGE34A.TAB;2
FEIGE66.TAB;2       FEIGE66A.TAB;2      FEIGE67.TAB;2       FEIGE67A.TAB;2
G138M31.TAB;2       G138M31A.TAB;2      G158M100.TAB;2      G158M100A.TAB;2
G191B2B.TAB;2       G191B2BA.TAB;2      G193M74.TAB;2       G193M74A.TAB;2
G24M9.TAB;2         G24M9A.TAB;2        G60M54.TAB;2        G60M54A.TAB;2
GD108.TAB;2         GD108A.TAB;2        GD248.TAB;2         GD248A.TAB;2
GD50.TAB;2          GD50A.TAB;2         GRWP705824.TAB;2    GRWP705824A.TAB;2
HD93521.TAB;2       HD93521A.TAB;2      HZ21.TAB;2          HZ21A.TAB;2
HZ4.TAB;2           HZ44.TAB;2          HZ44A.TAB;2         HZ4A.TAB;2
LTT9491.TAB;2       LTT9491A.TAB;2      NGC7293.TAB;4       NGC7293A.TAB;4
SA95M42.TAB;2       SA95M42A.TAB;2

Total of 50 files.
\end{verbatim}
%\normalsize %1-col

We reproduce here the {\tt aaareadme.*} files for the two sets of
tables.

The directory {\tt SPECTRAL\_ATLASROOT:[HST]} contains flux files for
HST spectrophotometric standards. The STSDAS tables (for 27 stars) in
the CALSPEC directory from STScI have been processed to give .TAB files
compatible with existing spectrophotometric standard star files already
used by FIGARO. Those .TAB files with a root name ending in an A have
wavelength and ABMAG listed, whilst those without an A at the end of the
rootname (except ETAUMA and GAMMAUMA) have wavelength and milliJy
listed, following the previous convention.

The basic reference for HST standards is Turnshek et al., Astron.  J.,
99, 1243, 1990. See also Bohlin, STScI Newsletter, Vol. 9, No. 2,
September, 1992 for details of the processing. This directory contains
both Category 1 and Category 2 HST standards (see Bohlin, 1992 for
listing).

For details of the processing of these files please refer to J. R.
Walsh at ST-ECF (jwalsh@eso.org or ESO::JWALSH).  Data last updated
August 1992.

The directory {\tt SPECTRAL\_ATLASROOT:[HST]} contains flux files for
the set of 25 spectrophotometric standards published by Oke (Astron. J.,
99, 1621, 1990). The STSDAS tables in the CALOBS directory from STScI
have been processed to give .TAB files compatible with existing
spectrophotometric standard star files already used by FIGARO.  Those
.TAB files with a root name ending in an A have wavelength and ABMAG
listed, whilst those without an A at the end of the rootname (except
ETAUMA and GAMMAUMA) have wavelength and milliJy listed, following the
previous convention.

Note that some of these standards have been previously observed
by Oke and others so tables may exist with the same or a similar
name.

For details of the processing of these files please refer to J. R.
Walsh at ST-ECF.  Data created 18 September 1991.


\section{Gauss: Format of Data File}

\vspace{5mm}
\begin{tiny}
\begin{verbatim}
    DATA ON GAUSSIAN FIT
    --------------------

 Spectrum file: TEST                            
Spectrum label: 

X and E.W. units are      Angstroms ; Y units are    Counts                 
Flux units are     Counts    *   Angstroms                 

                              G a u s s i a n   F i t                                                        !     C o n t i n u u m   F i t
    Peak Posn.   Peak Height     Sigma     Flux           E.W.    R.m.s.    Mean frac err    Order   Sigma Rej.  Frac err Rej.

    6563.336     0.2151E+04     0.2740   0.1476E+04     -8.688  0.7339E+02     1.8157          3       2.000       1.000
\end{verbatim}
\end{tiny}
\vspace{5mm}
%(The 132 column output lines have been split in two here to fit onto
%an 80 column terminal screen)

The mean fractional deviation is defined as:
$${1\over m}\sum_{j=n}^{n+m} \left|{(obs-fit)_j\over err_j}\right|$$
%           n+m
%           ----
%           \      Abs [ (Observed - Fitted)  / Error  ]
%           /                                 j        j
%           ----          
%           j=n 
%          ------------------------------------------------
%                                m                             

\end{document}
