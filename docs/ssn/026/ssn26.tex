\documentclass[11pt]{article}
\pagestyle{myheadings}

% -----------------------------------------------------------------------------
\newcommand{\stardoccategory}  {Starlink System Note}
\newcommand{\stardocinitials}  {SSN}
\newcommand{\stardocnumber}    {26.1}
\newcommand{\stardocsource}    {ssn\stardocnumber}
\newcommand{\stardocauthors}   {C A Clayton}
\newcommand{\stardocdate}      {15th August 1995}
\newcommand{\stardoctitle}     {Configuring, monitoring and tuning SPARC/Solaris systems}
% -----------------------------------------------------------------------------

\newcommand{\stardocname}{\stardocinitials /\stardocnumber}
\markright{\stardocname}
\setlength{\textwidth}{160mm}
\setlength{\textheight}{230mm}
\setlength{\topmargin}{-2mm}
\setlength{\oddsidemargin}{0mm}
\setlength{\evensidemargin}{0mm}
\setlength{\parindent}{0mm}
\setlength{\parskip}{\medskipamount}
\setlength{\unitlength}{1mm}

% -----------------------------------------------------------------------------
% Hypertext definitions.
% These are used by the LaTeX2HTML translator in conjuction with star2html.

% Comment.sty: version 2.0, 19 June 1992
% Selectively in/exclude pieces of text.
%
% Author
%    Victor Eijkhout                                      <eijkhout@cs.utk.edu>
%    Department of Computer Science
%    University Tennessee at Knoxville
%    104 Ayres Hall
%    Knoxville, TN 37996
%    USA

%  Do not remove the %begin{latexonly} and %end{latexonly} lines (used by
%  star2html to signify raw TeX that latex2html cannot process).
%begin{latexonly}
\makeatletter
\def\makeinnocent#1{\catcode`#1=12 }
\def\csarg#1#2{\expandafter#1\csname#2\endcsname}

\def\ThrowAwayComment#1{\begingroup
    \def\CurrentComment{#1}%
    \let\do\makeinnocent \dospecials
    \makeinnocent\^^L% and whatever other special cases
    \endlinechar`\^^M \catcode`\^^M=12 \xComment}
{\catcode`\^^M=12 \endlinechar=-1 %
 \gdef\xComment#1^^M{\def\test{#1}
      \csarg\ifx{PlainEnd\CurrentComment Test}\test
          \let\html@next\endgroup
      \else \csarg\ifx{LaLaEnd\CurrentComment Test}\test
            \edef\html@next{\endgroup\noexpand\end{\CurrentComment}}
      \else \let\html@next\xComment
      \fi \fi \html@next}
}
\makeatother

\def\includecomment
 #1{\expandafter\def\csname#1\endcsname{}%
    \expandafter\def\csname end#1\endcsname{}}
\def\excludecomment
 #1{\expandafter\def\csname#1\endcsname{\ThrowAwayComment{#1}}%
    {\escapechar=-1\relax
     \csarg\xdef{PlainEnd#1Test}{\string\\end#1}%
     \csarg\xdef{LaLaEnd#1Test}{\string\\end\string\{#1\string\}}%
    }}

%  Define environments that ignore their contents.
\excludecomment{comment}
\excludecomment{rawhtml}
\excludecomment{htmlonly}

%  Hypertext commands etc. This is a condensed version of the html.sty
%  file supplied with LaTeX2HTML by: Nikos Drakos <nikos@cbl.leeds.ac.uk> &
%  Jelle van Zeijl <jvzeijl@isou17.estec.esa.nl>. The LaTeX2HTML documentation
%  should be consulted about all commands (and the environments defined above)
%  except \xref and \xlabel which are Starlink specific.

\newcommand{\htmladdnormallinkfoot}[2]{#1\footnote{#2}}
\newcommand{\htmladdnormallink}[2]{#1}
\newcommand{\htmladdimg}[1]{}
\newenvironment{latexonly}{}{}
\newcommand{\hyperref}[4]{#2\ref{#4}#3}
\newcommand{\htmlref}[2]{#1}
\newcommand{\htmlimage}[1]{}
\newcommand{\htmladdtonavigation}[1]{}

% Define commands for HTML-only or LaTeX-only text.
\newcommand{\html}[1]{}
\newcommand{\latex}[1]{#1}

% Use latex2html 98.2.
\newcommand{\latexhtml}[2]{#1}

% Starlink cross-references and labels.
\newcommand{\xref}[3]{#1}
\newcommand{\xlabel}[1]{}

%  LaTeX2HTML symbol.
\newcommand{\latextohtml}{{\bf LaTeX}{2}{\tt{HTML}}}

%  Define command to recentre underscore for Latex and leave as normal
%  for HTML (severe problems with \_ in tabbing environments and \_\_
%  generally otherwise).
\newcommand{\setunderscore}{\renewcommand{\_}{{\tt\symbol{95}}}}
\latex{\setunderscore}

% -----------------------------------------------------------------------------
%  Debugging.
%  =========
%  Un-comment the following to debug links in the HTML version using Latex.

% \newcommand{\hotlink}[2]{\fbox{\begin{tabular}[t]{@{}c@{}}#1\\\hline{\footnotesize #2}\end{tabular}}}
% \renewcommand{\htmladdnormallinkfoot}[2]{\hotlink{#1}{#2}}
% \renewcommand{\htmladdnormallink}[2]{\hotlink{#1}{#2}}
% \renewcommand{\hyperref}[4]{\hotlink{#1}{\S\ref{#4}}}
% \renewcommand{\htmlref}[2]{\hotlink{#1}{\S\ref{#2}}}
% \renewcommand{\xref}[3]{\hotlink{#1}{#2 -- #3}}
%end{latexonly}
% -----------------------------------------------------------------------------
% Add any document-specific \newcommand or \newenvironment commands here

% -----------------------------------------------------------------------------
%  Title Page.
%  ===========
\begin{document}
\thispagestyle{empty}

%  Latex document header.
\begin{latexonly}
   CCLRC / {\sc Rutherford Appleton Laboratory} \hfill {\bf \stardocname}\\
   {\large Particle Physics \& Astronomy Research Council}\\
   {\large Starlink Project\\}
   {\large \stardoccategory\ \stardocnumber}
   \begin{flushright}
   \stardocauthors\\
   \stardocdate
   \end{flushright}
   \vspace{-4mm}
   \rule{\textwidth}{0.5mm}
   \vspace{5mm}
   \begin{center}
   {\Large\bf \stardoctitle}
   \end{center}
   \vspace{5mm}

%  Add heading for abstract if used.
%   \vspace{10mm}
%   \begin{center}
%      {\Large\bf Description}
%   \end{center}
\end{latexonly}

%  HTML documentation header.
\begin{htmlonly}
   \xlabel{}
   \begin{rawhtml} <H1> \end{rawhtml}
      \stardoctitle
   \begin{rawhtml} </H1> \end{rawhtml}

%  Add picture here if required.

   \begin{rawhtml} <P> <I> \end{rawhtml}
   \stardoccategory\ \stardocnumber \\
   \stardocauthors \\
   \stardocdate
   \begin{rawhtml} </I> </P> <H3> \end{rawhtml}
      \htmladdnormallink{CCLRC}{http://www.cclrc.ac.uk} /
      \htmladdnormallink{Rutherford Appleton Laboratory}
                        {http://www.cclrc.ac.uk/ral} \\
      Particle Physics \& Astronomy Research Council \\
   \begin{rawhtml} </H3> <H2> \end{rawhtml}
      \htmladdnormallink{Starlink Project}{http://www.starlink.ac.uk/}
   \begin{rawhtml} </H2> \end{rawhtml}
   \htmladdnormallink{\htmladdimg{source.gif} Retrieve hardcopy}
      {http://www.starlink.ac.uk/cgi-bin/hcserver?\stardocsource}\\

% HTML document table of contents (if used).
% ==========================================
% Add table of contents header and a navigation button to return
% to this point in the document (this should always go before the
% abstract \section). This places the table of contents on the title
% page. Do not use this if you want the normal behaviour.
%   \label{stardoccontents}
%   \begin{rawhtml}
%     <HR>
%     <H2>Contents</H2>
%   \end{rawhtml}
%   \htmladdtonavigation{\htmlref{\htmladdimg{contents_motif.gif}}
%                                            {stardoccontents}}

%  Start new section for abstract if used.
%  \section{\xlabel{abstract}Abstract}

\end{htmlonly}

% -----------------------------------------------------------------------------
%  Document Abstract. (if used)
%  ==================
% -----------------------------------------------------------------------------
%  Latex document Table of Contents. (if used)
%  ===========================================
%  Replace the \latexonlytoc command with \tableofcontents if you're
%  not only having a contents list on the title page.
\begin{latexonly}
   \setlength{\parskip}{0mm}
   \tableofcontents
   \setlength{\parskip}{\medskipamount}
   \markright{\stardocname}
\end{latexonly}
 -----------------------------------------------------------------------------

\section{Introduction}


The purpose of this brief document is to provide site managers with
a starting point to help you check and possibly improve the configuration
and performance of your systems without having to spend a lot of time
reading complex tuning manuals. The goal is not to fully
optimise your systems but to ensure that they are not misconfigured.
Hence, the following suggestions are limited to simple tasks which
are quick and easy to perform.

Essentially what follows is a series of tips, with only limit
explanation of the background to the issues which they address.
One consequence of this is that you may not understand all of
the terms discussed without referring to the Solaris documentation.
However, to explain everything fully would dramatically increase
the size of this document and would only repeat information which
is readily available in the Solaris documentation set.
Similarly, this paper does not discuss or follow formal performance tuning
processes for the removal of bottlenecks.
Instead it adopts the ``rule of thumb'' approach to the problem.
site managers interested in more detailed performance tuning
information should read

\begin{itemize}

\item {\bf Sun Performance and Tuning} by Adrian Cockcroft (SunSoft Press).
This is the best book on the subject, particularly the first 100 pages.
This is available from SunExpress for under \pounds 30.

\item An earlier version of the above book is available in the form of
two of Sun White Papers entitled {\bf Sun Performance Tuning Overview}
and {\bf Sun Performance Tuning Rules}. These are freely available via
the Starlink SPARC/Solaris operations WWW pages. The rules in the
second paper are being used as the basis of a Starlink performance
monitoring utility which, at the time of writing, is still under
development.

\item {\bf Security, Performance and Accounting Administration} (part
of the Solaris documentation set).

\item {\bf SMCC NFS Server Performance and Tuning Guide} (part of the Solaris
documentation set).

\item {\bf Network Administration Student Guide} (available as part of the
Sun Network Administration Course).

\end{itemize}

The present document refers to Solaris 2.4.

\section{Configuration}

\subsection{Swap space}
How big should your swap file be? The rule of thumb in
the Solaris manuals suggest that swap space should be
the difference between 64Mb and RAM size
i.e. you don't need a swap file if you have 64Mb of RAM or more. For
some of our applications, however, you will require more than 64Mb
of virtual memory so I would err on the side of configuring too
much rather than too little swap space -- disk space is cheap.

I would suggest 64Mb as a {\em minimum} on a heavily-used machine, 32Mb on
lightly-loaded machines (if you have any).

You can determine installed memory and swap usage on a machine with
the commands

\begin{quote}
{\tt
\# /usr/sbin/prtconf | grep Memory

\# /usr/sbin/swap -s
}
\end{quote}

If you do under-configure your swap space, you can always add a swap
file later ``on the fly''. Swap files can be created quickly and
also have the advantage of being removable
if you need disk space temporarily, but swapping to a file is less
efficient than swapping to a swap partition.
An example of when you might temporarily want to create additional
swap space via a swap file is when compiling a program which is
pushing the optimiser to its limits (e.g. {\tt DIPSO}).

For example, to create a 40Mb swap file, execute the following commands:

\begin{quote}
{\tt
\# mkfile 40M /opt/SWAP2\_RLSSP2

\# swap -a /opt/SWAP2\_RLSSP2
}
\end{quote}

You will also need to edit your {\tt /etc/vfstab} file so that the new
swap file is used after each reboot, giving the filename as the {\tt device
to mount} and the {\tt FS type} as swap.

An alternative way to create extra swap space is to use JumpStart and
re-install the system. However, that approach is beyond the scope of
the present discussion.

Do not create a swap file in a file system served from a remote machine.
Processes may be blown away if the remote machine becomes unavailable.

\subsection{X--terminal servers}

How much memory do you need on a machine supporting X--terminals for each
X-terminal supported? This is a commonly asked question but
unfortunately one that does not have a simple answer. It entirely
depends on what your users are doing and in what mode they use their
X-terminals.

For example, some users simply use the X-terminal's built-in {\tt
telnet} program to open a single window on the remote system. This
results in the production of a single process on that system. Generally
this type of user will use this approach since they wish to have
processes running on many different machines at once. The result is a
small load placed on each of several machines, 1--3Mb of RAM on each, rising
as the user starts up real applications.

Others log in via {\tt xdm}, use a window manager on the remote system
rather than the local X-terminal one and start up a number of processes
via their {\tt .xsession file}. Both types of behaviour are acceptable
(although some users automatically start up many unnecessary processes
every time they log in) but clearly the second type of user puts a
greater load on a single system than the first type. The author has observed
typical RAM usage by this type of user of 8--10Mb, again rising to typically
15--20Mb once that user start doing real work. Similar usages have been
seen at other sites.

Please note that these figures may not be applicable at your site.
They are only included here to give you but the most basic indication
of the amount of memory that you might need to support X--terminals.

Clearly, if you have less RAM available, there will be competition
between processes. This will not necessarily cause a performance
problem. Idle processes will have pages removed from physical memory as
other processes require that resource (with attendant overhead) and
usually will not be loaded back on a busy system unless sent a signal
by another process or activated by the user. However, the initial
loading of the image may cause other more worthy processes to start
swapping and hence it is more sociable not to start up processes which
are not going to be used.

\subsection{NFS and Network Loading}

Firstly, access to an NFS served disk is {\em much}, slower than to
a local disk. For example, the author has run the Starlink Benchmarking
suite (SSN/23)
and found elapsed runtimes can increase by a factor of up to {\em five}
when a remote disk served via NFS is used compared to a local disk.
The faster the machine, the greater the impact of the NFS-served disk
bottleneck. This is hardly surprising since NFS read and write rates
of 500 kbytes/second over Ethernet are considered fairly good results,
whereas fast SCSI disks run at 10Mbytes/second.
Furthermore, unnecessary remote disk accesses load the network,
penalizing other network users. Hence you should
strive to minimize the amount of heavy data processing that occurs
over NFS. If your users use local disks, they will get better performance
and they will not degrade the network performance for everyone else.

Try to make this clear to your users in any local documentation and at
SLUG meetings. Try also to bear this in mind when allocating disk space.
Some sites encourage small groups of users to just use a particular
machine and then ensure that their data is local to that machine.
Other sites have dedicated data processing workstations with a DAT drive
and plenty of local disk space. Users can then book these machines when
they have particularly heavy data reduction requirements. You will need
to consider the best solution for your site.

During the transition to Unix, your hardware base has grown incrementally.
Hence, your final configuration may not be optimal. Try to consider if
relocating peripherals such as disks and tape drives might reduce network load.
You may also wish to move certain users' home and data areas to a different
machine.

Our systems fall into the category of ``attribute--drive'' networks
i.e. the access to remote data is mostly random rather than synchronous.
In such a network, it is claimed that a single NFS server should
be able to support
20-25 clients (12-14 if fully active), more if CacheFS is used (see
later).

Ideally you want to avoid serving disks and have local disk access
where possible. In a highly distributed computing environment with many
CPUs and where you are forced to serve a lot of disks via NFS, then for
good performance, an NFS server should be dedicated to the task.  Note
that, a SPARC 2 or SPARC 5 has sufficient processor power for the job
so you do not need to tie up a SPARC 10 or 20 as your NFS server.

NFS runs in the kernel and hence a machine which is busy serving NFS
requests will give {\em very} poor interactive performance. I speak from bitter
experience! Configuring the NFS server to be a mail and print server is
a good way to use up the spare capacity on such a machine. CPU-hungry
theory batch jobs are another obvious way to use up those space CPU cycles,
but not ones requiring a lot of RAM.

NFS servers do not need vast amounts of  memory, not even for a disk
cache - Unix clients will cache commonly used files themselves. 16Mb is
sufficient, plus 16Mb for each ethernet interface. if you use it
as a print server and you use NeWSprint,
you are advised to install an additional 16Mb.
Note that these figures only apply to a dedicated NFS server. Your server
will need {\rm considerably} more memory if it is also supporting users.

If possible, mount your disk read-only to avoid the write-back of file
access times. NFS version 2 has reasonable read performance but can be poor
for writes.
NFS implements synchronous writes on the server. A synchronous NFS write
means that all data must be written to disk before the server can send an
acknowledgement back to the client.
Hence, exporting applications
read-only via NFS is acceptable but you want to avoid writing output from
programs over NFS if possible.

It has been promised that Solaris 2.5 will include NFS version 3. NFS
version 2 implements {\rm synchronous} write on the server i.e.
all data must be written to disk
before the server send an acknowledgement back to the client.  Version
3 will allow servers to cache client writes in memory until the client
requests the data be flushed to stable storage. This should result in a
significant performance improvement at certain installations.  Version 3
also removes file size and network transfer size limitations. It will
also use TCP rather than UDP as its preferred transport.

\subsection{SCSI disks}

If possible, don't put more than 2 heavily loaded disks on the same
SCSI bus. If you do, there will be contention and an increase in
response time for all disks on the bus.

It is advisable (where possible) to separate your ``slow'' SCSI disks
from your faster fast SCSI-2 disks. If you mix the slow and fast
devices on the same bus, the faster disks will have their transfer
rates (usually 10Mb/s) forced down to that of the slow SCSI disks (up to 5Mb/s).
Note, however, that it is O.K. to put a SCSI tape device which is
only used occasionally onto the same bus as a SCSI-2 disk. Performance
of the SCSI-2 disk will only be degraded when the tape drive is
actually in use.

Remember also that fast SCSI-2 devices require active termination
rather than the passive termination used on slow SCSI devices (an active
terminator has one or more voltage regulators to produce the
termination voltage, rather than using resistor voltage dividers, and
reduces noise). If you use a passive terminator, the performance of
your fast SCSI-2 devices will be degraded.

Note that it is possible to buy a SCSI S-bus expansion card for a few
hundred pounds. One could then put slow SCSI disks onto this
extra SCSI bus, ensuring that you don't slow down fast SCSI-2 devices by
putting the slower devices on the same bus.

Try to match the usage of a disk with its specification.  For example,
you should use your fastest disks for scratch space used for intensive
data processing and your slower, larger disks for long term storage
(i.e. users' home areas).  At Jodrell Bank, they have compared 4Gb
Barracuda-4 drives against the 9Gb Elite-9. When the disk is unloaded
(i.e. only one process is doing I/O) then the disks seem to give
similar I/O rates of 5--6 Mb/s, with perhaps the Barracuda-4
average slightly higher than the Elite-9 (6.0 $+/-$ 0.3 versus 5.6 $+/-$
0.3).  However, the Barracuda-4 seems to degrade more gracefully as
activity increases, actually appearing to sustain a slightly higher
aggregate rate for multiple I/O tests (e.g. 2 $\times$ 3.5 Mb/s)

Finally, if you have any kind of large filesystem or database which
will be accessed by many processes or users, it's best to spread it
over multiple spindles; for 8 Gb you can effectively double the
available I/O capacity by using 2 Barracuda-4 drives rather than a
single Elite-9.


\subsection{Optional filesystems}

\subsubsection{CacheFS}

It is worth mentioning Cachefs since this is relatively new, appearing
for the first time in Solaris 2.3. CacheFS is a caching mechanism
which improves performance by caching a slow file system,
such as a remote NFS-served file system, on another faster file system
such as a local disk. Thus in an NFS environment,
CacheFS reduces
network traffic and server load. The benefits of caching really show up on
busy networks with loaded servers, but a single client may not
see an improvement on a lightly loaded network with a powerful server.
Hence, although bundled with the operating system, CacheFS is not
enabled by default.

Details of how to set up a cache can be found in
the Solaris 2.x documentation in {\bf File System Administration}.
Note, however that only ``read-mostly'' file systems (e.g. {\tt
/usr/openwin, /usr/local} \& {\tt /usr/share/man}) are good candidates
for caching. File systems where data is generally read only once (e.g.
{\tt /var/mail}) will not benefit from caching and indeed you will get a
degradation in performance.

CacheFS can also be used to improve performance for file
systems on slow media, such as CD-ROM.

\subsubsection{tmpfs}

{\tt tmpfs} is another filesystem type. It maps a disk resident directory
(such as {\tt /tmp}) to virtual memory. If RAM is plentiful, read and write
access times to the directory using {\tt tmpfs} are significantly decreased,
since disk device latencies are avoided. However, if memory is low, data
residing in {\tt tmpfs} is swapped out to disk, and there is no advantage
over a disk resident filesystem.

{\tt tmpfs} should only be used for temporary data since a reboot or
crash will destroy the data in such a filesystem.

Note that the use of {\tt tmpfs} for {\tt /tmp} is enabled by default.
On a machine with a small amount of RAM on which {\tt /tmp} is filled
with larger files, you may wish to use a disk--based filesystem
for {\tt /tmp} rather than {\tt tmpfs}.

The following real-life example at a Starlink node shows the benefits
of RAM-based filesystems.

UCL found that when one user is running an echelle data reduction job,
this takes a fair while and the machine gives poor response
to all the other users. For example, they could type faster than the
machine could echo the characters. It appeared that the echelle process
was spending a lot of time in WAIT states while accessing data on disk.

UCL set up a local memory file system (DEC Unix equivalent of {\tt tmpfs})
and put the data in there. They found that the turnaround time for the
job decreased by a factor of 3 compared with using a local disk
(and a factor of 4.5 compared with a using a remote disk). Furthermore,
the general performance of the machine for other users was ``better''
when using the memory file system.

\section{Monitoring}

Monitoring your system {\em may} reveal a bottleneck which if removed will
improve performance.

The output from the system monitoring tools that come with Solaris
(e.g. {\tt vmstat, ps, netstat}, etc) is awkward to understand but can
give useful information, once you know what to look for.

More user-friendly tools for system monitoring, such as {\tt top},
and network monitoring tools such as {\tt intermon} \& {\tt ethermon}
are recommended. These are available via the Starlink SPARC/Solaris
operations pages. A monitoring utility based on Adrian Cockcroft's
{\bf Sun Performance Tuning Rules} is also in the pipeline.
These are available via the Starlink SPARC/Solaris operations WWW
pages.

The following basic monitoring tips use only the tools which come with
Solaris 2.x.


\subsection{CPU}

A CPU that is heavily loaded must share execution cycles amongst all
processes and each process waits in a queue for execution. Long
execution wait times may affect the ability of the process to
provide adequate response time, thus introducing a bottleneck.

You can monitor CPU usage with {\tt sar} (system activity resourcer)

\begin{quote}
{\tt
\# sar -u 1 10
}
\end{quote}

A high percentage of {\tt usr} CPU {\em might} indicate a problem or it might
simply show that the system is being fully utilized. In order to
determine if you have a CPU bottleneck, you also need to monitor
the CPU queue length.

\begin{quote}
{\tt
\# sar -q 1 10
}
\end{quote}

This shows how many processes are queued for execution.
{\tt runq-sz} (number of process threads in memory awaiting execution)
should not be consistently higher than 4 per CPU. Values above this
indicate that the system is CPU-bound. {\tt \%runocc} (\% of time that
the run queue was occupied) should be low since this indicates a good
distribution of processing time for each queued process.

You can monitor both together with

\begin{quote}
{\tt
\# sar -uq 1 10
}
\end{quote}


Also try

\begin{quote}
{\tt
\# vmstat 5
}
\end{quote}

If the {\tt procs r} entry is more than 4 times the number of CPUs, then
processes are having to wait for a slice of CPU and interactive response
is being degraded. This is an indication of a CPU bottleneck.
The {\tt procs b} entry indicates the number of processes which cannot
be executed because they are waiting for disk, terminal or network I/O
operations to complete. A large figure here indicates that the CPU is
not the bottleneck and that you should examine the health of your
other subsystems.

Continuous low values (less than 10) appearing in the {\tt id} (idle)
field of the {\tt cpu} column indicate that the CPU is very busy,
but as indicated above, not necessarily a bottleneck.




\subsection{Memory}

If a CPU accesses data directly from memory, rather than from disk,
performance is greatly increased since memory access is
{\em several orders of magnitude} faster than disk access - memory speeds
are measured in nanoseconds whereas disk speeds are measured in milliseconds.
Thus low memory availability
may result in increased disk access and a dramatic reduction in performance.
When you buy new systems, do not economize on RAM. The default systems
shipped are usually under-configured in this area to keep unit costs down.

The command
\begin{quote}
{\tt
\# sar -r 1 10
}
\end{quote}

will report unused memory pages ({\tt freemem}) and disk-based swap space
({\tt freeswap})
in 4K pages and 512K blocks respectively. {\tt freemem} is the number of
pages of RAM that are immediately ready to be used when a process starts
up or needs more memory. The absolute value of {\tt freemem} has no
useful meaning on its own. Its value relative to some other kernel
thresholds is what is important. However, continuous low (less than
6\% of installed memory) {\tt freemem} values may indicate a shortage
of available memory and a potential bottleneck.

\begin{quote}
{\tt
\# sar -g 10 2
}
\end{quote}

The {\tt pgscan/s} column indicates how busy the page daemon is. This
daemon only
runs when available memory is low. Continuous values above 20 per second
indicate a memory shortfall. This information can also be obtained with

\begin{quote}
{\tt
\# vmstat 5
}
\end{quote}

Here, the {\tt sr} column indicates how busy the page daemon is and again
continuous values above 20 per second indicate a memory shortfall.

\subsection{I/O system}

Improvements in CPU performance have far outstripped performance of
disks. Modern disks are considerably larger but not dramatically faster
then those we were using a few years ago. Hence, the I/O system is a prime
candidate for being the bottleneck in your system.

Disk bottlenecks occur when a system cannot read and/or write data to
disk faster enough. This is a common problem on NFS servers supporting
clients that request data that is spread across different physical locations
on the server disk filesystems.

How can you tell if a disk is overloaded? Run the command

\begin{quote}
{\tt
\# iostat -x 30
}
\end{quote}

Disks with are more than 30\% busy ({\tt\%b} entry) and which have an
I/O service time ({\tt svc\_t} entry) of more than 50ms are considered to
be slow. The server time is the time between a user process issuing, for
example, a read and the read completing. Hence, it is linked to
response time.

{\tt iostat} can also be used to measure how well disk loads are
spread across each disk on a server system.


\begin{quote}
{\tt
\# iostat -D 5
}
\end{quote}

The first line of output summarizes I/O statistics since the system first
booted. Each subsequent line represents activity for the specified interval
(here 5 seconds). Compare the {\tt util} fields for each disk and look for
a disk which is significantly more utilized than the others. If this is the
case, see if it is possible to move parts of your filesystem to another
disk to spread the load. If the loaded disk contains your swap areas,
then consider creating additional swap areas (partitions or files) on
one or more of the other disks.

\begin{quote}
{\tt
\# sar -d 10 2
}
\end{quote}

will also indicate unbalanced disk loads.


\begin{quote}
{\tt
\# sar -b 5 10
}
\end{quote}

gives buffer cache information. The buffer cache is {\em not} used to
cache all UFS disk I/O as it is in some flavours of Unix. In Solaris 2
it is used to cache header information, not data.

{\tt \%rcache} should be greater than
90 and {\tt \%wcache} should be greater than 65. If they are lower than this
you may need a larger buffer cache. The {\tt bufhwm} parameter in {\tt /etc/system}
can be used to specify the maximum size for the buffer cache (see Section 4).

\subsection{Network}

Unless you have recently upgraded you LAN, there is almost certainly room
for improvement in this area at your site.
The most basic measurement you can make is to determine the collision rate
for a single host. Use the command

\begin{quote}
{\tt
\# netstat -i 10
}
\end{quote}

The collision rate for a single host is {\tt colls*100/(output packets)}.
Network collision rates are computed by
collecting the network statistics by running this command on all
active machines, and averaging the network load by adding the total number
of collisions and dividing by the total number of {\tt output} packets.

Any figure over 5\%
indicates a heavily used (not necessarily overloaded) ethernet. Collisions
are a natural part of the functioning of Ethernet. However, too many
collisions reduces throughput and increases response time for your users.
Collision rates higher than 10\% signify an over-loaded network that
should be considered for segmentation.
Note that these figures relates to {\em collisions} and not {\rm load}.
Ethernet runs efficiently with loads of up to 40\%.
If you believe that your network is overloaded, contact the author to
discuss possible remedies.

You should also be aware of the {\tt snoop} program which allows you to
captures packets from the network and display their contents. This can
be useful for troubleshooting a network problem since it can be used
to locate the source and destination of many network anomalies such as
NFS retransmissions. However, {\tt snoop} can give you huge amounts of
data so it is important to use the appropriate qualifiers. For example,

\begin{quote}
{\tt
\# snoop -s 120 between rlssp0 rlssp2
}
\end{quote}

will only capture the first 120 bytes of a packet, which contains interesting
things such as the packet header, for packets between machines {\tt rlssp0}
and {\tt rlssp2}.

\subsection{NFS}

The {\tt nfsstat} command is useful for determining NFS client-server
workloads over a period of time.

\begin{quote}
{\tt
\# nfsstat -rc
}
\end{quote}

The {\tt calls} field displays the number of NFS operations that the server
serviced or a client requested. The {\tt badxid} field shows how many times
a client received a duplicate acknowledgement for a single request. The
{\tt retrans} field indicates how many times the client retransmitted
a request after not receiving an acknowledgement before the time period
expired.

If the {\tt badxid} and {\tt retrans} values are close, the server is not
responding to the client quickly enough. Optimizing server performance
should improve network performance as well, by reducing client retransmissions.

The client's {\em perceived} performance may improve by increasing the
time-out value ({\tt timeo}) option in the client's file system file
({\tt /etc/vfstab}), table or map to 25,50,100 \& 200.  Network
response will not improve but the client will experience fewer
timeouts, retransmissions and subsequent error messages.
Wait one day
between modifications and check to see if the number of timeouts is
decreasing.

If the {\tt badxid} value is much less than {\tt retrans} or 0, then
the network itself or the network interface on the server is suspect.

If the ratio of retransmissions to total calls {\tt retrans/calls*100)}
is greater than 5\%, the retransmission of rate it excessively high.
This may indicate that the client is accessing a server located in a
different subnet or network segment.  The intermediate routers or
bridges may be dropping the client's datagrams before they reach the
destination server, leading to excessive retransmissions.  NFS tends to
be slow when used across subnets since it uses UDP which (unlike TCP)
has no provision for adjusting data rate transfer rates to match the
packet switching capabilities of intermediate systems that route data
between client and server. Use the {\tt retrans} and {\tt timeo} NFS
mount options to compensate.  {\tt retrans} sets the number of NFS
retransmissions (the default is 5) and {\tt timeo} sets the NFS timeout
(the default is 11 (tenths of seconds)).  Network load will be reduced
due to less NFS retransmissions generated by the client.

To better
accommodate the datagram size handling capabilities of the intermediate
nodes, the retransmitted NFS message size can be decreased.  The read
size and write size are measured with the {\tt nfsstat} command
\begin{quote}
{\tt
\# nfsstat -m
}
\end{quote}

The read and write sizes can be adjusted in the client's {\tt /etc/vfstab}
(or equivalent map/table). 8192 is the maximum size. Try 2048 first.

The above command also shows NFS operation response times on a client.

Finally, the command

\begin{quote}
{\tt
\# netstat -s | fgrep udpInOverflows
}
\end{quote}

may be used to measure NFS server efficiency in scheduling and servicing
client requests. Non-zero values appearing in the {\tt udpInOverflows}
field indicate a shortage of server {\tt nfsd}s. Client performance may
suffer since there are not enough daemons running on the server to service
each client request. Adjust the number of {\tt nfsd}s spawned at boot
time in the system startup script {\tt /etc/rc3.d/S15nfs.server}. The
default is 8 usually (according to the man pages) and this may not be enough.
To cope with bursts of NFS traffic, a large number of {\tt nfsd}s
should be configured. If you configure too many, some may not be used, but
it is unlikely that there will be any adverse side effects.
The following table gives {\em guidelines} to help you size the number of
{\tt nfsd}s. A rule of thumb is two per active client, or the number in the
table below, whichever is greater.

\begin{table}[h]
\begin{center}
\begin{tabular}{||l|c||}        \hline
Desktop system which is & 4   \\
both a client and a server &  \\
\hline
Small dedicated server & 8-32 \\
\hline
Large NFS and compute server & 16-32 \\
\hline
Large NFS--only server & 32--64 \\
\hline
\end{tabular}
\end{center}
\end{table}

An NFS client will attempt to avoid sending requests for data if the data is
locally available in memory, or cache. Cache consistency checks verify that
the client only uses data stored in the cache if it is identical to the
data available on the server. You can avoid unnecessary cache consistency
checks (and hence reduce network traffic and server load) for file
systems that seldom change (e.g. CD-ROM catalogues, software disks. etc)
by increasing the {\tt actimeo} option in the client's {\tt /etc/vfstab}
(or equivalent table/map) to several minutes (e.g. 900 seconds). The
default is 30 seconds. Do not do this for evolving filesystems.

The book {\bf SMCC NFS Server Performance and Tuning guide} gives detailed
instructions on how to troubleshoot a poorly performing NFS system.
\section{Tuning}

There is very little tuning that can be done under Solaris 2. Most of
the tuning you can do really relates to configuration and has already
been discussed.

It is not
necessary to configure the kernel since it is dynamically linked at
runtime i.e. the kernel is no longer compiled. Hence, there is no
kernel configuration file in which to change the default values
of kernel parameters. This is now achieved by creating entries in
a system configuration file named {\tt /etc/system}.

Most of the tables that previously were statically allocated at boot time are
now dynamically allocated i.e. they will grow as needed up to a limit. This
limit is the value of the parameter that controls the size of the table.
Thus the parameters that used to indicate the size of the tables to be
allocated at boot time now indicate the maximum size that these tables may
reach.

The most significant parameter you can adjust in {\tt /etc/system} is {\tt
maxusers}. {\tt maxusers} is a single variable from which many system
parameters are derived (e.g. user process limit). The name is
historical -- it does not indicate the maximum number of users which
the system should be able to support. The default value of {\tt
maxusers} depends on the amount of installed RAM (by default, Maxusers
will be set to 8 on a 16Mb system, 32 on a 32Mb system, 40 on a 64Mb
system and 64 on a
128Mb system) but this default can be modified from within {\tt
/etc/system} with a command of the form

\begin{quote}
{\tt
set maxusers=40
}
\end{quote}

You may need to increase the value of {\tt maxusers} on a low--RAM machine
if you hit the maximum process limit. However, in most circumstances you
will not need to modify {\tt maxusers} ever.

If you have a larger server which many people log into with X--terminals,
then you may need to increase the pseudo-tty parameter {\tt pt\_cnt}
in {\tt /etc/system}. This has had to be done, for example, on the
large server at Leicester.

It should not be necessary to modify kernel parameters relating to paging
under Solaris 2.4.

See the man page {\tt system (4)} for full details of this file's
format but bear in mind that arbitrary tuning can create an unstable
machine or even an unbootable machine. Be very careful with {\tt set}
commands in {\tt /etc/system} since these cause automatic patching of
the kernel.

Incidently, you can kid your machine into thinking that it has less
memory installed than it has by setting the variable {\tt physmem} in
{\tt /etc/system}. The only possible reason that you might have for doing this
is in order to check that a heavy application which your users might be
developing will run on a system with less memory than your own.

\subsection{Multi-user systems}

Performance can be a problem on multi-user systems since under Unix
it is easy for a single user to grap a large fraction of
CPU, RAM or swap space. There is a school of thought that suggests
putting
greedy users onto a machine of their own to avoid them degrading the
service for others. However, this is not always possible.

It is possible to configure your system so that a single user cannot
grab most of one resource. However, the
process is rather hairy. One might expect that this can be done using
parameters in the system configuration file {\tt /etc/system}. However,
while this is possible under some flavours of SVR4, it cannot be done
with Solaris 2.
Instead, it is necessary to patch the kernel using
the program {\tt adb} (general-purpose debugger). This is very dangerous
and hence you should only consider undertaking this endeavour if you
have a serious problem with greedy users at your site which cannot
be resolved through isolation of those users or via local rules regarding
the usage of your machines. Instructions for the desperate can be found in
{\bf Sun Performance and Tuning}, pages 221--223.

The current system--wide resource limits can be found using the command

\begin{quote}
{\tt
\# sysdef -i
}
\end{quote}

Look under the section entitled Process Resource Limit Tunables.

An alternative to setting a system--wide limit on resource usage
is to use the {\tt limit} command. This is a shell built-in
function to set limitations on the system resources available to
the current shell and it's descendants.
It is possible to use this command in one of your global login files
to set limits but knowledgeable users can use the same command to
unset them again. Full details can be found in the {\tt limit}
man page.

The current process resource limits can be found using the {\tt limit}
command without qualifiers.

\end{document}
