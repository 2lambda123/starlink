\documentstyle[11pt,epsf,twoside]{article}
\pagestyle{myheadings}

% -----------------------------------------------------------------------------
% ? Document identification
\newcommand{\stardoccategory}  {Starlink User Note}
\newcommand{\stardocinitials}  {SUN}
\newcommand{\stardocsource}    {sun95.11}
\newcommand{\stardocnumber}    {95.11}
\newcommand{\stardocauthors}   {Malcolm J. Currie}
\newcommand{\stardocdate}      {2 June 1998}
\newcommand{\stardoctitle}     {KAPPA --- Kernel Application Package}
\newcommand{\stardocversion}   {0.11}
\newcommand{\stardocmanual}    {User's Guide}
% ? End of document identification
% -----------------------------------------------------------------------------

\newcommand{\stardocname}{\stardocinitials /\stardocnumber}
\markboth{\stardocname}{\stardocname}
%\markright{\stardocname}
\setlength{\textwidth}{160mm}
\setlength{\textheight}{230mm}
\setlength{\topmargin}{-2mm}
\setlength{\oddsidemargin}{0mm}
\setlength{\evensidemargin}{0mm}
\setlength{\parindent}{0mm}
\setlength{\parskip}{\medskipamount}
\setlength{\unitlength}{1mm}

% -----------------------------------------------------------------------------
%  Hypertext definitions.
%  ======================
%  These are used by the LaTeX2HTML translator in conjunction with star2html.

%  Comment.sty: version 2.0, 19 June 1992
%  Selectively in/exclude pieces of text.
%
%  Author
%    Victor Eijkhout                                      <eijkhout@cs.utk.edu>
%    Department of Computer Science
%    University Tennessee at Knoxville
%    104 Ayres Hall
%    Knoxville, TN 37996
%    USA

%  Do not remove the %\begin{rawtex} and %\end{rawtex} lines (used by 
%  star2html to signify raw TeX that latex2html cannot process).
%\begin{rawtex}
\makeatletter
\def\makeinnocent#1{\catcode`#1=12 }
\def\csarg#1#2{\expandafter#1\csname#2\endcsname}

\def\ThrowAwayComment#1{\begingroup
    \def\CurrentComment{#1}%
    \let\do\makeinnocent \dospecials
    \makeinnocent\^^L% and whatever other special cases
    \endlinechar`\^^M \catcode`\^^M=12 \xComment}
{\catcode`\^^M=12 \endlinechar=-1 %
 \gdef\xComment#1^^M{\def\test{#1}
      \csarg\ifx{PlainEnd\CurrentComment Test}\test
          \let\html@next\endgroup
      \else \csarg\ifx{LaLaEnd\CurrentComment Test}\test
            \edef\html@next{\endgroup\noexpand\end{\CurrentComment}}
      \else \let\html@next\xComment
      \fi \fi \html@next}
}
\makeatother

\def\includecomment
 #1{\expandafter\def\csname#1\endcsname{}%
    \expandafter\def\csname end#1\endcsname{}}
\def\excludecomment
 #1{\expandafter\def\csname#1\endcsname{\ThrowAwayComment{#1}}%
    {\escapechar=-1\relax
     \csarg\xdef{PlainEnd#1Test}{\string\\end#1}%
     \csarg\xdef{LaLaEnd#1Test}{\string\\end\string\{#1\string\}}%
    }}

%  Define environments that ignore their contents.
\excludecomment{comment}
\excludecomment{rawhtml}
\excludecomment{htmlonly}
%\end{rawtex}

%  Hypertext commands etc. This is a condensed version of the html.sty
%  file supplied with LaTeX2HTML by: Nikos Drakos <nikos@cbl.leeds.ac.uk> &
%  Jelle van Zeijl <jvzeijl@isou17.estec.esa.nl>. The LaTeX2HTML documentation
%  should be consulted about all commands (and the environments defined above)
%  except \xref and \xlabel which are Starlink specific.

\newcommand{\htmladdnormallinkfoot}[2]{#1\footnote{#2}}
\newcommand{\htmladdnormallink}[2]{#1}
\newcommand{\htmladdimg}[1]{}
\newenvironment{latexonly}{}{}
\newcommand{\hyperref}[4]{#2\ref{#4}#3}
\newcommand{\htmlref}[2]{#1}
\newcommand{\htmlimage}[1]{}
\newcommand{\htmladdtonavigation}[1]{}

%  Starlink cross-references and labels.
\newcommand{\xref}[3]{#1}
\newcommand{\xlabel}[1]{}

%  LaTeX2HTML symbol.
\newcommand{\latextohtml}{{\bf LaTeX}{2}{\tt{HTML}}}

%  Define command to re-centre underscore for Latex and leave as normal
%  for HTML (severe problems with \_ in tabbing environments and \_\_
%  generally otherwise).
\newcommand{\latex}[1]{#1}
\newcommand{\setunderscore}{\renewcommand{\_}{{\tt\symbol{95}}}}
\latex{\setunderscore}

%  Redefine the \tableofcontents command. This procrastination is necessary 
%  to stop the automatic creation of a second table of contents page
%  by latex2html.
\newcommand{\latexonlytoc}[0]{\tableofcontents}

% -----------------------------------------------------------------------------
%  Debugging.
%  =========
%  Remove % on the following to debug links in the HTML version using Latex.

% \newcommand{\hotlink}[2]{\fbox{\begin{tabular}[t]{@{}c@{}}#1\\\hline{\footnotesize #2}\end{tabular}}}
% \renewcommand{\htmladdnormallinkfoot}[2]{\hotlink{#1}{#2}}
% \renewcommand{\htmladdnormallink}[2]{\hotlink{#1}{#2}}
% \renewcommand{\hyperref}[4]{\hotlink{#1}{\S\ref{#4}}}
% \renewcommand{\htmlref}[2]{\hotlink{#1}{\S\ref{#2}}}
% \renewcommand{\xref}[3]{\hotlink{#1}{#2 -- #3}}
% -----------------------------------------------------------------------------
% ? Document-specific \newcommand or \newenvironment commands.

% Also define the html equivalents.

% degrees symbol
\newcommand{\dgs}{\hbox{$^\circ$}} 
\begin{htmlonly}
\renewcommand{\dgs}{{\rawhtml &deg;}} 
\end{htmlonly}

% arcminute symbol
\newcommand{\arcm}{\hbox{$^\prime$}} 
\begin{htmlonly}
\renewcommand{\arcm}{{\rawhtml &acute;}} 
\end{htmlonly}

% arcsec symbol
\newcommand{\arcsec}{\arcm\hskip -0.1em\arcm}
\begin{htmlonly}
\renewcommand{\arcsec}{{\rawhtml &quot;}} 
\end{htmlonly}

% hours symbol
\newcommand{\hr}{\hbox{$^{\rm h}$}}
\begin{htmlonly}
\renewcommand{\hr}{~hours}
\end{htmlonly}

% minutes symbol
\newcommand{\mn}{\hbox{$^{\rm m}$}}
\begin{htmlonly}
\renewcommand{\mn}{~minutes}
\end{htmlonly}

% seconds symbol
\newcommand{\scn}{\hbox{$^{\rm s}$}}
\begin{htmlonly}
\renewcommand{\scn}{~seconds}
\end{htmlonly}

% decimal-minutes symbol
\newcommand{\um}{\hskip-0.3em\hbox{$^{\rm m}$}\hskip-0.08em}
\begin{htmlonly}
\renewcommand{\um}{\mn} 
\end{htmlonly}

% decimal-degree symbol
\newcommand{\udeg}{\hskip-0.3em\dgs\hskip-0.08em}
\begin{htmlonly}
\renewcommand{\udeg}{{\rawhtml &deg;}} 
\end{htmlonly}

% decimal-second symbol
\newcommand{\us}{\hskip-0.27em\hbox{$^{\rm s}$}\hskip-0.06em}  
\begin{htmlonly}
\renewcommand{\us}{\scn} 
\end{htmlonly}

% decimal-arcminute symbol
\newcommand{\uarcm}{\hskip-0.28em\arcm\hskip-0.04em}  
\begin{htmlonly}
\renewcommand{\uarcm}{{\rawhtml &acute;}} 
\end{htmlonly}

% decimal-arcsecond symbol
\newcommand{\uarcs}{\hskip-0.27em\arcsec\hskip-0.02em}  
\begin{htmlonly}
\renewcommand{\uarcs}{{\rawhtml &quot;}}
\end{htmlonly}

% centre an asterisk
\newcommand{\lsk}{\raisebox{-0.4ex}{\rm *}}

% conditional text
\newcommand{\latexelsehtml}[2]{#1}
\begin{htmlonly}
  \renewcommand{\latexelsehtml}[2]{#2}
\end{htmlonly}

\hyphenation{which-ever}

% A kind of list item, like description, but with an easily adjustable
% item separation.  Note that the paragraph and fount-size change are
% needed to make the revised \baselinestretch work.
\newlength{\menuwidth}
\newlength{\menuindent}
\newcommand{\menuitem}[2]
  {{\bf #1} \settowidth{\menuwidth}{{\bf #1} }
  \setlength{\menuindent}{-0.5em}
  \addtolength{\menuwidth}{-2\menuwidth}
  \addtolength{\menuwidth}{\textwidth}
  \addtolength{\menuwidth}{\menuindent}
  \hspace{\menuindent}\parbox[t]{\menuwidth}{
  \renewcommand{\baselinestretch}{0.75}\small
  #2 \par \vspace{0.6ex}
  \renewcommand{\baselinestretch}{1.0}\normalsize} \\ }
\begin{htmlonly}
\newcommand{\menuitem}[2]
  {\item [\htmlref{#1}{#1}] #2}
\end{htmlonly}

\newcommand{\classitem}[1]{\item [\htmlref{#1}{#1}]}

% an environment for references (for the SST sstdiytopic command).
\newenvironment{refs}{\vspace{-4ex} % normally 3ex
                      \begin{list}{}{\setlength{\topsep}{0mm}
                                     \setlength{\partopsep}{0mm}
                                     \setlength{\itemsep}{0mm}
                                     \setlength{\parsep}{0mm}
                                     \setlength{\leftmargin}{1.5em}
                                     \setlength{\itemindent}{-\leftmargin}
                                     \setlength{\labelsep}{0mm}
                                     \setlength{\labelwidth}{0mm}}
                    }{\end{list}}

% Lines for breaking up Appendices A and B.
\newcommand{\jrule}{\noindent\rule{\textwidth}{0.45mm}}
\newcommand{\krule}{\vspace*{-1.5ex}
                    \item [\rm \rule{\textwidth}{0.15mm}]}

% Shorthands for hypertext links.
% -------------------------------
\newcommand{\AGIref}{\xref{AGI}{sun48}{}}
\newcommand{\ARDref}{\xref{ARD}{sun183}{}}
\newcommand{\ASTERIXref}{\xref{{\footnotesize ASTERIX}}{sun98}{}}
\newcommand{\CCDPACKref}{\xref{{\footnotesize CCDPACK}}{sun139}{}}
\newcommand{\CGSDRref}{\xref{{\footnotesize CGS4DR}}{sun27}{}}
\newcommand{\CONVERTref}{\xref{{\footnotesize CONVERT}}{sun55}{}}
\newcommand{\ECHOMOPref}{\xref{{\footnotesize ECHOMOP}}{sun152}{}}
\newcommand{\ESPref}{\xref{{\footnotesize ESP}}{sun180}{}}
\newcommand{\Figaroref}{\xref{{\footnotesize FIGARO}}{sun86}{}}
\newcommand{\FITSref}{\htmladdnormallink{FITS}{http://www.gsfc.nasa.gov/astro/fits/fits\_{}home.html}}
\newcommand{\GAIAref}{\xref{GAIA}{sun214}{}}
\newcommand{\GKSref}{\xref{GKS}{sun83}{}}
\newcommand{\GWMref}{\xref{GWM}{sun130}{}}
\newcommand{\HDSref}{\xref{HDS}{sun92}{}}
\newcommand{\HDSTRACEref}{\xref{{\footnotesize HDSTRACE}}{sun102}{}}
\newcommand{\ICLref}{\xref{{\footnotesize ICL}}{sg5}{}}
\newcommand{\IDIref}{\xref{IDI}{sun65}{}}
\newcommand{\IRCAMPACKref}{\xref{{\footnotesize IRCAMPACK}}{sun177}{}}
\newcommand{\IRAFref}{\htmladdnormallink{{\footnotesize IRAF}}{http://iraf.noao.edu/iraf-homepage.html}}
\newcommand{\IRASref}{\xref{{\footnotesize IRAS90}}{sun163}{}}
\newcommand{\JCMTDRref}{\xref{{\footnotesize JCMTDR}}{sun132}{}}
\newcommand{\NDFref}[1]{\htmlref{#1}{ap:NDFformat}}
\newcommand{\NDFextref}[1]{\xref{#1}{sun33}{}}
\newcommand{\PDAref}{\xref{PDA}{sun194}{}}
\newcommand{\PGPLOTref}{\htmladdnormallink{PGPLOT}{http://astro.caltech.edu/\~{}tjp/pgplot/}}
\newcommand{\PHOTOMref}{\xref{{\footnotesize PHOTOM}}{sun45}{}}
\newcommand{\PISAref}{\xref{{\footnotesize PISA}}{sun109}{}}
\newcommand{\PONGOref}{\xref{{\footnotesize PONGO}}{sun137}{}}
\newcommand{\PSMERGEref}{\xref{{\footnotesize PSMERGE}}{sun164}{}}
\newcommand{\SGSref}{\xref{SGS}{sun85}{}}
\newcommand{\SPECDREref}{\xref{{\footnotesize SPECDRE}}{sun140}{}}
\newcommand{\TSPref}{\xref{{\footnotesize TSP}}{sun66}{}}
\newcommand{\TWODSPECref}{\xref{{\footnotesize TWODSPEC}}{sun16}{}}


% SST definitions
% ---------------

% +
%  Name:
%     SST.TEX

%  Purpose:
%     Define LaTeX commands for laying out Starlink routine descriptions.

%  Language:
%     LaTeX

%  Type of Module:
%     LaTeX data file.

%  Description:
%     This file defines LaTeX commands which allow routine documentation
%     produced by the SST application PROLAT to be processed by LaTeX and
%     by LaTeX2html. The contents of this file should be included in the
%     source prior to any statements that make of the sst commands.

%  Notes:
%     The commands defined in the style file html.sty provided with LaTeX2html 
%     are used. These should either be made available by using the appropriate
%     sun.tex (with hypertext extensions) or by putting the file html.sty 
%     on your TEXINPUTS path (and including the name as part of the  
%     documentstyle declaration).

%  Authors:
%     RFWS: R.F. Warren-Smith (STARLINK)
%     PDRAPER: P.W. Draper (Starlink - Durham University)
%     MJC: Malcolm J. Currie (STARLINK)

%  History:
%     10-SEP-1990 (RFWS):
%        Original version.
%     10-SEP-1990 (RFWS):
%        Added the implementation status section.
%     12-SEP-1990 (RFWS):
%        Added support for the usage section and adjusted various spacings.
%     8-DEC-1994 (PDRAPER):
%        Added support for simplified formatting using LaTeX2html.
%     1995 October 4 (MJC):
%        Added goodbreaks and pagebreak[3] in various places to improve
%        pages breaking before headings, not immediately after.
%        Corrected banner width.
%     {enter_further_changes_here}

%  Bugs:
%     {note_any_bugs_here}

% -

%  Define length variables.
\newlength{\sstbannerlength}
\newlength{\sstcaptionlength}
\newlength{\sstexampleslength}
\newlength{\sstexampleswidth}

%  Define a \tt font of the required size.
\newfont{\ssttt}{cmtt10 scaled 1095}

%  Define a command to produce a routine header, including its name,
%  a purpose description and the rest of the routine's documentation.
\newcommand{\sstroutine}[3]{
   \goodbreak
   \markboth{{\stardocname}~ --- #1}{{\stardocname}~ --- #1}
   \rule{\textwidth}{0.5mm}
   \vspace{-7ex}
   \newline
   \settowidth{\sstbannerlength}{{\Large {\bf #1}}}
   \setlength{\sstcaptionlength}{\textwidth}
   \setlength{\sstexampleslength}{\textwidth}
   \addtolength{\sstbannerlength}{0.5em}
   \addtolength{\sstcaptionlength}{-2.0\sstbannerlength}
   \addtolength{\sstcaptionlength}{-4.9pt}
   \settowidth{\sstexampleswidth}{{\bf Examples:}}
   \addtolength{\sstexampleslength}{-\sstexampleswidth}
   \parbox[t]{\sstbannerlength}{\flushleft{\Large {\bf #1}}}
   \parbox[t]{\sstcaptionlength}{\center{\Large #2}}
   \parbox[t]{\sstbannerlength}{\flushright{\Large {\bf #1}}}
   \begin{description}
      #3
   \end{description}
}

%  Format the description section.
\newcommand{\sstdescription}[1]{\item[Description:] #1}

%  Format the usage section.
\newcommand{\sstusage}[1]{\pagebreak[3] \item[Usage:] \mbox{} \\[1.3ex] {\ssttt #1}}

%  Format the invocation section.
\newcommand{\sstinvocation}[1]{\sloppy \item[Invocation:]\hspace{0.4em}{\tt #1}}

%  Format the arguments section.
\newcommand{\sstarguments}[1]{
   \item[Arguments:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Format the returned value section (for a function).
\newcommand{\sstreturnedvalue}[1]{
   \item[Returned Value:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Format the parameters section (for an application).
\newcommand{\sstparameters}[1]{
   \goodbreak 
   \item[Parameters:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Format the output results parameters section (for an application).
\newcommand{\sstresparameters}[1]{
   \goodbreak 
   \item[Results Parameters:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Format the graphics style parameters section (for an application).
\newcommand{\sstgraphparameters}[1]{
   \goodbreak 
   \item[Graphics-style Parameters:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Format the examples section.
\newcommand{\sstexamples}[1]{
   \goodbreak
   \item[Examples:] \mbox{} \\
   \vspace{-3.5ex}
   \begin{description}
      #1
   \end{description}
}

%  Define the format of a subsection in a normal section.
\newcommand{\sstsubsection}[1]{ \item[{#1}] \mbox{} \\}

%  Define the format of a subsection in the examples section.
%\newcommand{\sstexamplesubsection}[2]{\sloppy
%\item[\parbox{\sstexampleslength}{\ssttt #1}] \mbox{} \\ #2 }
\newcommand{\sstexamplesubsection}[2]{\sloppy \item{\ssttt #1} \mbox{} \\ #2 }

%  Define the format of a long-example subsection in the examples section.
%\newcommand{\sstlongexamplesubsection}[3]{\sloppy
%\item[\ssttt \hspace{-0.5em}#1] {\ssttt #2} \mbox{} \\ #3}
\newcommand{\sstlongexamplesubsection}[3]{\sloppy \item{\ssttt #1} {\ssttt #2} \mbox{} \\ #3}

%  Format the notes section.
\newcommand{\sstnotes}[1]{\pagebreak[3] \item[Notes:] \mbox{} \\[1.3ex] #1}

%  Provide a general-purpose format for additional (DIY) sections.
\newcommand{\sstdiytopic}[2]{\goodbreak \item[{\hspace{-0.35em}#1\hspace{-0.35em}:}] \mbox{} \\[1.3ex] #2}

%  Format the implementation status section.
\newcommand{\sstimplementationstatus}[1]{
   \pagebreak[3] \item[{Implementation Status:}] \mbox{} \\[1.3ex] #1}

%  Format the bugs section.
\newcommand{\sstbugs}[1]{\item[Bugs:] #1}

%  Specify a variant of the itemize environment where the top separation
%  is reduced.  It is needed because a \vspace is ignored in the
%  \sstitemlist command.
\newenvironment{sstitemize}{%
  \vspace{-4.3ex}\begin{itemize}}{\end{itemize}}

%  Format a list of items while in paragraph mode.
\newcommand{\sstitemlist}[1]{
  \mbox{} \\
  \vspace{-3.5ex}
  \begin{sstitemize}
     #1
  \end{sstitemize}
}

%  Format a list of items while in paragraph mode, and where there
%  is a heading, thus the negative vertical space is not needed.
\newcommand{\ssthitemlist}[1]{
  \mbox{} \\
  \vspace{-3.5ex}
  \begin{itemize}
     #1
  \end{itemize}
}

%  Define the format of an item.
\newcommand{\sstitem}{\item}

%  Now define html equivalents of those already set. These are used by
%  latex2html and are defined in the html.sty files.
\begin{htmlonly}

%  Re-define \ssttt.
   \newcommand{\ssttt}{\tt}

%  sstroutine.
   \renewcommand{\sstroutine}[3]{
      \subsection{#1\xlabel{#1}-\label{#1}#2}
      \begin{description}
         #3
      \end{description}
   }

%  sstdescription
   \renewcommand{\sstdescription}[1]{\item[Description:]
      \begin{description}
         #1
      \end{description}
   }

%  sstusage
   \renewcommand{\sstusage}[1]{\htmlref{\item[Usage:]}{ap:usage} \mbox{} \\ {\ssttt #1}}

%  sstinvocation
   \renewcommand{\sstinvocation}[1]{\item[Invocation:]
      \begin{description}
         {\ssttt #1}
      \end{description}
   }

%  sstarguments
   \renewcommand{\sstarguments}[1]{
      \item[Arguments:]
      \begin{description}
         #1
      \end{description}
   }

%  sstreturnedvalue
   \renewcommand{\sstreturnedvalue}[1]{
      \item[Returned Value:]
      \begin{description}
         #1
      \end{description}
   }

%  sstparameters
   \renewcommand{\sstparameters}[1]{
      \htmlref{\item[Parameters:]}{se:param}
      \begin{description}
         #1
      \end{description}
   }

%  sstresparameters
   \renewcommand{\sstresparameters}[1]{
      \htmlref{\item[Results Parameters:]}{se:parout}
      \begin{description}
         #1
      \end{description}
   }

%  sstexamples
   \renewcommand{\sstexamples}[1]{
      \htmlref{\item[Examples:]}{ap:example}
      \begin{description}
         #1
      \end{description}
   }

%  sstsubsection
   \renewcommand{\sstsubsection}[1]{\item[{#1}]}

%  sstexamplesubsection
   \renewcommand{\sstexamplesubsection}[2]{\item[{\ssttt #1}] \\ #2}

%  sstnotes
   \renewcommand{\sstnotes}[1]{\item[Notes:]
      \begin{description}
         #1
      \end{description}
   }

%  sstdiytopic
   \renewcommand{\sstdiytopic}[2]{\item[{#1}]
      \begin{description}
         #2
      \end{description}
   }

%  sstimplementationstatus
   \renewcommand{\sstimplementationstatus}[1]{\item[Implementation Status:] 
      \begin{description}
         #1
      \end{description}
   }

%  sstitemlist
   \newcommand{\sstitemlist}[1]{
      \begin{itemize}
         #1
      \end{itemize}
   }
\end{htmlonly}

%  End of sst.tex layout definitions.
%.

% End of SST definitions
% ----------------------

%    Starlink definitions for \LaTeX\ macros used in MAN output
%
%  Description:
%    As much as possible of the output from the MAN automatic manual generator
%    uses calls to user-alterable macros rather than direct calls to built-in
%    \LaTeX\ macros. This file is a version of the MAN default definitions for
%    these macros modified for Starlink preferences.
%
%  Language:
%    \LaTeX
%
%  Support:
%    William Lupton, {AAO}
%    Alan Chipperfield (RAL)
%-
%  History:
%    16-Nov-88 - WFL - Add definitions to permit hyphenation to work on
%		 words containing special characters and in teletype fonts.
%    27-Feb-89 - AJC - Redefine \manroutine
%                      Added \manheadstyle
%                      Switch order of argument descriptors
%    07-Mar-89 - AJC - Narrower box for parameter description
%                      Remove Intro section and other unused bits
%
% permit hyphenation when in teletype font (support 9,10,11,12 point only -
% could extend), define lccodes for special characters so that the hyphen-
% ation algorithm is not switched off. Define underscore character to be
% explicit underscore rather than lots of kerns etc.

\typeout{Starlink MAN macros. Released 27th February 1989}

%\hyphenchar\nintt=`-\hyphenchar\tentt=`-\hyphenchar\elvtt=`-\hyphenchar\twltt=`-

\lccode`_=`_\lccode`$=`$

%    Macros used in the .TEX_SUMMARY file
%
%  Description:
%    There is a command to introduce a new section (mansection) and a list-like
%    environment (mansectionroutines) that handles the list of routines in the
%    current section. In addition a mansectionitem command can be used instead
%    of the item command to introduce a new routine in the current section.
%-

\newcommand {\mansection}[2]{\subsection{#1 --- #2}}

\newenvironment {mansectionroutines}{\begin{description}\begin{description}}%
{\end{description}\end{description}}

\newcommand {\mansectionitem}[1]{\item [#1:] \mbox{}}

%    Macros used in the .TEX_DESCR file
%
%  Description:
%    There is a command to introduce a new routine (manroutine) and a list-like
%    environment (manroutinedescription) that handles the list of paragraphs
%    describing the current routine. In addition a manroutineitem command can
%    be used instead of the item command to introduce a new paragraph for the
%    current routine.
%
%    Two-column tables (the ones that can occur anywhere and which are
%    triggered by "=>" as the second token on a line) are bracketed by a
%    new environment (mantwocolumntable). Other sorts of table are introduced
%    by relevant  environments (manparametertable, manfunctiontable and
%    manvaluetable). The definitions of these environments call various other
%    user-alterable commands, thus allowing considerable user control over such
%    tables... (to be filled in when the commands have been written)
%-

\newcommand {\manrule}{\rule{\textwidth}{0.5mm}}

%\newcommand {\manroutine}[2]{\subsection{#1 --- #2}}
\newlength{\speccaption}
\newlength{\specname}
\newcommand{\manroutine}[2]{\goodbreak
                          \markboth{{\stardocname}~ --- #1}{{\stardocname}~ --- #1}
                          \rule{\textwidth}{0.5mm}  % draw thick line
                          \settowidth{\specname}{{\Large {\bf #1}}}
                        % left and right box width is text width plus gap
                          \addtolength{\specname}{4ex}
                        % caption width is width of page less the two names
                        % less than empirical fudge factor
                          \setlength{\speccaption}{\textwidth}
                          \addtolength{\speccaption}{-2.0\specname}
                          \addtolength{\speccaption}{-4.9pt}
                        % move text up the page because \flushleft environ-
                        % ment creates a paragraph
                          \vspace{-7mm}
                          \newline
                          \parbox[t]{\specname}{\flushleft{\Large {\bf #1}}}
                          \parbox[t]{\speccaption}{\flushleft{\Large #2}}
                          \parbox[t]{\specname}{\flushright{\Large {\bf #1}}}
                          }

\newenvironment {manroutinedescription}{\begin{description}}{\end{description}}

\newcommand {\manroutineitem}[2]{\pagebreak[3]\item [#1:] #2\mbox{}}


% parameter tables

\newcommand {\manparametercols}{lllp{90mm}}

\newcommand {\manparameterorder}[3]{#2 & #3 & #1 &}

\newcommand {\manparametertop}{}

\newcommand {\manparameterblank}{\gdef\manparameterzhl{}\gdef\manparameterzss{}}

\newcommand {\manparameterbottom}{}

\newenvironment {manparametertable}{\gdef\manparameterzss{}%
\gdef\manparameterzhl{}\hspace*{\fill}\vspace*{-\partopsep}\begin{trivlist}%
\item[]\begin{tabular}{\manparametercols}\manparametertop}{\manparameterbottom%
\end{tabular}\end{trivlist}}

\newcommand {\manparameterentry}[3]{\manparameterzss\gdef\manparameterzss{\\}%
\gdef\manparameterzhl{\hline}\manparameterorder{#1}{#2}{#3}}


% list environments

\newenvironment {manenumerate}{\begin{enumerate}}{\end{enumerate}}

\newcommand {\manenumerateitem}[1]{\item [#1]}

\newenvironment {manitemize}{\begin{itemize}}{\end{itemize}}

\newcommand {\manitemizeitem}{\item}

\newenvironment
{mandescription}{\begin{description}\item{~}\vspace*{-5.6ex}\begin{description}}%
{\end{description}\end{description}}

\newcommand {\mandescriptionitem}[1]{\item [#1]}

\newcommand {\mantt}{\tt}

% manheadstyle for Starlink
\newcommand {\manheadstyle}{}

%\catcode`\_=12

% ? End of document specific commands
%------------------------------------------------------------------------------
%  Title Page.
%  ===========
\renewcommand{\thepage}{\roman{page}}
\begin{document}
\thispagestyle{empty}

%  Latex document header.
%  ======================
\begin{latexonly}
   CCLRC / {\sc Rutherford Appleton Laboratory} \hfill {\bf \stardocname}\\
   {\large Particle Physics \& Astronomy Research Council}\\
   {\large Starlink Project\\}
   {\large \stardoccategory\ \stardocnumber}
   \begin{flushright}
   \stardocauthors\\
   \stardocdate
   \end{flushright}
   \vspace{-4mm}
   \rule{\textwidth}{0.5mm}
   \vspace{5mm}
   \begin{center}
   {\Huge\bf  \stardoctitle \\ [2.5ex]}
   {\LARGE\bf \stardocversion \\ [4ex]}
   {\Huge\bf  \stardocmanual}
   \end{center}
   \vspace{5mm}

% ? Heading for abstract if used.
   \vspace{10mm}
   \begin{center}
      {\Large\bf Abstract}
   \end{center}
% ? End of heading for abstract.
\end{latexonly}

%  HTML documentation header.
%  ==========================
\begin{htmlonly}
   \xlabel{}
   \begin{rawhtml} <H1> \end{rawhtml}
      \stardoctitle\\
      \stardocversion\\
      \stardocmanual
   \begin{rawhtml} </H1> \end{rawhtml}

% ? Add picture here if required.
%  \htmladdimg{sun95_cover.gif}
% ? End of picture

   \begin{rawhtml} <P> <I> \end{rawhtml}
   \stardoccategory \stardocnumber \\
   \stardocauthors \\
   \stardocdate
   \begin{rawhtml} </I> </P> <H3> \end{rawhtml}
      \htmladdnormallink{CCLRC}{http://www.cclrc.ac.uk} /
      \htmladdnormallink{Rutherford Appleton Laboratory}
                        {http://www.cclrc.ac.uk/ral} \\
      \htmladdnormallink{Particle Physics \& Astronomy Research Council}
                        {http://www.pparc.ac.uk} \\
   \begin{rawhtml} </H3> <H2> \end{rawhtml}
      \htmladdnormallink{Starlink Project}{http://star-www.rl.ac.uk/}
   \begin{rawhtml} </H2> \end{rawhtml}
   \htmladdnormallink{\htmladdimg{source.gif} Retrieve hardcopy}
      {http://star-www.rl.ac.uk/cgi-bin/hcserver?\stardocsource}\\

%  HTML document table of contents. 
%  ================================
%  Add table of contents header and a navigation button to return to this 
%  point in the document (this should always go before the abstract \section). 
  \label{stardoccontents}
  \begin{rawhtml} 
    <HR>
    <H2>Contents</H2>
  \end{rawhtml}
  \renewcommand{\latexonlytoc}[0]{}
  \htmladdtonavigation{\htmlref{\htmladdimg{contents_motif.gif}}
        {stardoccontents}}

% ? New section for abstract if used.
  \section{\xlabel{abstract}Abstract}
% ? End of new section for abstract
\end{htmlonly}

% -----------------------------------------------------------------------------
% ? Document Abstract. (if used)
%   ==================
{\footnotesize KAPPA} is an applications package comprising about 180
general-purpose commands for image processing, data visualisation, and
manipulation of the standard Starlink data format---the NDF.  It is
intended to work in conjunction with Starlink's various specialised
packages.

In addition to the NDF, {\footnotesize KAPPA} can also process data in
other formats by using the `on-the-fly' conversion scheme.  Many
commands can process data arrays of arbitrary dimension, and others
work on both spectra and images.  {\footnotesize KAPPA} operates from
both the UNIX C-shell and the {\footnotesize ICL} command language.

This document describes how to use {\footnotesize KAPPA} and its
features.  There is some description of techniques too, including a
section on writing scripts.  This document includes several tutorials
and is illustrated with numerous examples.  The bulk of this document
comprises detailed descriptions of each command as well as classified
and alphabetical summaries. 

% ? End of document abstract
% -----------------------------------------------------------------------------
% ? Latex document Table of Contents (if used).
%  ===========================================
 \newpage
 \begin{latexonly}
   \setlength{\parskip}{0mm}
   \latexonlytoc
   \setlength{\parskip}{\medskipamount}
   \markboth{\stardocname}{\stardocname}
%  \markright{\stardocname}
 \end{latexonly}
% ? End of Latex document table of contents
% -----------------------------------------------------------------------------
% \newpage
\cleardoublepage
\renewcommand{\thepage}{\arabic{page}}
\setcounter{page}{1}

% The main text begins here.
% -----------------------------------------------------------------------------

\section{\xlabel{se_kappaintro}Introduction\label{se:kappaintro}}

\subsection{Background}
It is Starlink's aim to provide {\em maintainable\/}, {\em
portable\/}, and {\em extensible\/} applications packages that work in
harmony by sharing a common infrastructure toolkit, standards,
conventions and above all, a standard data format.  Individual
packages are no longer required to perform all functions, thus carry
less inertia, and are more adaptable to outside developments.
Additional functionality can be added piecemeal to the relevant
package.  New user interfaces, such as graphical, could be layered
within the toolkit for obtaining parameters and so make the
enhancement available to all applications that make use of those
tools.  A recent example of this approach has allowed us to access
`foreign data formats' throughout Starlink packages, because the
packages use a common infrastructure library.

An important part of the rationalisation is that applications are
unified by sharing the same basic data structure---the
\NDFref{NDF}\ (Extensible $N$-dimensional Data Format).  This contains
an $n$-dimensional data array that can store most astronomical data such
as spectra, images and spectral-line data cubes.  The NDF may also
contain information like a title, axis labels and units, error and
quality arrays.  There are also places in the NDF, called {\em
extensions}, to store any ancillary data associated with the data
array, even other NDFs.

\subsection{R\^{o}le of KAPPA}

The backbone of the applications packages is {\footnotesize KAPPA} ({\bf
K}ernel {\bf AP}plication {\bf PA}ckage).  It provides general-purpose
applications that have wide applicability, concentrating on image
processing, data visualisation, and manipulating
\xref{NDF}{sun33}{overview_of_an_ndf} components.  {\footnotesize KAPPA} provides
facilities that integrate with specialised Starlink packages such as
those for CCD reduction (\CCDPACKref ), stellar and galaxy photometry
(\PHOTOMref , \PISAref , \ESPref ), spectroscopy (\ECHOMOPref , \Figaroref ,
\SPECDREref , \TWODSPECref ), X-ray (\ASTERIXref ), graphics (\PONGOref ),
time-series polarimetry (\TSPref ), instrument-specific (\CGSDRref ,
\IRASref , \IRCAMPACKref , \JCMTDRref ) {\it etc.}  {\em Thus the
functionality of {\footnotesize KAPPA} should not be regarded in isolation.}

Nor {\footnotesize KAPPA} should not be perceived as a rival to {\footnotesize FIGARO}.
As {\footnotesize FIGARO} {\em applications\/} become more integrated with
Starlink packages, for example by using the NDF library, they should
be seen as complementary, with {\footnotesize FIGARO} concentrating on
spectroscopy and {\footnotesize KAPPA} image processing.  Of course, there is
some duplication for historical and ease-of-use reasons.

In a wider context, {\footnotesize KAPPA} offers facilities not in \IRAFref ,
for instance handling of data errors, quality masking, a graphics
database, availability from the shell, as well as more $n$-dimensional
applications, widespread use of data axes, and a different style. It
integrates with instrument packages developed at UK observatories.
With the automatic data conversion and the likelihood the {\footnotesize
KAPPA} and other Starlink packages will be available from within the
{\footnotesize IRAF} command language, you should be able to pick the best or
relevant tools from both systems to get the job done.

\subsection{Functionality of KAPPA}

\subsubsection{Applications}
Currently, {\footnotesize KAPPA} has about 180 commands that are available
both from the UNIX C-shell and from the \ICLref\ command language.
They provide the following facilities for data processing:

\begin{itemize}
\item \htmlref{FITS readers}{se:fitsreaders} that generate NDFs and
text tables, and the import and export of ancillary data through the
NDF \htmlref{FITS extension}{se:fitsairlock};
\item \htmlref{generation of test data}{cl:datagen}, and
\htmlref{NDF creation from text files}{TRANDAT};
\item setting and examining \htmlref{NDF components}{ap:NDFformat};
\item definition or \htmlref{calculation of a sky co-ordinate
system}{SETSKY} for use in conjunction with \IRASref\ tools;
\item arithmetic including a \htmlref{powerful application}{MATHS}
that handles expressions;
\item \htmlref{pixel and region editing}{cl:pixedit}, including polygons and
circles; re-flagging of bad pixels by value or by \htmlref{median
filtering}{GLITCH}; and \htmlref{pasting arrays over others}{PASTE};
\item \htmlref{masking}{se:masking} of regions, and of \htmlref{pixels
whose variances are too large}{ERRCLIP};
\item configuration change: \htmlref{flip}{FLIP},
\htmlref{rotate}{ROTATE}, \htmlref{shift}{SLIDE}, subset, change
dimensionality;
\item forming image mosaics; normalisation of NDF pairs;
\item \htmlref{compression and expansion of images}{cl:compexp};
\item \htmlref{generalised resampling}{cl:resample} of NDFs using
arbitrary transformations;
\item filtering: \htmlref{box}{BLOCK}, \htmlref{Gaussian}{GAUSMOOTH},
and \htmlref{median smoothing}{MEDIAN}; very efficient
\htmlref{Fourier transform}{FOURIER}, \htmlref{convolution}{CONVOLVE};
\item deconvolution: \htmlref{maximum-entropy}{MEM2D},
\htmlref{Lucy-Richardson}{LUCY}, \htmlref{Wiener filter}{WIENER};
\item \htmlref{two-dimensional-surface fitting}{cl:surfit};
\item statistics including \htmlref{ordered statistics}{HISTAT},
\htmlref{histogram}{HISTOGRAM}; \htmlref{pixel-by-pixel
statistics over a sequence of images}{MSTATS};
\item inspection of image values;
\item \htmlref{centroids of features}{CENTROID}, particularly stars;
\htmlref{stellar PSF fitting}{PSF};
\item detail enhancement using \htmlref{histogram equalisation}{HISTEQ}
and \htmlref{Laplacian}{LAPLACE}, convolution, edge enhancement via a
\htmlref{shadow effect}{SHADOW}, \htmlref{thresholding}{THRESH};
\item \htmlref{calculation of polarimetry images}{CALPOL};
\end{itemize}

There are also many applications for data visualisation:
\begin{itemize}
\item use of the \htmlref{graphics database}{se:agitate}, AGI, to pass
information about pictures between tasks; tools for the creation, labelling,
selection of pictures, and obtaining world and data co-ordinate
information from them;
\item \htmlref{image}{DISPLAY} and \htmlref{greyscale}{GREYPLOT} plots with
a selection of scaling modes and many options such as axes;
\item creation, selection, saving and manipulation of \htmlref{colour
tables and palettes}{se:coltab} (for axes, annotation, coloured markers
and borders);
\item \htmlref{snapshot}{SNAPSHOT} of an image display to hardcopy; and
\item line graphics: \htmlref{contouring}{TURBOCONT}, including
\htmlref{overlay}{CONTOVER}; histogram; \htmlref{line
plots}{LINPLOT} of 1-dimensional arrays, and \htmlref{multiple-line
plot}{MLINPLOT} of images; \htmlref{pie sections}{ELPROF}, and slices
through an image; \htmlref{vector plot}{VECPLOT} of image; all of
which offer some control of the appearance of plots.
\end{itemize} 

\subsubsection{General}
{\footnotesize KAPPA} handles \htmlref{bad pixels, and processes
quality}{se:masking}, variance, and other information stored within
NDFs \latexonly{(SUN/33 and Section~\ref{se:datastr})}.  In order to
achieve generality {\footnotesize KAPPA} does not process non-standard
extensions; however, it does not lose non-standard ancillary data
since it copies extensions to any NDFs that it creates.

{\footnotesize KAPPA} can also process data in other formats, such as FITS
and {\footnotesize IRAF}, using an automatic-data conversion facility
(SUN/55, SSN/20).  For historical reasons, 20 commands in {\footnotesize KAPPA}
still use the old \htmlref{IMAGE format}{ap:IMAGEformat}
\latexonly{(Appendix~\ref{ap:IMAGEformat})}, which does not include
all the features of the NDF.  A low-priority conversion process is
underway to transform the remaining to use the NDF-access
interface\latexonly{ (SUN/33)}.  Once completed {\footnotesize KAPPA} can
finally reach version 1.0.  See
\latexelsehtml{Appendix~\ref{ap:full}}{the \htmlref{Specifications of
KAPPA applications}{ap:full}} for details of which applications use the NDF.

Although oriented to image processing, many commands will work on NDFs
of arbitrary dimension and others operate on both spectra and images.
Many applications handle all non-complex data types directly, for
efficient memory and disc usage.  Those that do not will usually
undergo automatic data conversion to produce the desired result.

{\footnotesize KAPPA}'s graphics are device independent.  X-windows and
overlays are supported.

\subsection{Support}
{\footnotesize KAPPA} receives a high level of support, and bug reports
are dealt with promptly.

It is continuing to be developed and additional applications
functionality are envisaged, particularly a graphical-user interface
for image processing.  [If you wish to suggest new tasks or
enhancements to existing ones please contact the author.  Small jobs
may be undertaken quite quickly.  Requests involving substantial
pieces of work need to gain the support of others to be included in
Starlink's software plans, and so the best way to influence the
priority given to {\footnotesize KAPPA} is to contact the chairman of the
Image Processing Software Strategy Group.  The Starlink World Wide Web
pages will periodically contain news of plans and work in progress.]

\subsection{This document}
This document is arranged as follows.  First are two annotated {\footnotesize
KAPPA} tutorials to give you a quick summary of basic usage.  The main
text follows, which amplifies the points sketched in the demonstrations,
and describes other functionality and modes of use illustrated with
further examples.  Finally, there are extensive appendices, including a
classified list of commands and detailed descriptions of each command,
which are also available on a quick-reference card.


\newpage
\section{\xlabel{se_demo}Tutorials\label{se:demo}}

So the facilities summarised in the introduction sound appealing.  Now
you want to know how to access them, but the thick manual looks
daunting.  Actually, most of this manual comprises descriptions of each
application.  The best way to learn the basics is to try some example
sessions.

Login to a colour X-terminal or workstation.  Then enter the commands
following the prompts shown below.  The {\tt \%} is the shell prompt
string, which you don't type.  As we go along there will be
commentary explaining what is happening and why.  Let's begin. 

\subsection{From the C-shell}

\small
\begin{verbatim}
     % kappa
\end{verbatim}
\normalsize

This defines C-shell aliases for each {\footnotesize KAPPA} command,
includes the help information, and shows the version number.  It need
only be issued once per login session.  Thus you will see

\small
\begin{verbatim}
      KAPPA commands are now available -- (Version 0.9-0)
 
      Type kaphelp for help on KAPPA commands
\end{verbatim}
\normalsize

Let's run a {\footnotesize KAPPA} application.  \htmlref{CADD}{CADD} adds a
scalar constant to an \NDFref{NDF} file---the Starlink standard data
format---to make a new NDF file (usually called an {\em NDF\/} for
short.)  In this case ten is added to the pixels in {\tt
\$KAPPA\_DIR/comwest.sdf} to create {\tt test.sdf} in the current
directory.

\small
\begin{verbatim}
     % cadd $KAPPA_DIR/comwest 10 test
\end{verbatim}
\normalsize

There are three \htmlref{{\em parameters\/}}{se:param} qualifying
the CADD command: the names of the input and output NDFs and the
constant.  Notice that these parameters are separated by spaces.  Most
applications have a few of these positional parameters, usually the
most commonly used.  Parameter values on the command line are not
subsequently prompted for by the application.  Also you see that the
NDF file extensions are not given. The {\tt .sdf} extension indicates
that it was created by the \xref{Hierarchical Data System}{sun92}{}
\latexonly{ (SUN/92)}, or HDS for short.  Note that an arbitrary
{\tt .sdf} file is not necessarily an NDF.

Next we run the statistics task.  Here we have not given any parameters.
In this case the application will ask for the values of any parameters
it needs.

\small
\begin{verbatim}
     % stats
\end{verbatim}
\normalsize

The only parameter required is called NDF, and \htmlref{STATS}{STATS}
prompts us for it.

\small
\begin{verbatim}
     NDF - Data structure to analyse /@test/ >
\end{verbatim}
\normalsize

In this example, STATS wants to know for which NDF we require
statistical data.  The text between the {\tt //} delimiters is the
suggested default for the parameter.  By pressing the carriage return we
accept this default as the parameter's value.  Here the suggested
default is the name of the NDF created by CADD.  (Ignore the {\tt @} for
the moment---it just tells the application that it is a file.) {\footnotesize
KAPPA} remembers the last NDF used or created, and uses it for the
suggested default to save typing.  Since {\tt test} is the NDF whose
statistics we want we just hit the return key.  Again we exclude the
{\tt .sdf} extension.  Here is the output from STATS.

\small
\begin{verbatim}
        Pixel statistics for the NDF structure /home/scratch/mjc/test

           Title                     : Comet West, low resolution
           NDF array analysed        : DATA

              Pixel sum              : 11851773
              Pixel mean             : 180.8437
              Standard deviation     : 63.47324
              Minimum pixel value    : 13.89063
                 At pixel            : (59, 83)
                 Co-ordinate         : (58.5, 82.5)
              Maximum pixel value    : 255.9375
                 At pixel            : (248, 45)
                 Co-ordinate         : (247.5, 44.5)
              Total number of pixels : 65536
              Number of pixels used  : 65536 (100.0%)

\end{verbatim}
\normalsize

Of course, in your case the current directory will not be {\tt
/home/scratch/mjc}.  The NDF title is the unchanged from the 
{\tt \$KAPPA\_DIR/comwest} NDF.  This is the normal behaviour for tasks
that create a new NDF from an old one; they do, however, have a
parameter for changing this default.  To alter a defaulted parameter you
supply its new value on the command line.  Defaulted parameters exist to
prevent a long series of prompts where reasonable values can be defined,
and hence save time. (However, there is a way of being prompted for all
parameters of a command should you wish.)  

NDFs may contain three standard arrays---the data array, the data
variance and quality.  STATS can calculate statistics for any of
these.  By default, STATS uses the data array, as indicated here.

Next we wish to smooth our data.  \htmlref{GAUSMOOTH}{GAUSMOOTH}
performs a Gaussian smooth of neighbouring pixels.

\small
\begin{verbatim}
     % gausmooth
\end{verbatim}
\normalsize

Again we are prompted with the same suggested default, since we
have not created any new NDFs within STATS.  Say we don't want to
smooth that NDF, but the original one.  We just enter the name of the
NDF at the prompt.  Notice that we don't need the {\tt @}
prefix, since parameter IN expects a file.  (One occasion where you
would need it is when the filename is a number, {\it{e.g.}}\ if your
NDF was called 234 you must enter {\tt @234}, otherwise the parameter
system will think you are giving the integer 234.  Yes \ldots I 
know \ldots the parameter system is trying to be too clever.)

\small
\begin{verbatim}
     IN - Input NDF /@test/ > $KAPPA_DIR/comwest
\end{verbatim}
\normalsize

The description of parameter FWHM is too brief for us to select a value.
So we obtain some help on this parameter, and then GAUSMOOTH reprompts
for a value.  The smoothed NDF is written to the NDF called {\tt testsm}
in the current directory.

\small
\begin{verbatim}
     FWHM - Gaussian PSF full-width at half-maximum /5/ > ?

     GAUSMOOTH

       Parameters

         FWHM

           FWHM() = _REAL (Read)
              This specifies whether a circular or elliptical Gaussian
              point-spread function is used in smoothing a 2-dimensional
              image.  If one value is given it is the full-width at
              half-maximum of a 1-dimensional or circular Gaussian PSF.
              (Indeed only one value is permitted for a 1-dimensional
              array.)  If two values are supplied, this parameter becomes the
              full-width at half-maximum of the major and minor axes of an
              elliptical Gaussian PSF.  Values between 0.1 and 100.0 pixels
              should be given.  Note that unless a non-default value is
              specified for the BOX parameter, the time taken to perform the
              smoothing will increase in approximate proportion to the
              value(s) of FWHM.  The suggested default is the current value.
              will increase in approximate proportion to the value of FWHM.
                                                           
     FWHM - Gaussian PSF full-width at half-maximum /5/ >
     OUT - Output NDF > testsm
\end{verbatim}
\normalsize

Next we want to look at the result of our image processing.  First
thing to do is to select an image display.  The {\tt xwindows} device
becomes the current image display and remains so until the next
\htmlref{IDSET}{IDSET} command.  (You may need to enter the
{\tt xdisplay} command (\xref{SUN/129}{sun129}{}) to redirect
the output from the host computer to the screen in front you.)

\small
\begin{verbatim}
     % idset xwindows
\end{verbatim}
\normalsize

Now we actually display it on the screen.  Some applications have
many parameters, and it would be impractical to have to specify all
those preceding the ones we wanted to alter.  The solution is specify
the parameter by keyword.  Here we have requested that the scaling of
the data values to colour indices within the image display uses the
current percentile range.  Note that you may abbreviate the options
of a menu, such as offered by parameter MODE, to any unambiguous
string.

\small
\begin{verbatim}
     % display mode=pe accept
     Data will be scaled from 78.38278 to 235.3536.
\end{verbatim}
\normalsize

If you have just created the window, the image will not look much like
the comet, because the existing colour table is poor.  If we replace
the table with a greyscale ramp from white to black,

\small
\begin{verbatim}
     % lutneg
\end{verbatim}
\normalsize

we see a blurred image of the ubiquitous Comet West on the screen.

The \htmlref{ACCEPT keyword}{se:parspeckey} is a very useful feature.
It tells an application to accept all the suggested defaults.  In this
case \htmlref{DISPLAY}{DISPLAY} uses the current NDF and scales
between the current percentile limits---10 and 90.  The keyword can be
abbreviated to double backslash from the shell. Aside: the parameter
system actually requires a single backslash. From the shell, however,
backslash is a {\em metacharacter\/}, and so must be `escaped' to
treat the character literally.  One way is to place a {\tt
$\backslash$} before each metacharacter.  You can escape a series of
characters by placing them inside single quotes {\tt ' '}. Other
metacharacters to watch out for when using {\footnotesize KAPPA} include {\tt
[]()"}.

Next we want to make the image colourful.  There are a number of
predefined lookup tables, or you may create and modify your own.  Here
we've given the X-window a spectrum-like colour table. 

\small
\begin{verbatim}
     % lutspec
\end{verbatim}
\normalsize

Finally, we decide that we don't like the appearance and we want to
try the effect of `rotating' the colour table.
\htmlref{LUTROT}{LUTROT} tells you what to do.

\small
\begin{verbatim}
     % lutrot
     Move_Mouse to rotate the colour table.
     Press_Mouse_left to reset the colour table.
     Press_Mouse_right to end the rotation of the colour table.
\end{verbatim}
\normalsize

If now you move the mouse from left to right, the colours will move too.
Press the right-hand button when you have finished.

\subsection{From ICL}
\ICLref\ is currently the standard command language
designed for use with Starlink applications, like {\footnotesize KAPPA}.  The
main advantages for the {\footnotesize KAPPA} user are that shell
metacharacters like {\tt []()$\backslash$"} need not be escaped;
command names may be abbreviated; far fewer few executables need be
loaded, and therefore it is slightly faster than using the shell when
you want to invoke more than a few commands on a busy system; there is
a wide selection of intrinsic functions and floating-point arithmetic;
and results may be passed between applications via {\footnotesize ICL}
variables.  However, in these two demonstrations the command languages
are interchangeable apart from the accept backslash.

Let's start the second example.

\small
\begin{verbatim}
     % icl
\end{verbatim}
\normalsize

This starts {\footnotesize ICL}.  System, local and user-defined {\footnotesize ICL}
login files are invoked.  Here there is only a system login procedure
which sets up help on Starlink packages, and commands for setting
up definitions for those packages.  One of those commands is {\tt
kappa}; it is analogous to the {\tt kappa} command from the shell.
We enter it after receiving the {\footnotesize ICL} prompt.

You should see something like the following.

\small
\begin{verbatim}
    ICL (UNIX) Version 3.1 14/09/95

    Loading installed package definitions...

       - Type HELP package_name for help on specific Starlink packages
       -   or HELP PACKAGES for a list of all Starlink packages
       - Type HELP [command] for help on ICL and its commands

     ICL> kappa
 
        KAPPA commands are now available (Version 0.9-0).
 
        Type `help kappa' or `kaphelp' for help on KAPPA commands.
 
\end{verbatim}
\normalsize

Now we run an application, \htmlref{ADD}{ADD}, that adds the pixels in
{\tt \$KAPPA\_DIR/comwest.sdf} to those in {\tt \$KAPPA\_DIR/ccdframec.sdf}. 
Although these images have different
dimensions, the intersection is made.

\small
\begin{verbatim}
     ICL> add $KAPPA_DIR/comwest $KAPPA_DIR/ccdframec
\end{verbatim}
\normalsize

After the first {\footnotesize KAPPA} command is issued you'll see an arcane
message like this.

\small
\begin{verbatim}
     Loading /star/bin/kappa/kappa_mon into kappa_mon11601 (attached)
\end{verbatim}
\normalsize

It just tells you that the {\footnotesize KAPPA} monolith is being loaded.
You'll see similar messages for each of the three monoliths when they
are first wanted.

Since we did not give the name of the destination
\xref{NDF}{sun33}{overview_of_an_ndf} that will hold the co-added NDFs,
ADD prompts for it.  Notice that literal parameters are case insensitive.

\small
\begin{verbatim}
     OUT - Output NDF / / > demo1
\end{verbatim}
\normalsize

\small
\begin{verbatim}
     ICL> ndftrace \

        NDF structure /home/soft2/mjc/alpha_OSF1/kappa/package/demo1:
           Title:  Comet West, low resolution

        Shape:
            No. of dimensions:  2
            Dimension size(s):  256 x 256
            Pixel bounds     :  1:256, 1:256
            Total pixels     :  65536

        Data Component:
            Type        :  _REAL
            Storage form:  PRIMITIVE
            Bad pixels may be present
\end{verbatim}
\normalsize

This shows that the {\tt demo1} NDF has the same dimensions as the
smaller of the two NDFs.

We were going to display the image on the current image display, but
then changed our minds.  A {\tt !!} in response to a prompt
\htmlref{aborts}{se:abortnull} a task.

\small
\begin{verbatim}
     ICL> display demo1
     MODE - Method to define the scaling limits /'PERCENTILES'/ > !!
     !! SUBPAR: Abort (!!) response to prompt for parameter MODE
     OBEYW unexpected status returned from task "kapview_mon11601", action
     - "DISPLAY"
     ADAMERR  %PAR, Parameter request aborted
\end{verbatim}
\normalsize

We make the X-windows the current graphics device.
\latexelsehtml{Like the IDSET we saw earlier this}{This}
selection remains until we next issue a \htmlref{GDSET}{GDSET}
command.  It persists between shell and {\footnotesize ICL} sessions.
If you wish to see the current values of these
\htmlref{{\em global\/} parameters}{se:parglobals},
use the \htmlref{GLOBALS}{GLOBALS} command.  There are examples later.

\small
\begin{verbatim}
     ICL> gdset xwindows
\end{verbatim}
\normalsize

{\footnotesize KAPPA} uses the graphics database, which records the positions
and extents of graphs and images, collectively called pictures.

\small
\begin{verbatim}
     ICL> picgrid 2 1
\end{verbatim}
\normalsize

This instruction divides the display surface into two equally sized
pictures, side by side.  They are labelled 1 and 2 in the database.
Picture 1 is the current picture, in which future pictures are drawn,
unless we select a new current picture.

Thus in picture 1 we display an image of Comet West around which we
draw a yellow border.  The backslash causes the current scaling method
to be used.

\small
\begin{verbatim}
     ICL> display comwest border bcolour=yell \
     !! Error accessing file '/home/scratch/mjc/comwest.sdf' -
     !     No such file or directory
     !  HDS_OPEN: Error opening an HDS container file.
     !  NDF_ASSOC: Unable to associate an NDF structure with the '%IN' parameter.
\end{verbatim}
\normalsize

\htmlref{DISPLAY}{DISPLAY} could not find the a {\tt comwest.sdf} in
the current directory.  So there is an error message and we are
prompted.  This time we remember to add the environment variable.

\small
\begin{verbatim}
     IN - NDF to be displayed /@comwest_bas/ > $KAPPA_DIR/comwest
     Data will be scaled from 67.46276 to 226.5568.
\end{verbatim}
\normalsize
An image of Comet West should be visible to the left of the screen.

SHADOW creates an image that appears like a bas-relief.  We've called
the resulting NDF comwest\_bas.  The backslash causes the current NDF
to be the input NDF for \htmlref{SHADOW}{SHADOW}.

\small
\begin{verbatim}
     ICL> shadow out=comwest_bas \
\end{verbatim}
\normalsize

We select the right-hand picture created earlier.

\small
\begin{verbatim}
     ICL> picsel 2
\end{verbatim}
\normalsize

As above we display the current NDF, the bas-relief image, with a yellow
border on the right of the raw comet image.

\small
\begin{verbatim}
     ICL> display border bcolour=Yell \
     Data will be scaled from -4.721756 to 5.697861.
\end{verbatim}
\normalsize

The relief looks best with a greyscale colour table.  Note that this
does not affect the colour of the border.  \htmlref{LUTGREY}{LUTGREY}
is a procedure which calls a more-general application.  Since it is
the first procedure we've invoked there is a short pause while all the
{\footnotesize KAPPA} procedures are compiled and loaded.

\small
\begin{verbatim}
     ICL> lutgrey
     Loading procedure file $KAPPA_DIR/kappa_proc.icl
\end{verbatim}
\normalsize

Next we decide to make a hard copy of the bas-relief image. 
\htmlref{GREYPLOT}{GREYPLOT} does this and adds a key of grey levels
and their corresponding values.  The chosen device is {\tt ps\_l};
this overrides the {\tt xwindows} device for the duration of GREYPLOT.
If this name isn't recognised at your site, issue the
\htmlref{GDNAMES}{GDNAMES} command for a list of your local device
names.  Select the landscape PostScript device.  We scale between
wider limits to reduce the glare.

\small
\begin{verbatim}
     ICL> greyplot device=ps_l
     IN - NDF to be displayed /@comwest_bas/ >
     KEY - Do you want a key of the grey levels and a title ? /YES/ >
     MODE - Method to define the scaling limits /'SCALE'/ >
     BLACK - Value to be black in the plot? /187.5625/ > 11
     WHITE - Value to be white in the plot? /-183.453125/ > -8.33
\end{verbatim}
\normalsize

GREYPLOT does not send your plot to the printer, since this is hardware
and node dependent.  Therefore, you must issue a shell command
from {\footnotesize ICL} to perform this action.  That's not difficult---just
insert a {\tt !} before the UNIX command, and in most cases just issue
the command as if you were in the shell, like we do below.

\small
\begin{verbatim}
     ICL> lpr -P1 gks74.ps
\end{verbatim}
\normalsize

Shell aliases may also be used, so if {\tt ri} equates to {\tt rm -i},
we could remove any unwanted \xref{HDS}{sun92}{} files.  If you don't
have this symbol, as is likely, then you will receive the appropriate
error message from {\footnotesize ICL}.

\small
\begin{verbatim}
     ICL> ri *.sdf
\end{verbatim}
\normalsize

That's the end of the second demonstration.  Of course, these
introductions have only scratched the surface of what {\footnotesize KAPPA} can
do for you.  You should look at 
\latexelsehtml{Appendix~\ref{ap:classified}}{the
\htmlref{classified lists}{ap:classified}} to search for the desired
function, and then find more details in
\latexelsehtml{Appendix~\ref{ap:full}.}{the \htmlref{specifications}{ap:full}.}

If you get stuck or something untoward happens, there is a {\tt Hints}
help topic.

\newpage
\section{Getting started}

\subsection{Running KAPPA}
{\footnotesize KAPPA} runs from the C-shell and variants, and also from the
interactive command language---\ICLref\@.  Both run
monolithic programmes for efficiency.  Both have their advantages and
disadvantages.  Of the latter, the shell forces you to escape certain
characters, and {\footnotesize ICL} does not have a {\tt foreach} to loop
through a wildcarded list of NDFs. You may simply prefer the familiar
shell to {\footnotesize ICL}, though UNIX commands, including editing, are
accessible from {\footnotesize ICL} via a {\tt !} prefix.  This is not the
soapbox to expound the intrinsic merits of the two command languages,
but where there are differences affecting {\footnotesize KAPPA}, they'll be
indicated.  The choice is yours.

To run {\footnotesize KAPPA} from the shell just enter the following command.

\small
\begin{verbatim}
     % kappa
\end{verbatim}
\normalsize
This executes a procedure setting up aliases for {\footnotesize KAPPA}'s
command names and to make help information available.  Then you'll be
able to mix {\footnotesize KAPPA} commands with the familiar shell ones. 

If the {\tt kappa} command is not recognised, you probably haven't
enabled the Starlink software.  In your {\tt .cshrc} or {\tt .tcshrc}
file, you insert the line

\small
\begin{verbatim}
     source /star/etc/cshrc
\end{verbatim}
\normalsize
and in {\tt .login} you include the equivalent line

\small
\begin{verbatim}
     source /star/etc/login
\end{verbatim}
\normalsize
At non-Starlink sites the {\tt /star} path may be different.

To run {\footnotesize KAPPA} from {\footnotesize ICL} you have to start up the
command language if you are not already using it.  This requires just
one extra command, namely

\small
\begin{verbatim}
     % icl
\end{verbatim}
\normalsize
You will see any messages produced by system and user procedures, followed
by the {\tt ICL>} prompt.  Again there is a procedure for making the
commands known to the command language, and not unexpectedly, it too is

\small
\begin{verbatim}
     ICL> kappa
\end{verbatim}
\normalsize
Then you are ready to go.  Not too painful, was it?  In either case
you'll see message from {\footnotesize KAPPA} telling you which version is
ready for use.

So what do you get for your trouble?  Appendix~\ref{ap:summary} lists
in alphabetical order the 181 commands and their functions, and
\latexelsehtml{Appendix~\ref{ap:classified}}{there}
is a \htmlref{classified list}{ap:classified} of the same commands.
Many examples are given in subsequent sections.

\subsection{Issuing Commands}
To run an application you then can just give its name---you will be
prompted for any required parameters.  Alternatively, you may enter
parameter values on the command line specified by position or by
keyword.  More on this in
\latexelsehtml{Section~\ref{se:param}.}{\htmlref{Parameters}{se:param}}

Commands are interpreted in a case-independent way from \ICLref , but
from the shell they must be given in lowercase.  In {\footnotesize ICL},
commands may also be abbreviated provided they are unambiguous strings
with at least four characters.  Commands shorter than five characters,
therefore, cannot be shortened. So

\small
\begin{verbatim}
     ICL> CREF
     ICL> crefr
     ICL> CreFra
     ICL> CREFRAM
\end{verbatim}
\normalsize
would all run CREFRAME. Whereas

\small
\begin{verbatim}
     ICL> FITS
     ICL> FITSI
\end{verbatim}
\normalsize
would be ambiguous, since there are several commands beginning
{\tt FITS}, and two starting {\tt FITSI}, namely FITSIN and FITSIMP.

Note if other packages are active there is the small possibility of a
command-name clash.  Issuing such a
command will run that command in the package last activated.  You can
ensure running the {\footnotesize KAPPA} command by inserting a {\tt kap\_}
prefix before the command name.  For example,

\small
\begin{verbatim}
      % kap_rotate
\end{verbatim}
\normalsize
will execute {\footnotesize KAPPA}'s ROTATE application.  There may also be a
clash with UNIX commands and shell built-in functions, though there are
now far fewer conflicts than in earlier versions of {\footnotesize KAPPA}, with
only {\bf look} being ambiguous.  There is also a {\bf glob} in the
C-shell which might confuse you should you forget that
\htmlref{GLOBALS}{GLOBALS} cannot be abbreviated from the shell.

\medskip

{\large {\bf Since the {\normalsize{\bf KAPPA}} commands are the same
in both the shell and {\normalsize{\bf ICL}}, the {\tt \%} and {\tt
ICL>} prompts in the examples and description
\latexelsehtml{below}{in subsequent pages} are
interchangeable unless noted otherwise.}}

\subsection{\xlabel{se_kaphelp}Obtaining Help\label{se:kaphelp}}

\subsubsection{Entering the Help System}
To access the KAPPA help use \htmlref{KAPHELP}{KAPHELP}.

\small
\begin{verbatim}
     ICL> kaphelp
\end{verbatim}
\normalsize
The system responds by introducing {\footnotesize KAPPA}'s help library,
followed by a long list of topics for which help is available,
followed by the prompt {\tt Topic?}. These topics are mostly the
commands for running applications, but they also include global
information on matters such as parameters, data structures and
selecting a graphics device.

From \ICLref\ you can issue other commands for
obtaining help about {\footnotesize KAPPA}.

\small
\begin{verbatim}
     ICL> help kappa
     ICL> help packages
\end{verbatim}
\normalsize
The former is nearly equivalent to entering {\tt kaphelp}.  However,
it is less easy to use as it lacks many of the navigational aids of
KAPHELP.  The latter gives a summary of Starlink packages available
from {\footnotesize ICL}.  If you select the {\tt KAPPA} subtopic, you'll get
a precis of the package's facilities. (This is part of an index of
Starlink packages.)

If you have commenced running an application you can still access the
help library whenever you are prompted for a parameter.  See
\latexelsehtml{Section~\ref{se:parhelp}}{\htmlref{Entering the Help 
System}{se:parhelp}} for details.

\subsubsection{Navigating Help Hierarchies} 

The help information is arranged hierarchically.  The help system
enables you to navigate the library by prompting when it has either
presented a screen's worth of text or has completed displaying the
previously requested help.  The information displayed by the help
system on a particular topic includes a description of the topic and
a list of subtopics that further describe the topic.

You can select a new topic by entering its name or an unambiguous
abbreviation.  If you press the carriage-return key ({\tt{<CR>}}) you
will either continue reading a topic where there is further text to
show, or move up one level in the hierarchy.  Entering a {\tt CTRL/D}
(pressing D whilst holding the CTRL key) terminates the help session.
See the description of \htmlref{KAPHELP}{KAPHELP} for a full
list of the options available at prompts inside the help system, and
the rules for wildcarding and abbreviating topics.

\subsubsection{Help on KAPPA commands}
Help on an individual {\footnotesize KAPPA} application is simply achieved by
entering {\tt kaphelp} followed by the command name, for example

\small
\begin{verbatim}
     % kaphelp centroid
\end{verbatim}
\normalsize
will give the description and usage of the CENTROID command.  There
are subtopics which contain details of the parameters, including
defaults, and valid ranges; examples; notes expanding on the
description; implementation status; and occasionally timing. For
example,

\small
\begin{verbatim}
     ICL> kaphelp hist param ndf
\end{verbatim}
\normalsize
gives details of parameter {\tt NDF} in all applications prefixed by
{\tt HIST}.

(From {\footnotesize ICL} you can also invoke its help system, thus
\small
\begin{verbatim}
     ICL> help centroid
\end{verbatim}
\normalsize
is similar to {\tt kaphelp centroid}, though the {\footnotesize ICL} system has
drawbacks, and you are recommended to run KAPHELP.)

The instruction

\small
\begin{verbatim}
     ICL> kaphelp classified
\end{verbatim}
\normalsize
displays a list of subject areas as subtopics.  Each subtopic lists and
gives the function of each {\footnotesize KAPPA} application in that
classification.  There is also an alphabetic list which can be obtained
directly via the command

\small
\begin{verbatim}
     ICL> kaphelp summary
\end{verbatim}
\normalsize

\subsubsection{\xlabel{se_hypertext}Hypertext Help\label{se:hypertext}}

A modified version of this document exists in hypertext form.  One way
to access it is to use the
\xref{{\tt showme}}{sun188}{displaying_parts_of_documents} command

\small
\begin{verbatim}
     % showme sun95
\end{verbatim}
\normalsize
and a Web browser will appear, presenting the index to the hypertext
form of this document.  The hypertext permits easy location of
referenced documents and applications.  It also includes colour
illustrations.

The \xref{{\tt findme}}{sun188}{performing_keyword_searches}
command lets you search the Starlink documents by
keywords.  For instance,

\small
\begin{verbatim}
     % findme masking
\end{verbatim}
\normalsize
searches the document looking for the word ``masking'' in
them.  The level of searching depends on whether a match is found.
The search starts with the document title, the page (section)
titles, and finally the document text.  The deeper the search,
the longer it will take.  There are switches provided to limit the
level of the search.  The search string may include {\bf sed} or 
{\bf grep} regular expressions.  See \xref{SUN/188}{sun188}{}
or enter

\small
\begin{verbatim}
     % findme findme
     % findme showme 
\end{verbatim}
\normalsize
to learn more about the {\tt findme} and {\tt showme} commands.

\subsection{Changing the Current Directory in ICL}
You should change default directories in \ICLref\ using its
DEFAULT command, and not {\bf cd}.  Thus

\small
\begin{verbatim}
     ICL> default /home/scratch/dro
\end{verbatim}
\normalsize
makes {\tt /home/scratch/dro} the default directory for the {\footnotesize ICL}
session, and for existing and future subprocesses, including
application packages.

\subsection{\xlabel{se_exiting}Exiting an Application\label{se:exiting}}

In normal circumstances when you've finished using {\footnotesize KAPPA}
nothing need be done from the shell, but to end an \ICLref\ session,
enter the {\tt EXIT} command to return to the shell.

What if you've done something wrong, like entering the wrong value for a
parameter?  If there are further prompts you can enter the abort code
{\tt !!} to exit the application.  This is recommended even from the
shell because certain files like your NDFs may
become corrupted if you use a crude {\tt CTRL/C}.  If, however,
processing of the data has begun in the application, it is probably best
to let the task complete, unless it is a long job like image
deconvolution.  If you really must abort, {\tt CTRL/C} should be hit.
From \ICLref\ this ought to return you to a prompt, but the
processing will continue.  Then you can stop the running process by
`killing' it.  First find the task name

\small
\begin{verbatim}
     ICL> tasks
                           TASKNAME  Process Id

                   ndfpack_mon16528  15186
\end{verbatim}
\normalsize

and then kill it.

\small
\begin{verbatim}
     ICL> kill ndfpack_mon16528
\end{verbatim}
\normalsize
This removes a the {\footnotesize NDFPACK} monolith.  {\footnotesize NDFPACK} will be
loaded again once you enter one its commands.  If several attempts
with {\tt CTRL/C} fail to return you to an {\footnotesize ICL} prompt then
it's time for the heavy artillery---you may have to kill your window.
Once back to the shell enter {\tt icl} to return to {\footnotesize ICL}, and
then kill the process as described above.

If you have interrupted a task, it may be necessary to delete the
\htmlref{parameter file}{se:defaults} \latexonly{(Section~\ref{se:defaults})} and
the \htmlref{graphics database}{se:agitidy}
\latexonly{ (Section~\ref{se:agitidy})}.

\newpage
\section{\xlabel{se_param}Parameters\label{se:param}}

{\footnotesize KAPPA} is a command-driven package.  Commands have
{\em parameters\/} by which you can qualify their behaviour.
Parameters are obtained in response to prompts or supplied on a
command line.

For convenience, the main aspects of the parameter system
as seen by a user of {\footnotesize KAPPA} are described below, but note
that most of what follows is applicable to any Starlink
application.

\subsection{\xlabel{se_parsummary}Summary\label{se:parsummary}}

For your convenience, here is a summary of how to use parameters.
If you want more information, go to the appropriate section.

\begin{description}

\item Command-line values \newline
 On the command line values you can supply values by keyword or by position.
 See \latexelsehtml{Section~\ref{se:cmdlindef}}{\htmlref{Specifying Parameter
 Values on Command Lines}{se:cmdlindef}} for more details including abbreviated
 keywords.

\item {\tt ACCEPT, PROMPT, RESET} command-line special keywords \newline
 {\tt ACCEPT} accepts all the suggested defaults that would otherwise
 be prompted.  {\tt PROMPT} prompts for all the parameters not given on
 the command line, and {\tt RESET} resets all the suggested defaults to
 their initial values.  You can find more details and examples in
 \latexelsehtml{Section~\ref{se:parspeckey}.}{\htmlref{Special Keywords: ACCEPT,
 PROMPT, RESET}{se:parspeckey}.}

\item {\tt NAME - Prompt string /Suggested default/ $>$} \newline
 This is a schematic of a prompt.  NAME is the parameter's name.
 You normally respond with the value for the parameter, but there are
 special responses available (see below).
 If you just hit the return key, the suggested default becomes the
 parameter value.  Many parameters are defaulted without prompting.  See 
 \latexelsehtml{Section~\ref{se:defaults} and
 Section~\ref{se:parglobals}}{\htmlref{Defaults}{se:defaults} and
 \htmlref{Globals}{se:parglobals}} for more details.
\end{description}

Here is a list of some example parameter values to illustrate the
possible ways you can respond to a prompt.  Where there are command-line
differences, they are noted.

\begin{description}

\item {\tt 5409.12} \newline
 This is a scalar.  Numerical values can be integer, real, or double precision.

\item {\tt 12,34,56,78} \newline
 This is a vector.  They must be enclosed in {\tt [~]} if the array
 is supplied on the command line, or for character arrays.

\item {\tt [[11,21,31],[12,22,32]]} \newline
 This is a 3$\times$2 array.  Arrays of dimension $>$ 2 should
 appear in nested brackets.  See
 \latexelsehtml{Section~\ref{se:pararrays}}{\htmlref{Arrays}{se:pararrays}}
 for more about array values. 

\item {\tt T} \newline
\vspace{-5ex}
\item {\tt no} \newline
 This is a {\tt TRUE} value followed by a {\tt FALSE} values for
 logical parameters.  Acceptable values are {\tt TRUE}, {\tt FALSE},
 {\tt YES}, {\tt NO}, {\tt T}, {\tt F},  {\tt Y}, {\tt N} and their
 lowercase equivalents.  On the command line,
 the parameter name as a keyword means {\tt TRUE}.  If the name is
 prefixed with {\tt NO}, the value is set to {\tt FALSE}.

\item {\tt a string} \newline
\vspace{-5ex}
\item {\tt "a string"} \newline
 This is a string.  Strings need not be quoted at prompts.  Quotes are
 required on the command line if the string includes spaces or
 wildcards, or is a comma-separated array of strings.  There is more in
 \latexelsehtml{Section~\ref{se:parstrings}.}{\htmlref{Strings}{se:parstrings}.}
 Some parameters offer a selection from a menu to which you give
 an unambiguous abbreviation to select an option.  Other parameters
 can be numerical or a string.  (See
 \latexelsehtml{Section~\ref{se:parmenu}}{\htmlref{Menus}{se:parmenu}}
 for more information.)

\item {\tt filename} \newline
\vspace{-5ex}
\item {\tt @123} \newline
 This enters a filename (or tape drive).  You give a text filename
 verbatim, and NDFs without the file extension.  Foreign formats will
 usually have the file extension. Should the filename be a numerical
 value, it must be preceded by an {\tt @}.  There is more in
 \latexelsehtml{Section~\ref{se:parstrings}.}{\htmlref{Strings}{se:parstrings}.}

\item {\tt min} \newline
\vspace{-5ex}
\item {\tt max} \newline
 This selects the minimum- or maximum-allowed value, but not all
 parameters have a defined a range of permitted values.  See
 \latexelsehtml{Section~\ref{se:parminmax}.}{\htmlref{MIN and MAX
 parameter values}{se:parminmax}.}

\item {\tt !} \newline
 Enters the null value.  This has a variety of special meanings; which
 one will depend on the particular parameter.  For example, null might
 indicate that an output file is not to be created, or a loop is to be
 ended.  There are more examples in
 \latexelsehtml{Section~\ref{se:abortnull}.}{\htmlref{Abort and
 Null}{se:abortnull}.}

\item {\tt !!} \newline
 This aborts the application cleanly.

\item {\tt ?} \newline
\vspace{-5ex}
\item {\tt ??} \newline
 A single question mark presents the online help for the parameter,
 and then reprompts.  A double question mark leaves you in the help
 system to explore other help information.  See
 \latexelsehtml{Section~\ref{se:parhelp}}{\htmlref{Help}{se:parhelp}}
 for examples.  These special values are not supported from the
 command line.

\item \verb+\+ \newline
 This accepts the suggested default for the prompted parameter and the
 respective suggested defaults for all subsequent parameters for which
 prompting would otherwise occur.  On the command line \verb+\+
 is an abbreviation of the {\tt ACCEPT} keyword, and it
 applies to all parameters that would otherwise be prompted.  Note that
 from the shell you write \verb+\\+, as \verb+\+ is a
 shell metacharacter.


\end{description}

\subsection{\xlabel{se_defaults}Defaults\label{se:defaults}}

Command-line values are used mostly for those parameters that are
normally defaulted by the application.  Defaulted parameters enable
applications to have many options, say for controlling the appearance of
some graphical output, without making routine operations tedious because
of a large number of prompts.  The values of normally defaulted
parameters are given in 
\latexelsehtml{Appendix~\ref{ap:full}.}{the \htmlref{application
specifications}{ap:full}.}
You can also find them by obtaining online help on a specific
parameter.  They are enclosed in square brackets at the end of the
parameter description.

\small
\begin{verbatim}
     ICL> kaphelp median param *
\end{verbatim}
\normalsize
gives details of all parameters in application
\htmlref{MEDIAN}{MEDIAN}.  Other packages have similar help
commands.\latexelsehtml{\footnote{IMAGE-format applications
(Appendix~\ref{ap:IMAGEformat}) have their parameter help arranged
differently.  For each parameter there is a line that begins {\tt
vpath} which tells the parameter system from where it should try to
obtain a value or values for that parameter.  If it does not begin
{\tt 'PROMPT'} the parameter will be defaulted.  A vpath of {\tt
'DEFAULT'} indicates that the parameter will take the value given by
the default field; a vpath of {\tt 'DYNAMIC'} means that the
application has computed a suitable default, usually depending on the
values of other parameters or the dimensions of your data
array.}}{\footnote{\htmlref{IMAGE-format}{ap:IMAGEformat} applications
have their parameter help arranged differently.  For each parameter
there is a line that begins {\tt vpath} which tells the parameter
system from where it should try to obtain a value or values for that
parameter.  If it does not begin {\tt 'PROMPT'} the parameter will be
defaulted.  A vpath of {\tt 'DEFAULT'} indicates that the parameter
will take the value given by the default field; a vpath of {\tt
'DYNAMIC'} means that the application has computed a suitable default,
usually depending on the values of other parameters or the dimensions
of your data array.}} If you want to override one of these defaults,
then you must specify the parameter's value on the command line.

When you are prompted you will usually be given a suggested default
value in {\tt / /} delimiters.  You can choose to accept the default by
pressing carriage return.  For example, {\tt 64} is the suggested value
below.

\small
\begin{verbatim}
     XDIM - x dimension of output array /64/ >
\end{verbatim}
\normalsize
Alternatively, enter a different value

\small
\begin{verbatim}
     XDIM - x dimension of output array /64/ > 109
\end{verbatim}
\normalsize
to override the default.
Some defaults begin with an {\tt @}.

\small
\begin{verbatim}
     IN - Input image /@starfield/ >
\end{verbatim}
\normalsize
These are associated with files (text and \HDSref ) and devices (graphics
and tape).  If you want to override the default given, you do not have
to prefix your value with an {\tt @}, {\it{e.g.}}

\small
\begin{verbatim}
     DEVICE - Name of display device /@xwindows/ > x2w
\end{verbatim}
\normalsize
There are rare cases when the syntax is ambiguous, and it is then that you
need to include the {\tt @}.
\latexelsehtml{Section~\ref{se:parstrings}}{\htmlref{Strings}{se:parstrings}}
describes when the {\tt @} is needed.

From \ICLref\ the default value can be edited to save typing by
first pressing the {\tt <TAB>} key.  The editor behaves like the shell
command-line editor.

Defaults may change as data are processed.  Often the current (last)
value of the parameter will be substituted, or a dynamic value is
suggested depending on the values of other parameters.  Here is an
example comprising a loop within an application.

\small
\begin{verbatim}
     INPIC - Image to be inspected /@horsehead/ >
     Title = KAPPA - Gauss
     Array is 109 by 64 pixels
     CHOICE - Peep, Examine or List /'P'/ >

     XCEN - x centre pixel index of 7x7 box /55/ > 60
     YCEN - y centre pixel index of 7x7 box /32/ > 28
        .              .               .              .
        .              .               .              .
        .              .               .              .
     ANOTHER - Another inspection ? /YES/ >
     CHOICE - Peep, Examine or List /'P'/ >

     XCEN - x centre co-ordinate of 7x7 box /60/ > 66
     YCEN - y centre co-ordinate of 7x7 box /28/ >
\end{verbatim}
\normalsize
and so on.  Notice that the current values of the centres are the
suggested values at the second pair of prompts.

Current values of parameters are stored in a {\em parameter file}, and
so they persist between sessions.  For tasks run from the shell, this
is an HDS file {\tt \$ADAM\_USER/}{\it $<$application$>$}{\tt .sdf},
where {\it $<$application$>$} is the name of the application. (If the
environment variable {\tt \$ADAM\_USER} is not defined the parameter
file is situated in {\tt \$HOME/adam}).

Unfortunately, tasks invoked from \ICLref\ use a monolith parameter
file, which contains the individual application parameter files for
the members.  So for example, {\footnotesize KAPPA} has {\tt kappa\_mon.sdf},
{\tt kapview\_mon.sdf}, and {\tt ndfpack\_mon.sdf} stored in same
directory as the individual files.  This duality means that there are
two independent sets of current values for each task depending on
where you ran it from.

The parameter file should not be deleted unless the parameters
values are to be completely reset, or the file has been corrupted in
some way.  If you suspect the latter case, say after \htmlref{interrupting
a task}\latexonly{ (Section~\ref{se:exiting})}, run
\HDSTRACEref\ \latexonly{(SUN/102)} on the file to check its integrity.

\subsection{\xlabel{se_parglobals}Globals\label{se:parglobals}}

{\footnotesize KAPPA} stores a number of global parameters that are used as
defaults to reduce typing in response to prompts.  Global means that
they are shared between applications.  The most common is the last 
dataset (usually NDF) written or accessed.  In the example above, this was
{\tt horsehead.sdf}.  If you just press {\tt <CR>} to the prompt, the
global value is unchanged.  Only when you modify the parameter and the
application completes without error is the global value updated.

All global parameters are stored in HDS file {\tt
\$ADAM\_USER/GLOBAL.sdf}, or {\tt \$HOME/adam/GLOBAL.sdf} if the {\tt
\$ADAM\_USER} environment variable is not defined.  The full list is
given below.
\newline
\newline
\begin{tabular}{@{\hspace{5ex}}l@{~~ --- ~~}l}
{\tt GLOBAL.COORD\_SYSTEM}    & Current co-ordinate system.\\
{\tt GLOBAL.DATA\_ARRAY}      & Last NDF or foreign data file accessed or written.\\
{\tt GLOBAL.GRAPHICS\_DEVICE} & Current graphics workstation.\\
{\tt GLOBAL.IMAGE\_DISPLAY}   & Current image display (base).\\
{\tt GLOBAL.IMAGE\_OVERLAY}   & Current image-display overlay.\\
{\tt GLOBAL.INTERACTIONMODE}  & Current interaction mode. \\
{\tt GLOBAL.LUT} & Last lookup table file accessed or written. \\
{\tt GLOBAL.TRANSFORM} & Current transformation structure. \\
\end{tabular}

{\footnotesize KAPPA} uses the last DATA\_ARRAY written or accessed as
the suggested default value for the next prompt for an NDF structure
or foreign data format.  The same applies to the current lookup table
and transformation structure.  However, the remaining, including the
three graphics global parameters are defaulted---you will not be
prompted.  Details of how to control these parameters are given in the
relevant sections
\latexelsehtml{later---Section~\ref{se:devglobal} for the graphics devices,
Section~\ref{se:interaction} for the interaction mode, and
Section~\ref{se:co-ordsystem} for the co-ordinate systems.}{on
\htmlref{Selecting a Graphics Device}{se:devglobal},
\htmlref{Interaction Mode}{se:interaction}, and
\htmlref{Co-ordinate Systems}{se:co-ordsystem}.}

The values of all global parameters may be inspected with the
\htmlref{GLOBALS}{GLOBALS} task.  You can make them undefined using
\htmlref{NOGLOBALS}{NOGLOBALS}.

\small
\begin{verbatim}
     ICL> globals
     The current data file                : @/home/dro/jkt/ccdpic
     The current graphics device is       : @ps_l
     The current image-display device is  : @xwindows
     The current image-display overlay is : <undefined>
     The current lookup table file is     : @$KAPPA_DIR/spectrum_lut
     The current transformation is        : @/home/dro/deform/warpit
     The current interaction mode is      : <undefined>
     The current co-ordinate system is    : WORLD
\end{verbatim}
\normalsize
In the above example no interaction mode is defined.  The next time
you call an application which uses the interaction mode you
would be prompted for a value.   Likewise for the image-display
overlay. (Under normal circumstances you will not
have to enter the {\tt @} prefix yourself.) 

\subsection{\xlabel{se_parstrings}Strings\label{se:parstrings}}

Notice that the single or double quotes around strings given in
response to prompts for a character parameter can be omitted.  However,
on the command line these delimiters are needed if the string
contains spaces, otherwise the second and subsequent words could be
treated as separate positional parameters.

From the shell the quotes must be escaped.  For example,

\small
\begin{verbatim}
     % settitle myndf \"A new title\"
\end{verbatim}
\normalsize
would assign the title {\tt "A new title"} to the NDF called myndf.

To indicate that the parameter should come from a data-structure object,
prefix the name with an {\tt @} to tell the parameter system that it is
a file name, and not a literal value.

\small
\begin{verbatim}
     PLTITL - Plot title /' '/ @$ADAM_USER/galaxy.mytitle
\end{verbatim}
\normalsize
In this example PLTITL has the value of object MYTITLE in {\tt
galaxy.sdf}.  If the {\tt @} were omitted PLTITL would be {\tt
"\$ADAM\_USER/galaxy.mytitle"}.  You will need the {\tt @} prefix if your
file name is a number.  Note that the file extension should not be
included when giving the name of an \HDSref\ data file, including
\NDFref{NDFs}.  Otherwise HDS will look for an object called SDF
nested within the file.

Responses to prompts are case insensitive for comparison purposes.
Strings for character components in NDFs, plot captions and labels
are treated literally.

\subsection{\xlabel{se_pararrays}Arrays\label{se:pararrays}}

If a parameter requires an array of values, the series
should be in brackets separated by commas or spaces.  For example,

\small
\begin{verbatim}
     PERCENTILES - List of percentiles > [25,95.5,75]
\end{verbatim}
\normalsize
would input three values: 25, 95.5, and 75 into the real parameter
PERCENTILES.  If the application is expecting an exact number of values
you will be reprompted, either for all the values if you give too many,
or the remaining values if you supply too few.  There is one exception
where you can omit the brackets---a fairly common one---and that is in
response to a prompt for a one-dimensional numeric array as above.

From the shell you must escape the brackets.

\small
\begin{verbatim}
     % greyplot mode=pe percentiles=\[95,5\]
     % greyplot mode=pe percentiles='[95,5]'
     % greyplot mode=pe percentiles="[95,5]"
\end{verbatim}
\normalsize
All the above do this.  Single quotes have the advantage
that you can protect all the metacharacters that lie between
the quotes, so you don't need to escape each metacharacter.

Arrays of parameter values should appear in nested brackets.  For
example,

\small
\begin{verbatim}
     CVALUE - Component values > [[2,3],[5,4],[7,1]]
\end{verbatim}
\normalsize
supplies the values for a 2$\times$3-element parameter array, where
element (1,3) has value 7.

\subsection{\xlabel{se_abortnull}Abort and Null\label{se:abortnull}}

Responding to a prompt with a null value {\tt !} will not necessarily
cause the application to abort, but it can force a suitable default to
be used, where this is the most-sensible action.  A further use is
when an optional file may be created, such as a lookup table; a {\tt
!} entered in response to the prompt for the filename means that no
file is to be output.  Many tasks use null as a default for optional
files. In some applications, {\it{e.g.}}\ \htmlref{CRELUT}{CRELUT}, a
null ends an interactive loop.

Responding to a prompt with {\tt !!} will abort the application.  This
process includes the various tidying operations such as the unmapping
and closing of files.  Any other method of stopping an application
prematurely can leave files mapped or corrupted.

\subsection{\xlabel{se_parhelp}Help\label{se:parhelp}}

To get help about a parameter enter {\tt ?}.  Usually, this will give
access to the help-library
information for that parameter, for example,

\small
\begin{verbatim}
     BOX - Smoothing box size /3,3/ > ?

     BLOCK

       Parameters

         BOX(2) = _INTEGER (Read)
            The x and y sizes (in pixels) of the rectangular box to be
            applied to smooth the image. If only a single value is given,
            then it will be duplicated so that a square filter is used,
            except where the image is 1-dimensional, for which the box
            size along the insignificant dimension is set to 1.  The
            values given will be rounded up to positive odd integers,
            if necessary.

     BOX - Smoothing box size /3,3/ >
\end{verbatim}
\normalsize
and then immediately reprompt you for the parameter.  There are
occasions when information about the parameter is insufficient; you
may require to examine the examples or the description of related
parameters.  This can be achieved by entering {\tt ??} to the prompt.
You can then delve anywhere in the help information.  When you exit the
help system you're reprompted for the parameter.

For the old, \htmlref{IMAGE-format}{ap:IMAGEformat} applications, the
{\tt ?} does not enter the help system, and so the help is limited to
an unsatisfactory and miserly 120 characters:

\small
\begin{verbatim}
     DIRN - Direction of ramping > ?
       1 = L-R : 2 = R-L : 3 = B-T : 4 = T-B, where L is left, R is right, B is bott
     om and T is top
     DIRN - Direction of ramping > ?
\end{verbatim}
\normalsize
Fortunately, this form of parameter help is virtually phased out.

\subsection{\xlabel{se_parmenu}Menus\label{se:parmenu}}

Some parameters offer menus from which you select an option.
You do not have to enter the full option string, but merely a string
that selects a choice unambiguously.  In many cases this can be as
little as a single character.  Here is an example,

\small
\begin{verbatim}
     MODE - Method for selecting contour heights /'Free'/ > ?
         The method used to select the contour levels.  The options are
         described below.

           "Area"      - The contours enclose areas of the array for
                         which the equivalent radius increases by equal
                         increments.  You specify the number of levels.
           "Automatic" - The contour levels are equally spaced between
                         the maximum and minimum pixel values in the
                         array.  You supply the number of contour
                         levels.
           "Free"      - You define a series of contour values
                         explicitly.
           "Linear"    - You define the number of contours, the start
                         contour level and linear step between contours.
           "Magnitude" - You define the number of contours, the start
                         contour level and step between contours.  The
                         step size is in magnitudes so the nth contour
                         is dex(-0.4*(n-1)*step) times the start contour
                         level.

\end{verbatim}
\normalsize
where a {\tt F} would be sufficient to select the {\tt "Free"} option,
but at least two characters would be needed if you wanted {\tt "Area"}
or {\tt "Automatic"}.

Some parameters permit a mixture---a choice from a menu, or a
numerical value within a range.  The options are described in full in
\latexelsehtml{the help system and
Appendix~\ref{ap:full}.}{\htmlref{application specifications}{ap:full}.}

\subsection{\xlabel{se_envvar}Environment Variables\label{se:envvar}}

Environment variables operate both on the command line and prompts,
and both from the shell and \ICLref\@.  Thus if IMAGEDIR is
an environment variable pointing to a directory containing the
NDF called ngc1365, you could access it at a prompt as shown
below.

\small
\begin{verbatim}
     IN - Input image /@starfield/ > $IMAGEDIR/ngc1365
\end{verbatim}
\normalsize

\subsection{\xlabel{se_cmdlindef}Specifying Parameter Values on Command Lines
\label{se:cmdlindef}}

Parameters may be assigned values on the command line.  This is useful
for running tasks in batch mode and in procedures, and for specifying
the values of parameters that would otherwise be defaulted.  A
command-line parameter will prevent prompting for that parameter unless
there is an error with the given value, say giving an alphabetic
character string where a floating-point value is demanded.

There are two ways in which parameter values may be given on the
command line: by keyword and by position.  The two forms may be
mixed with care.  The parser looks for positional parameters then
keywords, so you can have some positional values followed by keyword
values.  See some of the examples presented in 
\latexelsehtml{Appendix~\ref{ap:full}.}{the \htmlref{application
specifications}{ap:full}.}

\subsubsection{\xlabel{se_parkeyword}Keyword\label{se:parkeyword}}

Keywords may appear in any order.
Here is an example of command-line defaults using keywords.

\small
\begin{verbatim}
     ICL> picdef current fraction=0.4
\end{verbatim}
\normalsize
FRACTION is a real parameter.  CURRENT is a logical parameter; by giving
just its name it is assigned the value {\tt TRUE}.  {\tt CURRENT=T} would
have the same effect.  To obtain a {\tt FALSE} value for a logical parameter
you add a {\tt NO} prefix to keyword, for example,

\small
\begin{verbatim}
     icl> picdef nocurrent
\end{verbatim}
\normalsize
would be equivalent to the following.

\small
\begin{verbatim}
     icl> picdef current=false
\end{verbatim}
\normalsize

\subsubsection{\xlabel{se_parabbrev}Abbreviations\label{se:parabbrev}}

There is an experimental system that allows you to abbreviate
parameter keywords to the minimum unambiguous length.  To use it, you
must first create an environment variable called {\tt ADAM\_ABBRV}
with an arbitrary value.

So for example
\small
\begin{verbatim}
     % setenv ADAM_ABBRV true
     % display mo=pe pe=\[5,95\] ba=blue
\end{verbatim}
\normalsize
would display an NDF between the 5 and 95 percentiles, and marking bad
pixels in blue.

If you give an ambiguous keyword, the parameter system will present
the list of possible keywords and ask you to select the one you
intended.  Suppose you want to display an image surrounded by axes
annotated using the NCAR fount, and you think that there is only one
parameter starting with F.

\small
\begin{verbatim}
     % display axes f=ncar
     !! Ambiguous keyword F used on the command line
     !  Matches with FILL
     !           and FONT
       - Specify the keyword unambiguously > fo
\end{verbatim}
\normalsize
When reprompted you just give an unambiguous abbreviation for the
desired parameter, that being FONT in this example.

\subsubsection{\xlabel{se_parposition}Position\label{se:parposition}}

Alternatively, you can specify command-line values by position.
Here is an example.

\small
\begin{verbatim}
     % thresh raw clipped 0 255
\end{verbatim}
\normalsize
This applies thresholds to the NDF called raw to form a new NDF
called clipped.  The values between 0 and 255 are unchanged.  Note that
trailing parameters may be omitted---NEWLO and NEWHI in the above
example---but intermediate ones may not.  The position of a parameter
can be found in the {\tt Usage} heading in
\latexelsehtml{Appendix~\ref{ap:full}}{the \htmlref{application
specifications}{ap:full}} or the help for the application.\footnote{In
\htmlref{IMAGE-format}{ap:IMAGEformat} applications
look in the help topic of the same name at the level beneath the topic
{\tt Parameters} in a given application.}

\subsubsection{\xlabel{se_keyvspar}Keyword versus Positional Parameters
\label{se:keyvspar}}

For tasks with a few parameters, using position is quick and
convenient. However, in complex programmes with many parameters it
would be tedious not only to enter all the intermediate values between
the ones you want to define, but also to remember them all.  Another
consideration is that some parameters do not have defined positions
because they are normally defaulted.  Keywords may also be
\htmlref{abbreviated}{se:parabbrev}\latexonly{(Section~\ref{se:parabbrev})}.
Thus the keyword technique is recommended for most parameters,
especially in scripts and procedures. Unabbreviated keywords insulate
scripts against new keywords and positional changes that are sometimes
needed.

See
\latexelsehtml{Section~\ref{se:custom}}{\htmlref{custom {\footnotesize KAPPA} 
commands}{se:customcom}}
if you want to learn how further to abbreviate command strings to
reduce typing for manual operation.

\subsubsection{\xlabel{se_parspecial}Special Behaviour\label{se:parspecial}}

Sometimes specifying a parameter on the command line induces different
behaviour, usually to inhibit a loop for procedures, or to eliminate
unnecessary processing.
For instance,

\small
\begin{verbatim}
     ICL> centroid blob init=[51,42] mode=i
\end{verbatim}
\normalsize
will determine the centroid near the point (51,42) in the NDF called
blob, and then it exits, whereas without the INIT value you would be
reprompted for a further initial position; and

\small
\begin{verbatim}
     % display galaxy mode=sc high=3000 low=1000
\end{verbatim}
\normalsize
prevents the calculation of the extreme values of the NDF called
galaxy that are normally given as suggested defaults for parameters
HIGH and LOW, because HIGH and LOW are already known.

\subsection{\xlabel{se_parspeckey}Special Keywords: ACCEPT, PROMPT, RESET
\label{se:parspeckey}}

Another way in which prompts and default values can be controlled is
by use of the keywords ACCEPT, PROMPT and RESET.

The RESET keyword causes the {\em suggested\/} default value of all
parameters (apart from those already specified before it on the
command line) to be set to the original values specified by the
application or its interface file.  In other words global and current
values are ignored.

The PROMPT keyword forces a prompt to appear for every application
parameter.  This can be useful if you cannot remember the name of a
defaulted parameter or there would be too much to type on the command
line.  However, it may prove tedious for certain applications that have
tens of parameters, most of which you normally never see.  You can abort
if it becomes too boring.

The ACCEPT keyword forces the parameter system to accept the suggested
default values either for every application parameter if the keyword
appears on the command line, or all subsequent parameters if it is
supplied to a prompt.  In other words, those parameters which would
normally be prompted with a value between {\tt / /} delimiters take
the value between those delimiters, {\it{e.g.}}\ XDIM we saw in
Section~\ref{se:defaults} would take the value {\tt 64}. Parameters
that are normally defaulted behave as usual.  The ACCEPT keyword needs
to be used with care in scripts because not every parameter has a
default, and therefore must be given on the command line for the
application to work properly.  For example,
\htmlref{CREFRAME}{CREFRAME} must have a value specified for parameter
OUTPIC, the name of the output data object. If we run the application
like this:

\small
\begin{verbatim}
     ICL> creframe accept
\end{verbatim}
\normalsize
it would fail in the sense that it would still have to prompt for a
value---it does not know where to write the output IMAGE
data file.  However, if we run CREFRAME like this:

\small
\begin{verbatim}
     ICL> creframe outpic=stars accept
\end{verbatim}
\normalsize
it would generate an output image using default values for all the
parameters except OUTPIC, and write the output to file {\tt stars.sdf}.
Another point to be wary of is that some applications have loops,
{\it{e.g.}}\ \htmlref{INSPECT}{INSPECT}, \htmlref{LOOK}{LOOK},
\htmlref{LUTABLE}{LUTABLE}, and if you use the ACCEPT keyword
it will only operate the first time the application gets a parameter
value.

Sometimes the keyword ACCEPT can be used without any parameter value
specifications on the command line.
For example, we could follow the above command by the command:

\small
\begin{verbatim}
     ICL> look accept
\end{verbatim}
\normalsize
and the central 7$\times$7 array of the image created by CREFRAME would
be displayed on your terminal without any parameter values being
prompted.  The symbol \verb+\+ has the same effect as ACCEPT
when used on the command line or at prompts, thus:

\small
\begin{verbatim}
     ICL> look \
\end{verbatim}
\normalsize
would have the same effect as the previous example---and is quicker
to type.  In command lines from the shell, the backslash is a
metacharacter and has to be escaped.  The easiest way to do that is
to double the backslash.

\small
\begin{verbatim}
     % look \\
\end{verbatim}
\normalsize

How do you find out which parameters have suggested defaults, as
opposed to those that are normally defaulted?  Well, a good rule of
thumb is that parameters for output files (images, lookup tables and
text) will not have a default, but the remainder will.  There are some
exceptions, such as where null is the default for optional files.
Consulting the description of the parameters should give the suggested
defaults, where appropriate.\footnote{This information is not given in
the old-style (\htmlref{IMAGE-format}{ap:IMAGEformat}) documentation.
For these you can look at the interface information in the KAPPA help
library given for each parameter, or the interface file itself
({\tt{\$KAPPA\_DIR/kappa\_mon.ifl}}).  If a parameter is given a suggested
default it will have a line beginning {\tt ppath} or a {\tt default}
line.} If you want to use ACCEPT for an automatic procedure or batch
job you could do some tests to find which parameters get prompted and
then put them on the command line in your procedure.

The RESET and ACCEPT keywords will work in tandem.  So for instance,

\small
\begin{verbatim}
     ICL> look reset accept
\end{verbatim}
\normalsize
will reset the suggested defaults of \htmlref{LOOK}{LOOK} to their
original, preset values, and accept these as the parameter values.

These special keywords may be abbreviated to no fewer than
two characters, if you have enabled
\htmlref{keyword-abbreviation}{se:parabbrev}\latexonly{ (Section~\ref{se:parabbrev})}.

\subsection{\xlabel{se_parminmax}MIN and MAX parameter values
\label{se:parminmax}}

Many parameters have well-defined ranges of allowed values. In some
cases it is useful to assign the maximum or minimum value to the
parameter.  Rather than give some numerical value, you can instead
supply {\tt MIN} to select the minimum-allowed value, and {\tt MAX}
to select the maximum.  This applies both on the command line and at
prompts.  In the example,

\small
\begin{verbatim}
     % block wlim=max
\end{verbatim}
\normalsize
parameter WLIM takes its maximum (1.0) meaning that if any of the
input pixels in the smoothing box is bad, the corresponding output
pixel is set bad.

Consult the reference section or the online help to see if a given
parameter has such a range.  If you attempt to use {\tt MIN} and
{\tt MAX} where there is no range defined, you'll see an error
message like

\small
\begin{verbatim}
    !! SUBPAR_MNMX: Parameter FONT - no upper limit set
\end{verbatim}
\normalsize
and you'll be invited to give another value.

\subsection{\xlabel{se_parout}Output Parameters\label{se:parout}}

Not only can programmes have parameters to which you supply values,
but they can also write out the results of their calculations to {\em
output\/} or {\em results parameters}.  This makes the results
accessible to subsequent tasks and to shell and \ICLref\ variables.
Example results are statistics like the standard deviation or the FWHM
of the point-spread function, the co-ordinates of points selected by a
cursor, or the attributes of an NDF.  They are not data files created
by the application.  In
\latexelsehtml{Appendix~\ref{ap:full}}{the \htmlref{application
specifications}{ap:full}} they are listed separately from
other parameters.

From the shell you can access these output parameters using the
{\footnotesize KAPPA} tool \htmlref{PARGET}{PARGET}.  Suppose that you
want to subtract the mean of an NDF called myndf to make an
a new NDF called outndf.

\small
\begin{verbatim}
    % stats myndf > /dev/null
    % set mean = `parget mean stats`
    % csub myndf $mean outndf 
\end{verbatim}
\normalsize
\htmlref{STATS}{STATS} calculates the statistics of myndf,
the displayed output being discarded.  PARGET reports the mean value
which is assigned to shell variable {\tt mean}.  Thereafter the mean
value is accessible as {\tt \$mean} in that process.  Thus the final
command subtracts the mean from myndf.

You can obtain vector values too.

\begin{verbatim}
    % ndftrace myndf > /dev/null
    % set axlab = `parget ndftrace axlabel`
    % display otherndf abslab=$axlab[1] ordlab=$axlab[2] axes 
\end{verbatim}
\normalsize
This displays the image otherndf surrounded by axes, but the plot's
axis labels originate from another dataset called myndf.
There are more examples in the \xref{{\sl C-Shell Cookbook}}{sc4}{}.

At the time of writing, ICL only permits scalar variables.  To do the
first example above from ICL, you would enter something like this.

\small
\begin{verbatim}
    ICL> stats myndf mean=(vmean)
    ICL> csub myndf (vmean) outndf 
\end{verbatim}
\normalsize
{\tt vmean} is an ICL variable.  The parentheses have the same effect
as the \verb+$+ in the C-shell example, meaning ``the value of'' the
variable.  Note that you can't redirect the output to {\tt /dev/null}.

If you use the \htmlref{PROMPT keyword}{se:parspeckey}
\latexonly{(see Section~\ref{se:parspeckey})} for an application with output
parameters, the programme will bizarrely prompt you for these.  It is
not asking for a value, but a {\em location\/} where to store the
value.  It is strongly recommended that you just accept the default
(normally zero) so that the values are written to their parameter
file, and hence permits PARGET to work.

\newpage
\section{\xlabel{se_graphdev}Graphics Devices and Files\label{se:graphdev}}

\subsection{\xlabel{se_selgradev}Selecting a Graphics Device
\label{se:selgradev}}

You can find the list of available devices and their names with task
\htmlref{GDNAMES}{GDNAMES}.  Names can be abbreviated provided they
remain unambiguous.

\subsubsection{\xlabel{se_devglobal}Global Parameters\label{se:devglobal}}

There are global parameters for the graphics devices.
\begin{description}
\item [Graphics Device] --- The device used for line graphics.
\item [Image Display] --- The device used for displaying images, but this
does not exclude the possibility of line graphics, such as axes, when
they are associated with the image.  The image-display device should
be capable of showing at least 24 colours or grey levels simultaneously,
and more for certain applications.
\item [Image-display Overlay] --- The device used for overlays on the image
display.  Again there is a restriction there should be at least 16
colours, except for X-windows overlays since these only have a single
colour.
\end{description}
The purpose of these global parameters is ostensibly to prevent
unnecessary prompting. However, there is an ulterior motive as well. 
The selection of devices outside of the graphics applications enables
us to perform other necessary actions just once. 

There are commands for selecting each type of device:
\htmlref{GDSET}{GDSET}, \htmlref{IDSET}{IDSET} and
\htmlref{OVSET}{OVSET} for the graphics device, image display and
overlay respectively.  For example,

\small
\begin{verbatim}
     ICL> idset xwindows
     % gdset ps_l
\end{verbatim}
\normalsize
A selection remains in force until you change it via another
device-selecting command, use \htmlref{NOGLOBALS}{NOGLOBALS}, or delete
the globals file.  The current choices can be inspected via the
\htmlref{GLOBALS}{GLOBALS} command.   If the
global parameter is undefined you will be prompted for the device if an
application requires it.

You can override the global parameter for the duration of a single
application by specifying it by keyword (normally {\tt DEVICE=}), or in
some applications, by position.  Here is an example.

\small
\begin{verbatim}
     ICL> contour device=ps_p
\end{verbatim}
\normalsize

\subsubsection{\xlabel{se_x-windows}X-windows\label{se:x-windows}}

The most commonly used devices are X-windows.  These can require a
little preparation before you select a device.  Starlink graphics use
\GWMref\ to manage windows.  It enables a window to persist between
separate applications; or to be shared by \GKSref\ and \IDIref\ applications,
and even programmes running on different machines.  See
\xref{SUN/130}{sun130}{} for details of {\footnotesize GWM} and how to
change your X-defaults file ({\tt{\$HOME/.Xdefaults}}), but the salient
points are given below.

If the window appears on a terminal or workstation other than the one
running the {\footnotesize KAPPA} executables you will need to redirect
output to your screen, if you have not already done so for some other
software.  You either use the \xref{{\tt xdisplay}}{sun129}{} command

\small
\begin{verbatim}
    % xdisplay myterm.mysite.mydomain.mycountry
\end{verbatim}
\normalsize
or set the DISPLAY environment variable to point to the address of
your screen.

\small
\begin{verbatim}
    % setenv DISPLAY myterm.mysite.mydomain.mycountry:0
\end{verbatim}
\normalsize
You substitute your machine's address or IP number.  (Ask your
computer manager.)

If you do not create the window before running {\footnotesize KAPPA}, the
first graphics application to open an X-windows device will create the
window, using certain defaults.  The defaults control amongst others
the foreground and background colours, the number of colours
allocated, the size and location of the window.  These defaults may be
altered with an X-defaults file, or a window created with the {\footnotesize
GWM} {\tt xmake} command.

\small
\begin{verbatim}
    % xmake xwindows -geom 600x450 -fg yellow -bg black
\end{verbatim}
\normalsize
This example makes a window of dimension 600-by-450 pixels, the
background colour is black and colour for the line graphics is
yellow. 

{\footnotesize GWM} also enables a single-colour overlay to be associated
with a window.  This lets you clear the overlay independently of the
image (task \htmlref{OVCLEAR}{OVCLEAR}), so you can annotate a picture
with other graphics. This is most useful in interactive applications
like \htmlref{ARDGEN}{ARDGEN}, \htmlref{CONTOVER}{CONTOVER},
\htmlref{INSPECT}{INSPECT} or \htmlref{SEGMENT}{SEGMENT}.  A
{\footnotesize GWM} overlay does have one drawback, however: it halves the
number of available colour indices.  If you want to use an X-windows
overlay, you must ensure that you have made the window with the {\tt
-overlay} qualifier to {\tt xmake}, or change your X-defaults file,
or use a {\tt +} suffix to the device name.

The following set up to place in your X-defaults file is a reasonable
compromise, as it maximises the number of colour indices for the
image-display window (xwindows), has a yellow overlay (xoverlay), and
a line-graphics window (x2windows).  In the defaults file there are
the following lines

\small
\begin{verbatim}
     Gwm*xwindows*colours:         80
     Gwm*xwindows*overlay:         True
     Gwm*xwindows*ovcolour:        Yellow
     Gwm*xwindows2*colours:        20
\end{verbatim}
\normalsize
and you can also set the sizes of the windows too.  Notice that the
second device name is x2windows, but the window name is xwindows2.
Don't ask why.  An overlay on this device is called x2overlay.  This
confusing name rule applies also to all but the first window of the
maximum of four windows allowed.

The device names can be abbreviated, to give unambiguous names.  Thus
you can enter {\tt xw} for the xwindows device, and {\tt xo} for its
overlay; {\tt x2w} for the x2windows device, and {\tt x2o} for its
overlay; and so on.  This is the reason for having device names as
they are.  If a window does not exist, you can create one with an
overlay by appending a {\tt +} to the name, so for example a device
name of {\tt x2w+} would open the x2windows device with an overlay
plane. 

The following tells {\footnotesize KAPPA} that these are the current devices.
These remain as global parameters, so you probably will not need to
issue these commands that often.

\small
\begin{verbatim}
     % idset xwindows
     % gdset x2windows
     % ovset xoverlay
\end{verbatim}
\normalsize
\bigskip

For an X-windows overlay, INSPECT in the cursor
mode uses dashed lines instead of changing colour, for example when
indicating the position of a region or slice.  Similarly for
CONTOVER when pen rotation is selected, and for
annotations in \htmlref{CURSOR}{CURSOR} and SEGMENT.

\subsection{Composite Hardcopy Plots}

{\footnotesize KAPPA} applications are modular so that you can build up
more-complicated plots, possibly add annotations with other
packages especially \PONGOref\@.  This is fine if the device is
some sort of screen.  However, when it is a hardcopy device,
\GKSref\ creates a separate file for each task you run.  So how
do you get the composite plot out on paper?  Well you can't just
append these files to create a single output.

What you must do is as follows.
\begin{itemize}
\item  Select an encapsulated PostScript device, be it colour or monochrome.
\item  Produce your graphics.  This will normally create files called
{\tt gks74.ps}, {\tt gks74.ps.1} {\it etc.}\  The first file it creates
has a last digit of one more than in the name of the last output file
you created in the directory.
\item Use \PSMERGEref\ to combine the plots.
\end{itemize}

To give an example, suppose we wanted to overlay a contour plot
on an image.

\small
\begin{verbatim}
    % idset epsfcol_l
    % display $KAPPA_DIR/comwest lut=$KAPPA_DIR/spectrum_lut mode=ra axes
    % contover device=epsfcol_l mode=au \\
    % psmerge gks74.ps* > myplot.ps
    % rm gks74.ps*
\end{verbatim}
\normalsize
You then print {\tt myplot.ps} to the colour PostScript printer.
PSMERGE also has options for scaling and rotating plots.

You can specify the output file name by appending the name to the
file, such as \linebreak
{\tt ps\_p;/home/scratch/dro/horse.ps}, which would place
the graphics output into file \linebreak 
{\tt /home/scratch/dro/horse.ps}.

\newpage
\section{\xlabel{se_datastr}Data structures\label{se:datastr}}

In an ideal world you would not need to know how your data are stored.
It would be transparent. (The trendies call it object orientation.) To
some extent Starlink applications achieve that through standard, but
extensible, data structures, and the ability to apparently operate on
other formats through the so-called \htmlref{`on-the-fly
conversion'}{se:autoconvert} (see
\latexonly{Section~\ref{se:autoconvert} and} \xref{SUN/55}{sun55}{}).
However, for historical reasons there are a number of data formats in
Starlink software, and at the moment {\footnotesize KAPPA} is no exception in
that it has two formats.

The main one is official standard data format for Starlink
applications---the \NDFextref{NDF}\ (Extensible $N$-dimensional Data
Format, SUN/33).  The second is the historical and flawed
\htmlref{IMAGE structure}{ap:IMAGEformat}, which is gradually being
phased out, and only affects twenty-one commands (the functionality of
which can mostly be achieved with other applications).  Both formats,
though, use \HDSref\ which has numerous advantages, not least that
{\em HDS files are portable between operating systems}; both have file
extension {\tt .sdf}.

The NDF is more detailed and has been carefully designed to facilitate
processing by both general applications like {\footnotesize KAPPA} and
specialist packages.  It contains an $n$-dimensional data array that
can store most astronomical data such as spectra, images and
spectral-line data cubes.  The NDF may also contain information like a
title, axis labels and units, error and quality arrays.  There are
also places in the NDF, called {\em extensions}, to store any
ancillary data associated with the data array, even other NDFs.

The two formats and the meaning of their components are described in
\latexelsehtml{Appendices~\ref{ap:NDFformat} and
\ref{ap:IMAGEformat}.}{in \htmlref{NDF standard components}{ap:NDFformat} and
\htmlref{IMAGE data format}{ap:IMAGEformat}.}  The former
includes the commands for manipulating the components.  Suffice to say
here that the IMAGE format is more or less a degenerate form of the NDF,
and so both formats can be used in parallel throughout {\footnotesize KAPPA};
those applications using the IMAGE format will have restricted
functionality, for example limited axis and quality processing, an
inability to handle \htmlref{NDF sections}{se:ndfsect}
\latexonly{(Section~\ref{se:ndfsect})}, and cannot use the
`on-the-fly' data-conversion scheme.

Both formats permit arrays to have seven dimensions, but some
applications only handle one-dimensional and/or two-dimen\-sional data
arrays.  The data and variance arrays are not constrained to a single
data type.  Valid types are the \htmlref{HDS numeric primitive
types}{ap:HDStypes}\latexonly{, see Appendix~\ref{ap:HDStypes}}.

Many applications are generic, that is they can work on all or some of
these data types directly.  This makes these applications faster, since
there is no need to make a copy of the data converted to the type
supported by the application.  If an application is not generic it only
processes {\_REAL} data.   Look in the {\tt Implementation Status} in
the help or the reference manual.  If none is given you can assume that
processing will occur in \_REAL.

In {\footnotesize KAPPA} the elements of the data array are often called 
\htmlref{{\em pixels}}{se:pixelindices}, even if the NDF is not
two-dimensional.

\subsection{Looking at the Data Structures}

You can look at a summary of the \NDFref{NDF}\ or
\htmlref{IMAGE}{ap:IMAGEformat} structure using the task
\htmlref{NDFTRACE}{NDFTRACE}, and obtain the values of NDF extension
components with the {\tt \htmlref{setext}{SETEXT} option=get} command.
\HDSTRACEref\ \latexonly{(SUN/102)} can be used to look at array values and
extensions.

\subsection{Editing the Data Structures}
There are facilities for editing \HDSref\ components, though these should be
used with care, lest you invalidate the file.  For instance, if you
were to erase the DATA\_ARRAY component of an \NDFref{NDF}, the file would no
longer be regarded as an NDF by applications software.

In {\footnotesize KAPPA}, \htmlref{ERASE}{ERASE} will let you remove any
component from within an HDS container file, but you have to know the
full path to the component.  \htmlref{SETEXT}{SETEXT} has options to
erase extensions and their contents, without needing to know how these
are stored within the HDS file.  It also permits you to create and
rename extension components, and assign new values to existing
components.  There are a number of commands for manipulating
FITS-header information stored in the NDF's FITS extension. These are
described in
\latexelsehtml{Section~\ref{se:fitsairlock}.}{\htmlref{the FITS
airlock}{se:fitsairlock}.}

\Figaroref\ offers some additional tasks (\xref{CREOBJ}{sun86}{CREOBJ},
\xref{DELOBJ}{sun86}{DELOBJ}, and \xref{RENOBJ}{sun86}{RENOBJ}) for
editing HDS components.

\subsection{\xlabel{se_native}Native Format\label{se:native}}
Although \HDSref\ files are portable you are recommended to copy them to
the host machine, and run application \htmlref{NATIVE}{NATIVE} on them
for efficiency gains.  NATIVE converts the data to the native format
of the machine on which you issue the command.  If you don't do this,
every time you access the data in your \NDFref{NDF}, say, this conversion
process occurs.  In the C-shell you can use a {\bf foreach} construct
to process several files.  The following converts all the HDS files in
the current directory.

\small
\begin{verbatim}
     % foreach file (*.sdf)
     > native $file:r
     > echo Converted $file
     > end
\end{verbatim}
\normalsize
The \verb+>+ is a prompt from the shell.

\newpage
\section{\xlabel{se_ndfsect}NDF Sections\label{se:ndfsect}}

You will frequently want to examine or process only a portion of your
dataset, be it to focus on a given object in an image, or a single
spectrum between nominated wavelengths, or a plane of a cube.  You
could use \htmlref{NDFCOPY}{NDFCOPY} or \htmlref{MANIC}{MANIC} in some
circumstances to make a new NDF containing the required data, but this
would be inconvenient as you would need more disc space, and to invent
and remember a new filename. In addition, MANIC currently does not
fully support the NDF, and so you could lose axis information, for
example.  You will be pleased to learn that there is a succinct and
powerful alternative that obviates the need to create a new file---the
NDF section.  The application just processes a `rectangular' subset,
or section, of the NDF that you nominate.  Certainly, it requires you
to learn a little syntax, but after you use it a few times it will
seem cheap at the price for the advantages it offers.  Please note
however, that sections cannot be given to an
\htmlref{IMAGE-format}{ap:IMAGEformat} application.

An NDF section is defined by specifying the bounds of the portion of
the NDF to be processed immediately following the name of the NDF.
You can do this in any place where an NDF name alone would suffice,
for example, on the command line or in response to a prompt or as a
default in an interface file.  The syntax is a series of subscripts
within parentheses and may be given in several ways.  Here is a simple
example.

\small
\begin{verbatim}
     ICL> stats cluster(101:200,51:150)
\end{verbatim}
\normalsize

This would derive statistics of a 100$\times$100-pixel region
starting at pixel indices (101,~51) in the NDF called cluster.
Alternatively, ranges of axis co-ordinates may be given instead of
pixel indices.  Besides giving lower and upper bounds as above, you may
specify a centre and extent.  Sections are not limited to
subsets---supersets are allowed.  See the paragraphs below for more
details of these features.

If you {\em do\/} want to make a new NDF from a portion of an existing one,
you should use the command NDFCOPY.  An NDF's shape may be changed
{\it in situ\/} by \htmlref{SETBOUND}{SETBOUND}.

{\em Note if you supply an NDF section on a C-shell command line,
you must escape the parentheses.}  For example, the following
are both equivalent to the earlier example.

\small
\begin{verbatim}
     % stats cluster"(101:200,51:150)"
     % stats cluster\(101:200,51:150\)
\end{verbatim}
\normalsize



\subsection{\xlabel{se_ndfsec-bounds}Specifying Lower and Upper Bounds
\label{se:ndfsec-bounds}}

The subscript expression appended to an NDF name to specify a section
may be given in several ways. One possible method (corresponding with
the example above) is to give the lower and upper bounds in each
dimension, as follows

\small
\begin{quote}
\begin{center}
{\tt NAME( a:b, c:d, ... )}
\end{center}
\end{quote}
\normalsize

where `{\tt a:b}', `{\tt c:d}', ({\it etc.}) specify the lower and upper
bounds. The bounds specified need not necessarily lie within the actual
bounds of the NDF, because {\em bad\/} pixels will be supplied in the
usual way, if required, to pad out the NDF's array components whenever
they are accessed. However, none of the lower bounds should exceed the
corresponding upper bound. 

Omitting any of the bounds from the subscript expression will cause the
appropriate (lower or upper) bound of the NDF to be used instead. If you
also omit the separating `:', then the lower and upper bounds of the
section will both be set to the same value, so that a single pixel will
be selected for that dimension. Omitting the bounds entirely for a
dimension (but still retaining the comma) will cause the entire extent
of that dimension to be used. Thus, 

\small
\begin{quote}
\begin{center}
{\tt IMAGE(,64)}
\end{center}
\end{quote}
\normalsize

could be used to specify row 64 of a 2-dimensional image, while

\small
\begin{quote}
\begin{center}
{\tt CUBE( 1, 257:, 100 )}
\end{center}
\end{quote}
\normalsize

would specify column 1, pixels 257 onwards, selected from plane number
100 of a 3-dimensional `data cube', forming a one-dimensional section.

\subsection{\xlabel{se_ndfsec-cenext}Specifying Centre and Extent
\label{se:ndfsec-cenext}}

An alternative form for the subscript expression involves specifying the
centre and extent of the region required along each dimension, as
follows

\small
\begin{quote}
\begin{center}
{\tt NAME( p}$\sim${\tt q, r}$\sim${\tt s, ... )}
\end{center}
\end{quote}
\normalsize

where `{\tt p}$\sim${\tt q}', `{\tt r}$\sim${\tt s}', ({\it etc.})
specify the centre and extent. Thus, 

\small
\begin{quote}
\begin{center}
{\tt NAME(100}$\sim${\tt 11,200}$\sim${\tt 5)}
\end{center}
\end{quote}
\normalsize

would refer to an 11$\times$5-pixel region of an image centred on pixel
(100,~200).

If the value before the delimiting `$\sim$' is omitted, it will default
to the index of the central pixel in that dimension (rounded downwards
if there are an even number of pixels). If the value following the
`$\sim$' is omitted, it will default to the number of pixels in that
dimension. Thus, 

\small
\begin{quote}
\begin{center}
{\tt IMAGE( }$\sim${\tt 100, }$\sim${\tt 100)}
\end{center}
\end{quote}
\normalsize

could be used to refer to a 100$\times$100-pixel region located
centrally within an image, while 

\small
\begin{quote}
\begin{center}
{\tt IMAGE( 10}$\sim${\tt , 20}$\sim$ {\tt )}
\end{center}
\end{quote}
\normalsize

would specify a section which is the same size as the original image, but
displaced so that it is centred on pixel (10,~20).

\subsection{\xlabel{se_ndfsec-axis}Using Axis Co-ordinates to Specify Sections
\label{se:ndfsec-axis}}

A further variation is that numerical values in subscript expressions
may be specified either as integers, in which case they refer to pixel
indices, or as floating-point numbers, in which case they refer to
{\em axis\/} (or {\em data}) co-ordinates 
\latexelsehtml{(for a description of {\em data\/} co-ordinates, see
Section~\ref{se:dataco-ordinates}).}{(see \htmlref{data
co-ordinates}{se:dataco-ordinates} for a description).}  Here a
``floating-point'' number is one containing a decimal point and/or an
exponent.  Double-precision arithmetic is used to process these
values, but either double- or single-precision notation may be used
when supplying them.  Both linear and non-linear {\em axis\/}
co-ordinates are supported, the values supplied being automatically
converted into the corresponding pixel indices before use. For
instance,

\small
\begin{quote}
\begin{center}
{\tt SPECTRUM(6500.0:7250.0)}
\end{center}
\end{quote}
\normalsize

could be used to select the appropriate region of a spectrum calibrated
in {\AA}ngstroms, while

\small
\begin{quote}
\begin{center}
{\tt SPECTRUM(6000.0}$\sim${\tt 500.0)}
\end{center}
\end{quote}
\normalsize

would select a region of the spectrum approximately from 5750 to 
6250.0~{\AA}ngstroms (the exact extent depending the values of the
axis co-ordinates), and

\small
\begin{quote}
\begin{center}
{\tt SPECTRUM(5500.0}$\sim${\tt 21)}
\end{center}
\end{quote}
\normalsize

would select a 21-pixel-wide region of the spectrum centred on
5500~{\AA}ngstroms.

Command \htmlref{NDFTRACE}{NDFTRACE} will show whether or not an NDF
has axis co-ordinates, and if so, it reports their extents.

\subsection{\xlabel{se_ndfsec-dim}Changing Dimensionality
\label{se:ndfsec-dim}}

The number of dimensions given when specifying an NDF section need not
necessarily correspond with the actual number of NDF dimensions,
although usually it will do so. 

If you specify fewer dimensions than there are NDF dimensions, then
any unspecified bounds will be set to (1:1) for the purposes of
identifying the pixels to which the section should refer. Conversely, if
extra dimensions are given, then the shape of the NDF will be padded
with extra bounds set to (1:1) in order to match the number of
dimensions. In all cases, the resulting section will have the number of
dimensions you have actually specified, the padding serving only to
identify the pixels to which the section should refer. 

In {\footnotesize KAPPA} there are a number of applications that can only
handle a fixed number of dimensions ({\it{e.g.}}\
\htmlref{DISPLAY}{DISPLAY}, \htmlref{LINPLOT}{LINPLOT},
\htmlref{MEDIAN}{MEDIAN}).  NDF sections permit such applications to
have wider applicability, since the applications can operate on full
NDFs of arbitrary dimensionality.  So for instance, DISPLAY could show
planes of a datacube.

\subsection{\xlabel{se_ndfsec-mixing}Mixing Bounds Expressions
\label{se:ndfsec-mixing}}

In the \htmlref{last example}{se:ndfsec-axis}
\latexonly{ (in Section~\ref{se:ndfsec-axis})} both {\em axis\/}
co-ordinates and pixel indices were mixed in the same subscript
expression. In fact, any of the features described earlier may be
combined when specifying an NDF section, the only restriction being that
when the shape of the resulting section is expressed in pixel indices,
the lower bound must not exceed the upper bound in any dimension. Thus,
all the following might be used as valid specifications for NDF
sections 

\small
\begin{quote}
\begin{center}
\begin{tabular}{l}
{\tt NDF(3.7)}\\
{\tt NDF(,5:)}\\
{\tt NDF(-77:13.8,,4)}\\
{\tt NDF(66}$\sim${\tt 9,4:17)}\\
{\tt NDF(}$\sim${\tt 5,6}$\sim${\tt )}\\
{\tt NDF(}$\sim${\tt ,:)}\\
{\tt NDF(5500.0}$\sim${\tt 150,)}\\
{\tt NDF(3.0}$\sim${\tt 1.5,-78.06D-3:13.0545,,,,)}\\
\end{tabular}
\end{center}
\end{quote}
\normalsize

Many other combinations are obviously possible.

\newpage
\section{\xlabel{se_ndfhistory}NDF History\label{se:ndfhistory}}

During a spring clean of directories to free some space (what d'
y'mean you don't?), most of us will have encountered data files whose
purpose and worth are long forgotten. We're reluctant to remove them
in case they contain irreplaceable data. Some people are very good and
make copious notes\ldots Even then the result of a casual experiment
might not be recorded.  For those who are lazy, such files can be a
frequent dilemma.  Even a quick look at a plot of the data is often
little assistance.  As you've probably surmised, the
\NDFextref{NDF}\ offers a solution.

Within an NDF you may record {\em history\/} information.  This is
usually a chronicle of the processing stages used to form the NDF, 
including the parameter values of the applications invoked; but it may
also include commentary you provide, for example, the rationale for
doing certain operations.

History is associated with individual NDFs; it is not some global
attribute of a data-processing session.  An NDF has a {\em history
update mode}, which remains with the NDF and any descendant NDF, until
the update mode is altered or the history erased.  By default, the update
mode is {\tt "Disabled"}, meaning that no history recording occurs.  To
permit history recording you must first switch it on, selecting from
three update modes---{\tt "Quiet"}, {\tt "Normal"}, and {\tt
"Verbose"}---which give increasingly more detailed information.
% There are some examples below, for you to gauge which is appropriate.

\subsection{\xlabel{se_conhistory}Control and Content of History Recording
\label{se:conhistory}}

Task \htmlref{HISSET}{HISSET} lets you set the history update mode.
The default is {\tt "Normal"}, thus here the command

\small
\begin{verbatim}
     % hisset hr1068
\end{verbatim}
\normalsize
switches normal history recording on for NDF hr1068.  Thereafter
whenever you alter this NDF, or create another NDF from it,
the task automatically records the name of the application which was
run, the date and time, a reference name that identifies the NDF, your
username, and some text comprising the command-line parameters and the
full path of the application.  In {\footnotesize KAPPA} the package 
name and version is appended to the application name.
This {\em default history recording\/} facility will gradually
become available in other Starlink packages as they are rebuilt.
Some of these packages may provide task-dependent additional text. 

If disc space is not a concern, you might prefer the verbose level.

\small
\begin{verbatim}
     % hisset hr1068 verbose
\end{verbatim}
\normalsize
the supplementary information being the machine type, and its operating
system name and version.

For small datasets, such as spectra, the history can amount to a
significant part of the NDF's size, so for these you might prefer
the quiet level.  This does not record the command line.

HISSET lets you switch off history recording, if you want to do
something `off the record', or erase the history altogether.

\small
\begin{verbatim}
     % hisset hr1068 disabled
     % hisset hr1068 erase
\end{verbatim}
\normalsize

\subsection{\xlabel{se_commenthistory}Adding Commentary to History Recording
\label{se:commenthistory}}

Once history recording is enabled, you can add commentary to an
NDF using \htmlref{HISCOM}{HISCOM}.

\small
\begin{verbatim}
     % hiscom hr1068 i "There may have been cloud during the integration."
\end{verbatim}
\normalsize
You aren't limited to single lines if you respond to the prompt for 
the comment.  You can give a series of lines, terminated by supplying
{\tt !}.

\small
\begin{verbatim}
     % hiscom hr1068
     COMMENT - Comment line > The dome may have been obstructing the telescope 
     COMMENT - Comment line > during the integration.  We are not sure that the
     COMMENT - Comment line > filter is correct either.
     COMMENT - Comment line > !
\end{verbatim}
\normalsize

If you prefer, you may edit some text into a file and append its
contents to the history records.  Thus

\small
\begin{verbatim}
     % hiscom hr1068 f file=comments.lis
\end{verbatim}
\normalsize
appends the text contained in {\tt comments.lis} to the history
records of NDF hr1068.

\subsection{\xlabel{se_listhistory}Listing History Records
\label{se:listhistory}}

At some point you will want to refer back to the history records.
The \htmlref{HISLIST}{HISLIST} task does this.

\small
\begin{verbatim}
     % hislist hr1068

        History listing for NDF structure /home/scratch/dro/hr1068:

        History structure created 1995 Sep 24 11:16:15.000

     1: 1995 Sep 24 11:16:15.000 - HISSET          (NDFPACK V1.0)

        Parameters: MODE='Normal' NDF=@hr1068
        Software: /star/bin/kappa/hisset
\end{verbatim}
\normalsize
Before you ask\ldots at present there are no parameters for selecting a
time interval and there is no output of the machine and username, but
they're not forgotten.

Here is another example showing a series of history records.

\small
\begin{verbatim}
     % hislist hr1068 \\

        History listing for NDF structure /home/scratch/dro/hr1068sm2:

        History structure created 1995 Nov 24 11:16:15.000

     1: 1995 Sep 24 11:16:15.000 - HISSET          (NDFPACK V1.0)

        Parameters: MODE='Normal' NDF=@hr1068
        Software: /star/bin/kappa/hisset

     2: 1995 Sep 24 11:19:53.000 - GAUSMOOTH       (KAPPA V0.9)

        Parameters: BOX=13 FWHM=5 IN=@hr1068 OUT=@hr1068sm TITLE=! WLIM=!
        Software: /star/bin/kappa/gausmooth

     3: 1995 Sep 24 11:20:15.000 - HISSET          (NDFPACK V1.0)

        History update mode changed from NORMAL to VERBOSE.
        Parameters: MODE='Normal' NDF=@hr1068sm
        Software: /star/bin/kappa/hisset

     4: 1995 Sep 24 11:20:49.000 - GAUSMOOTH       (KAPPA V0.9)

        Parameters: BOX=9 FWHM=3 IN=@hr1068sm OUT=@hr1068sm2 TITLE=! WLIM=!
        Software: /star/bin/kappa/gausmooth
        Machine: alpha, System: OSF1 214 (release V3.2)

     5: 1995 Sep 24 11:22:32.000 - HISCOM          (NDFPACK V1.0)

        Parameters: MODE='Interface' NDF=@hr1068sm2 WRAP=TRUE
        Software: /star/bin/kappa/hiscom
        The dome may have been obstructing the telescope during the integration.
        We are not sure that the filter is correct either.
\end{verbatim}
\normalsize

The first history item HISSET enabling history.  This was followed by
a smooth of the data with \htmlref{GAUSMOOTH}{GAUSMOOTH}.  Then the
recording level was set to verbose.  The fourth record recalls another
smooth, and this time you can see the machine details.  Finally, some
commentary is added with HISCOM.

\newpage
\section{\xlabel{se_agitate}Graphics Database\label{se:agitate}}

Have you ever faced the problem that you would like an application to
know about graphics drawn by some other programme?  For instance, you
display an image of the sky, then later you want to obtain the
co-ordinates of the stars within the image via the cursor.  There are
two main approaches to achieve this functionality.  The first is to
duplicate the display code in the cursor application.  This is
wasteful and inflexible.  The second is to store information about
{\em pictures\/} in a database that can be accessed by graphics
programmes.  A picture's position and extent are added to the database
immediately after the plot is created. The latter technique is
incorporated in \xref{AGI}{sun48}{} \latexonly{({\it{cf.}}\ SUN/48)}---the
graphics database---which can store information about plots on any
graphics device.

AGI also stores a {\em name\/} and a {\em comment\/} for each picture.
Optionally, a {\em label\/} may be added.  `Name' is something of a
misnomer---`type' would give a clearer indication of its purpose.  In
the same way `label' would have been called `name', but for the fact
that `name' was in use.  Confused?  Don't worry.  It will become clear
in a moment.

Currently, the name can take one of four values: {\tt "BASE"} or
{\tt "FRAME"} or {\tt "DATA"} or {\tt "KEY"}. {\tt "BASE"} is reserved for
the BASE picture; this is the picture that extends over the whole of the
plotting area.  {\tt "DATA"} indicates that the picture contains a
representation of data in a graphical form, {\it{e.g.}}\ a greyscale, an
histogram.  {\tt "KEY"} is used for keys---there's a surprise. 
{\tt "FRAME"} usually contains a collection of DATA pictures.  For instance, in
a contour plot the DATA picture is the area where contours may
potentially be drawn; whereas the FRAME picture comprises the annotated
axes and labels, the key, and data area.

The comment is a one-line description of the picture.  At present in
{\footnotesize KAPPA} the comment is just {\tt "KAPPA\_}{\it
$<$application$>$}{\tt{"}} where {\it $<$application$>$} is
the name of the application that created the picture.  However, if
there is a demand, commentary might be modified to be under your
control via a parameter in each graphics application, and that
defaults to the application name as above.  What you can control now
is the {\em label\/}.  This is a name you assign to a picture for easy
reference and selection.

All {\footnotesize KAPPA} graphics applications use AGI, where new plots are
being drawn or co-ordinates are required.  The best way to demonstrate
{\footnotesize KAPPA} and AGI in harness is to give some illustrated examples.

\subsection{\xlabel{se_agiaction}AGI and KAPPA in action\label{se:agiaction}}

Assuming that {\footnotesize KAPPA} is loaded and the graphics device---an
X-window of geometry 1024 by 780 pixels in the following examples---is
available.  To create such a window use the {\tt xmake}
(\xref{SUN/130}{sun130}{}) command

\small
\begin{verbatim}
     % xmake xwindows -geom 1024x780 -overlay -colours 64
\end{verbatim}
\normalsize
selecting an overlay and not too many colours.

First of all we make the X-window the current graphics device and
image display, and the X-window overlay the image-display overlay (as
described in
\latexelsehtml{Section~\ref{se:devglobal}).}{in \htmlref{global device
names}{se:devglobal}.)}  These will remain in force until changed.  The
following commands would not be necessary if the global parameters
already had these values.

\small
\begin{verbatim}
     ICL> gdset xwindows
     ICL> idset xwindows
     ICL> ovset xoverlay
\end{verbatim}
\normalsize
Next we shall clear the X-window, and purge the database of xwindows
and xoverlay pictures.  Note it does not clear the overlay, just the
database entries, because AGI knows that the two graphics devices use
the same screen, but \GKSref\ does not.

\small
\begin{verbatim}
     ICL> gdclear
     ICL> lutgrey
\end{verbatim}
\normalsize

\begin{latexonly}
   \begin{figure}[hbt]
   \begin{center}
   \leavevmode\epsfysize=125mm\epsfbox{sun95_agi1.eps}
   \caption{DISPLAYed CCD image as seen in the X-window except that it is
   the negative of what you would actually see.  This was done for clarity
   in high-contrast reproduction.  Yes it {\em could\/} be a lot worse!}
   \label{fi:agi1}
   \end{center}
   \end{figure}
\end{latexonly}

One picture remains in the database---the BASE---because AGI must
always have a current picture into which further plots are drawn.

Next we display a CCD frame scaled between defined limits.  (The NDF
used in this demonstration is {\tt \$KAPPA\_DIR/ccdframec}).  Details
of the displayed image are recorded in the graphics database, but the
current picture continues to be the BASE.  In other words {\em the
current picture on input is the current picture on output}.  This is
standard practice in {\footnotesize KAPPA}.  The only exceptions are
applications that manage the database rather than plotting
data.\footnote{An uncontrolled exit from an application,
{\it{e.g.}}\ {\tt CTRL/C} may leave the database in an abnormal state.}
If this rule was not enacted, pictures would become progressively
smaller, vanishing into the distance, since new pictures cannot be
drawn outside the current picture.  \htmlref{DISPLAY}{DISPLAY}, in
keeping with other graphics applications, does not write a label.
We'll see later how to annotate a picture with a label.

\small
\begin{verbatim}
     ICL> display $KAPPA_DIR/ccdframec centre=[390,300]
     MODE - Method to define scaling limits /'FLASH'/ > SC
     LOW - Low value for display /2250/ > 2300
     HIGH - High value for display /30790.99/ > 4700
\end{verbatim}
\normalsize

\begin{latexonly}
   \begin{figure}[hbt]
   \begin{center}
   \leavevmode\epsfysize=125mm\epsfbox{sun95_agi2.eps}
   \caption{CCD image DISPLAYed in the top-right quarter of the BASE picture.}
   \label{fi:agi2}
   \end{center}
   \end{figure}
\end{latexonly}

Unlike the earlier example where the image was plotted in the centre of the
screen, here it is shifted down and to the left (see Figure~
\latexelsehtml{\ref{fi:agi1}}{1}).
{\tt CENTRE=[390,300]} means position the pixel (390,300) in the array
at the centre of the current picture. As you can see the pixel can lie
outside the bounds of the array. By default DISPLAY places the
central pixel at the centre of the current picture.  When the current
picture is the BASE, this location is therefore at the centre of the
screen as we have here.

\begin{htmlonly}
   \label{fi:agi1}
   \htmladdimg{sun95_agi1.gif}

   Figure 1: DISPLAYed CCD image as seen in the X-window except that it is
   the negative of what you would actually see.  This was done for clarity
   in high-contrast reproduction.  Yes it {\em could\/} be a lot worse!
\end{htmlonly}

To illustrate this point and introduce a new command we shall plot
the same data array in a picture at the top right of the screen (Figure~
\latexelsehtml{\ref{fi:agi2}}{2}).

\begin{htmlonly}
   \label{fi:agi2}
   \htmladdimg{sun95_agi2.gif}

   Figure 2: CCD image DISPLAYed in the top-right quarter of the BASE picture.
\end{htmlonly}

\small
\begin{verbatim}
     ICL> picdef mode=tr
     ICL> display
     IN - NDF to be displayed /@$KAPPA_DIR/ccdframec/ >
     MODE - Method to define scaling limits /'SCALE'/ >
     LOW - Low value for display /2250/ > 2300
     HIGH - High value for display /30790.99/ > 4700
\end{verbatim}
\normalsize
\begin{latexonly}
   \begin{figure}[hbt]
   \begin{center}
   \leavevmode\epsfysize=125mm\epsfbox{sun95_agi3.eps}
   \caption{Magnified portion of the CCD image to the top right.}
   \label{fi:agi3}
   \end{center}
   \end{figure}
\end{latexonly}
\htmlref{PICDEF}{PICDEF} creates a new FRAME picture in the database.
It is half the linear size of the BASE picture, and is located in the
top-right corner (hence {\tt mode=tr}).  The picture is empty, but it
still becomes the new current picture.  Therefore when DISPLAY comes
along and puts the image in the centre of the current picture it plots
in the top-right corner of the screen.  You will also notice that the
magnification is correspondingly reduced by a factor of two, from 1.55
to 0.775.  DISPLAY has two parameters that control the $x$ and $y$
magnifications, but these default to the give the largest
magnification that displays the image without clipping or distortion.
Now we can exceed the default magnifications to produce an enlarged
portion of the image within the current picture.  (See
\latexelsehtml{Section~\ref{se:ndfsect}}{\htmlref{NDF
Sections}{se:ndfsect}} for an alternative way to displaying or
processing a rectangular portion of an NDF.) Only the part of the
magnified image that is located within the current picture will be
visible; there is no spillage into other pictures (Figure~
\latexelsehtml{\ref{fi:agi3}}{3}).

\begin{htmlonly}
   \label{fi:agi3}
   \htmladdimg{sun95_agi3.gif}

   Figure 3: Magnified portion of the CCD image to the top right.
\end{htmlonly}

\small
\begin{verbatim}
     ICL> display xmagn=2.5 ymagn=2.5
     IN - NDF to be displayed /@$KAPPA_DIR/ccdframec/ >
     MODE - Method to define scaling limits /'SCALE'/ >
     LOW - Low value for display /2250/ > 2300
     HIGH - High value for display /30790.99/ > 4700
\end{verbatim}
\normalsize
Before we plot any more pictures we can give the current picture---still
the frame created by PICDEF---a label.

\small
\begin{verbatim}
     ICL> piclabel eric
\end{verbatim}
\normalsize
If we move to or create a new current picture, and then want to return
to ERIC all we have to enter is

\small
\begin{verbatim}
     ICL> picsel eric
\end{verbatim}
\normalsize

\begin{latexonly}
   \begin{figure}[hbt]
   \begin{center}
   \leavevmode\epsfysize=125mm\epsfbox{sun95_agi4.eps}
   \caption{Contour plot added to bottom-right.}
   \label{fi:agi4}
   \end{center}
   \end{figure}
\end{latexonly}

The graphics database is not restricted to images.  To show this we
shall draw a contour plot in the bottom-right corner of the BASE
picture.  The new picture has the aspect ratio of the BASE picture.  Its
dimension is measured as a fraction of the linear size of the BASE
picture, which is controlled by the parameter {\tt FRACTION}.  The
current picture is not cleared before plotting so you can see through
the contour plot to the galaxy image behind (Figure~
\latexelsehtml{\ref{fi:agi4}}{4}).  The {\tt NOOUTLINE} prevents an
outline from being drawn around the picture.

\begin{htmlonly}
   \label{fi:agi4}
   \htmladdimg{sun95_agi4.gif}

   Figure 4: Contour plot added to bottom-right.
\end{htmlonly}

\small
\begin{verbatim}
     ICL> picdef mode=br fraction=0.65 nooutline
     ICL> turbocont $KAPPA_DIR/ccdframec(51:250,141:330) noclear ~
     pltitl="AGI demonstration"
     MODE - Method for selecting contour heights /'FREE'/ > ar
     NCONT - Give the number of contour heights /6/ > 5
     Contour heights used:
     2478.022,   2560,958,   2697.029,   3042.488,   4050.937.
\end{verbatim}
\normalsize
The PICDEF command allows you to define new pictures with any
aspect ratio.  To do this you can supply a value for the ASPECT
parameter, or define explicit limits ({\tt{mode=xy}}) or
use the cursor ({\tt{mode=cursor}})---the default.  Also, you may
constrain the new picture to lie within the current picture with
parameter {\tt CURRENT}.  In this case the fractional size and aspect
ratio relate to the current picture.

\small
\begin{verbatim}
     ICL> picdef current
     To select a point press the left button on the mouse or trackerball.
     To exit press the right button.
     Use the cursor to select 2 distinct points.
\end{verbatim}
\normalsize
If you try to select a point outside the current picture you will be told
and the cursor position is reset.

\small
\begin{verbatim}
     Point lies outside the allowed region.
     Co-ordinates are ( 0.5160462, 1.0269577E-02 ) and ( 0.8908858, 0.6046214 )
\end{verbatim}
\normalsize
Notice there is an outline of the region selected, as it is drawn
by default.

Now draw a greyscale plot and its key within the region.
The extent of the new current picture can be seen in silhouette against
the earlier pictures (Figure~
\latexelsehtml{\ref{fi:agi5}}{5}).

\begin{htmlonly}
   \label{fi:agi5}
   \htmladdimg{sun95_agi5.gif}

   Figure 5: Greyplot added within the region of contour picture defined by
   PICDEF's cursor.
\end{htmlonly}

\small
\begin{verbatim}
     ICL> greyplot $KAPPA_DIR/ccdframec pltitl="AGI demonstration"
     KEY - Do you want a key of the grey levels and a title ? /TRUE/ >
     MODE - Method to define scaling limits /'SCALE'/ >
     BLACK - Value to be black in the plot? /30790.99/ > 4700
     WHITE - Value to be white in the plot? /2250/ > 2300
\end{verbatim}
\normalsize

\begin{latexonly}
   \begin{figure}[ht]
   \begin{center}
   \leavevmode\epsfysize=125mm\epsfbox{sun95_agi5.eps}
   \caption{Greyplot added within the region of contour picture defined by
   PICDEF's cursor.}
   \label{fi:agi5}
   \end{center}
   \end{figure}
\end{latexonly}
PICDEF has one further mode---Array.  This enables you to create an
$n\times m$ grid of new FRAME pictures.  It also has a mechanism for
labelling all the pictures, so you can easily switch between the
elements of the picture array.  You might use the following command in
an \ICLref\ procedure to display a series of up to twelve spectra.

\small
\begin{verbatim}
     ICL> picdef mode=a prefix=spec xpic=3 ypic=4
\end{verbatim}
\normalsize
The bottom-left picture would be labelled SPEC1 and the rest are
numbered in sequence from left to right to SPEC12---the top-right
picture.  You'd call \htmlref{PICSEL}{PICSEL} to select each picture
in turn via the {\footnotesize ICL} SNAME function \latexonly{(see
Section~\ref{se:ICLproc})}, or use a {\bf while} loop in a C-shell
script\latexonly{ (see Section~\ref{se:cshscript})}.  Since this is a
common operation a shorthand command, \htmlref{PICGRID}{PICGRID}, is
available.  For instance,

\small
\begin{verbatim}
     ICL> picgrid 3 4
\end{verbatim}
\normalsize
is equivalent to the previous example, except that the pictures are
labelled 1 to 12.

You can see that montages of pictures can rapidly be built.
Occasionally, you will want some earlier picture to become the current
picture.  As we've seen labelled picture can be recalled via PICSEL,
but not all pictures will be labelled, especially ones with name {\tt
DATA}, because of the rule that applications must not change the
current picture.  Another way to select a new current picture is via
the command \htmlref{PICCUR}{PICCUR}.  It displays a cursor.  Move the
cursor to lie on top of the picture you require and select a point
following the instructions (usually by pressing the left-button of the
mouse), then exit (normally by hitting the right-hand mouse button).
Generally, this will be fine, but you can have cases where one plot is
still visible through a transparent plot drawn subsequently.  If the
later picture extends entirely over the image you require, PICCUR will
not let you access it. The moral is ``be careful when arranging your
pictures''.  A picture may only be partially obscured, so by moving
the cursor around and hitting the left-hand button you can often find
a portion that is topmost. PICCUR reports the name, comment and the
label (if it there is one) of the picture in which the cursor is
located to assist you.  It is usually quite obvious where pictures
begin and end, so in practice it is easier than described here.

There is another way of selecting a new current picture, and is
essential for accessing `obscured'
pictures---\htmlref{PICLIST}{PICLIST}.  Besides listing all the
pictures with a specified name for the current graphics device,
PICLIST offers a means of making any of the listed pictures the
current picture.  There are a number of synonym commands derived from
PICLIST; these let you select the last-created picture
(\htmlref{PICLAST}{PICLAST}) or the BASE picture
(\htmlref{PICBASE}{PICBASE}), amongst others.

Sometimes you will want to know where you can place another picture
without it hiding or overwriting an existing plot.  There are three
tasks for selecting suitable FRAME pictures:
\htmlref{PICEMPTY}{PICEMPTY}, \htmlref{PICENTIRE}{PICENTIRE},
and \htmlref{PICVIS}{PICVIS}.  Suppose, for instance, you have created
a substantial array of FRAME pictures, perhaps for plotting spectra
from a Fabry-Perot data cube.  Now you could select each FRAME picture
by remembering or computing its label and passing it to PICSEL. A
quicker approach is to run PICEMPTY, to select the next empty FRAME
picture before plotting each spectrum.


If you do get lost or forget what and where the current picture is,
the \htmlref{GDSTATE}{GDSTATE} command will come to your rescue.  You
can even plot an outline with the OUTLINE keyword if you can't
visualise the device co-ordinates.  The current picture does not have
a label.  If it did this too would be listed by GDSTATE.  PICLIST also
flags the current picture with a {\tt C} to the left of the picture
numbers.

\small
\begin{verbatim}
     ICL> gdstate
 
     Status of the xwindows window graphics device...
        Physical device: 
 
        The current picture is a FRAME picture.
        Comment: KAPPA_PICDEF
        World co-ordinates:
                    X = 0.5160462 to 0.8908858
                    Y = 1.0269577E-02 to 0.6046214
        Normalised device co-ordinates:
                    X = 0.3930821 to 0.6786044
                    Y = 7.822529E-03 to 0.4605515
 
\end{verbatim}
\normalsize

Let us select the first picture that we displayed (the image to the
bottom left).  Having done so we can then inspect some of the data in
that picture.  \htmlref{INSPECT}{INSPECT} looks for the topmost data
picture within the current picture.  This may be the current picture
itself. Using the database entries it knows the position and extent of
the picture, and so when you come to place the cursor on a point, its
pixel co-ordinates can be evaluated.  Notice that AGI remembers the
name of the source data array and so you need not re-enter it here.
If you want to inspect another NDF of the same dimensions you can
override the NDF stored in AGI by specifying the NDF you want on the
command line.  Sometimes you might obtain the wrong array; see
\latexelsehtml{Section~\ref{se:probwrongNDF}}{\htmlref{Problems
Problems}{se:probwrongNDF}} on what to do.  In the example below a
slice through the CCD frame is plotted on the overlay plane (Figure~
\latexelsehtml{\ref{fi:agi6}}{6}).

\begin{latexonly}
   \begin{figure}[hbt]
   \begin{center}
   \leavevmode\epsfysize=125mm\epsfbox{sun95_agi6.eps}
   \caption{A slice through the image drawn on the overlay plane.}
   \label{fi:agi6}
   \end{center}
   \end{figure}
\end{latexonly}
\begin{htmlonly}
   \label{fi:agi6}
   \htmladdimg{sun95_agi6.gif}

   Figure 6: A slice through the image drawn on the overlay plane.
\end{htmlonly}

\small
\begin{verbatim}
     ICL> piccur

     Use the graphics cursor to define the next point or picture...
        Press left button on mouse/trackerball to select a point.
        Press right button on mouse/trackerball to end input.

      X = 183.4733, Y = 264.575 in KAPPA_DISPLAY ( DATA )

     ICL> inspect

     For certain options, boxes will appear.  The functions are controlled
     by the mouse/trackerball buttons and keyboard...
        Press left button to select the operation shown in the left box.
        Press middle button/keyboard "2" to select the operation shown in the middle box.
        Press right button to select the operation shown in the right box.

     Current picture has name: DATA, comment: KAPPA_DISPLAY
     Using /star/bin/kappa/ccdframec as the input NDF.
     GDEVICE - Graphics device is to be used for line plots > xov
     OPTION - Option required /'Region'/ > SL
     Co-ordinates are ( 150, 175 ) and ( 211, 318 )
     Type the null character, !, if the slice is not to be saved.
     SLNAME - Name of file to save slice /!/ >
     OPTION - Option required /'Region'/ > E
\end{verbatim}
\normalsize
A word of warning about the device for GDEVICE.  Don't use the base
plane of the image display, in this case {\tt xwindows}.  Otherwise
your underlying images will be erased when a line plot is drawn.
The current-graphics-device scheme breaks down here.  Both
options: no default or occasionally a harmful default,
are undesirable.  Anyway you've been warned!

Using the same picture we can now demonstrate the contour-overlay
application, \htmlref{CONTOVER}{CONTOVER}.  The data array we have
employed so far is smoothed with \htmlref{GAUSMOOTH}{GAUSMOOTH}; the
origin is shifted with \htmlref{SETORIGIN}{SETORIGIN}, (moving the
image ten pixels left and twenty down); and then all values outside a
circle centred at pixel (160,~230), radius 70 pixels are set to zero
via \htmlref{OUTSET}{OUTSET} to make {\tt \mbox{ccdcircle.sdf}}. See Figure~
\latexelsehtml{\ref{fi:agi7}}{7}.  The plot is made to the current
overlay device, defined by \htmlref{OVSET}{OVSET}, namely {\tt xov}.
Currently, only pixel offsets may be used, but eventually complicated
transformations will be possible, say for overlaying a radio map on
the infra-red image of a source.  Note that CONTOVER reports the name
of the NDF used for the underlying image, so if for some reason it is
not the one you intended, just abort ({\tt{!!}}) at the prompt for
parameter OFFSET. (You can check what the reference object will be
with the \htmlref{PICIN}{PICIN} command.)

\begin{latexonly}
   \begin{figure}[hbt]
   \begin{center}
   \leavevmode\epsfysize=125mm\epsfbox{sun95_agi7.eps}
   \caption{Contour overlay of a different data array on top of the
   original CCD image.}
   \label{fi:agi7}
   \end{center}
   \end{figure}
\end{latexonly}
\begin{htmlonly}
   \label{fi:agi7}
   \htmladdimg{sun95_agi7.gif}

   Figure 7: Contour overlay of a different data array on top of the
   original CCD image.
\end{htmlonly}

\small
\begin{verbatim}
     ICL> contover
     NDF - Image to be contoured /@$KAPPA_DIR/ccdframec/ > ccdcircle
     Current picture has name: DATA, comment: KAPPA_DISPLAY
        Reference data object: /star/bin/kappa/ccdframec.
     OFFSET - x-y displacement in pixels /0,0/ > 10,20
     MODE - Method for selecting contour heights /'Free'/ >
     HEIGHTS - Give the heights of the contours > [2400,2500,2650,2900,3300,3900]
\end{verbatim}
\normalsize

Application PICCUR has a dual role.  It also provides a means of
determining the co-ordinates of points in pictures selected with the
cursor.  Again only the topmost picture at a given cursor location is
accessible.  Each time the left-hand button is pressed the co-ordinates
of the position in the topmost picture at that point is presented.
This is the dynamic mode ({\tt{"Dynamic"}}).  The other options let you
obtain co-ordinates in the reference frames of the current picture
({\tt{"Current"}}) or anchored ({\tt{"Anchor"}}) to the picture at the first
cursor position selected.

\small
\begin{verbatim}
     ICL> piccur

     Use the graphics cursor to define the next point or picture...
        Press left button on mouse/trackerball to select a point.
        Press right button on mouse/trackerball to end input.

      X = 245.6915, Y = 400.4704 in KAPPA_GREYPLOT ( DATA )
      X = 0.8398062, Y = 1.04369 in KAPPA_GREYPLOT ( FRAME )
      X = 239.562, Y = 279.4127 in KAPPA_CONTOUR ( DATA )
      X = 241.1404, Y = 298.2021
      X = 255.3456, Y = 293.2284
      X = 153.5614, Y = 236.91 in KAPPA_DISPLAY ( DATA ), label = ERIC.
      X = 3.2948352E-02, Y = 0.5845106 in KAPPA_CONTOUR ( FRAME )
      X = 0.5291266, Y = 1.805826 in KAPPA_GREYPLOT ( FRAME )
      X = 0.7304244, Y = 0.9760387 in KAPPA_CONTOUR ( FRAME )
      X = -7.8452587E-02, Y = 0.838221 in KAPPA_CONTOUR ( KEY )
      X = 0.7147477, Y = 0.5338036
      X = 0.7361856, Y = 0.5252284
\end{verbatim}
\normalsize
You can record the co-ordinates in a text file.

If you don't want to affect the graphics database, but still obtain
co-ordinates of features in an image, use the closely related
\htmlref{CURSOR}{CURSOR}, which also allows selected points to
be marked or joined.  It also permits you to remove previous points
if you make a mistake.  CURSOR has output parameters so that you can
pass the co-ordinates of the selected points to a script or other
applications.  There is also the {\tt XYcur} option in INSPECT; it marks
the points selected, and allows updating of lists of $x$-$y$ positions
stored in text files.

\subsection{\xlabel{se_agitidy}Tidying\label{se:agitidy}}

The contents of the graphics database are ephemeral.  Therefore you
should regularly purge the database entries with
\htmlref{GDCLEAR}{GDCLEAR} (or optionally via \htmlref{IDCLEAR}{IDCLEAR}
for the current image display), or delete the database file.  The
location of the AGI database can be controlled by setting the
environment variable AGI\_USER to the desired location.  If AGI\_USER
is undefined, the database file is placed in your home directory.
Thus by default the file is {\tt \$HOME/agi\_}{\it $<$node$>$}{\tt
.sdf}, where you substitute your computer's node name for {\it
$<$node$>$}, {\it{e.g.}}\ {\tt /home/dro/agi\_rlsaxp.sdf}. A new
database is created when you run an AGI application if none exists.
AGI will purge the database for a device if the graph window has
changed size, or if you switch between portrait and landscape formats
for a printer device.

\newpage
\section{\xlabel{se_co-ordsystem}Co-ordinate Systems
\label{se:co-ordsystem}}

{\footnotesize KAPPA} uses a number of co-ordinate systems; this section
describes them, and how to select one.  \htmlref{NDFTRACE}{NDFTRACE}
reports the bounds of an NDF in these various co-ordinate systems.

\subsection{\xlabel{se_pixelindices}Pixel Indices\label{se:pixelindices}}

The elements in an \NDFref{NDF's}\ array can be addressed by their
$n$-dimensional integer pixel indices.  The first or {\em origin\/}
pixel in an NDF need not have pixel index equal to one in each
dimension, though it usually will.  An NDF can have an arbitrary
origin.  By adjusting the origins of NDFs with
\htmlref{SETORIGIN}{SETORIGIN} you may combine the NDFs differently.  For
example, ADD adds two NDFs where they overlap. The overlap is defined
in terms of pixel indices. Thus only those pixels with the same pixel
index in both NDFs will be added.  The resultant NDF is the addition
in the intersection of the two input NDFs.

If you wish to select rectangular sections of pixels within an NDF
indices are fine and precise.  Since they are integer quantities,
these indices cannot represent a continuous co-ordinate system,
although the information stored in an NDF will almost always require
that positions within it be describable to sub-pixel accuracy.  For
example, when \htmlref{CENTROID}{CENTROID} determines the position of
a star in a 2-dimensional image it will inevitably give rise to a
non-integer result, for which a continuous ($x$-$y$) co-ordinate
system is required.

\subsection{\xlabel{se_pixelco-ordinates}Pixel Co-ordinates
\label{se:pixelco-ordinates}}

There are a number of ways in which a continuous co-ordinate system
can be defined for a regular array of pixels. In the absence of
other information, the Starlink convention is to use a {\em pixel
co-ordinate system\/} in which a pixel with indices ($i,j$) has its
centre at the position

\begin{quote}
\begin{center}
($i-\frac{1}{2},j-\frac{1}{2}$)
\end{center}
\end{quote}

and is taken to be one unit in extent in each dimension.
Pixel (1,1) would therefore be centred at the position (0.5,0.5) and would
have its `lower' and `upper' corners located at positions (0.0,0.0) and
(1.0,1.0) respectively, as follows

\begin{quote}
\begin{center}

\setlength{\unitlength}{1.0mm}
\begin{picture}(15,25)(0,-7.5)

\thicklines
\put(0,0){\line(1,0){10}}
\put(10,0){\line(0,1){10}}
\put(10,10){\line(-1,0){10}}
\put(0,10){\line(0,-1){10}}
\put(5,5){\circle*{0.7}}

\thinlines
\put(0,-4){\vector(0,1){3}}
\put(0,-5){\makebox(0,0)[tr]{(0.0,0.0)}}
\put(10,14){\vector(0,-1){3}}
\put(10,15){(1.0,1.0)}
\put(14,5){\vector(-1,0){8}}
\put(15,5){\makebox(0,0)[l]{(0.5,0.5)}}

\end{picture}
\end{center}
\end{quote}

This makes it possible to refer to fractional pixel positions---in
this case within a 2-dimensional array, although the principle can
obviously be extended to other numbers of dimensions. 

\subsection{\xlabel{se_dataco-ordinates}Data Co-ordinates
\label{se:dataco-ordinates}}

The pixel co-ordinate system described above defines how to convert pixel
indices into a set of continuous co-ordinates and therefore introduces a
co-ordinate {\em axis\/} which runs along each dimension of the NDF, as
follows

\begin{quote}
\begin{center}

\setlength{\unitlength}{0.65mm}
\begin{picture}(70,57)(-10,-7)

\multiput(10,10)(0,3){11}{\line(1,0){45}}
\multiput(10,10)(3,0){16}{\line(0,1){30}}
\put(32.5,43){\makebox(0,0)[b]{\scriptsize NDF Pixel Array}}

\thicklines
\put(0,0){\vector(1,0){60}}
\put(25,-5){\bf Axis 1}
\put(0,0){\vector(0,1){45}}
\put(0,48){\makebox(0,0)[b]{\bf Axis 2}}

\end{picture}
\end{center}
\end{quote}

The use of the pixel size to determine the units of these axes is rather
restrictive, however, and in practice we may want to use more realistic
physical units. This would allow a spectrum to be calibrated in
wavelength, for instance, or the output from a plate-measuring machine
to be related to axes calibrated in microns.   In {\footnotesize KAPPA}
these are called {\em Data\/} co-ordinates.

The NDF's {\em axis\/} components are designed to hold the extra information
needed to define more useful co-ordinate systems, so that realistic axes can
be associated with a NDF, along with {\em labels\/} and {\em units\/} for
these axes.

\subsection{\xlabel{se_selectcosys}Selecting a Co-ordinate System
\label{se:selectcosys}}

The pixel co-ordinate system is the default choice, and although it is
intended to be used only in the absence of other information,
{\footnotesize KAPPA} offers you a choice of co-ordinate system, even if
your NDF has physical co-ordinates in its axis centres.

There is a \htmlref{global parameter}{se:parglobals}---the current
co-ordinate system---that may be {\tt WORLD} or {\tt DATA}.  It is set
by a parameter called COSYS in each application that processes data
co-ordinates. For instance,

\small
\begin{verbatim}
     % display cosys=w axes \\
     % centroid
\end{verbatim}
\normalsize
makes the axes about the displayed image and the centroid positions in
the following task appear in pixel co-ordinates, regardless of whether
there are axes present or not.

Conversely,

\small
\begin{verbatim}
     % display cosys=d axes \\
     % centroid
\end{verbatim}
\normalsize
makes the axes about the displayed image and the centroid positions
appear in data co-ordinates, {\em provided there are axes present.}
Of course, the axes may just have a pixel co-ordinate system.

If the global parameter is undefined, applications will adopt 
a suitable default, depending on the application.  All default to
{\tt "Data"} except \htmlref{ARDMASK}{ARDMASK}, CENTROID, 
\htmlref{FITSURFACE}{FITSURFACE}, \htmlref{PSF}{PSF},
\htmlref{SEGMENT}{SEGMENT}, \htmlref{TRANSFORMER}{TRANSFORMER}, and
\htmlref{ZAPLIN}{ZAPLIN}.

When you draw a plot or an image, the graphics database records the
co-ordinate extents of the picture.  What was not said explicitly was
which co-ordinate system.  Looking at the output from
\htmlref{GDSTATE}{GDSTATE} and \htmlref{CURSOR}{CURSOR} shows that the
database deals in world co-ordinates.  Graphics packages like
\GKSref\ require co-ordinates to be linear and increase from left to
right and bottom to top.  This is not necessarily the case for
physical axes.  So are we going to lose our co-ordinate system between
applications?  No\ldots well almost certainly not. {\footnotesize KAPPA}
strives to find a linear transformation between your data co-ordinates
and pixel co-ordinates, or a logarithmic one in some cases.  If it
succeeds the transformation is stored in the database.  Thus if you
plot a spectrum with wavelength along the abscissa, you can then use
CURSOR to identify lines.

\small
\begin{verbatim}
     % linplot spectrum cosys=d \\
     % cursor


     Use the graphics cursor to define the next point...
        Press left button on mouse/trackerball to select a point.
        Press right button on mouse/trackerball to end input.

     X=3934.254, Y=196.118 in KAPPA_LINPLOT( DATA )
     X=3969.148, Y=196.118 in KAPPA_LINPLOT( DATA )
\end{verbatim}
\normalsize
The $x$ co-ordinates are the wavelengths in {\AA}ngstroms of the Calcium
II H and K lines.

If the global parameter is undefined you will be prompted for
a co-ordinate as required.

\newpage
\section{\xlabel{se_interaction}Interaction Mode\label{se:interaction}}

We have seen the different co-ordinate systems {\footnotesize KAPPA} uses.
Now we address how the applications obtain co-ordinate information
itself.  Applications that require co-ordinates are being modified to
permit a variety of mechanisms for obtaining those
co-ordinates.\footnote{Those converted so far are
\htmlref{ARDMASK}{ARDMASK}, CENTROID, INSPECT,
\htmlref{SEGMENT}{SEGMENT}, and \htmlref{ZAPLIN}{ZAPLIN}.}
The possibilities are as follows.
\begin{description}
\item [Cursor] --- This mode utilises the cursor of the current graphics
device.  For this to work the array must already be displayed as an
image, or a contour plot, or line plot (provided the application handles
1-dimensional data), and the picture is stored in the graphics database.
\item [Interface] --- This mode obtains co-ordinates from
the parameter system, usually in response to prompting.
\item [File] --- In this mode the application reads a text file
containing a list of co-ordinates in free format, one object per record.
There may be commentary lines in the file beginning with {\tt \#} or
{\tt !}.  The format and syntax of the files are {\it ad hoc}, and are
described in the application documentation.\footnote{A standard and
far-more powerful syntax for defining co-ordinates and regions within
arrays has been designed.}
\end{description}

Applications that permit these options have a parameter, called
MODE, by which you can control how positional data are to be acquired.
It would be tedious to have to specify a mode for each application,
therefore {\footnotesize KAPPA} has a
\htmlref{global parameter}{se:parglobals}---the interaction
mode---to which each application's interaction-mode parameter is
defaulted.  The global value remains in force until you change it by
assigning an application's interaction mode on the command line. The
following examples shows the effect of the global parameter.  For
compactness \htmlref{GLOBALS}{GLOBALS} will merely show the
interaction mode.

First we display an image on the {\tt xw} windows device.

\small
\begin{verbatim}
     ICL> gdset xw
     ICL> idset xw
     ICL> display $KAPPA_DIR/ccdframec mode=pe \
     Data will be scaled from 2366.001 to 2614.864.
     ICL> globals
     The current interaction mode is      : <undefined>
\end{verbatim}
\normalsize
Now we obtain the centroids of a couple of stellar/galaxian images via
each of the interaction modes.  First in cursor mode.  Note that
\htmlref{CENTROID}{CENTROID} obtains the name of the input NDF from the
graphics database in this mode.  If you need to preview which NDF is
going to be selected use the \htmlref{PICIN}{PICIN} command.

\small
\begin{verbatim}
     ICL> centroid mode=c
     Current picture has name: DATA, comment: KAPPA_DISPLAY.
     Using /star/bin/kappa/ccdframec as the input NDF.
     
     To select a point press the left button on the mouse or trackerball.
     To exit press the right button.
     Use the cursor to select one point.
 
     Input guess position was     86.23534, 295.0848
     Output centroid position is  86.41057, 295.1141
 
     Use the cursor to select one point.
 
     Input guess position was     73.32529, 318.9757
     Output centroid position is  72.76437, 318.9484
 
     Use the cursor to select one point.
\end{verbatim}
\normalsize
If we look at the global parameters again, indeed we see that it has
become cursor mode.  To show that it is global we can run
\htmlref{INSPECT}{INSPECT}.  Yes a cursor appears rather than a prompt
for the pixel value.

\small
\begin{verbatim}
     ICL> globals
     The current interaction mode is      : CURSOR
     ICL> inspect

     For certain options, boxes will appear.  The functions are controlled
     by the mouse/trackerball buttons and keyboard...
        Press left button to select the operation shown in the left box.
        Press middle button/keyboard "2" to select the operation shown in the middle box.
        Press right button to select the operation shown in the right box.

     Current picture has name: DATA, comment: KAPPA_DISPLAY.
     Using /star/bin/kappa/ccdframec as the input NDF.
     GDEVICE - Graphics device to be used for line plots /@xov/ >
     OPTION - Option required /'Region'/ > VA
 
        VALUE of image at 59, 294 is 2753.928
       
        VALUE of image at 149, 311 is 3727.465
 
     OPTION - Option required /'Region'/ > ex
\end{verbatim}
\normalsize
Now we'll see the effect of changing the mode parameter.  Note that
unless it is undefined or the application does not support the current
mode, you must change the mode on the command line.  First we shall
prompt for the co-ordinates.  A null ends the loop.

\small
\begin{verbatim}
     ICL> centroid mode=i
     NDF - Array to be analysed /@/star/bin/kappa/ccdframec/ >
     INIT - Guess at co-ordinates of star-like feature /108.8,403.5/ > 86,295

     Input guess position was     86, 295
     Output centroid position is  86.41057, 295.1141
     
     INIT - Guess at co-ordinates of star-like feature /86,295/ > 73.3,319
 
     Input guess position was     73.3, 319
     Output centroid position is  72.76437, 318.9484
 
     INIT - Guess at co-ordinates of star-like feature /73.3,319/ > !
\end{verbatim}
\normalsize
Finally, we can create a text file called {\tt starlist.dat} and run
CENTROID in file mode.  

\small
\begin{verbatim}
     ICL> cat > starlist.dat
     86 295
     73 320
     CTRL/D
     ICL> centroid mode=f
     COIN - File of initial positions /@centroid.lis/ > starlist.dat
     NDF - Array to be analysed /@$KAPPA_DIR/ccdframec/ >
 
     Input guess position was     86, 295
     Output centroid position is  86.41057, 295.1141
 
     Input guess position was     73, 320
     Output centroid position is  72.76437, 318.9484
\end{verbatim}
\normalsize
Such co-ordinate files can also be created interactively with images by
\htmlref{CURSOR}{CURSOR}, \htmlref{PICCUR}{PICCUR}, or the XYcur mode of
INSPECT.

\section{\xlabel{se_imagedisplay}Image-display Interface in KAPPA
\label{se:imagedisplay}}

{\footnotesize KAPPA} utilises \IDIref\ (Image-display Interface) for some
interactions with images.  However, \GKSref\ is still called to display
images, plot line graphics, and perform cursor interactions.  IDI is
much faster for some interactions, such as manipulating the colour
table, and provides functionality that is excluded from GKS, such as
pan and zoom.  The two graphics packages share windows through the
\xref{Graphics Window Manager}{sun130}{} ({\footnotesize GWM}\latexonly{,
SUN/130}) so that you need not know which {\footnotesize KAPPA} command
calls which package.

Not all IDI functionality is available in the Starlink X-windows
implementation.  For instance, there is no blinking capability on
X-windows.  The original data are also lost after zooming and clearing
due to a reasonable limitation of {\footnotesize GWM}.  Hence earlier
applications IDUNZOOM and IDVISIBLE have been withdrawn.  The withdrawal
of IDVISIBLE is actually not serious since images cannot be displayed
on the X-windows overlay.  You will probably want IDINVISIBLE to erase
only the overlay plane of an X-window.

IDI and GKS were selected for device independence, but since this is
less important today as X-windows and PostScript have become {\it de
facto\/} standards.  By relaxing the GKS straightjacket, GUI-based
image-processing and display tools dedicated to X-windows, will offer
much-improved interaction and performance.

\newpage
\section{\xlabel{se_coltab}Image-Display Colour Table and Palette
\label{se:coltab}}

An image display has a colour table (sometimes called the video lookup
table) which converts integer values in the display's memory into the
colours that you see on the monitor. There are usually somewhere
between 64 and 256 entries or {\em indices\/} in the colour table that
are numbered consecutively increasing from 0.  For example, if there
were 256 colour indices then you would be able display 256 different
colours simultaneously; and when you display an image your data values
would be scaled to map onto the integer range of the colour indices,
0--255.

In many systems the full colour table is used.  This has the
disadvantage that if you want to annotate images with captions or axes,
plot coloured borders about images, plot graphs {\it etc.}\, yet
simultaneously display
images with certain colour tables, there may be conflict of interests.
For instance, a linear greyscale colour table's first few colour indices
will be almost black.  By default, these same indices, particularly
index 1, are used by \GKSref/\SGSref\ for line graphics, thus
any plots will be invisible.  If you reset colour index 1 to white, the
appearance of your image alters.   Whenever you alter the colour table
to enhance the look of your image, it will affect the line graphics.

To circumvent this dilemma, {\footnotesize KAPPA} reserves a portion of the
colour table, called the {\em palette}, that is unaffected by changes
to the rest of the colour table. It is shown schematically below.  The
palette currently contains a fixed 16 indices.  $N$ is the total
number of indices.  In {\footnotesize KAPPA} the remainder of colour indices
is called the {\em colour table}.

\begin{center}
\begin{picture}(136,28)
% Thick outline.
\thicklines

% Draw the palette outline.  0.3 fudge to make lines match!
\put(2,15){\framebox(32,7.7){}}

% Draw the colour-table outline broken with dots to indicate an
% arbitrary length and so three frameboxes cannot be drawn.
\put(34,15){\line(1,0){50}}
\put(34,23){\line(1,0){50}}
\multiput(84.8,15)(2,0){4}{{\huge .}}
\multiput(84.8,22.4)(2,0){4}{{\huge .}}
\put(94,15){\line(1,0){40}}
\put(94,23){\line(1,0){40}}
\put(134,15){\line(0,1){8}}

% Switch to thin lines to mark the individual colour indices.
\thinlines

% Mark the colour indices as vertical lines.
\multiput(4,15)(2,0){15}{\line(0,1){8}}
\multiput(36,15)(2,0){25}{\line(0,1){8}}
\multiput(94,15)(2,0){20}{\line(0,1){8}}

% Label the colour indices.
\put(2,24){0}
\put(30,24){15}
\put(34,24){16}
\put(131,24){$N\!\!-\!\!1$}

% Make braces to indicate the two parts.
\put(18,10){\makebox(0,0)[c]{$\underbrace{\rule{31mm}{0mm}}$}}
\put(84,10){\makebox(0,0)[c]{$\underbrace{\rule{99mm}{0mm}}$}}

% Write the captions
\put(2,4){\makebox[32mm][c]{{\large Palette}}}
\put(34,4){\makebox[100mm][c]{{\large Colour Table}}}
\end{picture}
\end{center}

\subsection{\xlabel{se_lookuptables}Lookup Tables\label{se:lookuptables}}

In order to distinguish between the set of colours in the physical image
display (the colour table) and some external table of colours, the latter
is called a {\em lookup table}. Lookup tables comprise a series of
red, green and blue (RGB) intensities, each normalised to 1.0; they may
be stored in NDFs---indeed some are provided with {\footnotesize KAPPA}---or
be coded within applications.

A lookup table may be transferred into the display's colour table.
However, the number of indices in the colour table is usually not the
same as the number of colours in the lookup table and so a simple
substitution is not possible.  Therefore, {\footnotesize KAPPA} squeezes
or stretches the lookup table to make it fit in the available number of
colour-table indices.  Normally, linear interpolation between adjacent
lookup-table entries defines the resultant colour, though you can select
a nearest-neighbour algorithm.  The latter is suited to lookup tables
with sharp boundaries between contrasting colours, {\it{e.g.}}\ a series of
coloured blocks, and the former to smoothly varying lookup tables where
there are no obvious discontinuities, {\it{e.g.}}\ spectrum-like.

Let's have a few examples.

\small
\begin{verbatim}
     % lutheat
     % lutramps
     % lutread pastel
     % lutable li ex sawtooth nn
     % lutsave pirated
\end{verbatim}
\normalsize
\htmlref{LUTHEAT}{LUTHEAT} loads the standard `heat' lookup table into
the colour table using linear interpolation, whilst
\htmlref{LUTRAMPS}{LUTRAMPS} loads the standard coloured ramps using
the nearest neighbours in the lookup table.
\htmlref{LUTREAD}{LUTREAD} reads the lookup table stored in the
DATA\_ARRAY of the NDF called pastel and maps it onto the colour table
via linear interpolation.  In the fourth example the lookup table in
NDF sawtooth is mapped onto the colour table via a linear
nearest-neighbour method. {\tt ex} tells \htmlref{LUTABLE}{LUTABLE} to
read an external file.  In the final example
\htmlref{LUTSAVE}{LUTSAVE} saves the current colour table into a
lookup-table NDF called pirated. LUTSAVE is quite useful as you can
steal other people's attractive colour tables that they've carelessly
left in the display's memory!  It does not matter should the display
not have a palette, since

\small
\begin{verbatim}
     ICL> lutsave pirated full
\end{verbatim}
\normalsize
will save the full set of colour indices to the NDF.

\subsection{\xlabel{se_mancoltab}Manipulating Colour Tables
\label{se:mancoltab}}

{\footnotesize KAPPA} provides a number of commands for adjusting a colour
table.  \htmlref{LUTFLIP}{LUTFLIP}, \htmlref{LUTHILITE}{LUTHILITE},
\htmlref{LUTROT}{LUTROT}, \htmlref{LUTTWEAK}{LUTTWEAK} are all
\IDIref-based.  Remember that only the colour table will be flipped;
highlighted; rotated; stretched, or squashed, and/or have its origin
moved.  Thus during flipping, colour index 16 will swap with $N-1$, 17
with $N-2$ {\it etc.}\ \htmlref{CRELUT}{CRELUT} uses an enlarged
\htmlref{palette}{se:coltab} to modify a lookup table.

\subsection{\xlabel{se_creluts}Creating Lookup Tables\label{se:creluts}}

\subsubsection{From a Text File}
You can make a text file of the RGB intensities and use
\htmlref{TRANDAT}{TRANDAT} to create the \NDFref{NDF}, or manipulate
the colour table and then save it in a lookup-table NDF.  If you
choose the second option remember that all RGB intensities must lie in
the range 0.0--1.0, where 1.0 is the maximum intensity; and that equal
red, green, and blue intensities yields a shade of grey. So for
example if you want a six equal blocks of red, blue, yellow, pink,
sienna and turquoise you could create the text file {\tt col6.dat}
with contents

\small
\begin{verbatim}
     # Red, blue, yellow, pink, sienna, and turquoise LUT
     1.0 0.0 0.0
     0.0 0.0 1.0
     1.0 1.0 0.0
     0.9 0.56 0.56
     0.56 0.42 0.14
     0.68 0.92 0.92
\end{verbatim}
\normalsize
and then run TRANDAT to make the NDF called collut6.

\small
\begin{verbatim}
     % trandat col6 collut6 shape='[3,6]' auto
\end{verbatim}
\normalsize

\subsubsection{\xlabel{se_runcrelut}Running CRELUT\label{se:runcrelut}}

There is an interactive task called \htmlref{CRELUT}{CRELUT} for
creating lookup tables.  Let's be honest; it's a bit dated compared
with what is possible today with X-windows, but it works, and I still
use it.

CRELUT has a number of stages that cannot be expressed via the
examples in the reference manual, so here is a walk through with
commentary.  It would be a good idea to find an image display and
repeat the example yourself. CRELUT asks for an initial lookup table.
Since we don't have one we enter {\tt !}; this results in a greyscale
being loaded in the colour table.  Next we nominate an image for which
we want to tailor the lookup table.  In order to display it and to
compute an histogram we must also provide scaling limits. The chosen
limits encompass most data values.

\small
\begin{verbatim}
     ICL> crelut
     INLUT - NDF containing input lookup table /@$KAPPA_DIR/spectrum_lut/ > !
     NDF - Image to be displayed /@$KAPPA_DIR/ccdframec/ >
     LOW - Low value for image scaling /2250/ >
     HIGH - High value for image scaling /30790.990234375/ > 3000
\end{verbatim}
\normalsize
The image appears in grey above the histogram of values between 2250
and 3000.  Notice that the histogram shows the appearance of data
values given along the data-value axis, currently just grey
intensities.  Had we entered {\tt !} to parameter NDF, a ramp would
appear rather than the histogram.  Below the histogram are two rows of
16 enumerated colours and grey levels.  The bottom line is the
standard \htmlref{palette}{se:coltab}, as would be given by command
\htmlref{PALDEF}{PALDEF}, and an adjustable upper line.  To manipulate
the colour table (and hence generate a new lookup table) we select
palette colours and data-value ranges.  First we want to add some
colours of our own to the palette.

We first add a \htmlref{named colour}{ap:colset}---sienna---to the
palette, followed by a pale yellow given as RGB intensities.  We try
Violetred, but don't like it so the previous grey value is restored.
Orchid is paler than Violetred and more like what we want.  A null
ends the loop.  Note that once you have completed this section you
cannot add more colours to the palette.  The current RGB is given so
minor adjustments can be made.  Also you don't have to add palette
colours in numerical order.

\small
\begin{verbatim}
     Now you may add to the predefined palette. Numbers 16 to 31, are available.
     Type ! to complete the modifications.
     PALNUM - Number of the palette entry to be modified /16/ >
     The current RGB is 0,0,0.
     COLOUR - New palette colour > sienna
     OK - Accept this colour (Y/N)? /NO/ > y
     PALNUM - Number of the palette entry to be modified /17/ >
     The current RGB is 6.666667E-02,6.666667E-02,6.666667E-02.
     COLOUR - New palette colour > 1.0,1.0,0.3
     OK - Accept this colour (Y/N)? /NO/ > y
     PALNUM - Number of the palette entry to be modified /18/ >
     The current RGB is 0.1333333,0.1333333,0.1333333.
     COLOUR - New palette colour > violetred
     OK - Accept this colour (Y/N)? /NO/ >
     PALNUM - Number of the palette entry to be modified /18/ >
     The current RGB is 0.1333333,0.1333333,0.1333333.
     OUR - New palette colour > orchid
     OK - Accept this colour (Y/N)? /NO/ > y
     PALNUM - Number of the palette entry to be modified /19/ > !
\end{verbatim}
\normalsize
Now we change the colour table.  Notice the appearance of the histogram
alters.  Between 2300 and 2380 the colour smoothly varies from blue to
orchid. (We tried a narrower range but did not like it.)  Next a
block of sienna represents values in the range 2375--2400.  Notice that
the order of the colours matters, {\it{cf.}}\ the 2500--2600 range
where 2500 appears pale yellow and 2600 is red.  A null ends the
manipulation.

\small
\begin{verbatim}
     Now you may interactively change the lookup table (LUT). You give ranges of
     data values (that map to LUT pens) to be assigned colours obtained
     by interpolation of pairs of numbered colours selected from the palette.
     Type ! to complete the creation of the LUT.

     VALRANGE - Data-value range to be assigned colours > 2300,2350
     COLRANGE - Select one or two colours from the palette > 4,18

     VALRANGE - Data-value range to be assigned colours > 2300,2380
     COLRANGE - Select one or two colours from the palette > 4,18
 
     VALRANGE - Data-value range to be assigned colours > 2375,2400
     COLRANGE - Select one or two colours from the palette > 16
 
     VALRANGE - Data-value range to be assigned colours > 2900,3000
     COLRANGE - Select one or two colours from the palette > 6
 
     VALRANGE - Data-value range to be assigned colours > 2500,2600
     COLRANGE - Select one or two colours from the palette > 17,2
 
     VALRANGE - Data-value range to be assigned colours > !
\end{verbatim}
\normalsize
Finally, we save the lookup table in an NDF called YUK, but not the palette.
The original palette is restored.

\small
\begin{verbatim}
     Type the null character, !, if the created LUT is not to be saved.
     OUTLUT - NDF to save the lookup table > YUK
     OUTPAL - NDF to save the palette /!/ >
\end{verbatim}
\normalsize
 
\subsection{\xlabel{se_palette}Palette\label{se:palette}}

There are four commands for controlling the
\htmlref{palette}{se:coltab}\latexonly{ (Section~\ref{se:coltab})}, all
beginning PAL.  If you inherit the image display after a non-{\footnotesize
KAPPA} user or after a device reset, you will probably have to reset
the palette.  You can do this either by loading the default
palette---black, white, the primary then secondary colours, and eight
equally spaced grey levels---with the command \htmlref{PALDEF}{PALDEF};
or load a palette you've created yourself via \htmlref{PALREAD}{PALREAD}.
You modify the palette by changing individual colours within it using
\htmlref{PALENTRY}{PALENTRY}.  The colour
specification can be a \htmlref{named colour}{ap:colset}
\latexonly{(see Appendix~\ref{ap:colset} for a list)}, or RGB intensities.
For example,

\small
\begin{verbatim}
     % palentry 1 Skyblue
     % palentry 14 [1.0,1.0,0.3]
\end{verbatim}
\normalsize
would make palette index 1 sky blue and index 14 a pale yellow.  Once
you have a palette you like, save it in an NDF with
\htmlref{PALSAVE}{PALSAVE}.  \htmlref{IDSTATE}{IDSTATE}
reports the current palette colours by name, picking the nearest named
colour from the standard colour set given in 
\latexelsehtml{Appendix~\ref{ap:colset}.}{the set of named colours.}
The palette has limited {\em direct\/} use in {\footnotesize KAPPA} so far,
but it is expected to grow as more applications support different
colouring of the components of a plot.  One example, is in
\htmlref{DISPLAY}{DISPLAY} where you can have a coloured border for
effect (as seen on television news programmes).  The colour of the
border is selected from the palette or the nearest colour in the
palette is used.

Indirectly, the appearance of existing plots may be changed.  Palette
entry 0 is the background colour, and entries 1--5 correspond to
\SGSref\  pens or \PGPLOTref\ colour indices of the same number.  By
choosing a palette colour equal to the background colour, features may
be `erased'.

Note that the X-windows overlay only has one colour, and so does not have
a palette.

\newpage
\section{\xlabel{se_masking}Masking, Bad Values, and Quality\label{se:masking}}

{\em Masking\/} is the process by which you can exclude portions of
your data from data processing or analysis.  Suppose that you are
doing surface photometry of a bright galaxy, part of the data reduction
is to measure the background contribution around the galaxy and to
subtract it.  What you want to avoid is to include light from the galaxy
in your estimation of the background.  A convenient method for doing
this is to mask the galaxy during the background fitting.

There are two techniques used for masking.  One employs special {\em
bad\/} values, which are synonymous with {\em magic\/} or {\em
invalid\/} values.  These appear within the data or variance arrays
instead of the actual values, and indicate that the value is to be
ignored or is undefined.  They are destructive and so some people
don't like them, but you can always mask your data into a new,
temporary NDF.  If bad values weren't used what is a programme
supposed to do if it cannot compute a value, say divide by zero?  With
a little care, bad values are quite effective and they are used
throughout {\footnotesize KAPPA}.  By its nature, a bad value can only
indicate a logical condition about a data element---it is either good
or bad---and so this technique is sometimes called {\em flagging}.

In contrast, the second technique, uses a quality array.  This permits
many more attributes or qualities of the data to be associated with
each pixel.  In the current implementation there may be up to 255
integer values or 8 1-bit logical.  Thus quality can be regarded as
offering 8 logical masks extending over the data or variance arrays,
and can signify the presence or absence of a particular property if
the bit has value 1 or 0 respectively.  An application of quality to
satellite data might include the detector used to measure the value,
some indicator of the time each pixel was observed, was the
observation made within the Earth's radiation belts, and whether or
not the pixel contains a reseau mark.  By selecting only those data
with the appropriate quality values, you process only the data with
the desired properties.  This can be very powerful.  However, it does
have the drawback of having to store at least an extra byte per pixel
in your NDF.

The two methods are {\em not\/} mutually exclusive; the NDF permits
their simultaneous use in a dataset.

Now we'll look at both of these techniques in detail and demonstrating
the relevant {\footnotesize KAPPA} tasks.

\subsection{\xlabel{se_badmasking}Bad-pixel Masking\label{se:badmasking}}

Bad pixels are flagged with the Starlink standard values (see
Section~5 of \xref{SUN/39}{sun39}{}), which for
\htmlref{\_REAL}{ap:HDStypes} is the most-negative value possible.

In addition to tasks which routinely create bad values in the output
value is undefined, {\footnotesize KAPPA} offers many applications for
flagging pixels with certain properties or locations.

\subsubsection{\xlabel{se_ardwork}Doing it the ARD Way\label{se:ardwork}}

To mask a region or a series of regions within an NDF, you can
create an \xref{ASCII Region Definition}{sun183}{} (ARD) file.

\small
\begin{verbatim}
     % cat myard.ard
     PIXEL( 23, -17 )
     CIRCLE( -14.5, 318.7, 44.3 )
     ELLIPSE( 75.2, 296.6, 33, 16, 78 )
     POLYGON( 110, 115, 123, 132, 130, 125 )
     CIRCLE( 10, 10, 40 ) .AND. .NOT. CIRCLE( 10, 10, 30 )
     CTRL/D
\end{verbatim}
\normalsize

In this example, the regions are: the pixel with index (23,~$-$17), a
circle of radius 44.3 pixels about pixel co-ordinates (-14.5,~318.7),
an ellipse centred at (75.2,~296.6) with semi-major axis of 33 pixels
and semi-minor axis of 16 pixels, at orientation 78\dgs\ clockwise
from the $x$ axis, and an annulus centred on pixel (10,~10) between
radius 30 and 40 pixels.  There are several other keywords besides
those shown here.  ARD also has a powerful syntax for combining
regions, and you can set it up to use linear data co-ordinates or
your co-ordinate system.  These are described fully in 
\xref{SUN/183}{sun183}{}, but you'll be relieved to learn that there
is a shortcut.

If you are dealing with 2-dimensional data, {\footnotesize KAPPA} offers an
interactive graphical tool for generating ARD files.  To use
\htmlref{ARDGEN}{ARDGEN} you must first display your data on a device
with a cursor, such as an X-terminal.  \htmlref{DISPLAY}{DISPLAY}
with a greyscale lookup table is probably best for doing that.
The grey lets you see the coloured overlays clearly.

\small
\begin{verbatim}
     % ardgen demo.ard

     Current picture has name: DATA, comment: KAPPA_DISPLAY.
     Using NDF '/star/bin/kappa/ccdframec'

     Instructions for using the cursor...
        Press left button on mouse/trackerball to select a point.
        Press middle button on mouse/trackerball to see current cursor co-ordinates.
        Press right button on mouse/trackerball to end input.

     SHAPE - Region shape /'CIRCLE'/ >
\end{verbatim}
\normalsize
At this point you can select a shape.  Enter {\tt ?} to get the
list.  Once you've selected a shape you'll receive instructions.

\small
\begin{verbatim}
     SHAPE - Region shape /'COLUMN'/ > ellipse

     Region type is "ELLIPSE". Identify the centre, then one end of the semi-major
     axis, and finally one other point on the ellipse.
\end{verbatim}
\normalsize
Once you have defined one ellipse, you can define another or exit to
the OPTION prompt.  In addition to keyboard 1, pressing the right-hand
mouse button has the same effect.  Thus in the example, the new shape is
a rotated box.

\small
\begin{verbatim}
     Region completed. Identify another 'ELLIPSE' region...
     OPTION - Next operation to perform /'SHAPE'/ > rotbox

     Region type is "ROTBOX". Identify the two end points of any edge and then give
     a point on the opposite edge.
     Region completed. Identify another 'ROTBOX' region...
\end{verbatim}
\normalsize

If you make a mistake, enter {\tt List} at the OPTION prompt to see a list
of the regions.  Note the ``Region Index'' of the region(s) you wish to
remove, and select the {\tt Delete} option.  At the REGION prompt,
give a list of the regions you want to remove.  If you change your mind,
enter {\tt !} at the prompt for parameter REGIONS, and no regions are
deleted.

Now suppose you want to combine or invert regions in some way, you
supply {\tt Combine} at the OPTION prompt.  So suppose we have
created the following regions in {\tt \$KAPPA\_DIR/ccdframe}.

\small
\begin{verbatim}
       Region          Region Description
       Index
 
         1   -  ELLIPSE( 174.1, 234.4, 82.2, -43.5, 65.64783 )
         2   -  ELLIPSE( 168.1, 209.1, 29.4, -19.7, 9.441798 )
         3   -  ELLIPSE( 42.2, 244.1, 13, -10.3, 111.8452 )
         4   -  ROTBOX( 40.5, 219.2, 63.8, 38.3, 37.24281 )
         5   -  RECT( 141.5, 1.4, 143.9, 358.8 )
         6   -  POLYGON( 229.8, 247.7,
                         233.4, 247.7,
                         233.4, 258.6,
                         231, 267,
                         229.8, 265.8,
                         228.6, 256.2 )
\end{verbatim}
\normalsize

We want to form the region inside the first ellipse but not inside
the second.  This done in two stages.  First we invert the second
ellipse, meaning that pixels are included if they are not inside
this ellipse, by combining with the {\tt NOT} operator.

\small
\begin{verbatim}
     OPTION - Next operation to perform /'SHAPE'/ > comb
     OPERATOR - How to combine the regions /'AND'/ > not  
     OPERANDS - Indices of regions to combine or invert /6/ > 2
\end{verbatim}
\normalsize

This removes the original region 2, decrements the region numbers of
the other regions following 2 by one, so that region 3 becomes 2, 4
becomes 3, and so on.  A new region 7 is the inverted ellipse. The
renumbering makes it worth listing the regions before combining
regions.  The second stage is to combine it with region 1, using the
{\tt AND} operator.  This includes pixels if they are in both regions.
In this example, that means all the pixels outside the second ellipse
but which lie within the first.

\small
\begin{verbatim}
    OPTION - Next operation to perform /'SHAPE'/ > com
    OPERATOR - How to combine the regions /'AND'/ > 
    OPERANDS - Indices of regions to combine or invert /[6,7]/ > 1,6
\end{verbatim}
\normalsize

Here is another example of combination.  This creates a region for
pixels are included provided they are in one of two regions, but not
in both.  Here we apply the {\tt XOR} operator to the small ellipse
and the first rotated box.

\small
\begin{verbatim}
     OPTION - Next operation to perform /'SHAPE'/ > comb
     OPERATOR - How to combine the regions /'AND'/ > xor
     OPERANDS - Indices of regions to combine or invert /[4,5]/ > 1,2
\end{verbatim}
\normalsize

Here is the final set of regions.

\small
\begin{verbatim}
     OPTION - Next operation to perform /'SHAPE'/ > list


       Region          Region Description
       Index
 
         1   -  RECT( 141.5, 1.4, 143.9, 358.8 )
         2   -  POLYGON( 229.8, 247.7,
                         233.4, 247.7,
                         233.4, 258.6,
                         231, 267,
                         229.8, 265.8,
                         228.6, 256.2 )
 
         3   -  ( ELLIPSE( 174.1, 234.4, 82.2, -43.5, 65.64783 )
                  .AND.
                  ( .NOT. ELLIPSE( 168.1, 209.1, 29.4, -19.7, 9.441798 ) ) )
 
         4   -  ( ELLIPSE( 42.2, 244.1, 13, -10.3, 111.8452 )
                  .XOR.
                  ROTBOX( 40.5, 219.2, 63.8, 38.3, 37.24281 ) )
\end{verbatim}
\normalsize

Once you are done, enter {\tt "Exit"} at the OPTION prompt, and the
ARD file is created.  {\tt "Quit"} also leaves the programme, but
the ARD file is not made.

Having created the ARD file it is straightforward to generate a
masked image with \htmlref{ARDMASK}{ARDMASK}.

\small
\begin{verbatim}
     % ardmask $KAPPA_DIR/ccdframec demo.ard ardccdmask
\end{verbatim}
\normalsize

\begin{latexonly}
   \begin{figure}[hbt]
   \begin{center}
   \leavevmode\epsfysize=118mm\epsfbox{sun95_ardwork.eps}
   \caption{Masking of {\tt \$KAPPA\_DIR/ccdframec}. To the left shows
   the original ARDMASK regions, and to the right shows the final
   masked regions after some have been combined.}
   \label{fi:ardwork}
   \end{center}
   \end{figure}
\end{latexonly}
\begin{htmlonly}
   \label{fi:ardwork}
   \htmladdimg{sun95_ardwork.gif}

   Figure 8: Masking of {\tt \$KAPPA\_DIR/ccdframec}. To the left shows
   the original ARDMASK regions, and to the right shows the final
   masked regions after some have been combined.
\end{htmlonly}

Figure~
\latexelsehtml{\ref{fi:ardwork}}{8} shows the image with the original
regions outlined to the left.  Note only the section (:270,~:360) is
displayed.  To see where you have masked, use DISPLAY, which lets you
define a colour for bad pixels using the BADCOL parameter.

\small
\begin{verbatim}
     % display ardccdmask badcol=red \\
\end{verbatim}
\normalsize
To the right of Figure~
\latexelsehtml{\ref{fi:ardwork}}{8} is the final masked image.

\subsubsection{\xlabel{se_badsegment}SEGMENT and
ZAPLIN\label{se:badsegment}}

\htmlref{SEGMENT}{SEGMENT} is ostensibly for copying polygonal regions
from one NDF to another.  You may also use SEGMENT to copy bad pixels
into the polygonal regions by giving the null value for one of the two
input NDFs. For instance,

\small
\begin{verbatim}
     % segment in1=! in2=$KAPPA_DIR/ccdframec out=ccdmask
\end{verbatim}
\normalsize
NDF ccdmask will have bad values inside the polygons, whereas

\small
\begin{verbatim}
     % segment in2=! in1=$KAPPA_DIR/ccdframec out=ccdmask
\end{verbatim}
\normalsize
the pixels exterior to the polygons are flagged.  SEGMENT lets you
define the polygon vertices interactively, like in
\htmlref{ARDGEN}{ARDGEN}, but you can also use text files, or respond
to prompting.

\htmlref{ZAPLIN}{ZAPLIN} also has an option to fill in rectangular
areas when parameter ZAPTYPE has value {\tt Bad}.

\subsubsection{\xlabel{se_badspecial}Special Filters for Inserting Bad Values
\label{se:badspecial}}

There are applications that mask pixels if their values meet certain
criteria.

\htmlref{SETMAGIC}{SETMAGIC} flags those pixels with a nominated
value.  It is most useful during conversion of imported data whose
data system uses \htmlref{bad-pixel}{se:masking} values different from
Starlink's.

\htmlref{FFCLEAN}{FFCLEAN} removes defects smaller than a nominated
size from an image or vector \NDFref{NDF}.  It flags those pixels that
deviate from a smoothed version of the NDF by more than some number of
standard deviations from the local mean.

\htmlref{ERRCLIP}{ERRCLIP} flags pixels that have errors larger than
some supplied limit or signal-to-noise ratios below a threshold.  The
errors come from the \htmlref{VARIANCE}{apndf:variance} component of the NDF. 
Thus you can exclude unreliable data from analysis.

% The filtering applications have a WLIM parameter

\subsection{\xlabel{se_qualitymask}Quality Masking\label{se:qualitymask}}

All the \NDFref{NDF}\ tasks in {\footnotesize KAPPA} use quality yet there is
no obvious sign in individual applications how particular values of
\htmlref{quality}{se:masking} are selected.  What gives?  The meanings
attached to the quality bits will inevitably be quite specific for
specialist software packages, but {\footnotesize KAPPA} tasks aim to be
general purpose.  To circumvent this conflict there is an NDF
component called the {\em bad-bits mask} that forms part of the
quality information.  Like a QUALITY value, the bad-bits mask is an
unsigned byte.  Its purpose is to convert the eight quality flags into
a single logical value for each pixel, which can then be processed
just like a bad pixel.

When data are read from the NDF by mapping into memory, the quality of
each pixel is combined with the bad-bits mask; if a result of this
quality masking is FALSE, that pixel is assigned the bad value for
processing.  This does not change the original values stored in the NDF;
it only affects the mapped data.

So how do the quality and bad-bits mask combine to form a logical
value?  They form the bit-wise `AND' and test it for equality for 0.
None the wiser?  Regard each bit in the bad-bits mask as a switch
to activate detection of the corresponding bit in a pixel's quality.
The switch is on if it has value 1, and is off if it has value 0.
Thus if the pixel is flagged only if one or more of the 8 bits
has both quality and the corresponding bad-bit set to 1.  Here
are some examples:

\begin{center}
\begin{tabular}{lrr}
QUALITY:  & 10000001 & 10000001 \\
Bad-bits: & 01000100 & 01000101 \\
Bits on:  &          &       \^{} \\
Result:   & TRUE     & FALSE \\
\end{tabular}
\end{center}

The application \htmlref{SETBB}{SETBB} allows you to modify the
bad-bits mask in an NDF.  It allows you to specify the bit pattern in
a number of ways including decimal and binary as illustrated below.

\small
\begin{verbatim}
     % setbb RO950124 5
     % setbb RO950124 b101
\end{verbatim}
\normalsize
These both set the bad-bits mask to 00000101 for the NDF RO950124.
SETBB also allows you to combine an existing NDF bad-bits mask with
another mask using the operators AND and OR.  OR lets you switch on
additional bits without affecting those already on; AND lets you turn
off selected bits leaving the rest unchanged.

\small
\begin{verbatim}
     % setbb RO950124 b00010001 or
     % setbb RO950124 b11101110 and
\end{verbatim}
\normalsize
The first example sets bits 1 and 5 but leaves the other bits of
the mask unaltered, whereas the second switches off the same bits.

Now remembering which bit corresponds to which could be a strain on
the memory.  It would be better if some meaning was attached to each
bit through a name.  There are four general tasks in the
\IRASref\ package which address this. [We plan to integrate them into the
more-natural home of the {\footnotesize NDFPACK} sub-package of {\footnotesize KAPPA}.]
SETQUAL sets quality values and names; SHOWQUAL lists the named qualities;
REMQUAL removes named qualities; and QUALTOBAD uses a logical expression
containing the named quality properties to create a copy of your NDF
in which pixels satisfying the quality expression are set bad.
See Section~4.3 of \xref{SUN/163}{sun163}{} for details.

\subsection{\xlabel{se_removebad}Removing bad pixels\label{se:removebad}}

Sometimes having bad pixels present in your data is a nuisance, say
because some application outside of {\footnotesize KAPPA} does not
recognise them, or you want to integrate the flux of a source.
{\footnotesize KAPPA} offers a number of options for removing bad
values.  Which of these is appropriate depends on the reason why you
want to remove the bad pixels.

First you could replace the bad values with some other reasonable value,
such as zero.

\small
\begin{verbatim}
     % nomagic old new 0 comp=all
\end{verbatim}
\normalsize
Here dataset new is the same as dataset old except that any bad value
in the data or variance array has now become zero.

If you wanted some representative value used based upon neighbouring
pixels, use the \htmlref{GLITCH}{GLITCH} command.

\small
\begin{verbatim}
     % glitch old new where=bad
\end{verbatim}
\normalsize
This replaces the bad values in the data array (it is yet to support
variance) with the median of the eight neighbouring pixels.  This works
fine for isolated bad pixels but not for large blocks.  If your data are
generally flat, large areas can be replaced using the
\htmlref{FILLBAD}{FILLBAD} task.

\small
\begin{verbatim}
     % fillbad old new size=4
\end{verbatim}
\normalsize
The value of parameter SIZE should be about half the diameter of the
largest region of bad pixels.  Both the data array and variance arrays
are filled.

You may replace individual pixels or rectangular sections using
\htmlref{CHPIX}{CHPIX}.

\small
\begin{verbatim}
     % chpix old new
     SECTION - Section to be set to a constant /'55,123'/ >
     NEWVAL - New value for the section /'60'/ >
     SECTION - Section to be set to a constant /'1:30,-10:24'/ >
     NEWVAL - New value for the section /'-1'/ >
     SECTION - Section to be set to a constant /'1:30,-10:24'/ > !
\end{verbatim}
\normalsize
This replaces pixel (55,~123) with value 60, and the region from
(1,~$-$10) to (30,~24) with $-$1.  The final {\tt !} ends the loop of
replacements.  If you supply NEWVAL on the command line, only one
replacement occurs. 

It is also possible to paste other datasets where your bad values lie
with the \htmlref{PASTE}{PASTE} and \htmlref{SEGMENT}{SEGMENT} tasks.

\small
\begin{verbatim}
     % paste old fudge"(10:20,29:30)" out=new
\end{verbatim}
\normalsize
The dataset old is a copy of dataset new, except in the 22-pixel region
(10,~29) to (20,~30), where the values originate from the fudge dataset.

\newpage
\section{\xlabel{se_datainput}Getting Data into KAPPA\label{se:datainput}}

{\footnotesize KAPPA} utilises general data structures within an
\HDSref\ container file, with file extension {\tt .sdf}.  Most of
the examples in this documentation processing is performed on data in
this \NDFref{NDF}\ format generated from within {\footnotesize KAPPA}.
Generally, you will already have data in `foreign' formats, that is
formats other than the Starlink standard, particularly in the
\FITSref\ (Flexible Image Transport System),
\IRAFref , and \Figaroref\ DST formats.

\subsection{\xlabel{se_autoconvert}Automatic Conversion
\label{se:autoconvert}}

Although {\footnotesize KAPPA} tasks do not work directly with `foreign'
formats, they can made to appear that they do.  What happens is that
the format is converted `on-the-fly' to a scratch NDF, which is then
processed by {\footnotesize KAPPA}.  If the processing creates an output NDF
or modifies the scratch NDF, this may be back-converted `on-the-fly'
too, and not necessarily to the original data format.  At the end, the
scratch NDF is deleted.  So for example you could have an IRAF image
file, use BLOCK to filter the array, and output the resultant array as
a FITS file.

We must first define the names of the recognised formats and a file
extension associated with each format.  In practice you'll most likely
do this with the \xref{{\tt convert}}{sun55}{} command, which
creates these definitions for many popular formats.  The
file extension determines in which format a file is written.  There is
an environment variable called NDF\_FORMATS\_IN which defines the
allowed formats in a comma-separated list with the file extensions in
parentheses.  Here is an example.

\small
\begin{verbatim}
     % setenv NDF_FORMATS_IN 'FITS(.fit),IRAF(.imh),FIGARO(.dst)'
\end{verbatim}
\normalsize
Once defined it lets you run {\footnotesize KAPPA} tasks on FITS,
{\footnotesize IRAF}, or {\footnotesize FIGARO} files, like
\small
\begin{verbatim}
     % stats m51.fit
     % stats m51.dst
\end{verbatim}
\normalsize
would compute the statistics of a FITS file {\tt m51.fit}, and then
a {\footnotesize FIGARO} file {\tt m51.dst}.

The environment variable also defines a search order.  Had you entered

\small
\begin{verbatim}
     % stats m51
\end{verbatim}
\normalsize
\htmlref{STATS}{STATS} would first look for an NDF called m51 (stored in file {\tt
m51.sdf}). If it could not locate that NDF, STATS would then look for
a file called {\tt m51.fit}, and then {\tt m51.imh}, and finally {\tt
m51.dst}, stopping once a file was found and associating the
appropriate format with it. If none of the files exist, you'll receive
a ``file not found'' error message.

You can still define an
\latexelsehtml{NDF section (see Section~\ref{se:ndfsect})}{\htmlref{NDF
section}{se:ndfsect}} when you access an existing data file in a foreign
format.  Thus

\small
\begin{verbatim}
     % stats m51.imh"(100:200,200~81)"
\end{verbatim}
\normalsize
would derive the statistics for $x$ pixels between 100 and 200, and
$y$ pixels 160 to 240 in the {\footnotesize IRAF} file {\tt m51.imh}.

The conversion tasks may be your own for some private format, but
normally they will come from the
\xref{{\footnotesize CONVERT} package}{sun55}{}\latexonly{ (SUN/55)}.  If you want
to learn how to add conversions to the standard ones, you should
consult \xref{SSN/20}{ssn20}{}.

There is an environment variable that defines the format of new data
files.  This could be assigned the same value as NDF\_FORMATS\_OUT,
though they don't have to be.

\small
\begin{verbatim}
     % setenv NDF_FORMATS_OUT 'FITS(.fit),IRAF(.imh),FIGARO(.dst)'
\end{verbatim}
\normalsize
If you supply the file extension when a {\footnotesize KAPPA} task creates a
new dataset, and it appears in NDF\_FORMATS\_OUT, you'll get a file in
that format.  So for instance,

\small
\begin{verbatim}
     % ffclean in=m51.dst out=m51_cleaned.dst \\
\end{verbatim}
\normalsize

cleans {\tt m51.dst} and stores the result in {\tt m51\_cleaned.dst}.
On the other hand, if you only give the dataset name
\small
\begin{verbatim}
     % ffclean in=m51.dst out=m51_cleaned \\
\end{verbatim}
\normalsize
the output dataset would be the first in the NDF\_FORMATS\_OUT list.
Thus if you want to work predominantly in a foreign format, place it
first in the NDF\_FORMATS\_IN and NDF\_FORMATS\_OUT lists.

If you want to create an output NDF, you must insert a full stop at the
head of the list.

\small
\begin{verbatim}
     % setenv NDF_FORMATS_OUT '.,FITS(.fit),IRAF(.imh),FIGARO(.dst)'
\end{verbatim}
\normalsize
This is the recommended behaviour.  If you just want to propagate the
input data format, insert an asterisk at the start of the output-format
list.

\small
\begin{verbatim}
     % setenv NDF_FORMATS_OUT '*,.,FITS(.fit),IRAF(.imh),FIGARO(.dst)'
\end{verbatim}
\normalsize
This only affects applications which create a dataset using information
propagated from an existing dataset.  For instance, if the above
NDF\_FORMATS\_OUT were defined,

\small
\begin{verbatim}
     % ffclean in=m51.dst out=m51_cleaned \\
\end{verbatim}
\normalsize
would now create {\tt m51\_cleaned.dst}.  If there is no propagation
in the given application, the asterisk is ignored. 

You can retain the scratch NDF by setting the environment variable
NDF\_KEEP to 1.  This is useful if you intend to work mostly with NDFs
and will save the conversion each time you access the dataset.

The {\tt convert} command, which sets up definitions for the {\footnotesize
CONVERT} package defines the lists of input and output formats as
follows.

\small
\begin{verbatim}
     % setenv NDF_FORMATS_IN \
     'FITS(.fit),FIGARO(.dst),IRAF(.imh),STREAM(.das),UNFORMATTED(.unf),UNF0(.dat),
     ASCII(.asc),TEXT(.txt),GIF(.gif),TIFF(.tif),GASP(.hdr),COMPRESSED(.sdf.Z),
     GZIP(.sdf.gz),FITS(.fits),FITS(.fts),FITS(.FTS),FITS(.FITS),FITS(.FIT),
     FITS(.lilo),FITS(.lihi),FITS(.silo),FITS(.sihi),FITS(.mxlo),FITS(.rilo),
     FITS(.rihi),FITS(.vdlo),FITS(.vdhi),STREAM(.str)'

     % setenv NDF_FORMATS_OUT \
     '.,FITS(.fit),FIGARO(.dst),IRAF(.imh),STREAM(.das),UNFORMATTED(.unf),
     UNF0(.dat),ASCII(.asc),TEXT(.txt),GIF(.gif),TIFF(.tif),GASP(.hdr),
     COMPRESSED(.sdf.Z),GZIP(.sdf.gz)'
\end{verbatim}
\normalsize

See the {\footnotesize CONVERT} documentation for more details of these conversions.

\subsection{Other Routes for Data Import}

You can run {\footnotesize CONVERT} ({\it{cf.}}~SUN/55) directly to perform
conversions.  There is also \htmlref{TRANDAT}{TRANDAT}, which will read a text file of
data values, or co-ordinates and data values into an NDF, and 
\xref{ASCIN}{sun140}{ASCIN} in the \SPECDREref\ package\latexonly{ (SUN/140)}.

\subsection{\xlabel{se_fitsreaders}FITS readers\label{se:fitsreaders}}

The automatic conversion does not allow you the full control of the
conversion that direct use of a \FITSref\ reader offers and it does
not deal with the special properties of tape.
\htmlref{FITSIN}{FITSIN} will read, amongst others, simple FITS files
including blocked or group format and floating-point data from tape.
\htmlref{FITSDIN}{FITSDIN} is its counterpart for disc files.  
\latexonly{Let's see the FITS readers in action.}

\subsubsection{\xlabel{se_readfitstape}Reading FITS Tapes\label{se:readfitstape}}

\htmlref{FITSIN}{FITSIN} reads FITS files stored on tape.  For
efficiency, you should select the ``no-rewind'' device for the
particular tape drive, for example {\tt /dev/nrmt0h} on OSF/1 and
{\tt /dev/rmt/1n} on Solaris.

We ask for the second file on the tape, and the headers are displayed
so we can decide whether this is the file we want.  It is so we supply
a name of an NDF to receive the FITS file.  If it wasn't we would
enter {\tt !} to the OUT prompt.  The FMTCNV parameter asks whether
the data are to be converted to \htmlref{\_REAL}{ap:HDStypes}, using
the FITS keywords BSCALE and BZERO, if present.  If you are wondering
why there is {\tt (1)} after the file number, that's present because
FITS files can have sub-files, stored as FITS extensions.

\small
\begin{verbatim}
     % fitsin
     MT - Tape deck /@/dev/nrmt0h/ >
     The tape is currently positioned at file 1.
     FILES - Give a list of the numbers of the files to be processed > 2
     File # 2(1)  Descriptors follow:
     SIMPLE  =                    T
     BITPIX  =                   16
     NAXIS   =                    2
     NAXIS1  =                  400
     NAXIS2  =                  590
     DATE    = '03/07/88'                    /Date tape file created
     ORIGIN  = 'ING     '                    /Tape writing institution
     OBSERVER= 'CL      '                    /Name of the Observer
     TELESCOP= 'JKT     '                    /Name of the Telescope
     INSTRUME= 'AGBX    '                    /Instrument configuration
     OBJECT  = 'SYS:ARCCL.002'               /Name of the Object
     BSCALE  =                  1.0          /Multiplier for pixel values
     BZERO   =                  0.0          /Offset for pixel values
     BUNIT   = 'ADU     '                    /Physical units of data array
     BLANK   =                    0          /Value indicating undefined pixel
                 :                :                :
                 :                :                :
                 :                :                :
     END
     FMTCNV - Convert data? /NO/ >
     OUT - Output image > ff1
     Completed processing of tape file 2 to ff1.
     MORE - Any more files? /NO/ >
\end{verbatim}
\normalsize
We can trace the structure to reveal the 2-byte integer CCD image.  Notice
that the FITS headers are stored verbatim in a component .MORE.FITS.
This is the FITS extension.  The extension contents can be listed with
\htmlref{FITSLIST}{FITSLIST}.  There is more on this NDF extension and
its purpose in
\latexelsehtml{Section~\ref{se:fitsairlock}.}{the \htmlref{FITS
Airlock}{se:fitsairlock}.}

\small
\begin{verbatim}
     % hdstrace ff1
     FF1  <NDF>
 
        DATA_ARRAY(400,590)  <_WORD>   216,204,220,221,202,222,220,206,218,221,
                                       ... 216,218,218,204,221,218,219,222,221,218
        TITLE          <_CHAR*13>      'SYS:ARCCL.002'
        UNITS          <_CHAR*3>       'ADU'
        MORE           <EXT>           {structure}
           FITS(84)       <_CHAR*80>      'SIMPLE  =                    T','BI...'
                                          ... '   ...','         ING PACKEND','END'
   
     End of Trace.
\end{verbatim}
\normalsize
If you have many FITS files to read there is a quick method for
extracting all files or a selection.  In automatic mode the output
files are generated without manual intervention and the headers aren't
reported for efficiency.  Should you want to see the headers, write
them to a text file via the LOGFILE parameter.  The cost of automation
is a restriction on the names of the output files, but if you have
over a hundred files on a tape are you really going to name them
individually?

The following example extracts the fourth to sixth, and eighth files.
Note that the {\tt [~]} are needed because the value for parameter FILES is
a character array. 

\small
\begin{verbatim}
     % fitsin auto
     MT - Tape deck /@/dev/nrmt0h/ >
     FMTCNV - Convert data? /NO/ > y
     PREFIX - Prefix for the NDF file names? /'FITS'/ > JKT
     FILES - Give a list of the numbers of the files to be processed > [4-6,8]
     Completed processing of tape file 4 to JKT4.
     Completed processing of tape file 5 to JKT5.
     Completed processing of tape file 6 to JKT6.
     Completed processing of tape file 8 to JKT8.
     MORE - Any more files? /NO/ >
\end{verbatim}
\normalsize

You can list selected FITS headers from a FITS tape without attempting
to read in the data into NDFs by using \htmlref{FITSHEAD}{FITSHEAD}.
You can redirect its output to a file to browse at your leisure, and
identify the files you want to convert.  So for instance,

\small
\begin{verbatim}
     % fitshead /dev/nrmt1h > headers.lis
\end{verbatim}
\normalsize
lists all the FITS headers from a FITS tape on device {\tt /dev/nrmt1h}
to file {\tt headers.lis}.

After running FITSIN you may notice a file {\tt USRDEVDATASET.sdf} in
the current directory.  This \HDSref\ file records the current
position of the tape, so you can use FITSIN to read a few files, and
then run it again a little later, and FITSIN can carry on from where
you left off.  In other words FITSIN does not have to rewind to the
beginning of the tape to count files.  When you're finished you should
delete this file.

\subsubsection{Reading FITS Files}

Until comparatively recently there was officially no such thing as
disc FITS.  However, for many years {\it ad hoc}\ implementations have
existed.  Of these, \htmlref{FITSDIN}{FITSDIN} will handle files
adhering to the FITS rules for blocking (and more), but it doesn't
process byte-swapped `FITS' files.  Thus it can process files with
fixed-length records of semi-arbitrary length; so, for example, files
mangled during network transfer, which have 512-byte records rather
than the customary 2880, may be read.  However, it will not handle,
VAX FITS files as may be produced with \xref{{\footnotesize FIGARO's}
WDFITS}{sun86}{WDFITS}.  FITSDIN will accept a list of files with
wildcards.  However, a comma-separated list must be enclosed in
quotation marks.  Also wildcards must be protected.  Here are some
examples so you get the idea.

\small
\begin{verbatim}
     % fitsdin '*.fit'
     % fitsdin \*.fit
     ICL> fitsdin *.fit
     % fitsdin '"i*.fit,abc123.fts"'
     ICL> fitsdin "i*.fit,abc123.fts"
\end{verbatim}
\normalsize

In the following example a floating-point file is read (BITPIX=$-$32)
and so FMTCNV is not required.
\small
\begin{verbatim}
     % fitsdin '*.fits'

        2 files to be processed...

     Processing file number 1: /home/scratch/dro/gr.fits.
     File /scratch/dro/gr.fits(1)  Descriptors follow:
     SIMPLE  =                    T / Standard FITS format
     BITPIX  =                  -32 / No. of bits per pixel
     NAXIS   =                    2 / No. of axes in image
     NAXIS1  =                  512 / No. of pixels
     NAXIS2  =                  256 / No. of pixels
     EXTEND  =                    T / FITS extension may be present
     BLOCKED =                    T / FITS file may be blocked
     
     BUNIT   = 'none given      '   / Units of data values
     
     CRPIX1  =   1.000000000000E+00 / Reference pixel
     CRVAL1  =   0.000000000000E+00 / Coordinate at reference pixel
     CDELT1  =   1.000000000000E+00 / Coordinate increment per pixel
     CTYPE1  = '                '   / Units of coordinate
     CRPIX2  =   1.000000000000E+00 / Reference pixel
     CRVAL2  =   0.000000000000E+00 / Coordinate at reference pixel
     CDELT2  =   1.000000000000E+00 / Coordinate increment per pixel
     CTYPE2  = '                '   / Units of coordinate
 
     ORIGIN  = 'ESO-MIDAS'          / Written by MIDAS
     OBJECT  = 'artificial image'   / MIDAS desc.: IDENT(1)
             :                :                :
             :                :                :
             :                :                :
     HISTORY  ESO-DESCRIPTORS END     ................
     
     END
     OUT - Output image > gr
     Completed processing of disc file /home/scratch/dro/gr.fits to gr.
     File has illegal-length blocks (512). Blocks should be a multiple (1--10) of the
     FITS record length of 2880 bytes.
     Processing file number 2: /home/scratch/dro/indef.fits.
     File /home/scratch/dro/indef.fits(1)  Descriptors follow:
     SIMPLE  =                    T  /  FITS STANDARD
     BITPIX  =                   32  /  FITS BITS/PIXEL
     NAXIS   =                    2  /  NUMBER OF AXES
     NAXIS1  =                  256  /
     NAXIS2  =                   20  /
     BSCALE  =      3.7252940008E28  /  REAL = TAPE*BSCALE + BZERO
     BZERO   =      7.9999999471E37  /
     OBJECT  = 'JUNK[1/1]'  /
     ORIGIN  = 'KPNO-IRAF'  /
             :                :                :
             :                :                :
             :                :                :
     END
     OUT - Output image > iraf
     Completed processing of disc file /home/scratch/dro/indef.fits to iraf.
\end{verbatim}
\normalsize
\htmlref{NDFTRACE}{NDFTRACE} shows that the object name is written to
the NDF's title, that axes derived from the FITS headers are present,
and that gr is a \_REAL NDF.

\small
\begin{verbatim}
     % ndftrace gr
 
        NDF structure /home/scratch/dro/iraf:
           Title:  artificial image
           Units:  none given
 
        Shape:
           No. of dimensions:  2
           Dimension size(s):  512 x 256
           Pixel bounds     :  1:512, 1:256
           Total pixels     :  131072
 
        Axes:
           Axis 1:
              Label : Axis 1
              Units : pixel
              Extent: -0.5 to 511.5
 
           Axis 2:
              Label : Axis 2
              Units : pixel
              Extent: -0.5 to 255.5
 
        Data Component:
           Type        :  _REAL
           Storage form:  PRIMITIVE
           Bad pixels may be present
 
        Extensions:
              FITS             <_CHAR*80>
 
\end{verbatim}
\normalsize
Both FITSIN and FITSDIN write the FITS headers into an NDF extension
called FITS within your NDF.  The extension is a literal copy of all
the 80-character `card images' in order.  These can be inspected or
written to a file via the command FITSLIST.  There is more on this NDF
extension and its purpose in
\latexelsehtml{Section~\ref{se:fitsairlock}.}{the \htmlref{FITS
Airlock}{se:fitsairlock}.}

\subsection{\xlabel{se_fitsairlock}The FITS Airlock\label{se:fitsairlock}}

\subsubsection{\xlabel{se_ndfext}NDF Extensions\label{se:ndfext}}

An important feature of the \NDFref{NDF} is that it is designed to be
extensible.  The NDF has components whose meanings are well defined
and universal, and so they can be accessed by general-purpose
software, such as {\footnotesize KAPPA} and \CONVERTref\ provide; but the
NDF also allows independent {\em extensions\/} to be defined and
added, which can store auxiliary information to suit the needs of a
specialised software package.  (Note that the term extension here
refers to a structure within the NDF for storing additional data, and
is neither the file extension {\tt .sdf} nor extensions like BINTABLE
within the FITS file.) An extension is only processed by software that
understands the meanings obeys the processing rules of the various
components of the extension. Other programmes propagate the extension
information unaltered.

The existence of extensions makes it straightforward to write general
utilities for converting an arbitrary format into an NDF.  The idea
being that every specialist package should not have to have its own
conversion tools such as a FITS reader.  However, this still leaves the
additional data that requires specialist knowledge to move it into the
appropriate extension components.  The aim is to make the conversions
themselves extensible, with add-on operations to move the specialist
information to and from the extensions.  This is where the FITS
`airlock' comes in.

The \FITSref\ data format comprises a header followed by the data
array or table.  The header contains a series of 80-character lines
each of which contains the keyword name, a value and an optional
comment. There are also some special keywords for commentary.  The
meanings of most keywords are undefined, and so can be used to
transport arbitrary ancillary information, subject to FITS syntax
limitations.  There is a special NDF extension called FITS, which
mirrors this functionality, and may be added to an NDF.  It therefore
can act as an airlock between the general-purpose conversion tools and
specialist packages.

\subsubsection{\xlabel{se_fitsimpexp}Importing and Exporting from and
to the FITS Extension\label{se:fitsimpexp}}

The FITS extension comprises a 1-dimensional array of 80-character
strings that follow FITS-header formatting rules.  In the case of
\htmlref{FITSIN}{FITSIN} and \htmlref{FITSDIN}{FITSDIN}, each FITS
extension is a verbatim copy of the FITS header of the input file.
Other conversion tools like \xref{IRAF2NDF}{sun55}{IRAF2NDF} and
\xref{UNF2NDF}{sun55}{UNF2NDF} of \CONVERTref\ can also create a FITS
extension in the same fashion.  On export, standard conversion tools
propagate the FITS extension to any FITS headers or equivalent in the
foreign format.  However, information which is derivable from the
standard NDF components, such as the array dimensions, data units, and
linear axes, replaces any equivalent headers from FITS extension.

You use your knowledge, or the writer of the specialist package
provides import tools, to recognise certain FITS keywords and to
attribute meaning to them, and then to move or process their values
to make the specialist extensions.  One such is the PREPARE task in
\IRASref\@.  Similarly, the reverse operation---exporting the
extension information---can occur too, prior to converting the NDF
into another data format.

{\footnotesize KAPPA} offers two simple tools for the importing and exporting
of extension information: \htmlref{FITSIMP}{FITSIMP} and
\htmlref{FITSEXP}{FITSEXP}.  They both use a text file which acts as a
translation table between the FITS keyword and extension components.
Starting with FITSIMP, its translation table might look like this.

\small
\begin{verbatim}
     ORDER_NUMBER _INTEGER  ORDNUM 
     PLATE_SCALE  _REAL SCALE         ! The plate scale in arcsec/mm
     SMOOTHED  _LOGICAL FILTERED 
\end{verbatim}
\normalsize

It consists of three fields: the first is the name of the component in
the chosen extension, the second is the \htmlref{HDS data type}{ap:HDStypes}
of that component, and the third is the FITS keyword.  Optional
comments can appear following an exclamation mark. So if we placed
these lines in file {\tt imptable}, we could create an extension
called MYEXT of data type MJC\_EXT (if it did not already exist)
containing components ORDER\_NUMBER, PLATE\_SCALE, and SMOOTHED.

\small
\begin{verbatim}
     % fitsimp mydata imptable myext mjc_ext
\end{verbatim}
\normalsize
Should any of the keywords not exist in the FITS extension, you'll be
warned.  If the extension already exists, you don't need to specify
the extension data type.  FITSIMP will even handle hierarchical
keywords and those much-loved ING packets from La Palma.

Going in the opposite direction, the text translation file could look
like this
\small
\begin{verbatim}
     MYEXT.ORDER_NUMBER  ORDNUM(LAST) The spectral order number
     MYEXT.PLATE_SCALE   SCALE   The plate scale in arcsec/mm
     MYEXT.SMOOTHED  FILTERED 
\end{verbatim}
\normalsize
where the first column is the `name' of the extension component to be
copied to the FITS extension.  The `name' includes the extension name
and substructures.  The second column gives the FITS keyword to which
to write the value.  A further keyword in parentheses instructs FITSEXP
to place the new FITS header immediately before the header with that
keyword.  If the second keyword is absent from the translation-table
record or the FITS extension, the new header appears immediately before
the END header line in the FITS extension.  Thus the value of ORDER\_NUMBER
in extension MYEXT, creates a new keyword in the FITS extension called
ORDNUM, and it is located immediately prior the keyword LAST.

\subsubsection{\xlabel{se_list-fitsext}Listing the FITS Extension and
keywords\label{se:list-fitsext}}

If you don't want to be bothered with \NDFref{NDF} extensions, you
might just want to know the value of some FITS keyword, say the
exposure time, as part of your data processing. 
\htmlref{FITSLIST}{FITSLIST} lists the contents of the FITS extension
of an NDF or file.  You can even search for keywords with {\bf grep}.

\small
\begin{verbatim}
     % fitslist myndf | grep "ELAPSED ="
\end{verbatim}
\normalsize
This would find the keyword ELAPSED in the FITS extension of NDF
myndf.  (Keywords are 8 characters long and those with values are
immediately followed by an equals sign.)  However, the recommended
way is to use the \htmlref{FITSVAL}{FITSVAL} command.  Since this
command only reports the value, it is particularly useful in scripts
that need ancillary-data values during processing.  The following
obtains the value of keyword ELAPSED.

\small
\begin{verbatim}
     % fitsval myndf ELAPSED
\end{verbatim}
\normalsize

In a script you may need to know whether the keyword exists and take
appropriate action.

\small
\begin{verbatim}
     filterpre = `fitsexist myndf filter`
     if ( $filterpre == "TRUE" ) then
        filter = `fitsval myndf filter`
     else
        prompt -n "Filter > "
        set filter = $<
     endif
\end{verbatim}
\normalsize
Shell variable {\tt filterpres} would be assigned {\tt "TRUE"} when
the FILTER card is present, and {\tt "FALSE"} otherwise.  (The
{\tt `~`} quotes cause the enclosed command to be executed.)  So the
user of the script would be prompted for a filter name whenever the
NDF did not contain that information.

\subsubsection{\xlabel{se_manip-fitsext}Creating and Editing
the FITS Extension\label{se:manip-fitsext}}

Besides the conversion utilities, you can import your own FITS
extension using \htmlref{FITSTEXT}{FITSTEXT}.  You first prepare a
FITS-like header in a text file.  For example,

\small
\begin{verbatim}
     % fitstext myndf myfile
\end{verbatim}

\normalsize
places the contents of {\tt myfile} in the NDF called myndf. This is
not advised unless you are familiar with the rules for writing FITS
headers.  See the NOST {\sl A User's Guide to FITS}.  You find this
and other useful FITS documents, test files, and software at 
\htmladdnormallink{FITS Support Office Home
Page} {http://www.gsfc.nasa.gov/astro/fits/fits\_home.html}\latexonly{ (URL
{\tt http://www.gsfc.nasa.gov/astro/fits/fits\_home.html})}.  FITSTEXT does
perform some limited validation of the FITS headers, and informs you
of any problems it detects.  See the \htmlref{FITSHEAD}{FITSHEAD} Notes in
\latexelsehtml{Appendix~\ref{ap:full}}{\htmlref{application
specifications}{ap:full}} for details.

A safer bet for a hand-crafted FITS extension is to edit an existing
FITS extension to change a value, or use existing lines as templates
for any new keywords you wish to add.  \htmlref{FITSEDIT}{FITSEDIT}
lets you do this with your favourite text editor.  Define the
environment variable EDITOR to your editor, say

\small
\begin{verbatim}
     % setenv EDITOR jed
\end{verbatim}
\normalsize
to choose {\bf jed}.  If you don't do this, and EDITOR is unassigned,
FITSEDIT selects the {\bf vi} editor.  Then to edit the NDF extension
is simple.

\small
\begin{verbatim}
     % fitsedit myndf
\end{verbatim}
\normalsize
This edits the FITS extension of the NDF called myndf.
FITSEDIT extracts the file into a temporary file ({\tt{zzfitsedit.tmp}})
which you edit, and then uses FITSTEXT to restore the FITS extension.
It therefore has the same parsing of the edited FITS headers as FITSTEXT
provides.

\subsubsection{\xlabel{se_emanip-fitsext}Easy way to create and edit
the FITS Extension\label{se:emanip-fitsext}}

Should you wish to write a new value without knowing about FITS, or
in a script where manual editing is undesirable, the
\htmlref{FITSWRITE}{FITSWRITE} command does the job.  So for example,

\small
\begin{verbatim}
     % fitswrite myndf filter value=K
\end{verbatim}
\normalsize
will create a keyword FILTER with value {\tt K} in the FITS extension
of the NDF called myndf.  If the extension does not exist, this
command will first create it.

The \htmlref{FITSMOD}{FITSMOD} command has several editing options
including the ability to delete a keyword:

\small
\begin{verbatim}
     % fitsmod myndf airmass edit=delete
\end{verbatim}
\normalsize
here it removes the AIRMASS header; or rename a keyword:

\small
\begin{verbatim}
     % fitsmod myndf band rename newkey=filter
\end{verbatim}
\normalsize
as in this example, where keyword BAND becomes keyword FILTER; or
update an existing keyword:

\small
\begin{verbatim}
     % fitsmod myndf filter edit=u value=\$V comment='"Standard filter name"'
\end{verbatim}
\normalsize
this example modifies the comment string associated with the FILTER
keyword, leaving the value unchanged.

For routine operations requiring many operations on a dataset, FITSMOD
lets you specify the editing instructions in a text file.

\newpage
\section{\xlabel{se_procedures}Procedures\label{se:procedures}}

Applications from {\footnotesize KAPPA} and other packages can be
combined in procedures and scripts to customise and automate data
processing. In addition to giving literal values to application
parameters, you can include \ICLref\ or C-shell variables on the
command line, whose values are substituted at run time.  It is also
possible to write parameter data into variables, and hence pass them
to another application, or use the variables to control subsequent
processing.

\subsection{\xlabel{se_cshscript}C-shell scripts\label{se:cshscript}}

The \xref{{\sl C-shell Cookbook}}{sc4}{} contains many ingredients and
recipes, and features many {\footnotesize KAPPA} commands.  So there is little
point repeating them here other than to direct you to a documented
script in {\tt \$KAPPA\_DIR/multiplot.csh}.

\subsection{\xlabel{se_ICLproc}ICL Procedures\label{se:ICLproc}}

You should consult the \xref{{\sl ICL Users' Guide}}{sg5}{} for details
about writing {\footnotesize ICL} syntax, procedures, and functions, but you're a busy
researcher\ldots  For a quick overview the {\em two-page\/} summary on
``Writing ICL command files and procedures'' in SUN/101 is recommended
reading, even though much of the document is dated and still refers
to VMS.  Here we'll just show some example procedures that can be
adapted and cover points not mentioned in SUN/101.

Let's start with something simple.  You want to `flash' a series of
images, each with a yellow border.  First you write the following
procedure called FLASH.  It has one argument INPIC, that passes the name of
the NDF you want to display.  When you substitute an {\footnotesize ICL}
variable for a parameter value you enclose it in parentheses.  The lines
beginning with {\tt \{ } are comments.

\small
\begin{verbatim}
     PROC FLASH INPIC
     {
     { Procedure for displaying an image without scaling and a yellow border.
     {
        DISPLAY IN=(INPIC) MODE=FL BORDER BCOLOUR=Yellow
     END PROC
\end{verbatim}
\normalsize
To make {\footnotesize ICL} recognise your procedure you must `load' it.  The
command

\small
\begin{verbatim}
     ICL> LOAD FLASH
\end{verbatim}
\normalsize
will load the file {\tt FLASH.ICL}.
Thereafter in the {\footnotesize ICL} session you can invoke FLASH for many
NDFs. The following will display the NDFs called GORDON and FLOOD
side-by-side.

\small
\begin{verbatim}
     ICL> PICGRID 2 1 
     ICL> FLASH GORDON
     ICL> PICSEL 2
     ICL> FLASH FLOOD
\end{verbatim}
\normalsize
It would be tedious to have to load lots of individual procedures, but
you don't. If you have related procedures that you regularly require
they can be concatenated into a single file which you load.  Better
still is to add definitions for each of the procedures in your {\footnotesize
ICL} login file.  This is defined as the value of the ICL\_LOGIN
environment variable.  A reasonable place is in your home directory
and you'd define it like this.

\small
\begin{verbatim}
     % setenv ICL_LOGIN $HOME/login.icl
\end{verbatim}
\normalsize
However, the file doesn't have to be in your home directory, or called
{\tt login.icl}, but it's convenient to do so.  Suppose you have three
procedures: FLASH, PICGREY in file {\tt \$MY\_DIR/display\_proc.icl},
and FILTER in {\tt /home/user1/dro/improc.icl}.  In your {\tt
\$HOME/login.icl} you could add the following

\small
\begin{verbatim}
     defproc  flash     $MY_DIR/display_proc.icl
     defproc  sfilt     $HOME/user1/dro/improc.icl filter
     defproc  picgr(ey) $MY_DIR/display_proc.icl
\end{verbatim}
\normalsize
which defines three commands that will be available each time you
use {\footnotesize ICL}: FLASH which will run your FLASH
procedure, PICGREY to execute the PICGREY procedure, and SFILT which
runs the FILTER procedure.  In addition PICGREY can be abbreviated
to PICGR or PICGRE.  So now you can load and run your procedure.
Let's have some more example procedures.

Suppose you have a series of commands to run on a number of files.
You could create a procedure to perform all the stages of the
processing, deleting the intermediate files that it creates.

\small
\begin{verbatim}
     PROC UNSHARPMASK NDFIN CLIP NDFOUT

     { Insert ampersands to tell the command-line interpreter than these
     { strings are file names.
        IF SUBSTR( NDFIN, 1, 1 ) <> '@'
           NDFIN = '@' & (NDFIN)
        END IF
        IF SUBSTR( JUNK, 1, 1 ) <> '@'
           NDFOUT = '@' & (NDFOUT)
        END IF

     { Clip the image to remove the cores of stars and galaxies above
     { a nominated threshold.
        THRESH (NDFIN) TMP1 THRHI=(CLIP) NEWHI=(CLIP) \

     { Apply a couple of block smoothings with boxsizes of 5 and 13
     { pixels.  Delete the temporary files as we go along.
        BLOCK tmp1 tmp2 BOX=5
        ! rm tmp1.sdf
        BLOCK tmp2 tmp3 BOX=13
        ! rm tmp2.sdf

     { Multiply the smoothed image by a scalar.
        CMULT tmp3 0.8 tmp4
        ! rm tmp3.sdf

     { Subtract the smoothed and renormalised image from the input image.
     { The effect is to highlight the fine detail, but still retain some
     { of the low-frequency features.
        SUB (NDFIN) tmp4 (NDFOUT)
        ! rm tmp4.sdf
     END PROC
\end{verbatim}
\normalsize
There is a piece of syntax to note which often catches people out.
Filenames, data objects, and devices passed via {\footnotesize ICL} variables
to applications, such as NDFIN and NDFOUT in the above example, must
be preceded by an {\tt @}.

A common use of procedures is likely to be to duplicate processing
for several files.  Here is an example procedure that does that.  It uses
some intrinsic functions which look just like Fortran.

\small
\begin{verbatim}
     PROC MULTISTAT

     { Prompt for the number of NDFs to analyse.  Ensure that it is positive.
        INPUTI Number of frames:  (NUM)
        NUM = MAX( 1, NUM )

     { Find the number of characters required to format the number as
     { a string using a couple of ICL functions.
        NC = INT( LOG10( I ) ) + 1

     { Loop NUM times.
        LOOP FOR I=1 TO (NUM)

     { Generate the name of the NDF to be analysed via the ICL function
     { SNAME.
          FILE = '@' & SNAME('REDX',I,NC)

     { Form the statistics of the image.
          STATS NDF=(FILE)
        END LOOP
     END PROC
\end{verbatim}
\normalsize
If NUM is set to 10, the above procedure obtains the statistics of the
images named REDX1, REDX2, \dots REDX10. The {\footnotesize
ICL} variable FILE is in parentheses because its value is to be
substituted into parameter NDF. 

Here is another example, which could be used to flat field a series of
CCD frames.  Instead of executing a specific number of files, you
can enter an arbitrary sequence of NDFs.  When processing is
completed a !! is entered rather than an NDF name, and that exits the
loop.  Note the {\tt \~{}} continuation character (it's not
required but it's included for pedagogical reasons).
\pagebreak[3]

\small
\begin{verbatim}
     PROC FLATFIELD

     { Obtain the name of the flat-field NDF.  If it does not have a
     { leading @ insert one.
        INPUT "Which flat field frame?: " (FF)
        IF SUBSTR( FF, 1, 1 ) <> '@'
           FF = '@' & (FF)
        END IF

     { Loop until there are no further NDFs to flat field.
        MOREDATA = TRUE
        LOOP WHILE MOREDATA

     { Obtain the frame to flat field.  Assume that it will not have
     { an @ prefix. Generate a title for the flattened frame.
           INPUT "Enter frame to flat field (!! to exit): " (IMAGE)
           MOREDATA = IMAGE <> '!!'
           IF MOREDATA
              TITLE = 'Flat field of ' & (IMAGE)
              IMAGE = '@' & (IMAGE)

     { Generate the name of the flattened NDF.
              IMAGEOUT = (IMAGE) & 'F'
              PRINT Writing to (IMAGEOUT) 

     { Divide the image by the flat field.
              DIV IN1=(IMAGE) IN2=(FF) OUT=(IMAGEOUT) ~
                TITLE=(TITLE)
           END IF
        END LOOP
     END PROC
\end{verbatim}
\normalsize
Some {\footnotesize KAPPA} applications, particularly the statistical ones,
produce output parameters which can be passed between applications via
{\footnotesize ICL} variables. Here is an example to draw a
contour plot centred about a star in a nominated data array
from only the star's approximate position.  The region about the star is
stored in an output NDF file.  Note the syntax required to define the
value of parameter INIT; the space between the left bracket and
parenthesis is essential.

\small
\begin{verbatim}
     PROC COLSTAR FILE,X,Y,SIZE,OUTFILE

     {+
     {  Arguments:
     {     FILE = FILENAME (Given)
     {        Input NDF containing one or more star images.
     {     X = REAL (Given)
     {        The approximate x position of the star.
     {     Y = REAL (Given)
     {        The approximate y position of the star.
     {     SIZE = REAL (Given)
     {        The half-width of the region about the star's centroid to be
     {        plotted and saved in the output file.
     {     OUTFILE = FILENAME (Given)
     {        Output primitive NDF of 2*%SIZE+1 pixels square (unless
     {        constrained by the size of the data array or because the location
     {        of the star is near an edge of the data array.
     {-

     { Ensure that the filenames have the @ prefix.
        IF SUBSTR( FILE, 1, 1 ) <> '@'
           NDF = '@' & (FILE)
        ELSE
           NDF = (FILE)
        END IF
        IF SUBSTR( OUTFILE, 1, 1 ) <> '@'
           NDFOUT = '@' & (OUTFILE)
        ELSE
           NDFOUT = (OUTFILE)
        END IF

     { Search for the star in a 21x21 pixel box.  The centroid of the
     { star is stored in the ICL variables XC and YC.
        CENTROID NDF=(NDF) INIT=[ (X&','&Y)] XCEN=(XC) YCEN=(YC) ~
          MODE=INTERFACE SEARCH=21 MAXSHIFT=14

     { Convert the co-ordinates to pixel indices.
        IX = NINT( XC + 0.5 )
        IY = NINT( YC + 0.5 )

     { Find the upper and lower bounds of the data array to plot.  Note
     { this assumes no origin information in stored in the data file.
        XL = MAX( 1, IX - SIZE )
        YL = MAX( 1, IY - SIZE )
        XU = MAX( 1, IX + SIZE )
        YU = MAX( 1, IY + SIZE )

     { Create a new NDF centred on the star.
        NDFCOPY IN=(NDF)((XL):(XU),(YL):(YU)) OUT=(NDFOUT)

     { Draw a contour plot around the star on the current graphics device
     { at the given percentiles.
        TURBOCONT NDF=(NDFOUT) MODE=PE PERCENTILES=[80,90,95,99]

     { Exit if an error occurred, such as not being able to find a star
     { near the supplied position, or being unable to make the plot.
        EXCEPTION ADAMERR
           PRINT Unable to find or plot the star.
        END EXCEPTION
     END PROC
\end{verbatim}
\normalsize

Here is another that creates a fancy display of an image with axes and
a key showing data values.  Besides ICL syntax, it illustrates the use
of some graphics-database commands.  Again note the need to give an
expression combining the $x$-$y$ bounds of the key to the LBOUND and
UBOUND parameter arrays.

\small
\begin{verbatim}
     PROC FANCYLOOK NDF

     { Function:
     {    Plots a image with axes and colour-table key.

     { Insert the prefix to tell the parser that this is a file.
        IF SUBSTR( NDF, 1, 1 ) <> '@'
           FILE = '@' & (NDF)
        ELSE
           FILE = (NDF)
        END IF

     { Clear the current window.
        GDCLEAR CURRENT

     { Set the background and annotation colours.
        PALENTRY 0 Sienna
        PALENTRY 1 Yellow

     { Find the extent of the current picture and its aspect ratio.
        GDSTATE NCX1=(FX1) NCX2=(FX2) NCY1=(FY1) NCY2=(FY2) NOREPORT
        ASP = ( FX2 - FX1 ) / ( FY2 - FY1 )

     { Inquire the label of the current picture.  If it does not have one
     { label the current picture
        PICIN CURRENT LABEL=(ORIGLABEL) NOREPORT NAME=!
        IF ORIGLABEL = " "
           PICLABEL ORIGIN
           ORIGLABEL = "ORIGIN"
        END IF

     { Display the image with axes using the most-ornate fount.
        DISPLAY (FILE) MODE=PE AXES FONT=NCAR COSYS=D SCALOW=(LOW) SCAHIGH=(HIGH) \

     { Find the extent of the last frame picture, i.e. the one associated
     { with the last displayed image.
        PICFRAME
        GDSTATE NCX1=(DX1) NCX2=(DX2) NCY1=(DY1) NCY2=(DY2) NOREPORT

     { Determine the widths of the borders.
        XL = DX1 - FX1
        XR = FX2 - DX2
        YB = DY1 - FY1
        YT = FY2 - DY2

     { Restore the original current picture.
        PICSEL (ORIGLABEL)

     { Only plot a key if there is room. 
        IF MAX( XL, XR, YB, YT ) > 0.055

     { Determine which side has most room for the key.  First, see if the
     { key is vertical.
           IF MAX( XL, XR ) >= MAX( YB, YT )

     { Determine the width and height of the vertical key.  Bias to select
     { the right-hand side.  Part of the 0.75 is to allow for the wider
     { border to the left (0.19:0.05 of frame).  Try to obtain the same width,
     { subject to the constraint that it must fit inside the current picture. 
              ASPOBL = MAX( ASP, 1.0 )
              WIDTH = MIN( 1.2 * MAX( 0.75 * XL, XR ), 0.14 / ASPOBL )
              HEIGHT = 0.7 * ( DY2 - DY1 )

     { Define the bounds of the colour-table key.   Offset the key so that
     { there is no large gap between it and the image's axes.  The factors are
     { empirical, and no doubt could be improved with a more-sophisticated algorithm.
              DELTA = 0.12 * MAX( 0.0, DX2 - DX1 - 0.6 / ASPOBL )
              IF XL > 1.333 * XR
                 XK1 = MAX( 0.01, DX1 - WIDTH + DELTA )
              ELSE
                 XK1 = DX2 - DELTA * 5.0 / 19.0
              END IF
              XK2 = XK1 + WIDTH
              YK1 = 0.5 * ( DY2 + DY1 - HEIGHT )
              YK2 = YK1 + HEIGHT
           ELSE

     { Determine the width and height of the horizontal key.  Try to obtain
     { the same width, subject to the constraint that it must fit inside the
     { current picture. 
              ASPPRO = MIN( ASP, 1.0 )
              WIDTH = MIN( 1.2 * MAX( YB, YT ), 0.1 * ASPPRO )
              HEIGHT = 0.7 * ( DX2 - DX1 )

     { Define the bounds of the colour-table key.   Offset the key so that
     { there is no large gap between it and the image's axes.  The factors are
     { empirical, and no doubt could be improved with a more-sophisticated algorithm.
              DELTA = 0.11 * MAX( 0.0, DY2 - DY1 - 0.6 * ASPPRO )
              IF YB > YT
                 YK1 = MAX( 0.01, DY1 - WIDTH + DELTA )
              ELSE
                 YK1 = DY1 - DELTA * 9.0 / 15.0
              END IF
              YK2 = YK1 + WIDTH
              XK1 = 0.5 * ( DX2 + DX1 - HEIGHT )
              XK2 = XK1 + HEIGHT
           END IF

     { So far the units are in NDC.  LUTVIEW uses co-ordinates which go from
     { (0,0) to (1,1) for both axes.  So transform some of the co-ordinates.
           IF ASP >= 1
              YK1 = YK1 * ASP
              YK2 = YK2 * ASP
           ELSE
              XK1 = XK1 / ASP
              XK2 = XK2 / ASP
           END IF

     { Draw the key to fit within the current picture annotating with 
     { the scaling used in DISPLAY.
           LUTVIEW LOW=(LOW) HIGH=(HIGH) LBOUND=[ (XK1&','&YK1)] ~
             UBOUND=[ (XK2&','&YK2)] MODE=XY
        END IF
     END PROC
\end{verbatim}
\normalsize

\newpage
\section{\xlabel{se_probpage}Problems Problems\label{se:probpage}}
\subsection{Errors}
A detailed list of error codes and their meanings is not available.
{\footnotesize KAPPA} produces descriptive contextual error messages, which
are usually straightforward to comprehend.  Some of these originate in
the underlying infrastructure software.  Error messages from {\footnotesize
KAPPA} begin with the name of the application reporting the error. The
routine may have detected the error, or it has something to say about
the context of the error.

The remainder of the section describes some difficulties you may encounter
and how to overcome them.  Please suggest additions to this compilation.

\subsection{Unable to Obtain Work Space}

Error messages like ``Unable to create a work array'' may puzzle you.
They are accompanied by additional error messages that usually
pinpoint the reason for the failure of the application to complete.
Many applications require temporary or work space to perform their
calculations.  This space is stored in an \HDSref\ file within directory
{\tt \$HDS\_SCRATCH} and most likely is charged to your disc quota. (If you
have not redefined this environment variable, it will point to your
current directory.) So one cause for the message is insufficient disc
quota available to store the work space container file or to extend
it.  A second reason for the message is that your computer cannot
provide sufficient virtual memory to map the workspace.  In this case
you can try increasing your process limits using the C-shell built-in
function {\tt limit}.  You can find your current limits by entering
{\tt limit}.  You should see a list something like this.

\small
\begin{verbatim}
     cputime         unlimited
     filesize        unlimited
     datasize        131072 kbytes
     stacksize       2048 kbytes
     coredumpsize    unlimited
     memoryuse       89232 kbytes
     vmemoryuse      1048576 kbytes
     descriptors     4096 
\end{verbatim}
\normalsize
The relevant keywords are {\tt datasize} and the {\tt vmemoryuse}.  In
effect {\tt datasize} specifies the maximum total size of data files
you can map at one time in a single programme.  The default should be
adequate for most purposes and only need be modified for those working
with large images or cubes.  The {\tt vmemoryuse} specifies the maximum
virtual memory you can use.

\small
\begin{verbatim}
    % limit datasize 32768
\end{verbatim}
\normalsize
sets the maximum size of mapped data to 32 megabytes.  Values cannot
exceed the system limits.  You can list these with the {\tt -h}
qualifier.

\small
\begin{verbatim}
     % limit -h
     cputime         unlimited
     filesize        unlimited
     datasize        1048576 kbytes
     stacksize       32768 kbytes
     coredumpsize    unlimited
     memoryuse       89232 kbytes
     vmemoryuse      1048576 kbytes
     descriptors     4096 
\end{verbatim}
\normalsize
Although you can set your limits to the system maxima, it doesn't mean
that you should just increase your quotas to the limits. You might
become unpopular with some of your colleagues, especially if you
accidentally try to access a huge amount of memory.  If you cannot
accommodate your large datasets this way, you should fragment your
data array, and process the pieces separately.

After receiving this error message in an \ICLref\ session you may
need to delete the scratch file by hand.  The file is called {\tt
txxx.sdf}, where {\tt xxxx} is a process identifier.  A normal exit
from {\footnotesize ICL} will delete the work-space container file.

\subsection{\xlabel{se_probwrongNDF}Application Automatically Picks up the
Wrong NDF\label{se:probwrongNDF}}

Some applications read the name of the \NDFref{NDF}\ used to create a plot or
image from the graphics database in order to save typing.  Once in a
while you'll say ``that's not the one I wanted''.  This is because 
\AGIref\ finds the last {\tt DATA} picture situated within the current picture.
Abort the application via {\tt !!}, then use \htmlref{PICCUR}{PICCUR}
or \htmlref{PICLIST}{PICLIST} to select the required {\tt FRAME}
picture enclosing the {\tt DATA} picture, or even select the latter
directly.  You can override the AGI NDF also by specifying the
required NDF on the command line, provided it has pixels whose indices
lies within the world co-ordinates of the {\tt DATA} picture.  Thus

\small
\begin{verbatim}
     % inspect myndf
\end{verbatim}
\normalsize
will inspect the NDF called myndf.  The command \htmlref{PICIN}{PICIN} will
show the last DATA picture and its associated NDF.

\subsection{Unable to Store a Picture in the Graphics Database}
You may receive an error message, which says failed to store
such-and-such picture in the \htmlref{graphics database}{se:agitate}.
For some reason the database was corrupted due to reasons external to
{\footnotesize KAPPA}.  Don't worry, usually your plot will have appeared, and
to fix the problem run \htmlref{GDCLEAR}{GDCLEAR} or
\htmlref{IDCLEAR}{IDCLEAR} or delete the database file
({\tt{\$AGI\_USER/agi\_}}{\it $<$node$>$}{\tt.sdf}, where you substitute your
system's node name for {\it $<$node$>$}).  You will need to redraw the
last plot if you still require it, say for interaction.

\subsection{Line Graphics are Invisible on an Image Display}
The reason for invisible line graphics on your image display is
because it is drawn in black or a dark grey.  Most likely is that some
person has been using other software on your image display or that is
has been reset.  \htmlref{PALDEF}{PALDEF} will set up the default
colours for the palette, and so most line graphics will then appear in
white.  Alternatively,

\small
\begin{verbatim}
     % palentry 1 white
\end{verbatim}
\normalsize
will normally suffice.

\subsection{Error Obtaining a Locator to a Slice of an HDS array}
If the above error appears from DAT\_SLICE and you are (re)prompted
for an \NDFref{NDF}, the most likely cause is that you have asked an
\htmlref{IMAGE}{ap:IMAGE-format} application to process an
\htmlref{NDF section}{se:ndfsect}.  Use \htmlref{NDFCOPY}{NDFCOPY} to
make a subset before running the application in question, or process
the whole NDF.

\subsection{Badly placed ()'s}
This means that you have forgotten to `escape' parentheses, probably when
defining an NDF section.  Try inserting a backslash before each
parenthesis.

\small
\begin{verbatim}
     % stats myndf\(100:200,\)
\end{verbatim}
\normalsize

\subsection{Attempt to use 'positional' parameter value ({\tt{x}}) in an 
unallocated position}
Check the usage of the application you are running.  One way of
adding positional parameters unintentionally, is to forget to escape
the {\tt{"}} from the shell when supplying a string with spaces or
wildcards.  For example, this error would arise if we entered

\small
\begin{verbatim}
     % settitle myndf "A title"
\end{verbatim}
\normalsize
instead of say

\small
\begin{verbatim}
     % settitle myndf '"A title"'
\end{verbatim}
\normalsize
which protects all special characters between the single quotes.

\subsection{The choice {\tt x} is not in the menu.  The options
are\ldots}
You have either made an incorrect selection, or you have forgotten
to escape a metacharacter.  For the former, you can select a new
value from the list of valid values presented in the error message.
For the latter, part of another value is being
interpreted as a positional value for the parameter the task is
complaining about.

\small
\begin{verbatim}
     % linplot $KAPPA_DIR/spectrum pltitl="Red-giant plot"
     !! The choice plot is not in the menu.  The options are
     !     Data,Quality,Error,Variance.
     !  Invalid selection for parameter COMP.
\end{verbatim}
\normalsize
Here it thinks that {\tt plot} is a positional value.  Escape
the {\tt{"}} to cure the problem.

\small
\begin{verbatim}
     % linplot $KAPPA_DIR/spectrum pltitl='"Red-giant plot"'
\end{verbatim}
\normalsize

\subsection{\xlabel{se_fitsunixtape}``I've Got This FITS Tape''
\label{se:fitsunixtape}}

Certain combinations of magnetic tape produced on one model of tape
drive but read on a different model seem to generate parity errors
that are detected by the MAG\_ library that \htmlref{FITSIN}{FITSIN}
uses.  However, this doesn't mean that you won't be able to read your
FITS tape.  The UNIX tape-reading commands seem less sensitive to
these parity errors.

Thus you should first attempt to convert the inaccessible FITS files
on tape to disc files using the UNIX {\bf dd} command, and then use
the \htmlref{FITSDIN}{FITSDIN} application to generate the output NDF
or foreign format.  For example to convert a FITS file from device
{\tt /dev/nrst0} to an NDF called ndfname, you might enter

\small
\begin{verbatim}
     % dd if=/dev/nrst0 ibs=2880 of=file.fits
     % fitsdin files=file.fits out=ndfname
     % rm file.fits
\end{verbatim}
\normalsize
where {\tt file.fits} is the temporary disc-FITS file.  The 2880 is
the length of a FITS record in bytes.   Repeated {\bf dd} commands to
a no-rewind tape device (those with the {\tt n} prefix on OSF/1 and the
{\tt n} suffix on Solaris) will copy successive files.  To skip over
files or rewind the tape, use the {\bf mt} command.  For example,

\small
\begin{verbatim}
     % mt -f /dev/rmt/1n fsf 3
           :       :       :
     % mt -f /dev/rmt/1n asf 4
\end{verbatim}
\normalsize
moves the tape on device {\tt /dev/rmt/1n} forward three files,
then moves to the fourth file,

\small
\begin{verbatim}
     % mt bsf 2
\end{verbatim}
\normalsize
moves back two files on the default tape drive (defined by the
environment variable TAPE), and

\small
\begin{verbatim}
     % mt -f /dev/nrmt0h rewind
\end{verbatim}
\normalsize
rewinds to the start of the tape on device {\tt /dev/nrmt0h}.
Thus it is possible to write a script for extracting and converting a
series of files including ranges, just like FITSIN does.  

If the above approach fails, try another tape drive.

\subsection{\xlabel{se_probfitsin}FITSIN does not Recognise my FITS Tape
\xlabel{se_probfitsin}}

If you attempt to read a FITS magnetic tape with
\htmlref{FITSIN}{FITSIN}, you might receive an error like this

\vspace*{-\smallskipamount}
\small
\begin{verbatim}
     % fitsin
     % MT - Tape deck /@/dev/nrmt1h/ > /dev/nrmt3l
     !! Object '/DEV/NRMT3L' not found.
     !  DAT_FIND: Error finding a named component in an HDS structure.
     !  /dev/nrmt3l: MAG__UNKDV, Unable to locate device in DEVDATASET
\end{verbatim}
\vspace*{-\medskipamount}
\normalsize
when you enter the device name.  The magnetic-tape system uses an
\HDSref\ file called the device dataset (DEVDATASET) to store the
position of the tape between invocations of Starlink applications.
\goodbreak

When FITSIN is given a name, the magnetic-tape system validates the
name to check that it is a known device.  There should be a
{\tt devdataset.sdf} file (within {\tt /star/etc} at Starlink sites)
containing a list of at least all the available drives at your site.
What FITSIN is complaining about above, is that the device you have
given is not included in the DEVDATASET file.  Now this might be
because you mistyped the device name, or that the particular device is
not accessible on the particular machine, or because your computer manager
has not maintained the DEVDATASET when a drive was added.  You can look
at the contents of the DEVDATASET with this command.

\small
\begin{verbatim}
     % hdstrace /star/etc/devdataset
\end{verbatim}
\normalsize


Oh and one other point: make sure the tape is loaded in the drive.
Yes this mistake has happened (not mentioning any names) and it is
very hard to diagnose remotely.

\subsection{\xlabel{se_probweird}It Used to Work\ldots and Weird
Errors\label{se:probweird}}

There is a class of error that arises when an \HDSref\ file is corrupted.
The specific message will depend on the file concerned and where
in the file the corruption occurred.  The most likely reason for file
corruption is breaking into a task at the wrong moment, or trying
to write to a file at the same time. 

If you want to process simultaneously from different sessions---say
one interactive and another in batch---it is wise to redefine the
environment variables \$ADAM\_USER, and \$AGI\_USER if you want
graphics on the same machine. The environment variables should point
to a separate existing directory for each additional session.  This
will keep the \htmlref{global}{se:parglobals} and
\htmlref{application parameters}{se:defaults}, and the
\htmlref{graphics database}{se:agitate} separate for each session.

The way to look for corrupted HDS files is trace them.
Assuming that \$ADAM\_USER and \$AGI\_USER are defined,

\small
\begin{verbatim}
     % hdstrace $ADAM_USER/GLOBALS full
     % hdstrace $ADAM_USER/ardmask full
     % hdstrace $AGI_USER/agi_cacvad full
\end{verbatim}
\normalsize
traces the {\tt GLOBALS} file, the application you were running when the
weird error occurred (here \htmlref{ARDMASK}{ARDMASK}), and the
graphics database for machine {\tt cacvad}.  Once you have identified
the problem file, delete it.  If that proves to be the globals file,
you might want to retain the output from \HDSTRACEref , so that you can
restore their former values.  Deleting the graphics database is
something you should do regularly, so that's not a problem.

If you have been running {\footnotesize KAPPA} from \ICLref , you will need
to check of the integrity of the monolith parameter file, instead
the individual parameter file.  It will be one of these depending
on the type of task that failed: graphics, NDF components, or
the rest (mostly image processing) corresponding to these three
monolith interface files.

\small
\begin{verbatim}
     % hdstrace $ADAM_USER/kapview_mon full
     % hdstrace $ADAM_USER/ndfpack_mon full
     % hdstrace $ADAM_USER/kappa_mon full
\end{verbatim}
\normalsize


If that doesn't cure the problem, send me {\tt mjc@star.rl.ac.uk}
or the Starlink Software Librarian {\tt ussc@star.rl.ac.uk} a log of
the session demonstrating the problem, and we shall endeavour to
interpret it for you, and find out what's going wrong.

\newpage
\section{\xlabel{se_custom}Custom KAPPA\label{se:custom}}

\subsection{\xlabel{se_custom}Tasks\label{se:customtasks}}

{\footnotesize KAPPA} applications can be modified to suit your particular
requirements.  Since this document is not a programmer's guide,
instructions are not given here.  Programmers should contact the
author for details until a new Programmer's Guide appears to replace
the old \xref{SUN/101}{sun101}{}, which {\em was\/} a good summary of
Starlink infrastructure libraries and programming.

All the source files can be found in {\tt /star/kappa/*.tar} on
Starlink machines.  The {\tt /star} path may be different outside of
Starlink, so check with your computer manager.  There is a separate
tar file for each {\footnotesize KAPPA} subroutine library (with a {\tt
\_sub} suffix) and the interface files, with obvious names.  The
remaining files: the monolith routines, link scripts, include files,
the help source, shell scripts, {\footnotesize ICL} procedures, and test data
are in {\tt kappa\_source.tar}.  There is also a Starlink standard
{\tt makefile} and {\tt mk} script.

Here is a worked example.  Suppose that you have
\htmlref{\_REAL}{ap:HDStypes}-type datasets for which you want to
compute statistics including the skewness and kurtosis.  One way is to
modify STATS.  First to save typing define environment variables, say
STAR and KAP to point to where the Starlink software and {\footnotesize
KAPPA} source is stored.  Next we extract the source files to change.

\small
\begin{verbatim}
     % setenv STAR /star
     % setenv KAP /star/kappa
     % tar xf $KAP/kappa_sub.tar stats.
     % tar xf $KAP/kappa_ifls.tar stats.ifl
     % tar xf $KAP/kapgen_sub.tar kpg1_statr.f
     % tar xf $KAP/kapgen_sub.tar kpg1_stdsr.f
     % tar xf $KAP/kappa_source.tar kappa_link_adam
\end{verbatim}
\normalsize

We modify {\tt kpg1\_statr.f} to compute the additional statistics;
{\tt kpg1\_stdsr.f} to list the statistics; {\tt stats.f} to update the
documentation, to use the revised argument lists of the subroutines,
and to output the new statistics to parameters; and {\tt stats.ifl}
to add the output parameters.  {\tt kappa\_link\_adam} need not be
modified, but it is needed during linking.

Next some soft links to include files need to be made.

\small
\begin{verbatim}
     % star_dev
     % ndf_dev
     % prm_dev
     % par_dev
\end{verbatim}
\normalsize
For some other application and subroutines, you can find what is
needed by trying to compile them and see which include files the
compiler cannot locate.  You then invoke the appropriate package
definitions: {\em pkg}{\tt \_dev}, where {\em pkg\/} is the three-letter
package abbreviation.  Now compile the modified code.  This is for
OSF/1

\small
\begin{verbatim}
     % f77 -O -c -nowarn stats.f kpg1_statr.f kpg1_stdsr.f
\end{verbatim}
\normalsize
and this is for Solaris.

\small
\begin{verbatim}
     % f77 -O -PIC -c -w stats.f kpg1_statr.f kpg1_stdsr.f
\end{verbatim}
\normalsize
The {\tt -nowarn} and {\tt -w} prevent warning messages appearing.

Now link the task to produce a new {\tt stats} executable.

\small
\begin{verbatim}
     % alink stats.o -o stats kpg1_statr.o kpg1_stdsr.o \
     $KAPPA_DIR/libkapgen.a -L$STAR/lib `./kappa_link_adam`
\end{verbatim}
\normalsize
For other tasks you may need more KAPPA object libraries in addition
to {\tt libkapgen.a}.  There is a full list in {\tt \$KAP/makefile}
(look for {\tt OBJECT\_LIBRARIES}).  If you add further libraries, add
them in the order given within the makefile.

If you want to use {\footnotesize KAPPA} subroutines for your own application
here are words of warning: {\em the code may undergo alterations of
subroutine name or argument lists, and those without a {\em pkg\_\/}
prefix will either be replaced or renamed.} Therefore, you should copy
the modules you need.

\subsection{\xlabel{se_custom}Parameters\label{se:custompar}}

If you don't like {\footnotesize KAPPA}'s parameter defaults, or its choice
of which parameters get prompted for and which get defaulted, then you
can change them.  Extract the interface file from \linebreak
{\tt /star/kappa/kappa\_ifls.tar} to your work directory and make the
required modifications, and then recompile it.  See
\xref{SUN/115}{sun115}{} on the meanings and possible values of the
fieldnames, and how to recompile the interface file.  If you use
\ICLref , you'll need to modify a monolith interface file: {\tt
\$KAPPA\_DIR/kappa\_mon.ifl}, {\tt kapview\_mon.ifl} or {\tt
ndfpack\_mon.ifl}.  Finally, you will need to specify a search path
that includes the directory containing your modified interface file.

\small
\begin{verbatim}
     % setenv ADAM_IFL /home/scratch/dro/ifls:/usr/local/kappa
\end{verbatim}
\normalsize
This asks Starlink programmes to look in {\tt /home/scratch/dro/ifls}
to find the interface file, and if there isn't one to look in {\tt
/usr/local/kappa}.  If the interface file search is unsuccessful, the
directory containing the executable is assumed.  Thus if you've not
created your own interface file for a task, you'll get the released
version.  Of course, once you have done this, the documentation in
\latexelsehtml{Appendix~\ref{ap:full}}{the \htmlref{application
specifications}{ap:full}} will no longer be correct.

\subsection{\xlabel{se_custom}Commands\label{se:customcom}}

There is an easier method of tailoring {\footnotesize KAPPA} to your
requirements.  If you frequently use certain commands, especially
those with a long list of keywords and fixed values, you can define
some C-shell aliases or \ICLref\ symbols for the commands.  Like
the shell's {\tt \$HOME/.login}, {\footnotesize ICL} has a {\em login file}.
(See \xref{``ICL for Unix'' Appendix in SUN/144}{sun144}{icl_for_unix}, or 
\xref{SSN/64}{ssn64}{} for details.)  If you add symbols to this file,
each time you activate {\footnotesize ICL} these abbreviations will be
available to you without further typing.  What you should do is to
create a {\tt login.icl} in a convenient directory, and assign the
environment variable ICL\_LOGIN to that directory in your {\tt
\$HOME/.login} file.

It is possible to have several {\footnotesize ICL} login files, each for
different work in different directories.  Now to abbreviate a command
you put a DEFSTRING entry into the {\footnotesize ICL} login file.  For
example,

\small
\begin{verbatim}
     DEFSTRING MYC{ON} TURBOCONT CLEAR=F PENROT MODE=AU NCONT=7
\end{verbatim}
\normalsize

defines {\tt MYC} or {\tt MYCO} or {\tt MYCON} to run
\htmlref{TURBOCONT}{TURBOCONT} without clearing the screen, with pen
rotation and seven equally spaced contour heights.  The symbols are not
limited to {\footnotesize KAPPA}.  Indeed they can include shorthands for
shell commands.  For example,

\small
\begin{verbatim}
     DEFSTRING DA ls -al 
\end{verbatim}
\normalsize
would make DA produce a directory listing of all files with sizes and
modification dates.

You can check what the current login files are as follows.

\small
\begin{verbatim}
     % printenv | grep ICL_LOGIN
\end{verbatim}
\normalsize

For shell usage similar definitions can be made with aliases. For
example, 
 
\small
\begin{verbatim}
     % alias mycon turbocont clear=f penrot mode=au ncont=7
\end{verbatim}
\normalsize
 
is the equivalent of the DEFSTRING above, except that in keeping with
UNIX tradition the command is in lowercase, and the alias cannot be
abbreviated.

\section{Acknowledgements}

Several people have contributed complete {\footnotesize KAPPA} programmes, or
have upgraded earlier versions, or have written original code (which
eventually became included in {\footnotesize KAPPA} after reworking).
Particularly noteworthy contributions have been made by David Berry,
who wrote or improved the Fourier-transform-based commands, and
provided several other applications, converted some to use the NDF
library, and made improvements to existing routines.  Rodney
Warren-Smith converted several of the old IMAGE applications to use
the NDF\_ library, and supplied several other programmes, especially
ones that now form the basis of NDFPACK. Other contributions have come
from Steven Beard, Wei Gong, Rhys Morris, Jo Murray, Grant Privett,
and Richard Saxton.  The original {\footnotesize KAPPA} was derived from Mark
McCaughrean's {\footnotesize RAPI2D} and Ken Hartley's {\footnotesize ASPIC Kernel},
though little remains.  Thanks also to Rodney Warren-Smith for
permitting this document to include a few modified pages of his SUN/33
on NDF sections and co-ordinate systems; and to many useful
suggestions from users and programmers over the years including Chris
Clayton, Peter Draper, Jim Emerson, Horst Meyerdierks, Andy Scott, and
Martin Shaw.  Mike Lawden helped produce the quick-reference card.

\newpage
\appendix
\begin{small}
\section{\xlabel{ap_summary}An Alphabetical Summary of KAPPA Commands
\label{ap:summary}}
\begin{htmlonly}
\begin{description}
\end{htmlonly}

\menuitem{ADD}{
 Adds two NDF data structures.}
\menuitem{APERADD}{
 Derives statistics of pixels within a  specified circle of a 2-d data array.}
\menuitem{ARDGEN}{
 Creates a text file describing selected regions of an image.}
\menuitem{ARDMASK}{
 Uses an ARD file to set some pixels of an NDF to be bad.}
\menuitem{AXCONV}{
 Expands spaced axes in an NDF into the primitive form.}
\menuitem{AXLABEL}{
 Sets a new label value for an axis within an NDF data structure.}
\menuitem{AXUNITS}{
 Sets a new units value for an axis within an NDF data structure.}
\menuitem{BLOCK}{
 Smooths a 1- or 2-dimensional image using a square or rectangular box filter.}
\menuitem{CADD}{
 Adds a scalar to an NDF data structure.}
\menuitem{CALC}{
 Evaluates a mathematical expression.}
\menuitem{CALPOL}{
 Calculates polarisation parameters.}
\menuitem{CDIV}{
 Divides an NDF by a scalar.}
\menuitem{CENTROID}{
 Finds the centroids of star-like features in an NDF.}
\menuitem{CHPIX}{
 Replaces the values of selected pixels in an NDF.}
\menuitem{CMULT}{
 Multiplies an NDF by a scalar.}
\menuitem{COMPADD}{
 Reduces the size of an NDF by adding values in rectangular boxes.}
\menuitem{COMPAVE}{
 Reduces the size of an NDF by averaging values in rectangular boxes.}
\menuitem{COMPICK}{
 Reduces the size of an NDF by picking equally spaced pixels.}
\menuitem{CONTOUR}{
 Contours a 2-d NDF.}
\menuitem{CONTOVER}{
 Contours a 2-d NDF overlaid on an image displayed previously.}
\menuitem{CONVOLVE}{
 Convolves a pair of 1- or 2-dimensional NDFs together.}
\menuitem{CREFRAME}{
 Generates a test 2-d data array from a  selection of several types.}
\menuitem{CRELUT}{
 Creates or manipulates an image-display lookup table using a palette.}
\menuitem{CSUB}{
 Subtracts a scalar from an NDF data structure.}
\menuitem{CURSOR}{
 Reports the co-ordinates of points selected using the cursor.}
\menuitem{DISPLAY}{
 Displays a 1-d or 2-d NDF.}
\menuitem{DIV}{
 Divides one NDF data structure by another.}
\menuitem{DRAWSIG}{
 Draws ${\pm}n$ standard-deviation lines on a line plot.}
\menuitem{ELPROF}{
 Creates a radial or azimuthal profile of a 2-dimensional image.}
\menuitem{ERASE}{
 Erases an HDS object.}
\menuitem{ERRCLIP}{
 Removes pixels with large errors from an NDF.}
\menuitem{EXP10}{
 Takes the base-10 exponential of each pixel of  a data array.}
\menuitem{EXPE}{
 Takes the exponential of each pixel of a data  array (base e).}
\menuitem{EXPON}{
 Takes the exponential of each pixel   of a data array.}
\menuitem{FFCLEAN}{
 Removes defects from a substantially flat 1- or 2-dimensional NDF.}
\menuitem{FILLBAD}{
 Removes regions of bad values from a 2-dimensional NDF.}
\menuitem{FITSDIN}{
 Reads a FITS disc file composed of simple, group or table objects.}
\menuitem{FITSEDIT}{
 Edits the FITS extension of an NDF.}
\menuitem{FITSEXIST}{
 Inquires whether or not a keyword exists in a FITS extension.}
\menuitem{FITSEXP}{
 Exports NDF-extension information into an NDF FITS extension.}
\menuitem{FITSHEAD}{
 Lists the headers of FITS files.}
\menuitem{FITSIMP}{
 Imports FITS information into an NDF extension.}
\menuitem{FITSIN}{
 Reads a FITS tape composed of simple, group or table files.}
\menuitem{FITSLIST}{
 Lists the FITS extension of an NDF.}
\menuitem{FITSMOD}{
 Edits an NDF FITS extension via a text file or parameters.}
\menuitem{FITSTEXT}{
 Creates an NDF FITS extension from a text file.}
\menuitem{FITSURFACE}{
 Fits a polynomial surface to 2-dimensional data array.}
\menuitem{FITSVAL}{
 Reports the value of a keyword in the FITS extension.}
\menuitem{FITSWRITE}{
 Writes a new keyword to the FITS extension.}
\menuitem{FLIP}{
 Reverses an NDF's pixels along a specified dimension.}
\menuitem{FOURIER}{
 Performs forward and inverse Fourier transforms of 1- or 2-dimensional NDFs.}
\menuitem{GAUSMOOTH}{
 Smooths a 1- or 2-dimensional image using a Gaussian filter.}
\menuitem{GDCLEAR}{
 Clears a graphics device and purges its database entries.}
\menuitem{GDNAMES}{
 Shows which graphics devices are available.}
\menuitem{GDSET}{
 Selects a current graphics device.}
\menuitem{GDSTATE}{
 Shows the current status of a graphics device.}
\menuitem{GLITCH}{
 Replaces bad pixels in a 2-d data array with  the local median.}
\menuitem{GLOBALS}{
 Displays the values of the KAPPA global parameters.}
\menuitem{GREYPLOT}{
 Produces a greyscale plot of a 1-d or 2-d NDF.}
\menuitem{HISCOM}{
 Adds commentary to the history of an NDF.}
\menuitem{HISLIST}{
 Lists NDF history records.}
\menuitem{HISSET}{
 Sets the NDF history update mode.}
\menuitem{HISTAT}{
 Computes ordered statistics for an NDF's pixels using an histogram.}
\menuitem{HISTEQ}{
 Performs an histogram equalisation on an NDF.}
\menuitem{HISTOGRAM}{
 Computes an histogram of an NDF's values.}
\menuitem{IDCLEAR}{
 Clears an image display and purges its database entries.}
\menuitem{IDINVISIBLE}{
 Makes memory planes of an image-display device invisible.}
\menuitem{IDPAZO}{
 Pans and zooms an image-display device.}
\menuitem{IDSET}{
 Selects a current image-display device.}
\menuitem{IDSTATE}{
 Shows the current status of an image display.}
\menuitem{INSPECT}{
 Inspects a 2-d NDF in a variety of ways.}
\menuitem{KAPHELP}{
 Gives help about KAPPA.}
\menuitem{KSTEST}{
 Compares data sets using the Kolmogorov-Smirnov test.}
\menuitem{LAPLACE}{
 Performs a Laplacian convolution as an  edge detector in a 2-d data array.}
\menuitem{LINPLOT}{
 Draws a line plot of a 1-d NDF's data values against their axis co-ordinates.}
\menuitem{LOG10}{
 Takes the base-10 logarithm of each pixel of a  data array.}
\menuitem{LOGAR}{
 Takes the logarithm of each pixel of a data  array (specified base).}
\menuitem{LOGE}{
 Takes the natural logarithm of each pixel  of a data array.}
\menuitem{LOOK}{
 Outputs the values of a sub-array of a 2-d  data array to the screen or a text file.}
\menuitem{LUCY}{
 Performs a Richardson-Lucy deconvolution of a 1- or 2-dimensional array.}
\menuitem{LUTABLE}{
 Manipulates an image-display colour table.}
\menuitem{LUTBGYRW}{
 Loads the {\it BGYRW} lookup table.}
\menuitem{LUTCOL}{
 Loads the standard colour lookup table.}
\menuitem{LUTCONT}{
 Loads a lookup table to give the display the appearance of a contour plot.}
\menuitem{LUTFC}{
 Loads the standard false-colour lookup table.}
\menuitem{LUTFLIP}{
 Flips the colour table of an image-display device.}
\menuitem{LUTGREY}{
 Loads the standard greyscale lookup table.}
\menuitem{LUTHEAT}{
 Loads the {\it heat} lookup table.}
\menuitem{LUTHILITE}{
 Highlights a colour table of an image-display device.}
\menuitem{LUTIKON}{
 Loads the default {\it Ikon} lookup table.}
\menuitem{LUTNEG}{
 Loads the standard negative greyscale lookup table.}
\menuitem{LUTRAMPS}{
 Loads the coloured-ramps lookup table.}
\menuitem{LUTREAD}{
 Loads an image-display lookup table from an NDF.}
\menuitem{LUTROT}{
 Rotates the colour table of an image-display device.}
\menuitem{LUTSAVE}{
 Saves the current colour table of an image-display device in an NDF.}
\menuitem{LUTSPEC}{
 Loads a spectrum-like lookup table.}
\menuitem{LUTTWEAK}{
 Tweaks a colour table of an image-display device.}
\menuitem{LUTVIEW}{
 Draws a colour-table key.}
\menuitem{LUTZEBRA}{
 Loads a pseudo-contour lookup table.}
\menuitem{MAKESURFACE}{
 Creates a 2-dimensional NDF from the coefficients of a polynomial surface.}
\menuitem{MANIC}{
 Converts all or part of a data array from one  dimensionality to another.}
\menuitem{MATHS}{
 Evaluates mathematical expressions applied to NDF data structures.}
\menuitem{MEDIAN}{
 Smooths a 2-dimensional data array using a weighted median filter.}
\menuitem{MEM2D}{
 Performs a Maximum-Entropy deconvolution of a 2-dimensional NDF.}
\menuitem{MLINPLOT}{
 Draws a multi-line plot of a 2-d NDF's data values against their axis co-ordinates.}
\menuitem{MOSAIC}{
 Merges several non-congruent  2-d data arrays into one output data array.}
\menuitem{MSTATS}{
 Does cumulative statistics on  a 2-d sub-array over a sequence of data arrays.}
\menuitem{MULT}{
 Multiplies two NDF data structures.}
\menuitem{NATIVE}{
 Converts an HDS object to native machine data representation.}
\menuitem{NDFCOPY}{
 Copies an NDF (or NDF section) to a new location.}
\menuitem{NDFTRACE}{
 Displays the attributes of an NDF data structure.}
\menuitem{NOGLOBALS}{
 Resets the {\footnotesize KAPPA} global parameters.}
\menuitem{NOMAGIC}{
 Replaces all occurrences of magic value pixels in an NDF array with a new value.}
\menuitem{NORMALIZE}{
 Normalises one NDF to a similar NDF by calculating a scale factor and zero-point difference.}
\menuitem{NUMB}{
 Counts the number of elements of an NDF with values or absolute values above or below a threshold.}
\menuitem{OUTSET}{
 Sets pixels outside a specified  circle in a 2-d data array to a specified value.}
\menuitem{OVCLEAR}{
 Clears an image-display overlay.}
\menuitem{OVSET}{
 Selects a current image-display overlay.}
\menuitem{PALDEF}{
 Loads the default palette to a colour table.}
\menuitem{PALENTRY}{
 Enters a colour into an image display's palette.}
\menuitem{PALREAD}{
 Fills the palette of a colour table from an NDF.}
\menuitem{PALSAVE}{
 Saves the current palette of a colour table to an NDF.}
\menuitem{PARGET}{
 Obtains the value or values of an application parameter.}
\menuitem{PASTE}{
 Pastes a series of NDFs upon each other.}
\menuitem{PICBASE}{
 Selects the BASE picture from the graphics database.}
\menuitem{PICCUR}{
 Uses a cursor to select the current picture and to report the co-ordinates of points.}
\menuitem{PICDATA}{
 Selects the last DATA picture from the graphics database.}
\menuitem{PICDEF}{
 Defines a new graphics-database FRAME picture or an array of FRAME pictures.}
\menuitem{PICEMPTY}{
 Finds the first empty FRAME picture in the graphics database.}
\menuitem{PICENTIRE}{
 Finds the first unobscured and unobscuring FRAME picture in the graphics database.}
\menuitem{PICFRAME}{
 Selects the last FRAME picture from the graphics database.}
\menuitem{PICGRID}{
 Creates an array of FRAME pictures.}
\menuitem{PICIN}{
 Finds the attributes of a picture interior to the current picture.}
\menuitem{PICLABEL}{
 Labels the current graphics-database picture.}
\menuitem{PICLAST}{
 Selects the last picture from the graphics database.}
\menuitem{PICLIST}{
 Lists the pictures in the graphics database for a device.}
\menuitem{PICSEL}{
 Selects a graphics-database picture by its label.}
\menuitem{PICTRANS}{
 Transforms co-ordinates between the current and BASE pictures.}
\menuitem{PICVIS}{
 Finds the first unobscured FRAME picture in the graphics database.}
\menuitem{PICXY}{
 Creates a new FRAME picture defined by co-ordinate bounds.}
\menuitem{PIXDUPE}{
 Expands an NDF by pixel duplication.}
\menuitem{POW}{
 Takes the specified power of each  pixel of a data array.}
\menuitem{PSF}{
 Determines the parameters of a model star profile by fitting star images in a two-dimensional NDF.}
\menuitem{QUILT}{
 Generates a mosaic from equally sized 2-d data arrays, optionally specified
 from an ASCII file.}
\menuitem{RIFT}{
 Adds a scalar to a section of an NDF data structure to correct rift-valley defects.}
\menuitem{ROTATE}{
 Rotates a 2-dimensional NDF about its centre through any angle.}
\menuitem{SEGMENT}{
 Copies polygonal segments from one NDF to another.}
\menuitem{SETAXIS}{
 Sets values for an axis array component within an NDF data structure.}
\menuitem{SETBAD}{
 Sets new bad-pixel flag values for an NDF.}
\menuitem{SETBB}{
 Sets a new value for the quality bad-bits mask of an NDF.}
\menuitem{SETBOUND}{
 Sets new bounds for an NDF.}
\menuitem{SETEXT}{
 Manipulates the contents of a specified NDF extension.}
\menuitem{SETLABEL}{
 Sets a new label for an NDF data structure.}
\menuitem{SETMAGIC}{
 Replaces all occurrences of a given value in an NDF array with the bad value.}
\menuitem{SETNORM}{
 Sets a new value for one or all of an NDF's axis-normalisation flags.}
\menuitem{SETORIGIN}{
 Sets a new pixel origin for an NDF.}
\menuitem{SETSKY}{
 Makes an IRAS astrometry extension.}
\menuitem{SETTITLE}{
 Sets a new title for an NDF data structure.}
\menuitem{SETTYPE}{
 Sets a new numeric type for the data and variance components of an NDF.}
\menuitem{SETUNITS}{
 Sets a new units value for an NDF data structure.}
\menuitem{SETVAR}{
 Sets new values for the variance component of an NDF data structure.}
\menuitem{SHADOW}{
 Enhances edges in a 2-dimensional NDF using a shadow effect.}
\menuitem{SLIDE}{
 Realigns a 2-d data array via an $x$-$y$ shift.}
\menuitem{SNAPSHOT}{
 Dumps an image-display memory to a graphics hardcopy and optionally to an NDF.}
\menuitem{SQORST}{
 Squashes or stretches a 2-d  data array in either or both axes.}
\menuitem{STATS}{
 Computes simple statistics for an NDF's pixels.}
\menuitem{SUB}{
 Subtracts one NDF data structure from another.}
\menuitem{SUBSTITUTE}{
 Replaces all occurrences of a given value in an NDF array with another value.}
\menuitem{SURFIT}{
 Fits a polynomial or bi-cubic spline surface to 2-dimensional data array.}
\menuitem{THRESH}{
 Edits an NDF such that array values below and above two thresholds take constant values.}
\menuitem{TRANDAT}{
 Converts free-format text data into an NDF.}
\menuitem{TRANINVERT}{
 Inverts a transformation.}
\menuitem{TRANJOIN}{
 Joins two transformations.}
\menuitem{TRANMAKE}{
 Makes a transformation structure given its co-ordinate mappings.}
\menuitem{TRANSFORMER}{
 Applies a transformation to an NDF.}
\menuitem{TRANTRACE}{
 Lists the contents of a transformation structure.}
\menuitem{TRIG}{
 Performs a trigonometric  transformation on a data array.}
\menuitem{TURBOCONT}{
 Contours a 2-d NDF quickly.}
\menuitem{VECPLOT}{
 Plots a 2-dimensional vector map.}
\menuitem{WIENER}{
 Applies a Wiener filter to a 1- or 2- dimensional array.}
\menuitem{ZAPLIN}{
 Replaces regions in a 2-d NDF by bad values or by linear interpolation.}
\begin{htmlonly}
\end{description}
\end{htmlonly}

\newpage
\section{\xlabel{ap_classified}Classified KAPPA commands
\label{ap:classified}}

{\footnotesize KAPPA} applications may be classified in terms of their
functions as follows.

{\large
\begin{center}
{\bf DATA IMPORT \& EXPORT}
\end{center}
}

\begin{description}
%
\label{cl:datagen}
\item {\large\bf Image generation and input}

\begin{description}
\classitem{CREFRAME}
 Generates a test 2-d data array from a selection of several types.
\classitem{FITSDIN}
 Reads a FITS disc file composed of simple, group or table objects.
\classitem{FITSHEAD}
 Lists the headers of FITS files.
\classitem{FITSIMP}
 Imports FITS information into an NDF extension.
\classitem{FITSIN}
 Reads a FITS tape composed of simple, group or table files.
\classitem{MATHS}
 Evaluates mathematical expressions applied to NDF data structures.
\classitem{TRANDAT}
 Converts free-format data into an NDF.
\end {description}
\vspace*{0.7ex}
%
\item {\large\bf Preparation for output}
\begin{description}
\classitem{FITSEDIT}
 Edits the FITS extension of an NDF.
\classitem{FITSEXP}
 Exports NDF-extension information into an NDF FITS extension.
\classitem{FITSMOD}
 Edits an NDF FITS extension via a text file or parameters.
\classitem{FITSTEXT}
 Creates an NDF FITS extension from a text file.
\classitem{FITSWRITE}
 Writes a new keyword to the FITS extension.
\end {description}
\end {description}

\vspace*{-1ex}

{\large
\begin{center}
{\bf DATA DISPLAY}
\end{center}
}

\begin{description}
%
\item {\large\bf Detail enhancement}
\begin{description}
\classitem{HISTEQ}
 Performs an histogram equalisation on an NDF.
\classitem{LAPLACE}
 Performs a Laplacian convolution as an edge detector in a 2-d data array.
\classitem{SHADOW}
 Enhances edges in a 2-dimensional NDF using a shadow effect.
\classitem{THRESH}
 Edits an NDF such that array values below and above two thresholds take
 constant values.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Device selection}
\begin{description}
\classitem{GDNAMES}
 Shows which graphics devices are available.
\classitem{GDSET}
 Selects a current graphics device.
\classitem{IDSET}
 Selects a current image-display device.
\classitem{OVSET}
 Selects a current image-display overlay.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Display control}
\begin{description}
\classitem{CURSOR}
 Reports the co-ordinates of points selected using the cursor.
\classitem{GDCLEAR}
 Clears a graphics device and purges its database entries.
\classitem{GDSTATE}
 Shows the current status of a graphics device.
\classitem{IDCLEAR}
 Clears an image display and purges its database entries.
\classitem{IDINVISIBLE}
 Makes memory planes of an image-display device invisible.
\classitem{IDPAZO}
 Pans and zooms an image-display device.
\classitem{IDSTATE}
 Shows the current status of an image display.
\classitem{OVCLEAR}
 Clears an image-display overlay.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Graphics Database}
\begin{description}
\classitem{PICBASE}
 Selects the BASE picture from the graphics database.
\classitem{PICCUR}
 Uses a cursor to select the current picture and to report the co-ordinates of points.
\classitem{PICDATA}
 Selects the last DATA picture from the graphics database.
\classitem{PICDEF}
 Defines a new graphics-database FRAME picture or an array of FRAME pictures.
\classitem{PICEMPTY}
 Finds the first empty FRAME picture in the graphics database.
\classitem{PICENTIRE}
 Finds the first unobscured and unobscuring FRAME picture in the graphics database.
\classitem{PICFRAME}
 Selects the last FRAME picture from the graphics database.
\classitem{PICGRID}
 Creates an array of FRAME pictures.
\classitem{PICIN}
 Finds the attributes of a picture interior to the current picture.
\classitem{PICLABEL}
 Labels the current graphics-database picture.
\classitem{PICLAST}
 Selects the last picture from the graphics database.
\classitem{PICLIST}
 Lists the pictures in the graphics database for a device.
\classitem{PICSEL}
 Selects a graphics-database picture by its label.
\classitem{PICTRANS}
 Transforms co-ordinates between the current and BASE pictures.
\classitem{PICVIS}
 Finds the first unobscured FRAME picture in the graphics database.
\classitem{PICXY}
 Creates a new picture defined by co-ordinate bounds.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Lookup/Colour tables}
\begin{description}
\classitem{CRELUT}
 Creates or manipulates an image-display lookup table using a palette.
\classitem{LUTABLE}
 Manipulates an image-display colour table.
\classitem{LUTBGYRW}
 Loads the {\it BGYRW\/} lookup table.
\classitem{LUTCOL}
 Loads the standard colour lookup table.
\classitem{LUTCONT}
 Loads a lookup table to give the display the appearance of a contour plot.
\classitem{LUTFC}
 Loads the standard false-colour lookup table.
\classitem{LUTFLIP}
 Flips the colour table of an image-display device.
\classitem{LUTGREY}
 Loads the standard greyscale lookup table.
\classitem{LUTHEAT}
 Loads the {\it heat\/} lookup table.
\classitem{LUTHILITE}
 Highlights a colour table of an image-display device.
\classitem{LUTIKON}
 Loads the default {\it Ikon\/} lookup table.
\classitem{LUTNEG}
 Loads the standard negative greyscale lookup table.
\classitem{LUTRAMPS}
 Loads the coloured-ramps lookup table.
\classitem{LUTREAD}
 Loads an image-display lookup table from an NDF.
\classitem{LUTROT}
 Rotates the colour table of an image-display device.
\classitem{LUTSAVE}
 Saves the current colour table of an image-display device in an NDF.
\classitem{LUTSPEC}
 Loads a spectrum-like lookup table.
\classitem{LUTTWEAK}
 Tweaks a colour table of an image-display device.
\classitem{LUTVIEW}
 Draws a colour-table key.
\classitem{LUTZEBRA}
 Loads a pseudo-contour lookup table.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Output}
\begin{description}
\classitem{CONTOUR}
 Contours a 2-d NDF.
\classitem{CONTOVER}
 Contours a 2-d data array overlaid on an image displayed previously.
\classitem{DISPLAY}
 Displays a 1-d or 2-d NDF.
\classitem{DRAWSIG}
 Draws ${\pm}n$ standard-deviation lines on a line plot.
\classitem{ELPROF}
 Creates a radial or azimuthal profile of a 2-dimensional image.
\classitem{GREYPLOT}
 Produces a greyscale plot of a 1-d or 2-d NDF.
\classitem{INSPECT}
 Inspects a 2-d NDF in a variety of ways.
\classitem{LINPLOT}
 Draws a line plot of a 1-d NDF's data values against their axis co-ordinates.
\classitem{LOOK}
 Outputs the values of a sub-array of a 2-d data array to the screen or a
 text file.
\classitem{MLINPLOT}
 Draws a multi-line plot of a 2-d NDF's data values against their axis
 co-ordinates.
\classitem{SNAPSHOT}
 Dumps an image-display memory to a graphics hardcopy and
 optionally to an NDF.
\classitem{TURBOCONT}
 Contours a 2-d NDF quickly.
\classitem{VECPLOT}
 Plots a 2-dimensional vector map.
\end{description}
\vspace*{0.7ex}
%
\label{cl:palette}
\item {\large\bf Palette}
\begin{description}
\classitem{PALDEF}
 Loads the default palette to a colour table.
\classitem{PALENTRY}
 Enters a colour into an image display's palette.
\classitem{PALREAD}
 Fills the palette of a colour table from an NDF.
\classitem{PALSAVE}
 Saves the current palette of a colour table to an NDF.
\end{description}
\end{description}

\vspace*{-1ex}

{\large
\begin{center}
{\bf DATA MANIPULATION}
\end{center}
}

\begin{description}
%
\item {\large\bf Arithmetic}
\begin{description}
\classitem{ADD}
 Adds two NDF data structures.
\classitem{CADD}
 Adds a scalar to an NDF data structure.
\classitem{CDIV}
 Divides an NDF by a scalar.
\classitem{CMULT}
 Multiplies an NDF by a scalar.
\classitem{CSUB}
 Subtracts a scalar from an NDF data structure.
\classitem{DIV}
 Divides one NDF data structure by another.
\classitem{EXP10}
 Takes the base-10 exponential of each pixel of a data array.
\classitem{EXPE}
 Takes the exponential of each pixel of a data array (base $e$).
\classitem{EXPON}
 Takes the exponential of each pixel of a data array (specified base).
\classitem{LOG10}
 Takes the base-10 logarithm of each pixel of a data array.
\classitem{LOGAR}
 Takes the logarithm of each pixel of a data array (specified base).
\classitem{LOGE}
 Takes the natural logarithm of each pixel of a data array.
\classitem{MATHS}
 Evaluates mathematical expressions applied to NDF data structures.
\classitem{MULT}
 Multiplies two NDF data structures.
\classitem{POW}
 Takes the specified power of each pixel of a data array.
\classitem{SUB}
 Subtracts one NDF data structure from another.
\classitem{TRIG}
 Performs a trigonometric transformation on a data array.
\end {description}
\vspace*{0.7ex}
%
\item {\large\bf Combination}
\begin{description}
\classitem{CALPOL}
 Calculates polarisation parameters.
\classitem{KSTEST}
 Compares data sets using the Kolmogorov-Smirnov test.
\classitem{MOSAIC}
 Merges several non-congruent 2-d data arrays into one output data array.
\classitem{NORMALIZE}
 Normalises one NDF to a similar NDF by calculating a scale factor and zero
 difference.
\classitem{QUILT}
 Generates a mosaic from equally sized 2-d data arrays, optionally specified
 from an ASCII file.
\end{description}
\vspace*{0.7ex}
%
\label{cl:compexp}
\item {\large\bf Compression and expansion}
\begin{description}
\classitem{COMPADD}
 Reduces the size of an NDF by adding values in rectangular boxes.
\classitem{COMPAVE}
 Reduces the size of an NDF by averaging values in rectangular boxes.
\classitem{COMPICK}
 Reduces the size of an NDF by picking equally spaced pixels.
\classitem{PIXDUPE}
 Expands an NDF by pixel duplication.
\classitem{SQORST}
 Squashes or stretches a 2-d data array in either or both axes.
\classitem{TRANSFORMER}
 Applies a transformation to an NDF.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Configuration change}
\begin{description}
\classitem{FLIP}
 Reverses an NDF's pixels along a specified dimension.
\classitem{INSPECT}
 Inspects a 2-d NDF in a variety of ways.
\classitem{MANIC}
 Converts all or part of a data array from one dimensionality to another.
\classitem{NDFCOPY}
 Copies an NDF (or NDF section) to a new location.
\classitem{ROTATE}
 Rotates a 2-dimensional NDF about its centre through any angle.
\classitem{SETBOUND}
 Sets new bounds for an NDF.
\classitem{SLIDE}
 Realigns a 2-d data array via an $x$-$y$ shift.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Filtering}
\begin{description}
\classitem{BLOCK}
 Smooths a 1- or 2-dimensional image using a square or rectangular box filter.
\classitem{CONVOLVE}
 Convolves a pair of 1- or 2-dimensional NDFs together.
\classitem{FFCLEAN}
 Removes defects from a substantially flat 1- or 2-dimensional NDF.
\classitem{FOURIER}
 Performs forward and inverse Fourier transforms of 1- or 2-dimensional NDFs.
\classitem{GAUSMOOTH}
 Smooths a 1- or 2-dimensional image using a Gaussian filter.
\classitem{LUCY}
 Performs a Richardson-Lucy deconvolution of a 1- or 2-dimensional array.
\classitem{MEDIAN}
 Smooths a 2-dimensional data array using a weighted median filter.
\classitem{MEM2D}
 Performs a Maximum-Entropy deconvolution of a 2-dimensional NDF.
\classitem{WIENER}
 Applies a Wiener filter to a 1- or 2-dimensional array.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf HDS components}
\begin{description}
\classitem{ERASE}
 Erases an HDS object.
\classitem{NATIVE}
 Converts an HDS object to native machine data representation.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf NDF array components}
\begin{description}
\classitem{NDFCOPY}
 Copies an NDF (or NDF section) to a new location.
\classitem{SETBAD}
 Sets new bad-pixel flag values for an NDF.
\classitem{SETBB}
 Sets a new value for the quality bad-bits mask of an NDF.
\classitem{SETBOUND}
 Sets new bounds for an NDF.
\classitem{SETTYPE}
 Sets a new numeric type for the data and variance components of an NDF.
\classitem{SETVAR}
 Sets new values for the variance component of an NDF data structure.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf NDF axis components}
\begin{description}
\classitem{AXCONV}
 Expands spaced axes in an NDF into the primitive form.
\classitem{AXLABEL}
 Sets a new label value for an axis within an NDF data structure.
\classitem{AXUNITS}
 Sets a new units value for an axis within an NDF data structure.
\classitem{SETAXIS}
 Sets values for an axis array component within an NDF data structure.
\classitem{SETNORM}
 Sets a new value for one or all of an NDF's axis-normalisation flags.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf NDF character components}
\begin{description}
\classitem{SETLABEL}
 Sets a new label for an NDF data structure.
\classitem{SETTITLE}
 Sets a new title for an NDF data structure.
\classitem{SETUNITS}
 Sets a new units value for an NDF data structure.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf NDF extensions}
\begin{description}
\classitem{FITSEDIT}
 Edits the FITS extension of an NDF.
\classitem{FITSEXIST}
 Inquires whether or not a keyword exists in a FITS extension.
\classitem{FITSEXP}
 Exports NDF-extension information into an NDF FITS extension.
\classitem{FITSLIST}
 Lists the FITS extension of an NDF.
\classitem{FITSMOD}
 Edits an NDF FITS extension via a text file or parameters.
\classitem{FITSTEXT}
 Creates an NDF FITS extension from a text file.
\classitem{FITSVAL}
 Reports the value of a keyword in the FITS extension.
\classitem{FITSWRITE}
 Writes a new keyword to the FITS extension.
\classitem{SETEXT}
 Manipulates the contents of a specified NDF extension.
\classitem{SETSKY}
 Makes an IRAS astrometry extension.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf NDF History}
\begin{description}
\classitem{HISCOM}
 Adds commentary to the history of an NDF.
\classitem{HISLIST}
 Lists NDF history records.
\classitem{HISSET}
 Sets the NDF history update mode.
\end{description}
\vspace*{0.7ex}
%
\label{cl:pixedit}
\item {\large\bf Pixel editing and masking}
\begin{description}
\classitem{ARDGEN}
 Creates a text file describing selected regions of an image.
\classitem{ARDMASK}
 Uses an ARD file to set some pixels of an NDF to be bad.
\classitem{CHPIX}
 Replaces the values of selected pixels in an NDF.
\classitem{ERRCLIP}
 Removes pixels with large errors from an NDF.
\classitem{FFCLEAN}
 Removes defects from a substantially flat 1- or 2-dimensional NDF.
\classitem{FILLBAD}
 Removes regions of bad values from a 2-dimensional NDF.
\classitem{GLITCH}
 Replaces bad pixels in a 2-d data array with the local median.
\classitem{NOMAGIC}
 Replaces all occurrences of magic value pixels in an NDF array with
 a new value.
\classitem{OUTSET}
 Sets pixels outside a specified circle in a 2-d data array to a specified
 value.
\classitem{PASTE}
 Pastes a series of NDFs upon each other.
\classitem{RIFT}
 Adds a scalar to a section of an NDF data structure to correct rift-valley defects.
\classitem{SEGMENT}
 Copies polygonal segments from one NDF to another.
\classitem{SETMAGIC}
 Replaces all occurrences of a given value in an NDF array
 with the bad value.
\classitem{SUBSTITUTE}
 Replaces all occurrences of a given value in an NDF array with another value.
\classitem{ZAPLIN}
 Replaces regions in a 2-d NDF by bad values or by linear
 interpolation.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Polarimetry}
\begin{description}
\classitem{CALPOL}
 Calculates polarisation parameters.
\end{description}
\vspace*{0.7ex}
%
\label{cl:resample}
\item {\large\bf Resampling and transformations}
\begin{description}
\classitem{TRANINVERT}
 Inverts a transformation.
\classitem{TRANJOIN}
 Joins two transformations.
\classitem{TRANMAKE}
 Makes a transformation structure given its co-ordinate mappings.
\classitem{TRANSFORMER}
 Applies a transformation to an NDF.
\classitem{TRANTRACE}
 Lists the contents of a transformation structure.
\end{description}
\vspace*{0.7ex}
%
\label{cl:surfit}
\item {\large\bf Surface fitting}
\begin{description}
\classitem{FITSURFACE}
 Fits a polynomial surface to 2-dimensional data array.
\classitem{MAKESURFACE}
 Creates a 2-dimensional NDF from the coefficients of a polynomial surface.
\classitem{SURFIT}
 Fits a polynomial or spline surface to a 2-d data array using blocking.
\end{description}
\end{description}

\vspace*{-1ex}

{\large
\begin{center}
{\bf DATA ANALYSIS}
\end{center}
}

\begin{description}
%
\item {\large\bf Statistics}
\begin{description}
\classitem{APERADD}
 Derives statistics of pixels within a specified circle of a 2-d data array.
\classitem{HISTAT}
 Computes ordered statistics for an NDF's pixels using an histogram.
\classitem{HISTOGRAM}
 Computes an histogram of an NDF's values.
\classitem{INSPECT}
 Inspects a 2-d NDF in a variety of ways.
\classitem{MSTATS}
 Does cumulative statistics on a 2-d sub-array over a sequence of data arrays.
\classitem{NUMB}
 Counts the number of elements of an NDF with values or absolute values above
 or below a threshold.
\classitem{STATS}
 Computes simple statistics for an NDF's pixels.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Other}
\begin{description}
\classitem{CENTROID}
 Finds the centroids of star-like features in an NDF.
\classitem{NORMALIZE}
 Normalises one NDF to a similar NDF by calculating a scale factor
 and zero-point difference.
\classitem{PSF}
 Determines the parameters of a model star profile by fitting star images
 in a two-dimensional NDF.
\classitem{SURFIT}
 Fits a polynomial or spline surface to a 2-d data array.
\end{description}
\end{description}

\vspace*{-1ex}

{\large
\begin{center}
{\bf SCRIPTING TOOLS}
\end{center}
}

\begin{description}
\item{~}
\vspace*{-5.6ex}
%
\begin{description}
\classitem{CALC}
 Evaluates a mathematical expression.
\classitem{PARGET}
 Obtains the value or values of an application parameter.
\end{description}
\end{description}

\vspace*{-1ex}

{\large
\begin{center}
{\bf INQUIRIES \& STATUS}
\end{center}
}

\begin{description}
\item{~}
\vspace*{-5.6ex}
%
\begin{description}
\classitem{GLOBALS}
 Displays the values of the {\footnotesize KAPPA} global parameters.
\classitem{FITSEXIST}
 Inquires whether or not a keyword exists in a FITS extension.
\classitem{FITSLIST}
 Lists the FITS extension of an NDF.
\classitem{FITSVAL}
 Reports the value of a keyword in the FITS extension.
\classitem{NDFTRACE}
 Displays the attributes of an NDF data structure.
\classitem{NOGLOBALS}
 Resets the {\footnotesize KAPPA} global parameters.
\end{description}
\end{description}

\vspace*{-1ex}

{\large
\begin{center}
{\bf MISCELLANEOUS}
\end{center}
}

\begin{description}
\item{~}
\vspace*{-5.6ex}
%
\begin{description}
\classitem{KAPHELP}
 Gives help about {\footnotesize KAPPA}.
\end{description}
\end{description}
\end{small}

\section{\xlabel{ap_quotas}Quotas to run KAPPA\label{ap:quotas}}

No special quotas are needed to run {\footnotesize KAPPA}.  If you have large
datasets you might need to increase the datasize limit in the C-shell.

\small
\begin{verbatim}
    % limit datasize 65336
\end{verbatim}
\normalsize
sets the maximum size of a data file to 64 megabytes.  To list the
current values use the {\bf limit} command without any arguments.

\newpage
\section{\xlabel{ap_full}Specifications of KAPPA applications\label{ap:full}}
\subsection{Explanatory Notes}
For historical reasons, in this Appendix there are two formats---the
first, exemplified by \htmlref{ADD}{ADD}, is the new layout indicating
that this is one of the applications converted to use the NDF. An
example of the old style is \htmlref{APERADD}{APERADD}.  Applications
in this format use the moribund \htmlref{IMAGE format}{ap:IMAGEformat}.

\begin{description}
\item [NDF layout]
In this layout the specification of parameters has the following
format. 

\begin{verbatim}
     name  =  type (access)
        description
\end{verbatim}
This format also includes a {\em Usage\/} entry.  \label{ap:usage}
This shows how the application is invoked from the command line.  It
lists the positional parameters in order followed by any prompted
keyword parameters using a {\mbox ``KEYWORD=?''} syntax.  Defaulted
keyword parameters do not appear.  Positional parameters that are
normally defaulted are indicated by being enclosed in square brackets.
Keyword ({\it{i.e.}}\ not positional) parameters are needed where the
number of parameters are large, and usually occur because they depend
on the value of another parameter.  These are denoted by a curly
brace; the parameters on each line are related, and each line is
mutually exclusive.  An example should clarify.
\bigskip

{\ssttt \hspace*{1.0em}
        contour ndf [comp] mode ncont [key] [device] [smoothing]
        \newline\hspace*{2.5em}
        $\left\{ {\begin{tabular}{l}
                    FIRSTCNT=? STEPCNT=? \\
                    HEIGHTS=?
                   \end{tabular} }
        \right.$
        \newline\hspace*{2.9em}
        \makebox[0mm][c]{\footnotesize mode}
}
\bigskip

NDF, COMP, MODE, NCONT, KEY, DEVICE, and SMOOTHING are all positional
parameters.  Only NDF, MODE, and NCONT would be prompted if not given
on the command line.  The remaining parameters depend on the value of
MODE.  If the mode is to nominate a list of contour heights, HEIGHTS
will be needed (MODE~=~{\tt "Free"}); alternatively, if the mode requires a
start height and spacing between contours FIRSTCNT and STEPCNT should be
specified (MODE~=~{\tt "Linear"} or {\tt "Magnitude"}).  Note that there
are other modes which do not require additional information, and hence no
more parameters. 

There is also an {\em Examples\/} section. \label{ap:example}  This
shows how to run the application from the command line.  More often
you'll enter the command name and just some of the parameters, and be
prompted for the rest. {\em Note that from the C-shell you must escape
any special characters that appear in these examples.} For instance,
the fourth example of COMPAVE and the third of CONTOVER would be
written like this

\begin{verbatim}
     compave cosmos galaxy '[4,3]' weight title='"COSMOS compressed"'
     contover iras60'(200:300,100:350)' comp=d offset='[3,5]' \\
\end{verbatim}

from the C-shell.

\item [IMAGE layout]
In this layout the specification of parameters has the following format
\begin{verbatim}
    name    type    access    description
\end{verbatim}
\end{description}
\medskip

The following notes apply to both layouts.

Some parameters will only be used when another parameter has a certain
value or mode. These are indicated by the name of the mode in
parentheses at the end of the parameter description, but before any
default, {\it{e.g.}}\ parameter DEVICE in \htmlref{CENTROID}{CENTROID}
is only relevant when parameter MODE is {\tt "Cursor"}.

{\tt \%name} means the value of parameter {\it name}.

The description entry has a notation scheme to indicate 
normally defaulted parameters, {\it{i.e.}}\ those for which there will
be no prompt.
For such parameters a matching pair of square brackets ({\tt{[]}})
terminates the description.  The content between the brackets mean
\begin{description}
\item[{\tt{[]}}]
Empty brackets means that the default is created dynamically
by the application, and may depend on the values of other parameters.
Therefore, the default cannot be given explicitly.
\item[{\tt [,]}]
As above, but there are two default values that are created dynamically.
\item[{\tt [}{\rm default}{\tt{]}}]
Occasionally, a description of the default is given in normal type,
{\it{e.g.}}\ the size of the plotting region in a graphics application,
where the exact default values depend on the device chosen. 
\item[{\tt [default]}]
If the brackets contain a value in teletype-fount, this is the explicit
default value.
\end{description}

\newpage
\sstroutine{
   ADD
}{
   Adds two NDF data structures
}{
   \sstdescription{
      The routine adds two NDF data structures pixel-by-pixel to produce
      a new NDF.
   }
   \sstusage{
      add in1 in2 out
   }
   \sstparameters{
      \sstsubsection{
         IN1 = NDF (Read)
      }{
         First NDF to be added.
      }
      \sstsubsection{
         IN2 = NDF (Read)
      }{
         Second NDF to be added.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF to contain the sum of the two input NDFs.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the NDF supplied for parameter IN1 to be used
         instead. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         add a b c
      }{
         This adds the NDF called b to the NDF called a, to make the
         NDF called c.  NDF c inherits its title from a.
      }
      \sstexamplesubsection{
         add out=c in1=a in2=b title="Co-added image"
      }{
         This adds the NDF called b to the NDF called a, to make the
         NDF called c.  NDF c has the title {\tt "Co-added image"}.
      }
   }
   \sstnotes{
      If the two input NDFs have different pixel-index bounds, then
      they will be trimmed to match before being added. An error will
      result if they have no pixels in common.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CADD, CDIV, CMULT, CSUB, DIV, MATHS, MULT, SUB.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, HISTORY, and VARIANCE components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Units processing is not supported at present and therefore the
         UNITS component is not propagated.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.

      }
   }
}
\manroutine {{\manheadstyle{APERADD}}}{ Derives statistics of pixels within a
  specified circle of a 2-d data array.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine takes an input 2-d data array in an {\mantt{IMAGE}} structure
  and bins up all the pixels that lie within specified circles to
  either increase the signal-to-noise over that region, or to
  simulate a circular-aperture measurement of the image.

  WARNING: This simple task does not divide the light of pixels
  spanning the circle.  If the pixel's centre lies within the circle,
  its full value is included in the summation; if it lies outside
  the circle, the pixel value is excluded from the summation.
  Therefore this task is not suitable for accurate aperture
  photometry, especially where the aperture diameter is less than
  about ten times the pixel size.  Use PHOTOM where accuracy, rather
  than speed, is paramount.

  The following are displayed: the standard deviation of the
  intensity of the pixels within the aperture before binning, the
  integrated value over the aperture, and the calculated mean level
  and reduced noise after binning.

  The magic-value method is used for processing bad data.

\manroutineitem {Invocation }{}
  APERADD

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{LOGFILE}} }{{\mantt{FILENAME}}}
  Name of the text file to record the statistics. If null,
  there will be no logging. {\mantt [!]}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}} }{{\mantt{IMAGE}}}
  Input {\mantt{IMAGE}} structure containing data array to be processed.
\manparameterentry {{\mantt{READ}} }{{\mantt{XCEN}} }{{\mantt{\_REAL}}}
  {$x$} co-ordinate of the circle centre.
\manparameterentry {{\mantt{READ}} }{{\mantt{YCEN}} }{{\mantt{\_REAL}}}
  {$y$} co-ordinate of the circle centre.
\manparameterentry {{\mantt{READ}} }{{\mantt{DIAM}} }{{\mantt{\_REAL}}}
  Diameter of the circle in pixels.
\manparameterentry {{\mantt{READ}} }{{\mantt{AGAIN}} }{{\mantt{\_LOGICAL}}}
  If true then another aperture can be chosen.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{NUMPIX}} }{{\mantt{\_INTEGER}}}
  The number of pixels within the aperture.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{TOTAL}} }{{\mantt{\_REAL}}}
  The total of the pixel values within the aperture.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{MEAN}} }{{\mantt{\_REAL}}}
  The mean of the pixel values within the aperture.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{SIGMA}} }{{\mantt{\_REAL}}}
  The standard deviation of the pixel values within the aperture.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{NOISE}} }{{\mantt{\_REAL}}}
  The standard deviation of the pixel values within the aperture after
  binning.
\end{manparametertable}
\manroutineitem {Deficiencies }{}
  The circle centre must lie somewhere on the array, which is
  by far the most likely option, but it not totally general.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\newpage
\sstroutine{
   ARDGEN
}{
   Creates a text file describing selected regions of an image
}{
   \sstdescription{
      This is an interactive tool for selecting regions of a displayed
      image using a cursor, and then stores a description of the
      selected regions in a text file in the form of an `ARD Description'
      (see SUN/183).  This text file may subsequently be 
      used in conjunction with packages such as {\footnotesize CCDPACK}
      or {\footnotesize ESP}.

      The application initially obtains a value for the SHAPE parameter
      and then allows you to identify either one or many regions of the
      specified shape, dependent on the value of parameter STARTUP.
      When the required regions have been identified, a value is
      obtained for parameter OPTION, and that value determines what
      happens next.  Options include obtaining further regions,
      changing the current region shape, listing the currently defined
      regions, leaving the application, {\it etc.}\  Once the selected action
      has been performed, another value is obtained for OPTION, and
      this continues until you choose to leave the application.

      Instructions on the use of the cursor are displayed when the
      application is run.  The points required to define a region of
      the requested shape are described whenever the current region
      shape is changed using parameter SHAPE.  Once the points required
      to define a region have been given an outline of the entire
      region is drawn on the graphics device using the pen specified by
      parameter PALNUM.

      In the absence of any other information, subsequent application
      will use the union ({\it i.e.}\ the logical OR) of all the defined
      regions.  However, regions can be combined in other ways using the
      COMBINE option (see parameter OPTION).  For instance, two regions
      originally defined using the cursor could be replaced by their
      region of intersection (logical AND), or a single region could be
      replaced by its own exterior (logical NOT).  Other operators can
      also be used (see parameter OPERATOR).
   }
   \sstusage{
      ardgen ardout shape option [device] [startup] [palnum] [poicol]
         \newline\hspace*{1.5em}
         $\left\{ {\begin{tabular}{l}
            operands=? operator=? \\
            regions=? \\
                   \end{tabular} }
        \right.$
        \newline\hspace*{1.9em}
        \makebox[0mm][c]{\small option}
   }
   \sstparameters{
      \sstsubsection{
         ARDOUT = FILENAME (Write)
      }{
         Name of the text file in which to store the description of the
         selected regions.
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device on which the regions are to be selected.
         {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         OPERANDS() = \_INTEGER (Read)
      }{
         A pair of indices for the regions which are to be combined
         together using the operator specified by the parameter OPERATOR.
         If the operator is {\tt "NOT"}, then only one region index need be
         supplied.  Region indices are displayed by the {\tt "List"} option
         (see parameter OPTION).
      }
      \sstsubsection{
         OPERATOR = LITERAL (Read)
      }{
         The operator to use when combining two regions into a single
         region.  The pixels included in the resulting region depend on
         which of the following operators is selected.

         \begin{description}
         \item {\tt "AND"} --- Pixels are included if they are in both
                 of the regions specified by parameter OPERANDS.
         \item {\tt "EQV"} --- Pixels are included if they are in both
                 or neither of the regions specified by parameter OPERANDS.
         \item {\tt "NOT"} --- Pixels are included if they are not inside
                 the region specified by the parameter OPERANDS.
         \item {\tt "OR"}  --- Pixels are included if they are in either
                 of the regions specified by parameter OPERANDS.  Note, an
                 OR operator is implicitly assumed to exist between each
                 pair of adjacent regions unless some other operator is
                 specified.
         \item {\tt "XOR"} --- Pixels are included if they are in one, but not both,
                 of the regions specified by parameter OPERANDS.
         \end{description}
      }
      \sstsubsection{
         OPTION = LITERAL (Read)
      }{
         A value for this parameter is obtained when you choose to end
         cursor input (by pressing the relevant button as described
         when the application starts up).  It determines what to do
         next.  The following options are available:

         \begin{description}
         \item {\tt "Combine"} --- Combine two previously defined regions
                  into a single region using a Boolean operator, or invert
                  a previously defined region using a Boolean .NOT.
                  operator.  See parameters OPERANDS and OPERATOR.  The
                  original regions are deleted and the new combined
                  (or inverted) region is added to the end of the
                  list of defined regions.
         \item {\tt "Delete"} --- Delete previously defined regions, see
                  parameter REGIONS.
         \item {\tt "Exit"} --- Write out the currently defined regions to
                  a text file and exit the application.
         \item {\tt "List"} --- List the textual descriptions of the currently
                  defined regions on the screen.  Each region is described
                  by an index value, a `keyword' corresponding to the shape,
                  and various arguments describing the extent and position of
                  the shape.  These arguments are described in the
                  {\tt "}Notes{\tt "} section below.
         \item {\tt "Multi"} --- The cursor is displayed and you can then
                  identify multiple regions of the current shape, without
                  being re-prompted for OPTION after each one.  These
                  regions are added to the end of the list of currently
                  defined regions.  If the current shape is {\tt "Polygon"},
                  {\tt "Frame"} or {\tt "Whole"} (see parameter SHAPE) then
                  multiple regions cannot be defined and 
                  selected option automatically reverts to {\tt "Single"}.
         \item {\tt "Single"} --- The cursor is displayed and you can then
                  identify a single region of the current shape.  You are
                  re-prompted for parameter OPTION once you have
                  defined the region.  The identified region is
                  added to the end of the list of currently defined
                  regions.
         \item {\tt "Shape"} --- Change the shape of the regions created by the
                  {\tt "Single"} and {\tt "Multi"} options.  This causes a
                  new value for parameter SHAPE to be obtained.
         \item {\tt "Quit"} --- Quit the application without saving the currently
                   defined regions.
         \end{description}
      }
      \sstsubsection{
         PALNUM= \_INTEGER (Read)
      }{
         The pen number with which to outline the selected regions.
         {\tt [3]}
      }
      \sstsubsection{
         POICOL = \_INTEGER (Read)
      }{
         The pen number used to mark the cursor positions which define
         each region.  {\tt [2]}
      }
      \sstsubsection{
         REGIONS() = LITERAL (Read)
      }{
         The list of regions to be deleted.  Regions are numbered
         consecutively from 1 and can be listed using the {\tt "List"} option
         (see parameter OPTION).  Single regions or a set of adjacent
         regions may be specified, {\it e.g.}\ assigning {\tt [4,6-9,12,14-16]}
         will delete regions 4,6,7,8,9,12,14,15,16.  (Note that the brackets
         are required to distinguish this array of characters from a
         single string including commas.  The brackets are unnecessary
         when there is only one item.)  The numbers need not be in
         ascending order.

         If you wish to delete all the regions enter the wildcard $*$.
         {\tt 5-$*$} will delete from 5 to the last region.
      }
      \sstsubsection{
         SHAPE= LITERAL (Read)
      }{
         The shape of the regions to be defined using the cursor.
         After selecting a new shape, you are immediately requested to
         identify multiple regions as if {\tt "Multi"} had been specified for
         parameter OPTION.  The currently available shapes are listed below.

         \begin{description}
         \item {\tt "Box"} --- A rectangular box with sides parallel to the
               image axes, defined by its centre and one of its corners.
         \item {\tt "Circle"}  --- A circle, defined by its centre and radius.
         \item {\tt "Column"}  --- A single column of pixels.
         \item {\tt "Ellipse"} --- An ellipse, defined by its centre, one end
               of the major axis, and one other point which can be anywhere
               on the ellipse.
         \item {\tt "Frame"} --- The whole image excluding a border of constant
               width, defined by a single point on the frame.
         \item {\tt "Point"} --- A single pixel.
         \item {\tt "Polygon"} --- Any general polygonal region, defined
               by up to 200 vertices.
         \item {\tt "Rectangle"} --- A rectangular box with sides parallel to
               the image axes, defined by a pair of diagonally opposite corners.
         \item {\tt "Rotbox"} --- A rotated box, defined by both ends of
                       an edge, and one point on the opposite edge.
         \item {\tt "Row"}  --- A single row of pixels.
         \item {\tt "Whole"} --- The whole of the displayed image.
         \end{description}
      }
      \sstsubsection{
         STARTUP = LITERAL (Read)
      }{
         Determines if the application starts up in {\tt "Multi"} or
         {\tt "Single"} mode (see parameter OPTION). {\tt ["Multi"]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         ardgen extract.txt circle exit startup=single
      }{
         This example allows you to create a text file ({\tt extract.txt})
         describing a single circular region of the image displayed on
         the current graphics device.  The application immediately exits
         after the region has been identified.  This example may be
         useful in scripts or command procedures since there is no
         prompting.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         An image must previously have been displayed on the graphics
         device.

         \sstitem
         The arguments for the textual description of each shape are as
         follows  (all co-ordinates are pixel, {\it i.e.}\ world co-ordinates):

         \begin{description}
         \item {\tt "Box"} --- The co-ordinates of the centre, followed by the
               lengths of the two sides.
         \item {\tt "Circle"} --- The co-ordinates of the centre, followed by
               the radius.
         \item {\tt "Column"} --- The $x$ co-ordinate of the column.
         \item {\tt "Ellipse"} --- The co-ordinates of the centre, followed
               by the lengths of the semi-major and semi-minor axes, followed
               by the angle between the $x$ axis and the semi-major axis
               (in radians).
         \item {\tt "Frame"} --- The width of the border.
         \item {\tt "Point"} --- The co-ordinates of the pixel.
         \item {\tt "Polygon"} --- The co-ordinates of each vertex in the
               order given.
         \item {\tt "Rectangle"} --- The co-ordinates of two diagonally
               opposite corners.
         \item {\tt "Rotbox"} --- The co-ordinates of the box centre, followed by the
               lengths of the two sides, followed by the angle
               between the first side and the $x$ axis (in radians).
         \item {\tt "Row"} --- The $y$ co-ordinate of the row.
         \item {\tt "Whole"} --- No arguments.
         \end{description}
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ARDMASK; CCDPACK; ESP.
   }
}
\sstroutine{
   ARDMASK
}{
   Uses an ARD file to set some pixels of an NDF to be bad
}{
   \sstdescription{
      This task allows regions of a NDF's data array to be masked, so
      that they can be excluded from subsequent data processing.  ARD
      (ASCII Region Definition) descriptions stored in a text
      file define which pixels of the data array are masked.  An
      output NDF is created which is the same as the input file except
      that all pixels specified by the ARD file have been assigned the
      bad value.
   }
   \sstusage{
      ardmask in ardfile out
   }
   \sstparameters{
      \sstsubsection{
         ARDFILE = FILENAME (Read)
      }{
         The name of the ARD file containing a description of the parts
         of the image to be masked out, {\it i.e.}\ set to bad.  The suggested
         default is the current value or {\tt ardfile.dat} if there is no
         current value.
      }
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either {\tt "World"}
         or {\tt "Data"}.  If COSYS = {\tt "World"} the co-ordinates used in the
         ARD file are pixel co-ordinates or indices.  If COSYS = {\tt "Data"}
         the co-ordinates used in the ARD file are interpreted as data
         co-ordinates, provided the NDF contains axes that map onto
         pixel co-ordinates using linear transformations.  If there are
         no axes, pixel co-ordinates are assumed; if axes are present
         but non-linear, the task fails.  COSYS={\tt "World"} is recommended.
         {\tt [}Current co-ordinate system{\tt ]}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The name of the source NDF.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The name of the masked NDF.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the input NDF to the output NDF. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         ardmask a1060 galaxies.ard a1060\_sky title="A1060 galaxies masked"
      }{
         This flags pixels defined by the ARD file galaxies.ard within
         the NDF called a1060 to create a new NDF called a1060\_sky.
         a1060\_sky has a title={\tt "A1060 galaxies masked"}.  This might be
         to flag the pixels where bright galaxies are located to
         exclude them from sky-background fitting.
      }
      \sstexamplesubsection{
         ardmask in=ic3374 ardfil=ardfile.txt out=ic3374a
      }{
         This example uses as the source image the NDF called ic3374
         and sets the pixels specified by the ARD description contained
         in ardfile.txt to the bad value.  The resultant image is
         output to the NDF called ic3374a.  The title is unchanged.
      }
      \sstexamplesubsection{
         ardmask in=ic3374 ardfil=ardfile.txt out=ic3374a cosys=data
      }{
         As the previous example except that the ARD file is
         written using data co-ordinates.
      }
   }
   \sstdiytopic{
      ASCII-region-definition Descriptors
   }{
      The ARD file may be created by ARDGEN or written manually.  In the
      latter case consult SUN/183 for full details of the ARD
      descriptors and syntax; however, much may be learnt from looking
      at the ARD files created by ARDGEN and the ARDGEN documentation.
      There is also a summary with examples in the main body of SUN/95
      and the online help.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, UNITS, HISTORY, and VARIANCE components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.

      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ARDGEN.
   }
}
\newpage
\sstroutine{
   AXCONV
}{
   Expands spaced axes in an NDF into the primitive form
}{
   \sstdescription{
      This application routine converts {\it in situ\/} an NDF's axis centres
      in the `spaced' form into `simple' form.  Applications using the
      NDF\_ library, such as {\footnotesize KAPPA}, are not currently capable
      of supporting spaced arrays, but there are packages that produce NDF
      files with this form of axis, notably {\footnotesize ASTERIX}.
      This application provides a temporary method of allowing
      {\footnotesize KAPPA} {\it et al.}\ to handle these NDF datasets.
   }
   \sstusage{
      axconv ndf
   }
   \sstparameters{
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF to be modified.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         axconv rosat256
      }{
         This converts the spaced axes in the NDF called rosat256 into
         simple form.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: SETAXIS.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         Only axes with a real data type are created.
      }
   }
}
\sstroutine{
   AXLABEL
}{
   Sets a new label value for an axis within an NDF data structure
}{
   \sstdescription{
      This routine sets a new value for a label component of an
      existing NDF AXIS data structure.  The NDF is accessed in update
      mode and any pre-existing label component is over-written with a
      new value.  Alternatively, if a `null' value ({\tt !}) is given for the
      LABEL parameter, then the NDF's axis label component will be
      erased.  If an AXIS structure does not exist, a new one whose
      centres are pixel co-ordinates is created.
   }
   \sstusage{
      axlabel ndf label dim
   }
   \sstparameters{
      \sstsubsection{
         DIM = \_INTEGER (Read)
      }{
         The axis dimension for which the label is to be modified.
         There are separate labels for each NDF dimension.  The value
         must lie between 1 and the number of dimensions of the NDF.
         This defaults to 1 for a 1-dimensional NDF.  The suggested
         default is the current value.  {\tt []}
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF data structure in which an axis label component is to
         be modified.
      }
      \sstsubsection{
         LABEL = LITERAL (Read)
      }{
         The value to be assigned to the NDF's axis label component
         ({\it e.g.}\ {\tt "Wavelength"} or {\tt "Fibre index"}).  LABEL describes the
         quantity measured along the axis.  This value may later be
         used by other applications for labelling graphs or as a
         heading for columns in tabulated output.  The suggested
         default is the current value.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         axlabel ngc253 "Offset from nucleus" 2
      }{
         Sets the label component of the second axis dimension of the
         NDF structure ngc253 to have the value {\tt "Offset from nucleus"}.
      }
      \sstexamplesubsection{
         axlabel ndf=spect label=Wavelength
      }{
         Sets the axis label component of the 1-dimensional NDF
         structure spect to have the value {\tt "Wavelength"}.
      }
      \sstexamplesubsection{
         axlabel datafile label=! dim=3
      }{
         By specifying a null value ({\tt !}), this example erases any
         previous value of the label component for the third dimension
         in the NDF structure datafile.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: AXUNITS, SETAXIS, SETLABEL.
   }
}
\sstroutine{
   AXUNITS
}{
   Sets a new units value for an axis within an NDF data structure
}{
   \sstdescription{
      This routine sets a new value for a units component of an
      existing NDF AXIS data structure.  The NDF is accessed in update
      mode and any pre-existing units component is over-written with a
      new value.  Alternatively, if a `null' value ({\tt !}) is given for the
      UNITS parameter, then the NDF's axis units component will be
      erased.  If an AXIS structure does not exist, a new one whose
      centres are pixel co-ordinates is created.
   }
   \sstusage{
      axunits ndf units dim
   }
   \sstparameters{
      \sstsubsection{
         DIM = \_INTEGER (Read)
      }{
         The axis dimension for which the units is to be modified.
         There are separate units for each NDF dimension.  The value
         must lie between 1 and the number of dimensions of the NDF.
         This defaults to 1 for a 1-dimensional NDF.  The suggested
         default is the current value.  {\tt []}
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF data structure in which an axis units component is to
         be modified.
      }
      \sstsubsection{
         UNITS = LITERAL (Read)
      }{
         The value to be assigned to the NDF's axis units component
         ({\it e.g.}\ {\tt "Pixels"} or {\tt "km/s"}).  UNITS describes the physical units
         of the quantity measured along the axis.  This value may later
         be used by other applications for labelling graphs and other
         forms of display where the NDF's axis co-ordinates are shown.
         The suggested default is the current value.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         axunits ngc253 "arcsec" 2
      }{
         Sets the units component of the second axis dimension of the
         NDF structure ngc253 to have the value {\tt "arcsec"}.
      }
      \sstexamplesubsection{
         axunits ndf=spect units=Angstrom
      }{
         Sets the axis units component of the 1-dimensional NDF
         structure spect to have the value {\tt "Angstrom"}.
      }
      \sstexamplesubsection{
         axunits datafile units=! dim=3
      }{
         By specifying a null value ({\tt !}), this example erases any
         previous value of the units component for the third dimension
         in the NDF structure datafile.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: AXLABEL, SETAXIS, SETUNITS.
   }
}
\sstroutine{
   BLOCK
}{
   Smooths a 1- or 2-dimensional image using a square or rectangular box
   filter
}{
   \sstdescription{
      This application applies a square or rectangular box filter to a
      1- or 2-dimensional image so as to smooth it.  Each output pixel
      is either the mean or the median of the input pixels within the
      filter box.  The mean estimator provides one of the fastest
      methods of smoothing an image and is often useful as a
      general-purpose smoothing algorithm when the exact form of the
      smoothing point-spread function is not important.  The image is
      held in an NDF data structure.
   }
   \sstusage{
      block in out box [estimator]
   }
   \sstparameters{
      \sstsubsection{
         BOX(2) = \_INTEGER (Read)
      }{
         The $x$ and $y$ sizes (in pixels) of the rectangular box to be
         applied to smooth the image.  If only a single value is given,
         then it will be duplicated so that a square filter is used,
         except where the image is 1-dimensional, for which the box size
         along the insignificant dimension is set to 1.  The values
         given will be rounded up to positive odd integers, if
         necessary.
      }
      \sstsubsection{
         ESTIMATOR = LITERAL (Read)
      }{
         The method to use for estimating the output pixel values. It can
         be either {\tt "Mean"} or {\tt "Median"}. {\tt ["Mean"]}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF containing the 1- or 2-dimensional image to which
         box smoothing is to be applied.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The output NDF which is to contain the smoothed image.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF. A null value will cause
         the title of the input NDF to be used. {\tt [!]}
      }
      \sstsubsection{
         WLIM = \_REAL (Read)
      }{
         If the input image contains bad pixels, then this parameter
         may be used to determine the number of good pixels which must
         be present within the smoothing box before a valid output
         pixel is generated.  It can be used, for example, to prevent
         output pixels from being generated in regions where there are
         relatively few good pixels to contribute to the smoothed
         result.

         By default, a null ({\tt !}) value is used for WLIM, which causes
         the pattern of bad pixels to be propagated from the input
         image to the output image unchanged.  In this case, smoothed
         output values are only calculated for those pixels which are
         not bad in the input image.

         If a numerical value is given for WLIM, then it specifies the
         minimum fraction of good pixels which must be present in the
         smoothing box in order to generate a good output pixel.  If
         this specified minimum fraction of good input pixels is not
         present, then a bad output pixel will result, otherwise a
         smoothed output value will be calculated.  The value of this
         parameter should lie between 0.0 and 1.0 (the actual number
         used will be rounded up if necessary to correspond to at least
         1 pixel). {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         block aa bb 9
      }{
         Smooths the 2-dimensional image held in the NDF structure aa,
         writing the result into the structure bb.  The smoothing box is
         9 pixels square.  If any pixels in the input image are bad,
         then the corresponding pixels in the output image will also be
         bad.  Each output pixel is the mean of the corresponding input
         pixels.
      }
      \sstexamplesubsection{
         block spectrum spectrums [5,1] median title="Smoothed spectrum"
      }{
         Smooths the 1-dimensional data in the NDF called spectrum
         using a box size of 5 pixels, and stores the result in the NDF
         structure spectrums.  Each output pixel is the median of the
         corresponding input pixels.  If any pixels in the input image
         are bad, then the corresponding pixels in the output image
         will also be bad.  The output NDF has the title {\tt "Smoothed
         spectrum"}.
      }
      \sstexamplesubsection{
         block ccdin(123,) ccdcol [1,9] 
      }{
         Smooths the 123$^{\rm rd}$ column in the 2-dimensional NDF called ccdin
         using a box size of 9 pixels, and stores the result in the NDF
         structure ccdcol.  The first value of the smoothing box is
         ignored as the first dimension has only one element.  Each
         output pixel is the mean of the corresponding input pixels.
      }
      \sstexamplesubsection{
         block in=image1 out=image2 box=[5,7] estimator=median
      }{
         Smooths the 2-dimensional image held in the NDF structure
         image1 using a rectangular box of size 5$\times$7 pixels.  The
         smoothed image is written to the structure image2.  Each
         output pixel is the median of the corresponding input pixels.
      }
      \sstexamplesubsection{
         block etacar etacars box=[7,1] wlim=0.6
      }{
         Smooths the specified image data using a rectangular box 7$\times$1
         pixels in size. Smoothed output values are generated only if
         at least 60\% of the pixels in the smoothing box are good,
         otherwise the affected output pixel is bad.
      }
   }
   \sstdiytopic{
      Timing
   }{
      When using the mean estimator, the execution time is approximately
      proportional to the number of pixels in the image to be smoothed and
      is largely independent of the smoothing box size. This makes the
      routine particularly suitable for applying heavy smoothing to an image.
      Execution time will be approximately doubled if a variance array is
      present in the input NDF.

      The median estimator is much slower than the mean estimator, and is
      heavily dependant on the smoothing box size.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CONVOLVE, FFCLEAN, GAUSMOOTH, MEDIAN; Figaro: ICONV3,
      ISMOOTH, IXSMOOTH, MEDFILT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, UNITS, and HISTORY components of the input NDF
         and propagates all extensions.  In addition, if the mean
         estimator is used, the VARIANCE component is also processed.
         If the median estimator is used, then the output NDF will have
         no VARIANCE component, even if there is a VARIANCE component
         in the input NDF.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.
         The bad-pixel flag is also written for the data and variance arrays.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using single-precision floating point, or double
         precision if appropriate.
      }
   }
}
\sstroutine{
   CADD
}{
   Adds a scalar to an NDF data structure
}{
   \sstdescription{
      The routine adds a scalar ({\it i.e.}\ constant) value to each pixel of
      an NDF's data array to produce a new NDF data structure.
   }
   \sstusage{
      cadd in scalar out
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         Input NDF data structure, to which the value is to be added.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF data structure.
      }
      \sstsubsection{
         SCALAR = \_DOUBLE (Read)
      }{
         The value to be added to the NDF's data array.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the NDF supplied for parameter IN to be used
         instead. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         cadd a 10 b
      }{
         This adds ten to the NDF called a, to make the NDF called b.
         NDF b inherits its title from a.
      }
      \sstexamplesubsection{
         cadd title="HD123456" out=b in=a scalar=17.3
      }{
         This adds 17.3 to the NDF called a, to make the NDF called b.
         NDF b has the title {\tt "HD123456"}.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ADD, CDIV, CMULT, CSUB, DIV, MATHS, MULT, SUB.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, UNITS, HISTORY, and VARIANCE components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.

      }
   }
}
\sstroutine{
   CALC   
}{
   Evaluates a mathematical expression
}{
   \sstdescription{
      This task evaluates an arithmetic expression and reports the
      result.  It main role is to perform floating-point arithmetic in
      scripts.  A value {\tt "Bad"} is reported if there was an error
      during the calculation, such as a divide by zero.
   }
   \sstusage{
      calc exp [prec] fa-fz=? pa-pz=?
   }
   \sstparameters{
      \sstsubsection{
         EXP = LITERAL (Read)
      }{
         The mathematical expression to be evaluated, {\em e.g.}
         {\tt "-2.5$*$LOG10(PA)"}.  In this expression constants may either be
         given literally or represented by the variables PA, PB, \ldots
         PZ.  The expression may contain sub-expressions represented by
         the variables FA, FB, \ldots FZ.  Values for those
         sub-expressions and constants which appear in the expression
         will be requested via the application's parameter of the same
         name.

         FORTRAN 77 syntax is used for specifying the expression, which
         may contain the usual intrinsic functions, plus a few extra
         ones.  An appendix in SUN/61 gives a full description of the
         syntax used and an up-to-date list of the functions available.
         The arithmetic operators ($+$,-,/,$*$,$*$$*$) follow the normal order
         of precedence.  Using matching (nested) parentheses will
         explicitly define the order of expression evaluation.  The
         expression may be up to 132 characters long.
      }
      \sstsubsection{
         FA-FZ = LITERAL (Read)
      }{
         These parameters supply the values of `sub-expressions' used
         in the expression EXP.  Any of the 26 may appear; there is no
         restriction on order.  These parameters should be used when
         repeated expressions are present in complex expressions, or to
         shorten the value of EXP to fit within the 132-character limit.
         Sub-expressions may contain references to other
         sub-expressions and constants (PA-PZ).  An example of using
         sub-expressions is:
         \begin{description}
         \item EXP $>$ {\tt PA$*$ASIND(FA/PA)$*$X/FA}
         \item FA $>$ {\tt SQRT(X$*$X$+$Y$*$Y)}
         \item PA $>$ {\tt 10.1}
         \end{description}
         where the parameter name is to the left of $>$ and its value is
         to the right of the $>$.
      }
      \sstsubsection{
         PA-PZ = \_DOUBLE (Read)
      }{
         These parameters supply the values of constants used in the
         expression EXP and sub-expressions FA-FZ.  Any of the 26 may
         appear; there is no restriction on order.  Using parameters
         allows the substitution of repeated constants using one
         reference.  This is especially convenient for constants with
         many significant digits.  It also allows easy modification of
         parameterised expressions provided the application has not
         been used with a different EXP in the interim.  The parameter
         PI has a default value of 3.14159265359D0.  An example of
         using parameters is:
         \begin{description}
         \item EXP $>$ {\tt SQRT(PX$*$PX$+$PY$*$PY)$*$EXP(PX-PY)}
         \item PX $>$ {\tt 2.345}
         \item PY $>$ {\tt -0.987}
         \end{description}
         where the parameter name is to the left of $>$ and its value is
         to the right of the $>$.
      }
      \sstsubsection{
         PREC = LITERAL (Read)
      }{
         The arithmetic precision with which the transformation
         functions will be evaluated when used.  This may be either
         {\tt "\_REAL"} for single precision, {\tt "\_DOUBLE"} for double precision,
         or {\tt "\_INTEGER"} for integer precision.  Elastic precisions are
         used, such that a higher precision will be used if the input
         data warrant it.  So for example if PREC = {\tt "\_REAL"}, but
         double-precision data were to be transformed, double-precision
         arithmetic would actually be used.  The result is reported
         using the chosen precision.  {\tt ["\_REAL"]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         RESULT = LITERAL (Write)
      }{
         The result of the evaluation.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
      {\rm Shell usage:}
      }{
      The syntax in the following examples apply to the shell.
      }
      \sstexamplesubsection{
         calc "27.3$*$1.26"
      }{
         The reports the value of the expression 27.3$*$1.26, {\em i.e.}\ 34.398.
      }
      \sstexamplesubsection{
         calc exp="(pa$+$pb$+$pc$+$pd)/4.0" pa=\$med1 pb=\$med2 pc=\$med3 pd=\$med4
      }{
         This reports the average of four values defined by script
         variables med1, med2, med3, and med4.
      }
      \sstexamplesubsection{
         calc "42.6$*$pi/180"
      }{
         This reports the value in radians of 42.6 degrees.
      }
      \sstexamplesubsection{
         calc "(mod(PO,3)$+$1)/2" prec=\_integer po=\$count
      }{
         This reports the value of the expression
         {\tt "(mod(\$count,3)$+$1)/2)"}
         where {\tt \$count} is the value of the shell variable count.  The
         calculation is performed in integer arithmetic, thus if
         count equals 2, the result is 1 not 1.5.
      }
      \sstexamplesubsection{
         calc "sind(pa/fa)$*$fa" fa="log(abs(pb$+$pc))" pa=2.0e-4 pb=-1 pc=\$x
      }{
         This evaluates sind(0.0002/log(abs(\$x$-$1)))$*$log(abs(\$x$-$1)) where
         {\tt \$x} is the value of the shell variable x.
      }
      \sstexamplesubsection{
      {\rm {\footnotesize ICL} usage:}
      }{
      For {\footnotesize ICL} usage only those expressions containing
      parentheses need to be in quotes, though {\footnotesize ICL}
      itself provides the arithmetic.  So the above examples would be
      }
      \sstexamplesubsection{
         calc 27.3$*$1.26
      }{
         The reports the value of the expression 27.3$*$1.26, {\em i.e.}\ 34.398.
      }
      \sstexamplesubsection{
         calc exp="(pa$+$pb$+$pc$+$pd)/4.0" pa=(med1) pb=(med2) pc=(med3) pd=(med4)
      }{
         This reports the average of four values defined by {\footnotesize ICL}
         variables med1, med2, med3, and med4.
      }
      \sstexamplesubsection{
         calc 42.6$*$pi/180
      }{
         This reports the value in radians of 42.6 degrees.
      }
      \sstexamplesubsection{
         calc "(mod(PO,3)$+$1)/2" prec=\_integer po=(count)
      }{
         This reports the value of the expression
         {\tt "(mod((count),3)$+$1)/2)"}
         where {\tt (count)} is the value of the {\footnotesize ICL}
         variable count.  The calculation is performed in integer
         arithmetic, thus if count equals 2, the result is 1 not 1.5.
      }
      \sstexamplesubsection{
         calc "sind(pa/fa)$*$fa" fa="log(abs(pb$+$pc))" pa=2.0e-4 pb=-1 pc=(x)
      }{
         This evaluates sind(0.0002/log(abs((x)$-$1)))$*$log(abs((x)$-$1)) where
         {\tt (x)} is the value of the {\footnotesize ICL} variable x.
      }
   }
   \sstimplementationstatus{
      On OSF/1 systems an error during the calculation results in a
      core dump.  On Solaris, undefined values are set to one.  These
      are due to problems with the TRANSFORM infrastructure.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: MATHS.
   }
}

\sstroutine{
   CALPOL
}{
   Calculates polarisation parameters
}{
   \sstdescription{
      This routine calculates various parameters describing the
      polarisation described by four intensity arrays analysed at 0\dgs,
      45\dgs, 90\dgs, and 135\dgs\ to a reference direction.  Variance
      values are
      stored in the output NDFs if all the input NDFs have variances and
      you give a true value for parameter VARIANCE.

      By default, three output NDFs are created holding percentage
      polarisation, polarisation angle and total intensity. However,
      NDFs holding other quantities, such as the Stokes parameters, can
      also be produced by over-riding the default null values
      associated with the corresponding parameters.  The creation of any
      output NDF can be suppressed by supplying a null value for the
      corresponding parameter.

      There is an option to correct the calculated values of percentage
      polarisation and polarised intensity to take account of the
      statistical bias introduced by the asymmetric distribution of
      percentage polarisation (see parameter DEBIAS).  This correction
      subtracts the variance of the percentage polarisation from the
      squared percentage polarisation, and uses the square root of this
      as the corrected percentage polarisation.  The corresponding
      polarised intensity is then found by multiplying the corrected
      percentage polarisation by the total intensity.  Returned variance
      values take no account of this correction.
   }
   \sstusage{
      calpol in1 in2 in3 in4 p theta i
   }
   \sstparameters{
      \sstsubsection{
         DEBIAS = \_LOGICAL (Read)
      }{
         {\tt TRUE} if a correction for statistical bias is to be made to
         percentage polarisation and polarised intensity.  This
         correction cannot be used if any of the input NDFs do not
         contain variance values, or if you supply a {\tt FALSE} value
         for parameter VARIANCE. {\tt [FALSE]}
      }
      \sstsubsection{
         I = NDF (Write)
      }{
         An output NDF holding the total intensity derived from all four
         input NDFs.
      }
      \sstsubsection{
         IN1 = NDF (Read)
      }{
         An NDF holding the measured intensity analysed at an angle of
         0\dgs\ to the reference direction.  The primary input NDF.
      }
      \sstsubsection{
         IN2 = NDF (Read)
      }{
         An NDF holding the measured intensity analysed at an angle of
         45\dgs\ to the reference direction.  The suggested default
         is the current value.
      }
      \sstsubsection{
         IN3 = NDF (Read)
      }{
         An NDF holding the measured intensity analysed at an angle of
         90\dgs\ to the reference direction.  The suggested default
         is the current value.
      }
      \sstsubsection{
         IN4 = NDF (Read)
      }{
         An NDF holding the measured intensity analysed at an angle of
         135\dgs\ to the reference direction.  The suggested default
         is the current value.
      }
      \sstsubsection{
         IA = NDF (Write)
      }{
         An output NDF holding the total intensity derived from input
         NDFs IN1 and IN3. {\tt [!]}
      }
      \sstsubsection{
         IB = NDF (Write)
      }{
         An output NDF holding the total intensity derived from input
         NDFs IN2 and IN4. {\tt [!]}
      }
      \sstsubsection{
         IP = NDF (Write)
      }{
         An output NDF holding the polarised intensity. {\tt [!]}
      }
      \sstsubsection{
         P = NDF (Write)
      }{
         An output NDF holding percentage polarisation.
      }
      \sstsubsection{
         Q = NDF (Write)
      }{
         An output NDF holding the normalised Stokes parameter, $Q$. {\tt [!]}
      }
      \sstsubsection{
         U = NDF (Write)
      }{
         An output NDF holding the normalised Stokes parameter, $U$. {\tt [!]}
      }
      \sstsubsection{
         THETA = NDF (Write)
      }{
         An output NDF holding the polarisation angle in degrees.
      }
      \sstsubsection{
         VARIANCE = \_LOGICAL (Read)
      }{
         {\tt TRUE} if output variances are to be calculated.  This parameter
         is only accessed if all input NDFs contain variances, otherwise
         no variances are generated.  {\tt [TRUE]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         calpol m51\_0 m51\_45 m51\_90 m51\_135 m51\_p m51\_t m51\_i ip=m51\_ip
      }{
         This example produces NDFs holding percentage polarisation,
         polarisation angle, total intensity and polarised intensity,
         based on the four NDFs M51\_0, m51\_45, m51\_90 and m51\_135.
      }
      \sstexamplesubsection{
         calpol m51\_0 m51\_45 m51\_90 m51\_135 m51\_p m51\_t m51\_i ip=m51\_ip novariance
      }{
         As above except that variance arrays are not computed.
      }
      \sstexamplesubsection{
         calpol m51\_0 m51\_45 m51\_90 m51\_135 m51\_p m51\_t m51\_i ip=m51\_ip
      }{
         As the first example except that there is a correction for
         statistical bias in the percentage polarisation and polarised
         intensity, assuming that all the input NDFs have a VARIANCE
         array.
      }
      \sstexamplesubsection{
         calpol m51\_0 m51\_45 m51\_90 m51\_135 q=m51\_q p=m51\_p
      }{
         This example produces NDFs holding the Stokes $Q$ and $U$
         parameters, again based on the four NDFs M51\_0, m51\_45, m51\_90
         and m51\_135.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         A bad value will appear in the output data and variance arrays
         when any of the four input data values is bad, or if the total
         intensity in the pixel is not positive.  The output variance
         values are also undefined when any of the four input variances is
         bad or negative, or any computed variance is not positive, or
         the percentage polarisation is not positive.

         \sstitem
         If the four input NDFs have different pixel-index bounds, then
         they will be trimmed to match before being added.  An error will
         result if they have no pixels in common.

         \sstitem
         The output NDFs are deleted if there is an error during the
         formation of the polarisation parameters.

         \sstitem
         The output NDFs obtain their QUALITY, AXIS information, and
         TITLE from the IN1 NDF.  The following labels and units are also
         assigned:

         \begin{tabular}[h]{lll}
            I &  {\tt "Total Intensity"} &       UNITS of IN1 \\
            IA & {\tt "Total Intensity"} &       UNITS of IN1 \\
            IB & {\tt "Total Intensity"} &       UNITS of IN1 \\
            IP & {\tt "Polarised Intensity"} &   UNITS of IN1 \\
            P &  {\tt "Percentage Polarisation"} & {\tt "\%"} \\
            Q &  {\tt "Stokes Q"} &              --- \\
            U &  {\tt "Stokes U"} &              --- \\
            THETA & {\tt "Polarisation Angle"} & {\tt "Degrees"} \\
         \end{tabular}
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: VECPLOT; IRCAMPACK: POLCAL, POLMAPC, POLMAPD, POLSKY, \linebreak
      POLSMOOTH, POLZAP; TSP.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY, VARIANCE,
         LABEL, TITLE, UNITS, and HISTORY components of the input NDF
         and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using single-precision floating point.

      }
   }
}
\sstroutine{
   CDIV
}{
   Divides an NDF by a scalar
}{
   \sstdescription{
      This application divides each pixel of an NDF by a scalar
      (constant) value to produce a new NDF.
   }
   \sstusage{
      cdiv in scalar out
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         Input NDF structure whose pixels are to be divided by a
         scalar.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF structure.
      }
      \sstsubsection{
         SCALAR = \_DOUBLE (Read)
      }{
         The value by which the NDF's pixels are to be divided.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         A title for the output NDF.  A null value will cause the title
         of the NDF supplied for parameter IN to be used instead.
         {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         cdiv a 100.0 b
      }{
         Divides all the pixels in the NDF called a by the constant
         value 100.0 to produce a new NDF called b.
      }
      \sstexamplesubsection{
         cdiv in=data1 out=data2 scalar=-38
      }{
         Divides all the pixels in the NDF called data1 by $-$38 to give
         data2.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ADD, CADD, CMULT, CSUB, DIV, MATHS, MULT, SUB.
   }
   \sstimplementationstatus{

      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, UNITS, HISTORY, and VARIANCE components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.
         Arithmetic is carried out using the appropriate floating-point
         type, but the numeric type of the input pixels is preserved in
         the output NDF.
      }
   }
}
 
\sstroutine{
   CENTROID
}{
   Finds the centroids of star-like features in an NDF
}{
   \sstdescription{
      This routine takes an NDF and returns the co-ordinates of the
      centroids of features in its data array given approximate initial
      co-ordinates.  A feature is a set of connected pixels which are
      above or below the surrounding background region.  For example, a
      feature could be a star or galaxy on the sky.  The centroid is
      obtained iteratively from computing marginal profiles in a
      search region about the initial position, and subtracting the
      background.  The loop is repeated up to a maximum number of
      iterations, though it normally terminates when a desired accuracy
      has been achieved.  Typically, for stars better than 0.1 pixel
      is readily attainable, but the accuracy is affected by noise,
      non-Gaussian and overlapping features.  The error in the centroid
      position may be estimated by a Monte-Carlo method using the
      data variance to generate realisations of the data about the
      feature.  Each realisation is processed identically to the actual
      data, and statistics are formed to derive the standard deviations.

      Positions may be expressed in either pixel or data co-ordinates.
      Three methods are available for obtaining the initial positions.

      \begin{enumerate}
      \item From the parameter system, usually in response to prompting.
      \item By a placing a graphics cursor of a nominated device on the
         feature.  To do this the data array must already be
         displayed as an image, or a contour or line plot, and the
         picture stored in the graphics database.
      \item By reading a text file containing a list of co-ordinates
         in free format, one object per record.  There may be
         commentary lines in the file beginning with '{\tt \#}' or
         '{\tt !}'.
      \end{enumerate}

      In the first two modes the application loops asking for new
      feature co-ordinates until told to quit or encounters an error.
      Features within NDFs of dimension 1 to 7 may be located, except
      in the second method, where the graphics interface limits
      constrain the NDF to be one- or two-dimensional---the most common
      cases.

      The results may optionally be written to a log file that includes
      details of the input parameters.  This is intended for the human
      reader.  If the centroid positions are to be fed into another
      application requiring a list of co-ordinates, {\it e.g.}\ for photometry,
      a co-ordinate list file may also be written.
   }
   \sstusage{
      centroid ndf [mode] init [search] [maxiter] [maxshift] [toler]
   }
   \sstparameters{
      \sstsubsection{
         CERROR = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, errors in the centroid position will be calculated.
         The input NDF must contain the data's variance in order
         to compute errors. {\tt [FALSE]}
      }
      \sstsubsection{
         COIN =  FILENAME (Read)
      }{
         Name of the text file containing the initial co-ordinates
         of star-like images.   The co-ordinates should be arranged
         in free-format columns $x$ then $y$ then $z$ {\it etc.}, one record per
         image.  The file may contain comment lines with the first
         character \# or !.  (XYlist mode)
      }
      \sstsubsection{
         COOUT =  FILENAME (Read)
      }{
         Name of the text file to contain the centroid co-ordinates
         of the star-like images.  It is a co-ordinate file which can
         be used by other applications, and contains the position of
         one object per record.  If COOUT is null, {\tt !}, no output
         co-ordinate file will be created. {\tt [!]}
      }
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either
         {\tt "World"} or {\tt "Data"}.  If COSYS = {\tt "Data"} the
         input co-ordinates and the
         centroids are to be expressed in data co-ordinates, otherwise
         pixel (world) co-ordinates are used.  The data co-ordinates are
         converted to and from pixel indices via the NDF's axis values;
         if there is no axis information within the NDF, world
         co-ordinates are then used, except in Cursor mode where the
         transformation, if present, is taken from the last DATA
         picture in the graphics database.  If COSYS = {\tt "World"} pixel
         co-ordinates are used throughout.  {\tt [}Current co-ordinate
         system{\tt ]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device whose the cursor is used to select the
         images for which centroids are to be calculated. (Cursor
         mode) {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         INIT()  = \_DOUBLE (Read)
      }{
         Guess co-ordinates of the feature to be centroided.  The
         co-ordinates must lie within the bounds of the NDF.  If
         the initial co-ordinates are supplied on the command line
         only one centroid will be found; otherwise the application
         will ask for further guesses, which may be terminated by
         supplying the null value ({\tt !}). (Interface mode)
      }
      \sstsubsection{
         LOGFILE  =  FILENAME (Read)
      }{
         Name of the text file to log the results.  If null, there
         will be no logging.  Note this is intended for the human reader
         and is not intended for passing to other applications. {\tt [!]}
      }
      \sstsubsection{
         MARK = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, the point selected by the cursor will be marked by a
         cross. (Cursor mode) {\tt [FALSE]}
      }
      \sstsubsection{
         MAXITER  =  \_INTEGER (Read)
      }{
         Maximum number of iterations to be used in the search.  It must
         be in the range 1--9.  The dynamic default is 3. {\tt [9]}
      }
      \sstsubsection{
         MAXSHIFT()  =  \_REAL (Read)
      }{
         Maximum shift in each dimension allowed between the guess and
         output positions in pixels.  Each must lie in the range
         0.0--26.0.  If only a single value is given, then it will be
         duplicated to all dimensions. The dynamic default is half of
         SEARCH $+$ 1. {\tt [9.0]}
      }
      \sstsubsection{
         MODE  =  LITERAL (Read)
      }{
         The mode in which the initial co-ordinates are to be obtained.
         {\tt "Interface"} means from the parameter system, {\tt "Cursor"}
         enables selection by graphics cursor, and {\tt "File"} reads
         them from a text file. {\tt [}Current interaction mode{\tt ]}
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         The NDF structure containing the data array to be analysed.
      }
      \sstsubsection{
         NSIM =  \_INTEGER (Read)
      }{
         The number of simulations or realisations using the variance
         information in order to estimate the error in the centroid
         position.  The uncertainty in the centroid error decreases
         as one over the square root of NSIM. The range of acceptable
         values is 3--10000. {\tt [100]}
      }
      \sstsubsection{
         POSITIVE  =  \_LOGICAL (Read)
      }{
         {\tt TRUE} if array features are positive above the background.
         {\tt [TRUE]}
      }
      \sstsubsection{
         SEARCH()  =  \_INTEGER (Read)
      }{
         Size in pixels of the search box to be used. If only a single
         value is given, then it will be duplicated to all dimensions
         so that a square, cube or hypercube region is searched.
         Each value must be odd and lie in the range 3--51.  {\tt [9]}
      }
      \sstsubsection{
         TOLER  =  \_REAL (Read)
      }{
         Accuracy in pixels required in centroiding.  Iterations will
         stop when the shift between successive centroid positions
         is less than the accuracy.  The accuracy must lie in the range
         0.0--2.0. {\tt [0.05]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         CENTRE()  = \_DOUBLE (Write)
      }{
         Co-ordinates of the centroid of the feature.  Note
         that only the last centroid is recorded in this parameter.
      }
      \sstsubsection{
         XCEN  =  \_DOUBLE (Write)
      }{
         $x$ co-ordinate of the centroid of the star-like feature.  Note
         that only the last centroid is recorded in this parameter.
         This provided in addition to CENTRE for convenience until
         {\footnotesize ICL} permits arrays.
      }
      \sstsubsection{
         YCEN  =  \_DOUBLE (Write)
      }{
         $y$ co-ordinate of the centroid of the star-like feature.  Note
         that only the last centroid is recorded in this parameter.
         This provided in addition to CENTRE for convenience until
         {\footnotesize ICL} permits arrays.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         centroid cluster cu
      }{
         This finds the centroids in the NDF called cluster via the
         graphics cursor on the current graphics device.
      }
      \sstexamplesubsection{
         centroid cluster cu search=21
      }{
         This finds the centroids in the NDF called cluster via the
         graphics cursor on the current graphics device.  The search
         box for the centroid is 21 pixels in each dimension.
      }
      \sstexamplesubsection{
         centroid cluster i [21.7,5007.1] cosys=d
      }{
         This finds the centroid of the object in the 2-dimensional NDF called
         cluster around the data co-ordinate (21.7,5007.1).
      }
      \sstexamplesubsection{
         centroid arp244(6,,) i [40,30] cosys=w toler=0.01
      }{
         This finds the 2-dimensional centroid of the feature near pixel
         (6,40,30) in the 3-dimensional NDF called arp244 via the
         graphics cursor on the current graphics device.  The centroid
         must be found to 0.01 pixels.
      }
      \sstexamplesubsection{
         centroid cluster i [40,30] cosys=w xcen=(xp) ycen=(yp)
      }{
         This finds the centroid of the object in the 2-dimensional NDF
         called cluster around the pixel co-ordinate (40.0,30.0).  The
         centroid co-ordinates are written to {\footnotesize ICL}
         variables XP and YP for use in other applications.
      }
      \sstexamplesubsection{
         centroid cluster mode=xy coin=objects.dat logfile=centroids.log
      }{
         This finds the centroids in the NDF called cluster.  The
         initial positions are given in the text file {\tt objects.dat} in
         the current co-ordinate system.  A log of the input parameter
         values, initial and centroid positions is written to the text
         file {\tt centroids.log}.
      }
      \sstexamplesubsection{
         centroid cluster mode=xy coin=objects.dat coout=centres.dat
      }{
         As above, except there is no logfile; instead a co-ordinate
         file called centres.dat is generated.
      }
      \sstexamplesubsection{
         centroid cluster i [40,30] cosys=w nopositive
      }{
         This finds the centroid of the object in the 2-dimensional NDF called
         cluster around the pixel co-ordinate (40.0,30.0).  The feature
         must have values below that of the surrounding background.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PSF.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         The processing of bad pixels and all non-complex numeric types
         is supported.

         \sstitem
         Data co-ordinates are processed in double precision.  When data
         co-ordinates are being used, and the cursor mode is selected or
         the NDF contains axis information, the results appear in double
         precision.  Double precision arithmetic is needed to prevent
         a loss of precision for certain co-ordinate transformations stored
         within the graphics database or for double-precision axis centres.

         \sstitem
         The format of the logfile is different for 2-dimensional from other
         dimensions for historical reasons.  The 2-dimensional has a tabular layout
         with headings.  Logging awaits a proper table system to make this
         consistent.
      }
   }
}

\sstroutine{
   CHPIX
}{
   Replaces the values of selected pixels in an NDF
}{
   \sstdescription{
      This application replaces selected elements of an NDF array
      component with specified values.  The task loops until there are
      no more elements to change, indicated by a null value in response
      to a prompt.  For non-interactive processing, supply the value of
      parameter NEWVAL on the command line.
   }
   \sstusage{
      chpix in out section newval [comp]
   }
   \sstparameters{
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The name of the NDF array component to be modified.  The
         options are: {\tt "Data"}, {\tt "Error"}, {\tt "Quality"} or
         {\tt "Variance"}. {\tt "Error"} is the alternative to
         {\tt "Variance"} and causes the
         square of the supplied replacement value to be stored in the
         output VARIANCE array.
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         NDF structure containing the array component to be modified.
      }
      \sstsubsection{
         NEWVAL = LITERAL (Read)
      }{
         Value to substitute in the output array element or elements.
         The range of allowed values depends on the data type of the
         array being modified.  NEWVAL={\tt "Bad"} instructs that the bad
         value appropriate for the array data type be substituted.
         Placing NEWVAL on the command line permits only one section
         to be replaced.  If there are multiple replacements, a null
         value ({\tt !}) terminates the loop.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF structure containing the modified version of
         the array component.
      }
      \sstsubsection{
         SECTION = LITERAL (Read)
      }{
         The elements to change.  This is defined as an NDF section, so
         that ranges can be defined along any axis, and be given as
         pixel indices or axis (data) co-ordinates.  So for example
         {\tt "3,4,5"} would select the pixel at (3,4,5); {\tt "3:5,"} would
         replace all elements in columns 3 to 5; {\tt ",4"} replaces line 4.
         See {\tt "}NDF sections{\tt "} in SUN/95, or the online documentation for
         details.  A null value ({\tt !}) terminates the loop during multiple
         replacements.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the input NDF to the output NDF.
         {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         chpix rawspec spectrum 55 100
      }{
         Assigns the value 100 to the pixel at index 55 within the
         one-dimensional NDF called rawspec, creating the output NDF
         called spectrum.
      }
      \sstexamplesubsection{
         chpix rawspec spectrum 10:19 0 error
      }{
         Assigns the value 0 to the error values at indices 10 to 19
         within the one-dimensional NDF called rawspec, creating the
         output NDF called spectrum.  The rawspec dataset must have a
         variance compoenent.
      }
      \sstexamplesubsection{
         chpix in=rawimage out=galaxy section="$\sim$20,100:109" newval=bad
      }{
         Assigns the bad value to the pixels in the section $\sim$20,100:109
         within the two-dimensional NDF called rawimage, creating the
         output NDF called galaxy.  This section is the central
         20 pixels along the first axis, and pixels 110 to 199 along the
         second.
      }
      \sstexamplesubsection{
         chpix in=zzcha out=zzcha\_c section="45,21," newval=-1
      }{
         Assigns value $-$1 to the pixels at index (45,~21) within all
         planes of the three-dimensional NDF called zzcha, creating
         the output NDF called zzcha\_c.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ARDMASK, FILLBAD, GLITCH, NOMAGIC, SEGMENT, SETMAGIC, SUBSTITUTE, ZAPLIN;
      Figaro: CSET, ICSET, NCSET, TIPPEX.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         The routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF;
         and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.

      }
   }
}
\sstroutine{
   CMULT
}{
   Multiplies an NDF by a scalar
}{
   \sstdescription{
      This application multiplies each pixel of an NDF by a scalar
      (constant) value to produce a new NDF.
   }
   \sstusage{
      cmult in scalar out
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         Input NDF structure whose pixels are to be multiplied by a
         scalar.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF structure.
      }
      \sstsubsection{
         SCALAR = \_DOUBLE (Read)
      }{
         The value by which the NDF's pixels are to be multiplied.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         A title for the output NDF.  A null value will cause the title
         of the NDF supplied for parameter IN to be used instead.
         {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         cmult a 12.5 b
      }{
         Multiplies all the pixels in the NDF called a by the constant
         value 12.5 to produce a new NDF called b.
      }
      \sstexamplesubsection{
         cmult in=rawdata out=newdata scalar=-19
      }{
         Multiplies all the pixels in the NDF called rawdata by $-$19 to
         give newdata.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ADD, CADD, CDIV, CSUB, DIV, MATHS, MULT, SUB.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, UNITS, HISTORY, and VARIANCE components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.
         Arithmetic is carried out using the appropriate floating-point
         type, but the numeric type of the input pixels is preserved in
         the output NDF.
      }
   }
}
\sstroutine{
   COMPADD
}{
   Reduces the size of an NDF by adding values in rectangular boxes
}{
   \sstdescription{
      This application takes an NDF data structure and reduces it in
      size by integer factors along each dimension.  The compression
      is achieved by adding the values of the input NDF within
      non-overlapping `rectangular' boxes whose dimensions are the
      compression factors.  The additions may be normalised to correct
      for any bad values present in the input NDF.
   }
   \sstusage{
      compadd in out compress [wlim]
   }
   \sstparameters{
      \sstsubsection{
         AXWEIGHT = \_LOGICAL (Read)
      }{
         When there is an AXIS variance array present in the NDF and
         AXWEIGHT={\tt TRUE} the application forms weighted averages of the
         axis centres using the variance.  For all other conditions
         the non-bad axis centres are given equal weight during the
         averaging to form the output axis centres. {\tt [FALSE]}
      }
      \sstsubsection{
         COMPRESS( ) = \_INTEGER (Read)
      }{
         Linear compression factors to be used to create the output
         NDF.  There should be one for each dimension of the NDF.  If
         fewer are supplied the last value in the list of compression
         factors is given to the remaining dimensions.  Thus if a
         uniform compression is required in all dimensions, just one
         value need be entered.  All values are constrained to be in
         the range one to the size of its corresponding dimension.  The
         suggested default is the current value.
      }
      \sstsubsection{
         IN  = NDF (Read)
      }{
         The NDF structure to be reduced in size.
      }
      \sstsubsection{
         NORMAL = \_LOGICAL (Read)
      }{
         When there are bad pixels present in the summation box these
         are ignored.  Therefore a simple addition of the input-array
         component's values will yield a result discordant with
         neighbouring output pixels that were formed from summation of
         all the pixels in the box.  When NORMAL={\tt TRUE} the output values
         are normalised: the addition is multiplied by the ratio of the
         number of pixels in the box to the number of good pixels
         therein to arrive at the output value.  When NORMAL={\tt FALSE} the
         output values are always just the sum of the good pixels.
         {\tt [TRUE]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         NDF structure to contain compressed version of the input NDF.
      }
      \sstsubsection{
         PRESERVE = \_LOGICAL (Read)
      }{
         If the input data type is to be preserved on output then this
         parameter should be set true.   However, this may result in
         overflows for integer types and hence additional bad values
         written to the output NDF.  If this parameter is set false
         then the output data type will be one of \_REAL or \_DOUBLE,
         depending on the input type. {\tt [FALSE]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the input NDF to the output NDF. {\tt [!]}
      }
      \sstsubsection{
         WLIM = \_REAL (Read)
      }{
         If the input NDF contains bad pixels, then this parameter
         may be used to determine the number of good pixels which must
         be present within the addition box before a valid output
         pixel is generated.  It can be used, for example, to prevent
         output pixels from being generated in regions where there are
         relatively few good pixels to contribute to the smoothed
         result.

         WLIM specifies the minimum fraction of good pixels which must
         be present in the summation box in order to generate a good
         output pixel.  If this specified minimum fraction of good
         input pixels is not present, then a bad output pixel will
         result, otherwise the output value will be the sum of the
         good values.  The value of this parameter should lie between
         0.0 and 1.0 (the actual number used will be rounded up if
         necessary to correspond to at least 1 pixel). {\tt [0.3]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         compadd cosmos galaxy 4
      }{
         This compresses the NDF called cosmos summing four times in
         each dimension, and stores the reduced data in the NDF called
         galaxy.  Thus if cosmos is two-dimensional, this command
         would result in a sixteen-fold reduction in the array
         components.
      }
      \sstexamplesubsection{
         compadd cosmos galaxy 4 wlim=1.0
      }{
         This compresses the NDF called cosmos adding four times in
         each dimension, and stores the reduced data in the NDF called
         galaxy.  Thus if cosmos is two-dimensional, this command
         would result in a sixteen-fold reduction in the array
         components.  If a summation box contains any bad pixels, the
         output pixel is set to bad.
      }
      \sstexamplesubsection{
         compadd cosmos galaxy 4 0.0 preserve
      }{
         As above except that a summation box need only contains a
         single non-bad pixels for the output pixel to be good, and
         galaxy's array components will have the same as those in
         cosmos.
      }
      \sstexamplesubsection{
         compadd cosmos galaxy [4,3] nonormal title="COSMOS compressed"
      }{
         This compresses the NDF called cosmos adding four times in
         the first dimension and three times in higher dimensions, and
         stores the reduced data in the NDF called galaxy.  Thus if
         cosmos is two-dimensional, this command would result in a
         twelve-fold reduction in the array components.  Also, if there
         are bad pixels there will be no normalistion correction for the
         missing values.  The title of the output NDF is {\tt "}COSMOS
         compressed{\tt "}.
      }
      \sstexamplesubsection{
         compadd in=arp244 compress=[1,1,3] out=arp244cs
      }{
         Suppose arp244 is a huge NDF storing a spectral-line data
         cube, with the third dimension being the spectral axis.
         This command compresses arp244 in the spectral dimension,
         adding every three pixels to form the NDF called arp244cs.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The axis centres and variances are averaged, whilst the widths
         are summed and always normalised for bad values.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: BLOCK, COMPAVE, COMPICK, PIXDUPE, SQORST, TRANSFORMER;
      Figaro: ISTRETCH, YSTRACT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, VARIANCE,
         LABEL, TITLE, UNITS, and HISTORY components of the input NDF
         and propagates all extensions.  QUALITY is not
         processed since it is a series of flags, not numerical values.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         Any number of NDF dimensions is supported.

      }
   }
}
\sstroutine{
   COMPAVE
}{
   Reduces the size of an NDF by averaging values in rectangular
   boxes
}{
   \sstdescription{
      This application takes an NDF data structure and reduces it in
      size by integer factors along each dimension.  The compression
      is achieved by averaging the input NDF within non-overlapping
      `rectangular' boxes whose dimensions are the compression factors.
      The averages may be weighted when there is a variance array
      present.
   }
   \sstusage{
      compave in out compress [wlim]
   }
   \sstparameters{
      \sstsubsection{
         AXWEIGHT = \_LOGICAL (Read)
      }{
         When there is an AXIS variance array present in the NDF and
         AXWEIGHT={\tt TRUE} the application forms weighted averages of the
         axis centres using the variance.  For all other conditions
         the non-bad axis centres are given equal weight during the
         averaging to form the output axis centres. {\tt [FALSE]}
      }
      \sstsubsection{
         COMPRESS( ) = \_INTEGER (Read)
      }{
         Linear compression factors to be used to create the output
         NDF.  There should be one for each dimension of the NDF.  If
         fewer are supplied the last value in the list of compression
         factors is given to the remaining dimensions.  Thus if a
         uniform compression is required in all dimensions, just one
         value need be entered.  All values are constrained to be in
         the range one to the size of its corresponding dimension.  The
         suggested default is the current value.
      }
      \sstsubsection{
         IN  = NDF (Read)
      }{
         The NDF structure to be reduced in size.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         NDF structure to contain compressed version of the input NDF.
      }
      \sstsubsection{
         PRESERVE = \_LOGICAL (Read)
      }{
         If the input data type is to be preserved on output then this
         parameter should be set true.   However, this will probably
         result in a loss of precision.  If this parameter is set false
         then the output data type will be one of \_REAL or \_DOUBLE,
         depending on the input type. {\tt [FALSE]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the input NDF to the output NDF. {\tt [!]}
      }
      \sstsubsection{
         WEIGHT = \_LOGICAL (Read)
      }{
         When there is a variance array present in the NDF and
         WEIGHT={\tt TRUE} the application forms weighted averages of the
         data array using the variance.  For all other conditions
         the non-bad pixels are given equal weight during averaging.
         {\tt [FALSE]}
      }
      \sstsubsection{
         WLIM = \_REAL (Read)
      }{
         If the input NDF contains bad pixels, then this parameter
         may be used to determine the number of good pixels which must
         be present within the averaging box before a valid output
         pixel is generated.  It can be used, for example, to prevent
         output pixels from being generated in regions where there are
         relatively few good pixels to contribute to the smoothed
         result.

         WLIM specifies the minimum fraction of good pixels which must
         be present in the averaging box in order to generate a good
         output pixel.  If this specified minimum fraction of good
         input pixels is not present, then a bad output pixel will
         result, otherwise an averaged output value will be calculated.
         The value of this parameter should lie between 0.0 and 1.0
         (the actual number used will be rounded up if necessary to
         correspond to at least 1 pixel). {\tt [0.3]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         compave cosmos galaxy 4
      }{
         This compresses the NDF called cosmos averaging four times in
         each dimension, and stores the reduced data in the NDF called
         galaxy.  Thus if cosmos is two-dimensional, this command
         would result in a sixteen-fold reduction in the array
         components.
      }
      \sstexamplesubsection{
         compave cosmos galaxy 4 wlim=1.0
      }{
         This compresses the NDF called cosmos averaging four times in
         each dimension, and stores the reduced data in the NDF called
         galaxy.  Thus if cosmos is two-dimensional, this command
         would result in a sixteen-fold reduction in the array
         components.  If an averaging box contains any bad pixels, the
         output pixel is set to bad.
      }
      \sstexamplesubsection{
         compave cosmos galaxy 4 0.0 preserve
      }{
         As above except that an averaging box need only contains a
         single non-bad pixels for the output pixel to be good, and
         galaxy's array components will have the same as those in
         cosmos.
      }
      \sstexamplesubsection{
         compave cosmos galaxy [4,3] weight title="COSMOS compressed"
      }{
         This compresses the NDF called cosmos averaging four times in
         the first dimension and three times in higher dimensions, and
         stores the reduced data in the NDF called galaxy.  Thus if
         cosmos is two-dimensional, this command would result in a
         twelve-fold reduction in the array components.  Also, if there
         is a variance array present it is used to form weighted means
         of the data array.   The title of the output NDF is {\tt "}COSMOS
         compressed{\tt "}.
      }
      \sstexamplesubsection{
         compave in=arp244 compress=[1,1,3] out=arp244cs
      }{
         Suppose arp244 is a huge NDF storing a spectral-line data
         cube, with the third dimension being the spectral axis.
         This command compresses arp244 in the spectral dimension,
         averaging every three pixels to form the NDF called arp244cs.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The axis centres and variances are averaged, whilst the widths
         are summed and always normalised for bad values.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: BLOCK, COMPADD, COMPICK, PIXDUPE, SQORST, TRANSFORMER;
      Figaro: ISTRETCH.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, VARIANCE,
         LABEL, TITLE, UNITS, and HISTORY components of the input NDF
         and propagates all extensions.  QUALITY is not
         processed since it is a series of flags, not numerical values.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         Any number of NDF dimensions is supported.

      }
   }
}
\sstroutine{
   COMPICK
}{
   Reduces the size of an NDF by picking equally spaced pixels
}{
   \sstdescription{
      This application takes an NDF data structure and reduces it in
      size by integer factors along each dimension.  The input NDF is
      sampled at these constant compression factors or intervals along
      each dimension, starting from a defined origin, to form an output
      NDF structure.  The compression factors may be different in each
      dimension.
   }
   \sstusage{
      compick in out compress [origin]
   }
   \sstparameters{
      \sstsubsection{
         COMPRESS( ) = \_INTEGER (Read)
      }{
         Linear compression factors to be used to create the output
         NDF.  There should be one for each dimension of the NDF.  If
         fewer are supplied the last value in the list of compression
         factors is given to the remaining dimensions.  Thus if a
         uniform compression is required in all dimensions, just one
         value need be entered.  All values are constrained to be in
         the range one to the size of its corresponding dimension.  The
         suggested default is the current value.
      }
      \sstsubsection{
         IN  = NDF (Read)
      }{
         The NDF structure to be reduced in size.
      }
      \sstsubsection{
         ORIGIN( ) = \_INTEGER (Read)
      }{
         The pixel indices of the first pixel to be selected.
         Thereafter the selected pixels will be spaced equally by
         COMPRESS() pixels.  The origin must lie within the first
         selection intervals, therefore the $i^{\rm th}$ origin must be in the
         range LBND($i$) to LBND($i$)$+$COMPRESS($i$)$-$1, where LBND($i$) is the
         lower bound of the $i^{\rm th}$ dimension.  The suggested default is
         the first array element.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         NDF structure to contain compressed version of the input NDF.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the input NDF to the output NDF. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         compick cosmos galaxy 4
      }{
         This compresses the NDF called cosmos selecting every fourth
         array element along each dimension, starting from the first
         element in the NDF, and stores the reduced data in the NDF
         called galaxy.
      }
      \sstexamplesubsection{
         compick cosmos galaxy 4 [3,2]
      }{
         This compresses the two-dimensional NDF called cosmos
         selecting every fourth array element along each dimension,
         starting from the pixel index (3,2), and stores the
         reduced data in the NDF called galaxy.
      }
      \sstexamplesubsection{
         compick in=arp244 compress=[1,1,3] out=arp244cs
      }{
         Suppose arp244 is a huge NDF storing a spectral-line data
         cube, with the third dimension being the spectral axis.
         This command compresses arp244 in the spectral dimension,
         sampling every third pixel, starting from the first wavelength
         at each image position, to form the NDF called arp244cs.
      }
   }
   \sstnotes{
      Each output dimension is calculated to give the most elements
      possible given the compression factor.  It is evaluated from the
      expression (input dimension - \%ORIGIN) / \%COMPRESS + 1.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: BLOCK, COMPADD, COMPAVE, PIXDUPE, SQORST, TRANSFORMER;
      Figaro: ISTRETCH.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY, VARIANCE,
         LABEL, TITLE, UNITS, and HISTORY components of the input NDF
         and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         Any number of NDF dimensions is supported.

      }
   }
}
 
\sstroutine{
   CONTOUR
}{
   Contours a 2-d NDF
}{
   \sstdescription{
      This application contours an image of a 2-dimensional NDF on the current
      graphics device.  The image may be part or whole of the data
      array, but also the variance or quality can be shown.  The plot
      is situated within the current graphics-database picture.

      The contour plot resides within optional, annotated and enumerated
      axes.  An optional, but recommended, key may be drawn to the
      right of the contour plot.  It reports the NDF's units if there
      are any, and only contour heights actually plotted are included.
      There are seven methods for selecting contours.

      The contouring algorithm has single-pixel resolution.
   }
   \sstusage{
      contour ndf [comp] mode ncont [key] [device]
        \newline\hspace*{1.5em}
        $\left\{ {\begin{tabular}{l}
                    firstcnt=? stepcnt=? \\
                    heights=? \\
                    percentiles=?
                   \end{tabular} }
        \right.$
        \newline\hspace*{1.9em}
        \makebox[0mm][c]{\small mode}
   }
   \sstparameters{
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the graphics device is to be cleared before display
         of the array. {\tt [TRUE]}
      }
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The NDF component to be contoured.  It may be {\tt "Data"},
         {\tt "Quality"}, {\tt "Variance"}, or {\tt "Error"} (where
         {\tt "Error"} is the alternative to {\tt "Variance"} and causes
         the square root of the variance values to be taken before
         plotting contours).  If {\tt "Quality"} is specified, then
         the quality values are treated as numerical values (in the
         range 0 to 255).  {\tt ["Data"]}
      }
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either {\tt "World"}
         or {\tt "Data"}.  {\tt "World"} makes pixel co-ordinates to appear on axes.
         If COSYS = {\tt "Data"} the NDF's axis information is used to
         annotate axes.  {\tt [}Current co-ordinate system{\tt ]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The plotting device. {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         FILL = \_LOGICAL (Read)
      }{
         The contour plot normally has square pixels, in other words
         a length along each axis corresponds to the same number of
         pixels.  However, for images with markedly different
         dimensions this default behaviour may not be suitable or give
         the clearest plot.  When FILL is {\tt TRUE}, the square-pixel
         constraint is relaxed and the contour plot is the largest
         possible within the current picture.  When FILL is {\tt FALSE}, the
         pixels are square.  The suggested default is the current
         value.  {\tt [FALSE]}
      }
      \sstsubsection{
         FIRSTCNT = \_REAL (Read)
      }{
         Height of the first contour (Linear and Magnitude modes).
         The suggested value is the current value.
      }
      \sstsubsection{
         HEIGHTS() = \_REAL (Read)
      }{
         Contour levels (Free mode).  The suggested default is the
         current value.
      }
      \sstsubsection{
         KEY = \_LOGICAL (Read)
      }{
         A key of the contour level versus pixel value is to be
         produced. {\tt [TRUE]}
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The method used to select the contour levels.  The options are
         described below.
         \begin{description}
         \item {\tt "Area"} --- The contours enclose areas of the array for
                         which the equivalent radius increases by equal
                         increments.  You specify the number of levels.
         \item {\tt "Automatic"} --- The contour levels are equally spaced between the maximum
                        the maximum and minimum pixel values in the
                        array.  You supply the number of contour levels.
         \item {\tt "Equalised"} --- You define the number of equally spaced
                          percentiles.
         \item {\tt "Free"} --- You specify a series of contour values
                        explicitly.
         \item {\tt "Linear"} --- You define the number of contours, the start
                        contour level and linear step between contours.
         \item {\tt "Magnitude"} --- You define the number of contours, the start
                        contour level and step between contours.  The
                        step size is in magnitudes so the $n^{\rm th}$
                        contour is 10$^{-0.4*(n-1)*{\rm step}}$ times the
                        start contour level.
         \item {\tt "Percentiles"} --- You specify a series of percentiles.
         \end{description}

         The suggested default is the current value, which is initially
         {\tt "Free"}.
      }
      \sstsubsection{
         NCONT = \_INTEGER (Read)
      }{
         The number of contours required (all modes except Free and
         Percentiles).  It must be between 1 and 50.  If the number is
         large, the plot may be cluttered and take longer to produce.
         {\tt 6}, the initial suggested default, gives reasonable results.
         The current value becomes the suggested default.
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         NDF structure containing the 2-dimensional image to be contoured.
      }
      \sstsubsection{
         PERCENTILES() = \_REAL (Read)
      }{
         Contour levels given as percentiles.  The values must lie
         between 0.0 and 100.0. (Percentiles mode).  The suggested
         default is the current value.
      }
      \sstsubsection{
         PXSIZE = \_REAL (Read)
      }{
         The length ($x$ axis) of the plot in metres. {\tt [}Maximum that can
         fit in the current picture whilst preserving square pixels{\tt ]}
      }
      \sstsubsection{
         PYSIZE = \_REAL (Read)
      }{
         The length ($y$ axis) of the plot in metres. {\tt [}Maximum that can
         fit in the current picture whilst preserving square pixels{\tt ]}
      }
      \sstsubsection{
         STATS = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, the LENGTH and NUMBER statistics are computed.
         {\tt [FALSE]}
      }
      \sstsubsection{
         STEPCNT = \_REAL (Read)
      }{
         Separation between contour levels, linear for Linear mode
         and in magnitudes for Magnitude mode.  The suggested value is
         the current value.
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         ABSLAB = LITERAL (Read)
      }{
         Label for the plot abscissa, in which NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  If axis information is present the suggested
         default is the NDF's axis label followed by the units, in
         parentheses.  If an error occurs obtaining the label the
         suggested default is {\tt "X"}. {\tt []}
      }
      \sstsubsection{
         AXES = \_LOGICAL (Read)
      }{
         {\tt TRUE} if labelled and annotated axes are to be drawn around the
         contour plot.  The annotations are either the data
         co-ordinates from the NDF axis components, provided these are
         present and linear and COSYS = {\tt "Data"}; otherwise pixel
         co-ordinates are used.  {\tt [TRUE]}
      }
      \sstsubsection{
         BORDER = \_LOGICAL (Read)
      }{
         BORDER is {\tt TRUE} if a box is to be drawn about the contour plot.
         This is only accessed when there are no axes required.
         {\tt [TRUE]}
      }
      \sstsubsection{
         CONCOL = LITERAL (Read)
      }{
        The colour of the contour lines on devices that support colour.
        The options are described below.

         \begin{description}
         \item {\tt "MAX"}  --- The maximum colour index in the image
                          display colour lookup table.
         \item {\tt "MIN"}  --- The minimum (non-reserved) colour index in
                          the image-display colour lookup table.
         \item {\bf An integer} --- The actual colour index.  It is
                          constrained between 0 and the maximum colour
                          index available on the device.
         \item {\bf A named colour} --- Uses the named colour from the
                          palette, and if it is not present, the nearest
                          colour from the palette is selected.
         \end{description}

         If the colour is to remain unaltered as the lookup table is
         manipulated choose an integer between 0 and 15, or a named
         colour.  This parameter will be ignored if PENROT = {\tt TRUE}.
         {\tt [}The current value, but equals {\tt 1} (the foreground
         colour) if there is no current value.{\tt ]}
      }
      \sstsubsection{
         DASHED = \_REAL (Read)
      }{
         The height below which the contours will be drawn with dashed
         lines.  A null value ({\tt !}) means all contours are drawn with
         solid lines.  {\tt [!]}
      }
      \sstsubsection{
         FONT = LITERAL (Read)
      }{
         The fount to be used for the line graphics.  It can be either
         {\tt "NCAR"} for the NCAR fancy characters and {\tt "GKS"} for the standard
         GKS san-serif fount.  The former is intended for hardcopy
         publication-quality plots, since it is relatively slow; the
         latter is intended for normal interactive graphics requiring
         rapid plotting, and it is clearer on small plots. The
         suggested default is the current value. {\tt ["GKS"]}
      }
      \sstsubsection{
         MAJTIC( 2 ) = \_REAL (Read)
      }{
         The parameter controlling the numbers of major tick marks
         for the $x$ and $y$ axes.  (Number used is between MAJTIC$+$2 and
         5$*$MAJTIC/2$+$4.) {\tt [3.,3.]}
      }
      \sstsubsection{
         MINTIC( 2 ) = \_REAL (Read)
      }{
         The number of minor tick marks between each major tick mark
         for the $x$ and $y$ axes.  A negative value forces the graphics
         package to compute appropriate values. {[-1.,-1.]}
      }
      \sstsubsection{
         ORDLAB = LITERAL (Read)
      }{
         Label for the plot ordinate, in which NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  If axis information is present the suggested
         default is the NDF's axis label followed by the units, in
         parentheses.  If an error occurs obtaining the label the
         suggested default is {\tt "Y"}. {\tt []}
      }
      \sstsubsection{
         OUTTIC = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the axis tick marks are to appear on the outside of
         the axes instead of inside. By default, the tick marks are
         drawn outside the contouring region to eliminate
         intersections of ticks with the contours. {\tt [TRUE]}
      }
      \sstsubsection{
         PENROT = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, the plotting pens are cycled through the contours to
         aid identification of the contour heights.  {\tt [FALSE]}
      }
      \sstsubsection{
         PLTITL = LITERAL (Read)
      }{
         The title of the plot.  Up to about 40 characters can be
         accommodated, and NCAR fancy founts may be embedded
          when FONT = {\tt "NCAR"}. If an error
         occurs obtaining the title, it is defaulted to {\tt "Contour plot"}.
         {\tt [}The NDF title{\tt ]}
      }
      \sstsubsection{
         THICK = \_REAL (Read)
      }{
         The thickness of the axes and annotations in the plot, where
         1.0 is the normal thickness. Currently, this is only available
         on a few devices.  It must take a value in the range 0.5--10.0.
         {\tt [1.0]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         LENGTH() = \_REAL (Write)
      }{
         The total length in pixels of the contours at each selected
         height.  These values are only computed when STATS = {\tt TRUE}.
         A 1-dimensional array of values giving the data co-ordinates of
         the centre of the (first) maximum-valued pixel found in the
         NDF array.
      }
      \sstsubsection{
         NUMBER() = \_INTEGER (Write)
      }{
         The number of closed contours at each selected height.
         Contours are not closed if they intersect a bad pixel or the
         edge of the image.  These values are only computed when
         STATS = {\tt TRUE}.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         contour myfile d $\backslash$
      }{
         Contours the data array in the NDF called myfile on the current
         graphics device.  All other settings are defaulted, so for
         example the current mode for determining heights is used, and
         a key is plotted.
      }
      \sstexamplesubsection{
         contour taurus1(100:199,150:269,4) $\backslash$
      }{
         Contours a 2-dimensional portion of current array component in
         the NDF cube called taurus1 on the current graphics device.
         The portion extends from pixel (100,~150,~4) to pixel
         (199,~269,~4).  All other settings are defaulted, so for
         example, the NDF's title adorns the plot, and a key is
         plotted.
      }
      \sstexamplesubsection{
         contour ngc6872 mode=au ncont=5 device=ps\_l concol=white
      }{
         Contours the data array in the NDF called ngc6872 on the ps\_l
         graphics device.  Five equally spaced contours between the
         maximum and minimum data values are drawn in white.
         A key is plotted.
      }
      \sstexamplesubsection{
         contour ndf=ngc6872 mode=au ncont=5 cosys=w device=ps\_l
      }{
         As above except that the pens are cycled (which will normally
         give rise to various dashed patterns on this device).  The
         axes have pixel co-ordinates.
      }
      \sstexamplesubsection{
         contour ngc6872 mode=li firstcnt=10 stepcnt=2 ncont=4 noaxes
      }{
         Contours the data array in the NDF called ngc6872 on the
         current graphics device.  Four contours at heights 10, 12, 14,
         and 16 are drawn.  A key is plotted, but no axes surround the
         contour plot.
      }
      \sstexamplesubsection{
         contour ss443 mode=pe percentiles=[80,90,95,98,99,99.9] stats
      }{
         Contours the data array in the NDF called ss443 on the current
         graphics device.  Contours at heights corresponding to the 80,
         90, 95, 98, 99, and 99.9 percentiles are drawn in the current
         colour.  A key is plotted.  Contour statistics are computed.
      }
      \sstexamplesubsection{
         contour mode=eq ncont=5 dashed=0 concol=red ndf=skyflux
      }{
         Contours the data array in the NDF called skyflux on the
         current graphics device.  Contours at heights corresponding to
         the 10, 30, 50, 70 and 90 percentiles are drawn in red.  Those
         contours whose values are negative will appear as dashed
         lines.  A key is plotted.
      }
      \sstexamplesubsection{
         contour comp=d nokey penrot $\backslash$
      }{
         Contours the portion of the data array in the current NDF on
         the current graphics device using the current method for
         height selection.  The NDF's title is the plot's title.  No
         key is drawn.  The appearance of the contours cycles every
         third contour.
      }
      \sstexamplesubsection{
         contour comp=v mode=fr heights=[10,20,40,80] title=Variance
      }{
         Contours the variance array in the current NDF on the
         current graphics device.  Contours at 10, 20, 40 and 80 are
         drawn. {\tt "Variance"} is the title of the plot.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The application stores a number of pictures in the graphics
         database in the following order: a FRAME of the specified size
         containing the title, annotated axes, contours and key; a DATA
         picture which is stored with world co-ordinates in units of data
         pixels; and a KEY picture to store the key if present.  The DATA
         picture also may have double-precision data co-ordinates derived
         from the NDF axis components provided these are linear and
         different from pixel co-ordinates; the data co-ordinates are
         stored via a linear transformation.  The NDF associated with the
         plot is stored by reference with the DATA picture.  On exit the
         current database picture for the chosen device reverts to the
         input picture.

         \sstitem
         There are some options for setting the characteristics of the
         contour lines.  By default, solid lines are drawn with the same
         colour as the axes and key, namely the foreground colour.  The
         colour will depend on the graphics device chosen, but it is often
         black for printers or white for terminals.  The alternatives to
         override this default behaviour are listed below.

         \begin{enumerate}
         \item Set a colour for all contours using parameter CONCOL.
         \item Request dashed contours below some threshold given by
               parameter DASHED and solid lines for other heights.  All
               contours have either the foreground colour or that
               prescribed by parameter CONCOL.
         \item Cycle the pens modulo 3 for each contour height actually
               plotted by setting PENROT = {\tt TRUE}.  The characteristics of
               the second and third line styles will depend on the chosen
               graphics device.  An image display or pen plotter will draw
               coloured lines using palette entries 1 to 3; whereas a
               window overlay, or monochrome laser printer or terminal
               will draw a variety of dashed or thicker lines.
         \item Combine options 2 and 3.  However, palette colours 1 to 3
               will always be used and CONCOL ignored.  The contours below
               the threshold continue the cycle through the three colours.
               There may be some confusion on devices that already use
               dashed lines, so this is only suitable for devices
               supporting at least three colours simultaneously.
         \end{enumerate}

         Pen rotation takes precedence over colour control through CONCOL. 
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CONTOVER, TURBOCONT; Figaro: ICONT; SPECDRE: SPECCONT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         Only real data can be processed directly.  Other non-complex
         numeric data types will undergo a type conversion before the
         contour plot is drawn.

         \sstitem
         Bad pixels and automatic quality masking are supported.

      }
   }
}
\begin{htmlonly}
Click \htmladdnormallink{here}{contour_exam1.gif} to see an example
plot with PENROT={\tt TRUE} (11k).
\end{htmlonly}

\newpage 
\sstroutine{
   CONTOVER
}{
   Contours a 2-d NDF overlaid on an image displayed previously
}{
   \sstdescription{
      This application draws a contour plot of a 2-dimensional NDF
      using an efficient algorithm.  The array may be part or whole of
      the data array, but also the variance or quality can be shown.

      The contour plot is drawn over an existing image that is
      displayed on the chosen graphics workstation or its overlay,
      provided the displayed image has been recorded in the
      graphics database.  (This will be the case for other display routines
      in KAPPA.)  The contour plotting occurs within the current picture
      only if it is a DATA picture, otherwise contours are overlaid in
      the last DATA picture within the current picture.  This
      application assumes that the world co-ordinate systems of the data
      array and the displayed image are both in pixel units, but not to
      the same origins.  Pixel $x$-$y$ offsets may be given to match the
      contour plot with the image, provided some contouring will be
      visible.  These displacements are in the sense image co-ordinate
      minus the data-array co-ordinate for an arbitrary fiducial point.

      The contouring algorithm has only pixel resolution, and
      so the contours are not smooth, but this makes the processing
      much faster.  There are seven methods for selecting contours.

      The best way to use this application is to first display an image
      on the base plane of an image display, make this the current
      picture, and then plot contours on the overlay plane, clearing
      the overlay picture each time. This enables more than one attempt
      at getting the correct contour heights.  The underlying image will
      not be erased. (Note that if you do not make the underlying image
      the current picture, the contour plot becomes the last DATA
      picture, and so any subsequent $x$-$y$ offsets should be set to 0,0 to
      prevent successive contour plots being incorrectly located.)
   }
   \sstusage{
      contover ndf [comp] offset mode ncont [device]
        \newline\hspace*{1.5em}
        $\left\{ {\begin{tabular}{l}
                    firstcnt=? stepcnt=? \\
                    heights=? \\
                    percentiles=?
                   \end{tabular} }
        \right.$
        \newline\hspace*{1.9em}
        \makebox[0mm][c]{\small mode}
   }
   \sstparameters{
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         True if the graphics device is to be cleared before display
         of the array. It should only be true for an overlay device.
         {\tt [TRUE]}
      }
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The NDF component to be contoured.  It may be {\tt "Data"},
         {\tt "Quality"}, {\tt "Variance"}, or {\tt "Error"} (where
         {\tt "Error"} is the alternative to {\tt "Variance"} and causes
         the square root of the variance values to be taken before
         plotting contours).  If {\tt "Quality"} is specified, then
         the quality values are treated as numerical values (in the
         range 0 to 255).  {\tt ["Data"]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The plotting device. The device must be in one of the following
         GNS categories: IMAGE\_DISPLAY, IMAGE\_OVERLAY, WINDOW,
         WINDOW\_OVERLAY, or MATRIX\_PRINTER.
         {\tt [}Current image-display-overlay device{\tt ]}
      }
      \sstsubsection{
         FIRSTCNT = \_REAL (Read)
      }{
         Height of the first contour (Linear and Magnitude modes).
      }
      \sstsubsection{
         HEIGHTS() = \_REAL (Read)
      }{
         Contour levels (Free mode).  The suggested default is the
         current value.
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The method used to select the contour levels.  The options are
         described below.
         \begin{description}
         \item {\tt "Area"} --- The contours enclose areas of the array for
                         which the equivalent radius increases by equal
                         increments.  You specify the number of levels.
         \item {\tt "Automatic"} --- The contour levels are equally spaced between the maximum
                        the maximum and minimum pixel values in the
                        array.  You supply the number of contour levels.
         \item {\tt "Equalised"} --- You define the number of equally spaced
                          percentiles.
         \item {\tt "Free"} --- You specify a series of contour values
                        explicitly.
         \item {\tt "Linear"} --- You define the number of contours, the start
                        contour level and linear step between contours.
         \item {\tt "Magnitude"} --- You define the number of contours, the start
                        contour level and step between contours.  The
                        step size is in magnitudes so the $n^{\rm th}$
                        contour is 10$^{-0.4*(n-1)*{\rm step}}$ times the
                        start contour level.
         \item {\tt "Percentiles"} --- You specify a series of percentiles.
         \end{description}

         The suggested default is the current value, which is initially
         {\tt "Free"}.
      }
      \sstsubsection{
         NCONT = \_INTEGER (Read)
      }{
         The number of contours required (all modes except Free and
         Percentiles).  It must be between 1 and 50.  If the number is
         large, the plot may be cluttered and take longer to produce.
         {\tt 6}, the initial suggested default, gives reasonable results.
         The current value becomes the suggested default.
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         NDF structure containing the 2-d image to be contoured.
      }
      \sstsubsection{
         OFFSET( 2 ) = \_INTEGER (Read)
      }{
         $x$-$y$ offsets of the input data-array with respect to the
         displayed image ({\it i.e.}\ $x_{\rm data} - x_{\rm image}$
         followed by $y_{\rm data} - y_{\rm image}$ for any fiducial
         point).   These are
         constrained so that some part of the contour plot will be
         overlaid on the displayed image.  The suggested default is
         [0,0], {\it i.e.}\ no shift.
      }
      \sstsubsection{
         PENROT = \_LOGICAL (Read)
      }{
         If TRUE, the plotting pens are cycled through the contours to
         aid identification of the contour heights.  It is ignored
         when annotation is selected. {\tt [FALSE]}
      }
      \sstsubsection{
         STEPCNT = \_REAL (Read)
      }{
         Separation between contour levels, linear for Linear mode
         and in magnitudes for Magnitude mode.
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         CONCOL = LITERAL (Read)
      }{
        The colour of the contour lines on devices that support colour.
        The options are described below.

         \begin{description}
         \item {\tt "MAX"}  --- The maximum colour index in the image
                          display colour lookup table.
         \item {\tt "MIN"}  --- The minimum (non-reserved) colour index in
                          the image-display colour lookup table.
         \item {\bf An integer} --- The actual colour index.  It is
                          constrained between 0 and the maximum colour
                          index available on the device.
         \item {\bf A named colour} --- Uses the named colour from the
                          palette, and if it is not present, the nearest
                          colour from the palette is selected.
         \end{description}

         If the colour is to remain unaltered as the lookup table is
         manipulated choose an integer between 0 and 15, or a named
         colour.  This parameter will be ignored if PENROT = {\tt TRUE}.
         {\tt [}The current value, but equals {\tt 1} (the foreground
         colour) if there is no current value.{\tt ]}
      }
      \sstsubsection{
         DASHED = \_REAL (Read)
      }{
         The height below which the contours will be drawn with dashed
         lines.  A null value ({\tt !}) means all contours are drawn with
         solid lines.  This facility is only available when ANNOTA =
         {\tt FALSE}. {\tt [!]}
      }
      \sstsubsection{
         THICK = \_REAL (Read)
      }{
         The thickness of the lines and NCAR-fount characters in the plot, where
         {\tt 1.0} is the normal thickness.  Currently, this is only available
         on a few devices.  It must take a value in the range 0.5--10.0.
         {\tt [1.0]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         contover myfile d [-20,7] $\backslash$
      }{
         Contours the data array in the NDF called myfile on the
         current image-display overlay device; the overlay is displaced
         such that pixel ($i$,~$j$) in myfile corresponds to pixel
         ($i-20$,~$j+$7) in
         the displayed image.  All other settings are defaulted, so for
         example the current method for determining heights is used,
         and as much of myfile will be contoured that fits into the
         current picture.
      }
      \sstexamplesubsection{
         contover ndf=ngc6872 mode=au ncont=5 offset=[0,0]
      }{
         Contours the data array in the NDF called ngc6872 on the
         current image-display overlay device.  Five equally spaced
         contours between the maximum and minimum data values are
         drawn.  There is no offset between the contour plot and the
         displayed image; this can be useful for comparing an NDF
         before and after some processing, {\it e.g.}\ smoothing.
      }
      \sstexamplesubsection{
         contover iras60(200:300,100:350) comp=d offset=[3,5] $\backslash$
      }{
         Contours the portion of the data array in the NDF called iras60
         on the current image-display overlay using the current method
         for height selection.  The maximum portion of the data array
         that can be contoured goes from pixel (200,100) to (300,350).
         The overlay is displaced such that pixel ($i$,~$j$) in the
         data array corresponds to pixel ($i+$3,~$j+$5) in the displayed
         image.
      }
      \sstexamplesubsection{
         contover comp=v mode=fr heights=[10,20,40,80] device=xov $\backslash$
      }{
         Contours the variance array in the current NDF on the xov
         device.  Contours at 10, 20, 40 and 80 are
         drawn.  There is no displacement between the variance contour
         plot and the displayed image.
      }
      \sstexamplesubsection{
         contover mode=eq ncont=5 dashed=15 pencol=blue ndf=skyflux
      }{
         Contours the data array in the NDF called skyflux on the
         current image-overlay device.  Contours at heights
         corresponding to the 10, 30, 50, 70 and 90 percentiles are
         drawn in blue (if available).  Those contours whose values less
         than 15 will appear as dashed lines.  There is no displacement
         between the contour plot and the displayed image.
      }
      \sstexamplesubsection{
         contover xx1 mode=pe percentiles=[90,95,98,99] pencol=white
         noclear device=epsf\_l
      }{
         Contours the data array in the NDF called xx1 on the epsf\_l
         device.  White contours at heights corresponding to the 90, 95,
         98, and 99 percentiles are drawn.  The display is not cleared.
         There is no displacement.  The output file could be combined
         with a DISPLAY plot (using PSMERGE) to make a hardcopy of a
         contour plot on a dark image.
      }
  }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The application records the contour plot as a DATA picture
         with world co-ordinates in units of data pixels in the graphics
         database.  The DATA picture may also may have double-precision
         data co-ordinates derived from the NDF axis components provided
         these are linear and different from pixel co-ordinates; the data
         co-ordinates are stored via a linear transformation.  The NDF
         associated with the plot is stored by reference with the DATA
         picture.  On exit the current database picture for the chosen
         device reverts to the input picture.
         picture for the chosen device reverts to the input picture.

         \sstitem
         There are some options for setting the characteristics of the
         contour lines.  By default, solid lines are drawn with the same
         colour as the axes and key, namely the foreground colour.  The
         colour will depend on the graphics device chosen, but it is often
         black for printers or white for terminals.  The alternatives to
         override this default behaviour are listed below.

         \begin{enumerate}
         \item Set a colour for all contours using parameter CONCOL.
               The choices may be quite restrictive on certain devices,
               for example a window overlay only has one colour.  Use
               the PALENTRY command to change this colour.
         \item Request dashed contours below some threshold given by
               parameter DASHED and solid lines for other heights.  All
               contours have either the foreground colour or that
               prescribed by parameter CONCOL.
         \item Cycle the pens modulo 3 for each contour height actually
               plotted by setting PENROT = {\tt TRUE}.  The characteristics of
               the second and third line styles will depend on the chosen
               graphics device.  An image display or pen plotter will draw
               coloured lines using palette entries 1 to 3; whereas a
               window overlay, or monochrome laser printer or terminal
               will draw a variety of dashed or thicker lines.
         \item Combine options 2 and 3.  However, palette colours 1 to 3
               will always be used and CONCOL ignored.  The contours below
               the threshold continue the cycle through the three colours.
               There may be some confusion on devices that already use
               dashed lines, so this is only suitable for devices
               supporting at least three colours simultaneously.
         \end{enumerate}

         Pen rotation takes precedence over colour control through CONCOL. 
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CONTOUR, TURBOCONT; Figaro: ICONT; SPECDRE: SPECCONT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         Only real data can be processed directly.  Other data types
         will undergo a type conversion before the contour plot is drawn.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

      }
   }
}
\sstroutine{
   CONVOLVE
}{
   Convolves a pair of 1- or 2-dimensional NDFs together
}{
   \sstdescription{
      This application smooths a 1- or 2-dimensional NDF using a
      Point-Spread Function given by a second NDF.  The output NDF is
      normalised to the same mean data value as the input NDF,
      and is the same size as the input NDF.
   }
   \sstusage{
      convolve in psf out xcentre ycentre
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF containing the image to be smoothed.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The output NDF which is to contain the smoothed image.
      }
      \sstsubsection{
         PSF = NDF (Read)
      }{
         An NDF holding the Point-Spread Function (PSF) with which the
         input image is to be smoothed. An error is reported if the PSF
         contains any bad pixels.  The PSF can be centred anywhere
         within the image (see parameters XCENTRE and YCENTRE).  A
         constant background is removed from the PSF before use.  This
         background level is equal to the minimum of the absolute value
         in the four corner pixel values.  The PSF is assumed to be zero 
         beyond the bounds of the supplied NDF. 
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         A title for the output NDF.  A null ({\tt !}) value means using the
         title of the input NDF. {\tt [!]}
      }
      \sstsubsection{
         WLIM = \_REAL (Read)
      }{
         If the input image contains bad pixels, then this parameter
         may be used to determine the number of good pixels which must
         be present within the smoothing box before a valid output
         pixel is generated.  It can be used, for example, to prevent
         output pixels from being generated in regions where there are
         relatively few good pixels to contribute to the smoothed
         result.

         By default, a null ({\tt !}) value is used for WLIM, which causes
         the pattern of bad pixels to be propagated from the input
         image to the output image unchanged.  In this case, smoothed
         output values are only calculated for those pixels which are
         not bad in the input image.

         If a numerical value is given for WLIM, then it specifies the
         minimum total weight associated with the good pixels in the
         smoothing box required to generate a good output pixel
         (weights for each pixel are defined by the normalised PSF).
         If this specified minimum weight is not present, then a bad
         output pixel will result, otherwise a smoothed output value
         will be calculated.  The value of this parameter should lie
         between 0.0 and 1.0.  A value of 0.0 will result in a good
         output pixel being created even if only one good input pixel
         contributes to it.  A value of 1.0 will result in a good output
         pixel being created only if all the input pixels which
         contribute to it are good. {\tt [!]}
      }
      \sstsubsection{
         XCENTRE = \_INTEGER (Read)
      }{
         The $x$ pixel index (column number) of the centre of the PSF
         within the supplied PSF array.  The suggested default is the
         centre of the PSF array.  (This is how the PSF command would
         generate the array.)
      }
      \sstsubsection{
         YCENTRE = \_INTEGER (Read)
      }{
         The $y$ pixel index (line number) of the centre of the PSF
         within the supplied PSF array.  The suggested default is the
         centre of the PSF array.  (This is how the PSF command would
         generate the array.)
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         convolve ccdframe iraspsf ccdlores 50 50
      }{
         The image in the NDF called ccdframe is convolved using the
         PSF in NDF iraspsf to create the smoothed image ccdlores.  The
         centre of the PSF image in iraspsf is at pixel indices
         (50,~50).  Any bad pixels in the input image are propagated to
         the output.
      }
      \sstexamplesubsection{
         convolve ccdframe  iraspsf ccdlores 50 50 wlim=1.0
      }{
         As above, but good output values are only created for pixels
         which have no contributions from bad input pixels.
      }
      \sstexamplesubsection{
         convolve ccdframe iraspsf ccdlores $\backslash$
      }{
         As in the first example except the centre of the PSF is located
         at the centre of the PSF array.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The algorithm used is based on the multiplication of the
         Fourier transforms of the input image and PSF image.

         \sstitem
         A PSF can be created using the PSF command or MATHS if the
         PSF is an analytic function.

      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: BLOCK, FFCLEAN, GAUSMOOTH, MATHS, MEDIAN, PSF; Figaro:
      ICONV3, ISMOOTH, IXSMOOTH, MEDFILT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of the
         input NDF and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using double-precision floating point.

      }
   }
}
\manroutine {{\manheadstyle{CREFRAME}}}{ Generates a test 2-d data array from a
  selection of several types.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine allows you to generate several different types
  of 2-d data array for test purposes. The data array is written to
  an output {\mantt{IMAGE}} structure.  The types of array are summarised as
  follows:

\begin{mandescription}
\mandescriptionitem {Random}  - between 0 and 1, or specified limits
\mandescriptionitem {Constant} - 0 or at a specified value
\mandescriptionitem {Noisy}  - Poissonian or Gaussian noise about a
  specified mean
\mandescriptionitem {Ramped}  - between specified minimum and maximum values
  and a choice of four directions
\mandescriptionitem {Gaussian} - a random distribution of 2-d Gaussians of
  defined FWHM and range of maximum peak values on a specified
  background, with optional invalid pixels and bad column. There is a
  choice of distributions for the Gaussians: fixed, or inverse square
  radially from the array centre. (In essence it is equivalent to a
  simulated star field.) The {$x$}-{$y$} position and peak
  value of each Gaussian may be stored in a Fortran
  formatted file, or reported to you. Magic-value bad
  data may be included randomly, and/or in a column or line of the array.
\end{mandescription}

  The maximum size of generated array is 4096{$\times$}4096 pixels,
  though generally test data should be much smaller.

\manroutineitem {Invocation }{}
  CREFRAME

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure for the generated data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title for the output {\mantt{IMAGE}} structure. \mbox{\mantt ['KAPPA - Creframe']}
\manparameterentry {{\mantt{READ}} }{{\mantt{IDIMS}}  }{{\mantt{\_INTEGER}}}
  {$x$} and {$y$} dimensions of the output data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{TYPED}}  }{{\mantt{\_CHAR}}}
  Type of data to be generated. The options are {\mantt{GS}} --- Gaussian;
  {\mantt{RR}} --- random 0 -- 1; {\mantt{RP}} --- random Poisson noise
  about mean; {\mantt{RL}} --- random with set limits;
  {\mantt{FL}} --- flat;   {\mantt{BL}} --- zeroes;
  {\mantt{RA}} --- ramps; and {\mantt{GN}} --- Gaussian noise about mean.
\manparameterentry {{\mantt{READ}} }{{\mantt{HIGH}}  }{{\mantt{\_REAL}}}
  High value used in the generated data array.
   ({\mantt{RA}} and {\mantt{RL}} types)
\manparameterentry {{\mantt{READ}} }{{\mantt{LOW}}  }{{\mantt{\_REAL}}}
  Low value used in the generated data array.
   ({\mantt{RA}} and {\mantt{RL}} types)
\manparameterentry {{\mantt{READ}} }{{\mantt{DIRN}}  }{{\mantt{\_INTEGER}}}
  Direction of the ramp. 1 means left to right, 2 is right to
  left, 3 is bottom to top, and 4 is top to bottom.
  ({\mantt{RA}} type)
\manparameterentry {{\mantt{READ}} }{{\mantt{MEAN}}  }{{\mantt{\_REAL}}}
  Mean value used in the generated data array.
  ({\mantt{FL}} and {\mantt{RP}} types)
\end{manparametertable}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{SIGMA}}  }{{\mantt{\_REAL}}}
  Standard deviation of noise to be used in the generated data
  array. ({\mantt{GN}} type)
\manparameterentry {{\mantt{READ}} }{{\mantt{MAX}}  }{{\mantt{\_REAL}}}
  Peak Gaussian intensity to be used in the generated data array.
 ({\mantt{GS}} type)
\manparameterentry {{\mantt{READ}} }{{\mantt{MIN}}  }{{\mantt{\_REAL}}}
  Lowest Gaussian intensity to be used in the generated data array.
 ({\mantt{GS}} type)
\manparameterentry {{\mantt{READ}} }{{\mantt{BACKGROUND}}  }{{\mantt{\_REAL}}}
  Background intensity to be used in the generated data array.
 ({\mantt{GS}} type)
\manparameterentry {{\mantt{READ}} }{{\mantt{NGAUSS}}  }{{\mantt{\_INTEGER}}}
  Number of Gaussian star-like images to be generated. ({\mantt{GS}} type)
\manparameterentry {{\mantt{READ}} }{{\mantt{SEEING}}  }{{\mantt{\_REAL}}}
  Seeing FWHM in pixels ({\em not} the same as the standard
  deviation). ({\mantt{GS}} type)
\manparameterentry {{\mantt{READ}} }{{\mantt{DISTRIB}}  }{{\mantt{\_CHAR}}}
  Radial distribution of the Gaussians to be used; alternatives
  weightings are {\mantt{FIX}} {\mantt{=}} fixed distance; and
  {\mantt{RSQ}} {\mantt{=}} {${1\over r^2}$}. ({\mantt{GS}} type)
  \mbox{\mantt ['FIX']}
\manparameterentry {{\mantt{READ}} }{{\mantt{BADPIX}}  }{{\mantt{\_LOGICAL}}}
  Whether or not bad pixels are to be included.  ({\mantt{GS}} type)
  \mbox{\mantt [FALSE]}
\manparameterentry {{\mantt{READ}} }{{\mantt{FRACTION}} }{{\mantt{\_REAL}}}
  Fraction of bad pixels to be included. ({\mantt{GS}} type)
  \mbox{\mantt [0.01]}
\manparameterentry {{\mantt{READ}} }{{\mantt{BADCOL}}  }{{\mantt{\_LOGICAL}}}
  Whether or not a bad column is to be included. ({\mantt{GS}} type)
  \mbox{\mantt [FALSE]}
\manparameterentry {{\mantt{READ}} }{{\mantt{SCREEN}}  }{{\mantt{\_LOGICAL}}}
  True if the Gaussian parameters are reported to you. ({\mantt{GS}} type)
  \mbox{\mantt [FALSE]}
\manparameterentry {{\mantt{READ}} }{{\mantt{FILENAME}} }{{\mantt{\_CHAR}}}
  Filename for the output of the Gaussian parameters.  ({\mantt{GS}} type)
  \mbox{\mantt [!]}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

 
\sstroutine{
   CRELUT
}{
   Creates or manipulates an image-display lookup table using a
   palette
}{
   \sstdescription{
      This application allows a lookup table to be created or modified
      interactively on a chosen image display from a palette of
      colours.  All plotting is performed within the current
      graphics-database picture for that device.  The phases in the
      creation or manipulation of the lookup table are enumerated below.
      \begin{enumerate}
      \item The initial colour table is read from an NDF lookup-table file
         or a greyscale used if there is no input lookup table.
      \item The name of an NDF containing a 2-dimensional array is obtained and the
         array is scaled and displayed in the top half of the picture
         at the largest magnification without distortion.  Below this an
         histogram of the values between the scaling limits is drawn
         with the colour index of each bin corresponding to the bin's
         scaled value. Thus colours in the image and the histogram
         match.   Axes of number versus data value are plotted about
         the histogram. If a null character, {\tt !}, is given then no NDF
         array is read and a ramp is produced instead of the histogram.
         An axis of pen numbers in the lookup table is drawn around the
         ramp.
      \item A numbered palette is drawn below the histogram.  A palette
         created in an earlier run of CRELUT may be restored from an
         NDF.  Otherwise the palette comprises eight coloured blocks
         (black, white, red, green, blue, yellow, magenta and cyan)
         with palette numbers 0--7, an eight-level greyscale (8--15)
         and a sixteen-level greyscale (16--31).  Palette numbers
         16--31 may be replaced randomly by colours you define.  The
         colours are specified by either the giving the red, green, blue
         intensities; or by name.  The loop is terminated by a null.
      \item Inside a loop you select the palette colour(s) to be
         assigned to the first and last pen numbers of a band within
         the lookup table.  For convenience, where there is an image
         and histogram the equivalent data values are entered rather
         than pen numbers directly, though they are converted to the
         nearest pens in the lookup table.  Linear interpolation
         between the two palette colours yields the lookup-table
         colours inside the band.  Should only one colour be given then
         all the pens in the requested range are set to that colour.
         Pen numbers may be re-used indefinitely and assigned new
         colours if the desired effect is not obtained.  (The histogram
         of the array is produced to assist in a sensible choice).  The
         loop is terminated by a null in response to either of the
         prompts.
      \item The lookup table may be saved in an NDF.  A null response, 
         {\tt !}, to the request for the name of the file in which the table is
         to be stored will result in the table not being saved.
         Likewise the palette may be saved in an NDF.
      \end{enumerate}
   }
   \sstusage{
      crelut inlut outlut ndf [comp] low high [inpal] [outpal] [device]
   }
   \sstparameters{
      \sstsubsection{
         COLOUR() = LITERAL (Read)
      }{
         A colour to be added to the palette at the entry given by
         parameter PALNUM.  It is specified in one of two ways.
         \begin{itemize}
           \item A named colour from the standard colour set, which may
           be abbreviated.  If the abbreviated name is ambiguous the
           first match is selected.  The case of the name is ignored.
           Some examples are {\tt "Seagreen"}, {\tt "Violet"}, and
           {\tt "Orchid"}.

           \item Normalised red, green, and blue intensities separated by
           commas or spaces.  Each value must lie in the range 0.0--1.0.
           For example, {\tt "1.0,1.0,0.5"} would give a pale yellow.
         \end{itemize}
         To exit the loop that obtains new palette colours enter a null
         character ({\tt !}) in response to the prompt.
      }
      \sstsubsection{
         COLRANGE() = \_INTEGER (Read)
      }{
         The numbers of the palette colours to be allocated to a range
         of pens within the lookup table.  One or two palette colours
         may be entered.  If only one is given all the range of pens
         are assigned that colour.  If two palette colours are given
         the colour of a pen is obtained by linear interpolation
         between the two colours at the fractional position of the pen
         in the range of colour indices.  Allowed values are 0--31.
      }
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The NDF component to be displayed.  It may be {\tt "Data"},
         {\tt "Quality"}, {\tt "Variance"}, or {\tt "Error"} (where
         {\tt "Error"} is the alternative to {\tt "Variance"} and causes
         the square root of the variance values to be displayed).
         If {\tt "Quality"} is specified, then the quality values are
         treated as numerical values (in the range 0 to 255).  {\tt ["Data"]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display to be used.  The device must be in
         one of the following GNS categories: IMAGE\_DISPLAY,
         IMAGE\_OVERLAY, or WINDOW, and have at least 48 colour indices.
         At least 120 colour indices is recommended.  The device must
         also not reset when the device is opened (since the new colour
         table would be lost).  {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         HIGH = \_DOUBLE (Read)
      }{
         This is the highest value in the 2-dimensional data array used for
         scaling and computing the histogram.  All larger array values
         are set to the highest colour index when HIGH is greater than
         LOW, otherwise all array values greater than HIGH are set to
         the lowest colour index.  The dynamic default is the maximum
         data value.
      }
      \sstsubsection{
         INLUT = NDF (Read)
      }{
         Name of the NDF containing the initial lookup table as its data
         array.  The LUT must be 2-dimensional, the first dimension
         being 3, and the second is arbitrary.  The method used to
         compress or expand the colour table if the second dimension is
         different from the number of unreserved colour indices is
         controlled by parameter NN.  Also the LUT's values must lie in
         the range 0.0--1.0.  If INLUT is null ({\tt !}) a
         greyscale is used.
      }
      \sstsubsection{
         INPAL = NDF (Read)
      }{
         Name of the NDF containing the initial palette as its data
         array.  The palette must be 2-dimensional, the first dimension
         being 3, and the second 32.  If the second dimension is
         greater than 32 only the first 32 colours are used; if it has
         less than 32 just fill as much of the palette as is possible
         starting from the first colour.  The palette's values must lie
         in the range 0.0--1.0.  If INPAL is null ({\tt !}) the default
         palette is loaded.
      }
      \sstsubsection{
         LOW = \_DOUBLE (Read)
      }{
         The array value that scales to the lowest pen in the colour
         table, and the minimum value to be included in the histogram.
         All smaller array values are set to the lowest colour
         index when LOW is less than HIGH, otherwise all array values
         smaller than LOW are set to the highest colour index.   The
         dynamic default is the minimum data value.
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         Input NDF data structure containing the image to be displayed
         to show the effect of the created colour table.
      }
      \sstsubsection{
         NN = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the input lookup table is mapped to the colour table by
         using the nearest-neighbour method.  This preserves sharp
         edges and is better for lookup tables with blocks of colour.
         If NN is {\tt FALSE}, linear interpolation is used, and this is
         suitable for smoothly varying colour tables. {\tt [FALSE]}
      }
      \sstsubsection{
         OK = \_LOGICAL (Read)
      }{
         {\tt TRUE} when the palette colour just produced is acceptable.
      }
      \sstsubsection{
         OUTLUT = NDF (Write)
      }{
         The output lookup table.
      }
      \sstsubsection{
         OUTPAL = NDF (Write)
      }{
         The palette used to create the lookup table.
      }
      \sstsubsection{
         PALNUM = \_INTEGER (Read)
      }{
         The number of the palette entry whose colour is to be
         modified.  (The numbers are plotted on the palette.) It is
         used within a loop to modify up to sixteen entries in the
         palette.  Entering a null, {\tt !}, will end that loop.  The
         suggested default is the next palette number.  PALNUM must lie
         in the range 16--31.
      }
      \sstsubsection{
         PENRANGE() = \_INTEGER (Read)
      }{
         The range of pen numbers in the lookup table which is about to
         be allocated a set of colours from the palette.  PENRANGE is
         only used when there is no image and histogram plotted.  The
         pen number can be read from the axis below the ramp.  If one
         pen number is given, only this pen is altered, and it is given
         the first palette colour of COLRANGE.  If two are supplied,
         the first pen number entered will take the first palette
         colour entered, and the second pen is assigned the second
         palette colour.  The pens must lie in the range zero to the
         maximum number of available pens.
      }
      \sstsubsection{
         PTITLE = LITERAL (Read)
      }{
         Title for the output palette NDF. {\tt ["KAPPA - Crelut"]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output lookup table NDF. {\tt ["KAPPA - Crelut"]}
      }
      \sstsubsection{
         VALRANGE() = \_DOUBLE (Read)
      }{
         The range of data values in the histogram/image which is to
         be allocated a set of colours from the palette, and hence be
         assigned to a part of the lookup table.  VALRANGE is only used
         when there is an image and histogram plotted.  The data value
         may be read from the axis below the histogram.  If one data
         value is given, only the single pen in the lookup table
         corresponding to the value is altered, and it is given the
         first palette colour of COLRANGE.  If two values are supplied,
         the first data value entered will take the first palette
         colour entered, and the second data value is assigned the
         second palette colour.  The data values must lie in the range
         PVLO--PVHI.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         Note:
      }{
         Since the application is highly interactive and contains loops
         it is not possible to give one-line commands to perform a
         complete operation.  Therefore the examples show how to
         control the input and output data and not the interactive
         manipulation of the colour table.
      }
      \sstexamplesubsection{
         crelut heat bizarre hh12 $\backslash$
      }{
         Reads a lookup table in an NDF called heat.  If resampling of
         the lookup table is required it achieved via linear
         interpolation. The lookup table after the manipulation is
         stored in NDF bizarre.  The data array in NDF hh12 is scaled
         between its minimum and maximum values and displayed in the
         top half of the current picture on the current image-display
         device.  Also drawn is an histogram of the intensities.
      }
      \sstexamplesubsection{
         crelut heat bizarre hh12 inpal=mypal $\backslash$
      }{
         As above except a palette created previously via the OUTPAL
         parameter.  This palette is in an NDF called mypal.
      }
      \sstexamplesubsection{
         crelut inlut=! deluxe hh12 v low=100 high=400 $\backslash$
      }{
         A greyscale lookup table is manipulated and the result
         is stored in NDF deluxe.  The variance array in NDF hh12 is
         scaled between 100 and 400, and displayed in the top half of
         the current picture on the current image-display device.  Also
         drawn is an histogram of the intensities between those limits.
      }
      \sstexamplesubsection{
         crelut heat bizarre ndf=! device=xwindows $\backslash$
      }{
         Reads a lookup table in an NDF called heat.  If resampling of
         the lookup table is required it achieved via the
         nearest-neighbour method.  The lookup table after the
         manipulation is stored in NDF bizarre.  A linear ramp is
         displayed in the lower half of the current picture on the
         xwindows device.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The application stores, in the order given, the following
         pictures in the graphics database: a frame comprising the data
         picture, the histogram or ramp and the palette; the data-array
         picture with world co-ordinates in units of data pixels; the
         histogram/ramp frame picture including the histogram/ramp plus
         the annotated axes; and the histogram with world co-ordinates in
         units of data values and number, or the ramp with units of pen
         numbers and normalised frequency.  The NDF associated with the
         image/histogram plots is stored by reference with the DATA
         picture.  On exit the current database picture for the chosen
         device reverts to the input picture.

         \sstitem
         Bad pixels will appear with the lowest colour index in the
         plot.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: LUTABLE, LUTFLIP, LUTREAD, LUTROT, LUTSAVE, LUTTWEAK, \linebreak
      LUTVIEW, PALREAD, PALSAVE; Figaro: COLOUR.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         The magic-value method is used for processing bad data.

         \sstitem
         This application will handle data in all numeric types, though
         type conversion to integer will occur for unsigned byte and word
         images.
      }
   }
}
\sstroutine{
   CSUB
}{
   Subtracts a scalar from an NDF data structure
}{
   \sstdescription{
      The routine subtracts a scalar ({\it i.e.}\ constant) value from each
      pixel of an NDF's data array to produce a new NDF data structure.
   }
   \sstusage{
      csub in scalar out
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         Input NDF data structure, from which the value is to be
         subtracted.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF data structure.
      }
      \sstsubsection{
         SCALAR = \_DOUBLE (Read)
      }{
         The value to be subtracted from the NDF's data array.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the NDF supplied for parameter IN to be used
         instead. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         csub a 10 b
      }{
         This subtracts ten from the NDF called a, to make the NDF
         called b.  NDF b inherits its title from a.
      }
      \sstexamplesubsection{
         csub title="HD123456" out=b in=a scalar=21.9
      }{
         This subtracts 21.9 from the NDF called a, to make the NDF
         called b.  NDF b has the title {\tt "HD123456"}.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ADD, CADD, CDIV, CMULT, DIV, MATHS, MULT, SUB.
   }
   \sstimplementationstatus{

      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, UNITS, HISTORY, and VARIANCE components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.

      }
   }
}
\sstroutine{
   CURSOR
}{
   Reports the co-ordinates of points selected using the cursor.
}{
   \sstdescription{
      This reads co-ordinates from the chosen graphics device and
      displays them on your terminal.  There is commentary that
      describes which buttons should be pressed to select or erase a
      point, or exit.  Optionally, the co-ordinates may be stored in a
      text file.

      For each selected cursor position its Cartesian co-ordinates are
      reported.  If the co-ordinate frame changes between selected
      positions the comment, name and any label associated with the new
      graphics-database picture are appended to the message.

      There are three modes of operation to define which co-ordinate
      system/picture is to be used.  These are ANCHOR, CURRENT and
      DYNAMIC. See the parameter MODE for details.

      In ANCHOR or DYNAMIC modes there is an option to select only
      pictures of a certain name in the database. This is most useful
      when DATA pictures are covered by transparent FRAME pictures.
   }
   \sstusage{
      cursor [mode] [name] [logfile] [device]
   }
   \sstparameters{
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either
         {\tt "World"} or {\tt "Data"}.  {\tt "World"} makes the world
         co-ordinates of the cursor position to be reported.  World
         co-ordinates that relate to a location in a data array will be
         in array pixels.  If COSYS = {\tt "Data"} the graphics database
         is examined for data co-ordinates stored via a transformation. 
         Data co-ordinates are arbitrary but most often they will be a
         linear or logarithmic transformation of the world co-ordinates.
         For example, the $x$ co-ordinate of a spectrum would be given in
         pixels if COSYS = {\tt "World"}, but if COSYS = {\tt "Data"} the
         $x$ co-ordinate could be in wavelength units, such as
         {\AA}ngstroms.  If the database does not
         have a world-to-data transformation for a given picture, the
         value of this parameter is irrelevant and world co-ordinates
         will be reported for that picture. {\tt [}Current
         co-ordinate system{\tt ]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation.  This device must support cursor
         interaction, and belong to one of the following classes:
         TERMINAL, IMAGE\_DISPLAY, IMAGE\_OVERLAY, WINDOW, and
         WINDOW\_OVERLAY.  {\tt [}The current graphics device{\tt ]}
      }
      \sstsubsection{
         DOUBLE = \_LOGICAL (Read)
      }{
         If true co-ordinates will be reported, written to the output
         parameters, and stored in the text file in double precision,
         otherwise single precision is used.  {\tt [FALSE]}
      }
      \sstsubsection{
         LOGFILE = FILENAME (Write)
      }{
         The name of the text file in which the co-ordinates of points
         selected with the cursor may be stored.  A null string ({\tt !})
         means that no file is created. The suggested default is the
         current value. {\tt [!]}
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The mode defining the co-ordinate system/picture in which
         cursor positions are returned.  There are three options.
         \begin{description}
            \item {\tt "Current"} selects the current picture in the AGI
            database and reports the position of a point selected by
            the cursor.  If the point does not lie within the picture,
            an extrapolated position is reported.

            \item {\tt "Dynamic"} selects the topmost picture in the AGI
            database which encompasses that position selected.  Thus
            the second and subsequent cursor hits may result in the
            selection of a new picture.

            \item {\tt "Anchor"} lets the first cursor hit select a
            picture which remains current throughout the running of
            the application.  If subsequent cursor hits fall outside
            the extent of this picture, a position extrapolated from
            the picture's co-ordinate system is reported.
         \end{description}
         {\tt ["Dynamic"]}
      }
      \sstsubsection{
         NAME = LITERAL (Read)
      }{
         Only pictures of this name are to be selected.  A null string
         ({\tt !}) or blanks means that pictures of all names may be selected.
         NAME is ignored when MODE = {\tt "Current"}.
         {\tt [!]}
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         COLOUR = LITERAL (Read)
      }{
         The colour in which to draw any graphics specified by
         parameter PLOT.  The options are described below.

         \begin{description}
         \item [{\tt "MAX"}]  --- The maximum colour index used for the
                              display of the image.
         \item [{\tt "MIN"}]  --- The minimum colour index used for the
                              display of the image.
         \item [An integer]   --- The actual colour index.  It is constrained
                              between 0 and the maximum colour index
                              available on the device.
         \item [A named colour] --- Uses the named colour from the palette, and
                            if it is not present, the nearest colour
                            from the palette is selected.
         \end{description}

         If the colour is to remain unaltered as the lookup table is
         manipulated choose an integer between 0 and 15, or a named
         colour.  The suggested default is the current value.

         This parameter is ignored on window overlays, where the
         overlay colour is used.  (Use the PALENTRY command to change
         this colour.)  An overlay has the advantage that the crosses
         or polygon can be erased using OVCLEAR once this task is
         completed.  The parameter is also ignored for character-cell
         terminals.
         {\tt [}The current value, but equals {\tt "Green"} if there is
         no current value.{\tt ]}
      }
      \sstsubsection{
         PLOT = LITERAL (Read)
      }{
         The type of graphics to be used to mark the position of each
         selected point.  PLOT can take any of the following values:

         \begin{description}
         \item {\tt "Cross"} --- Each point is marked by a cross.

         \item {\tt "None"} --- No graphics are produced.

         \item {\tt "Poly"} --- Causes each point to be joined by a 
                     straight line to the previous point.  The last
                     point is joined to the first point.
         \end{description}

         The initial default is {\tt "None"}, then subsequently it is the
         current value.  {\tt []}
      }
   }
   \sstresparameters{
      \sstsubsection{
         NUMBER = \_DOUBLE (Write)
      }{
         The number of points selected with the cursor and stored in
         output parameters XP and YP.
      }
      \sstsubsection{
         XC = \_DOUBLE (Write)
      }{
         The $x$ co-ordinate of the last point selected with the cursor.
         This is only written when parameter NUMBER is positive.
      }
      \sstsubsection{
         XP() = \_DOUBLE (Write)
      }{
         The $x$ co-ordinates of the points selected with the cursor.
         The number of values is given by parameter NUMBER, and
         therefore this parameter is only written when parameter NUMBER
         is positive.
      }
      \sstsubsection{
         YC = \_DOUBLE (Write)
      }{
         The $y$ co-ordinate of the last point selected with the cursor.
         This is only written when parameter NUMBER is positive.
      }
      \sstsubsection{
         YP() = \_DOUBLE (Write)
      }{
         The $y$ co-ordinates of the points selected with the cursor.
         The number of values is given by parameter NUMBER, and
         therefore this parameter is only written when parameter NUMBER
         is positive.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         cursor
      }{
         This obtains the co-ordinates of any visible picture for the
         current graphics device by use of the cursor.
      }
      \sstexamplesubsection{
         cursor colour=blue plot=cross
      }{
         As above except that the points are marked with blue crosses.
      }
      \sstexamplesubsection{
         cursor xc=(xpos) yc=(ypos)
      }{
         As the first example.  The $x$ and $y$ co-ordinates of the
         last-selected point is written to {\footnotesize ICL}
         variables xpos and ypos.
      }
      \sstexamplesubsection{
         cursor cosys=w
      }{
         This obtains the world co-ordinates of any visible picture for
         the current graphics device by use of the cursor.
      }
      \sstexamplesubsection{
         cursor current graphon
      }{
         This obtains the co-ordinates of any visible picture in the
         reference frame of the current picture of the Graphon device.
      }
      \sstexamplesubsection{
         cursor logfile=stars.dat name=data
      }{
         This obtains the co-ordinates of any visible DATA picture
         for the current graphics device.  The $x$-$y$ co-ordinates are
         stored in the text file called {\tt stars.dat}.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Should an error occur trying to obtain the BASE picture for
         ANCHOR or DYNAMIC modes, the current picture is unchanged.

         \sstitem
         In DYNAMIC and ANCHOR modes, if the cursor is situated at a
         point where there are no pictures of the selected name, the
         co-ordinates in the BASE picture are reported.

         \sstitem
         The maximum number of points is 500.

         \sstitem
         Points can be removed (the runtime instructions state how),
         starting from the most-recent one.  For an overlay device, the plotted
         points or polygon lines disappear.  For other devices, the
         erroneous points and lines are plotted with pen 4 of the palette.
      }   
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CURSOR, INSPECT, PICCUR; Figaro: ICUR, IGCUR.
   }
}
 
\sstroutine{
   DISPLAY
}{
   Displays a 1-d or 2-d NDF
}{
   \sstdescription{
      This application displays an image of a 1- or 2-dimensional NDF on the current
      image-display device.  The image may be the data array, but also
      variance or quality can be shown.  The image is situated within
      the current picture with the maximum magnification without
      clipping or distorting the image, though the exact positioning
      and magnification can be controlled.  The colour mapping has
      several scaling methods described below.  All the available
      colour indices are used save a few reserved for annotations.

      Only the parts of the displayed image that lie within the current
      picture are visible;  the rest is clipped.  Should the image be
      too large to fit onto the current picture at unit magnification
      (if you demand this magnification), then there is an option to squash the
      array in order to make it just fit; otherwise the portion of the
      data array visible within the current picture is displayed as
      originally requested.

      Annotated axes and a title, or a coloured border may be drawn
      around the displayed image.
   }
   \sstusage{
      display in [comp] clear [device] mode [centre] [xmagn] [ymagn]
         [out] 
        \newline\hspace*{1.5em}
        $\left\{ {\begin{tabular}{l}
                    low=? high=? \\  
                    percentiles=? \\
                    sigmas=? 
                   \end{tabular} }
        \right.$
        \newline\hspace*{1.9em}
        \makebox[0mm][c]{\small mode}
   }
   \sstparameters{
      \sstsubsection{
         CENTRE( 2 ) = \_DOUBLE (Read)
      }{
         These two values control the position of the displayed image.
         Specifically, if COSYS = {\tt "WORLD"} they are the pixel indices of
         the NDF image that are to lie at the centre of the current
         picture, but they are not limited to the bounds of the NDF
         array.  If COSYS = {\tt "DATA"} they are the data co-ordinates to
         lie at the centre of the picture, and are limited by the
         bounds of the NDF array.  The CENTRE parameters permit you to
         display a portion of an NDF about a specified pixel at high
         magnification.  The application attempts to display as much of
         the NDF array it can at the magnification, so do not expect a
         symmetric image about the chosen centre.  If you do not
         specify a magnification with centering, it may result in a
         small displayed image.  Further it may not be possible to have
         precisely the pixel you want at the centre of the image; the
         displacement decreases as the magnification is increased.
         CENTRE is disabled when FILL is {\tt TRUE}. 
         {\tt [}Centre of the image{\tt ]}
      }
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the current picture is to be cleared before the
         display of the image. {\tt [FALSE]}
      }
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The NDF component to be displayed.  It may be {\tt "Data"},
         {\tt "Quality"}, {\tt "Variance"}, or {\tt "Error"} (where
         {\tt "Error"} is the alternative to {\tt "Variance"} and causes
         the square root of the variance values to be displayed).
         If {\tt "Quality"} is specified, then the quality values are
         treated as numerical values (in the range 0 to 255).  {\tt ["Data"]}
      }
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either
         {\tt "WORLD"} or {\tt "DATA"}.  {\tt "WORLD"} makes pixel
         co-ordinates to appear on axes and the centering is defined
         in pixels.  If COSYS = {\tt "DATA"} the
         NDF's axis information is used to annotate axes and to control
         the position of the displayed image.  {\tt [}Current co-ordinate
         system{\tt ]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the graphics device used to display the image.
         The device must be in one of the following GNS categories:
         IMAGE\_DISPLAY, IMAGE\_OVERLAY, MATRIX\_PRINTER, or WINDOW, and
         have at least 24 colour indices or greyscale intensities.
         {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
      FILL = \_LOGICAL (Read)
      }{
         The display normally has square pixels, in other words a
         length along each axis corresponds to the same number of
         pixels.  However, for images with markedly different
         dimensions, such as two-dimensional spectra, this default
         behaviour may not be suitable or give the clearest plot.  When
         FILL is {\tt TRUE}, the square-pixel constraint is relaxed and the
         displayed image is the largest possible within the current
         picture.  When FILL is {\tt FALSE}, the pixels are square.  When
         FILL is {\tt TRUE} it disables the CENTRE, XMAGN, and YMAGN
         parameters.  The suggested default is the current value.
         {\tt [FALSE]}
      }
      \sstsubsection{
         HIGH = \_DOUBLE (Read)
      }{
         The array value that scales to the highest pen in the colour
         table.  All larger array values are set to the highest colour
         index when HIGH is greater than LOW, otherwise all array values
         greater than HIGH are set to the lowest colour index.  The
         dynamic default is the maximum data value.  There is an
         efficiency gain when both LOW and HIGH are given on the
         command line, because the extreme values need not be computed.
         (Scale mode)
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         Input NDF data structure containing the image to be displayed.
      }
      \sstsubsection{
         LOW = \_DOUBLE (Read)
      }{
         The array value that scales to the lowest pen in the colour
         table.  All smaller array values are set to the lowest colour
         index when LOW is less than HIGH, otherwise all array values
         smaller than LOW are set to the highest colour index.   The
         dynamic default is the minimum data value.  There is an
         efficiency gain when both LOW and HIGH are given on the
         command line, because the extreme values need not be computed.
         (Scale mode)
      }
      \sstsubsection{
         LUT = NDF (Read)
      }{
         Name of the NDF containing a lookup table as its data array;
         the lookup table is written to the image-display's colour
         table.  The purpose of this parameter is to provide a means of
         controlling the appearance of the image on certain devices,
         such as colour printers, that do not have a dynamic colour
         table, {\it i.e.}\ the colour table is reset when the device is
         opened.  If used with dynamic devices, such as windows or
         Ikons, the new colour table remains after this application has
         completed. A null, {\tt !}, means that the existing colour table will
         be used.

         The LUT must be 2-dimensional, the first dimension
         being 3, and the second being arbitrary.  The method used to
         compress or expand the colour table if the second dimension is
         different from the number of unreserved colour indices is
         controlled by parameter NN.  Also the LUT's values must lie in
         the range 0.0--1.0. {\tt [!]}
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The type of scaling to be applied to the array.  The options
         are described below.
         \begin{description}
         \item {\tt "Faint"} --- The image is scaled from the mean minus one
                         standard deviation to the mean plus seven
                         standard deviations.  The scaling values are
                         reported so that the faster Scale mode may be
                         utilised later.
         \item {\tt "Flash"} --- The image is flashed onto the screen without
                         any scaling at all.  This is the fastest
                         option.
         \item {\tt "Percentiles"} --- The image is scaled between the values
                         corresponding to two percentiles.  The scaling
                         values are reported so that the faster Scale
                         mode may be utilised later.
         \item {\tt "Range"} --- The image is scaled between the minimum and
                         maximum data values.
         \item {\tt "Scale"} --- You define the upper and lower limits
                         between which the image is to be scaled.  The
                         application reports the maximum and the minimum
                         values for reference and makes these defaults
                         respectively.
         \item {\tt "Sigmas"} --- The image is scaled between two
                         standard-deviation limits.  The scaling values
                         used are reported so that the faster Scale mode
                         may be utilised later.
         \end{description}
      }
      \sstsubsection{
         NN = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the input lookup table is mapped to the colour table by
         using the nearest-neighbour method.  This preserves sharp
         edges and is better for lookup tables with blocks of colour.
         If NN is {\tt FALSE}, linear interpolation is used, and this is
         suitable for smoothly varying colour tables.  NN is ignored
         unless LUT is not null. {\tt [FALSE]}
      }
      \sstsubsection{
         NUMBIN  =  \_INTEGER (Read)
      }{
         The number of histogram bins used to compute percentiles for
         scaling. (Percentiles mode) {\tt [2048]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The scaled section of the NDF displayed; it also does not have
         values that equal the reserved portion of the colour table.
         The output NDF is intended to be used as the input data in
         conjunction with SCALE={\tt FALSE}.  It will be vertically
         inverted with respect to the input array because of GKS
         convention.  If it has a null value ({\tt !}) no output NDF will be
         created.  This parameter is ignored when SCALE={\tt FALSE}. {\tt [!]}
      }
      \sstsubsection{
         PERCENTILES( 2 ) = \_REAL (Read)
      }{
         The percentiles that define the scaling limits. For example,
         {\tt [25,75]} would scale between the quartile values. (Percentile
         mode)
      }
      \sstsubsection{
         SCALE = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the input array is scaled according to the value of
         parameter MODE.  If it is {\tt FALSE}, MODE is ignored, and the input
         array is displayed as is.  There is no scaling, inversion
         or avoidance of annotation pens.  SCALE={\tt FALSE} is intended to
         be used with arrays previously scaled by this or similar
         applications which have already performed the scaling,
         inversion and exclusion.  It provides the quickest method of
         image display within this application. {\tt [TRUE]}
      }
      \sstsubsection{
         SIGMAS( 2 ) = \_REAL (Read)
      }{
         The standard-deviation bounds that define the scaling limits.
         To obtain values either side of the mean both a negative and
         a positive value are required.  Thus {\tt [$-$2,3]} would scale
         between the mean minus two and the mean plus three standard
         deviations.  {\tt [3,$-$2]} would give the negative of that.
      }
      \sstsubsection{
         SQUASH = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the array is to be squashed otherwise it is displayed
         as is with clipping.  This parameter is only used when the $x$
         and $y$ magnifications are both one, and the image would be
         clipped.  It is not used in Flash mode.
      }
      \sstsubsection{
         XMAGN = \_REAL (Read)
      }{
         The magnification (zooming) in the $x$ direction.  Unit
         magnification means that one NDF pixel maps to one
         display-device pixel.  It is ignored when FILL is {\tt TRUE}.
         {\tt [}Maximum that gives square pixels and
         just fills the current database picture{\tt ]}
      }
      \sstsubsection{
         YMAGN = \_REAL (Read)
      }{
         The magnification (zooming) in the $y$ direction.  Unit
         magnification means that one NDF pixel maps to one
         display-device pixel.  It is ignored when FILL is {\tt TRUE}.
         {\tt [\%XMAGN]}
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         ABSLAB  =  LITERAL (Read)
      }{
         Label for the plot abscissa, in which NCAR fancy founts
         may be embedded when FONT = {\tt "NCAR"}.  This parameter is only used when the axes
         option is selected.  If axis information is present the
         suggested default is the NDF's axis label followed by the
         units, in parentheses.  If an error occurs obtaining the label
         the suggested default is {\tt "X"}. {\tt []}
      }
      \sstsubsection{
         AXES = \_LOGICAL (Read)
      }{
         {\tt TRUE} if labelled and annotated axes are to be drawn around the
         displayed image.  The annotations are either the data
         co-ordinates from the NDF axis components, provided these are
         present and linear and COSYS = {\tt "DATA"}; otherwise pixel
         co-ordinates are used.  {\tt [FALSE]}
      }
      \sstsubsection{
         BADCOL = LITERAL (Read)
      }{
         The colour to give a bad pixel in the display.  There are a
         number of options described below.
         \begin{description}
           \item [{\tt "MAX"}]  - The maximum colour index used for the
                            display of the image.
           \item [{\tt "MIN"}]  - The minimum colour index used for the
                            display of the image.
           \item [An integer] - The actual colour index. It is constrained
                            between 0 and the maximum colour index
                            available on the device.
           \item [A named colour] - Uses the named colour from the palette, and
                            if it is not present, the nearest colour
                            from the palette is selected.
         \end{description}
         If the colour is to remain unaltered as the lookup table is
         manipulated choose an integer between 0 and 15, or a named
         colour.  The suggested default is the current value. {\tt [}The
         current value, but equals {\tt "MIN"} if there is no current
         value.{\tt ]}
      }
      \sstsubsection{
         BCOLOUR = LITERAL (Read)
      }{
         The colour of the border.  It is only accessed if BORDER is
         {\tt TRUE}.  There are a number of options described below.
         \begin{description}
           \item [{\tt "MAX"}]    - The maximum palette colour index.
           \item [{\tt "MIN"}]    - The background colour.
           \item [An integer] - The actual colour index in the palette.  It
                              is constrained to be between 0 and 15.
           \item [A named colour] - Uses the named colour from the
                              palette, and if it is not present, the nearest
                              colour from the palette is selected.
         \end{description}
         The suggested default is the current value. {\tt [}The current
         value, but equals {\tt "Yellow"} if there is no current value.{\tt ]}
      }
      \sstsubsection{
         BORDER = \_LOGICAL (Read)
      }{
         {\tt TRUE} if a coloured border is to be drawn around the
         displayed image.  If AXES is {\tt TRUE} the value of BORDER will
         be ignored and no border will be drawn.  The colour and width
         of the border is controlled by parameters BCOLOUR and
         BWIDTH. {\tt [FALSE]}
      }
      \sstsubsection{
         BWIDTH( 2 ) = \_REAL (Read)
      }{
         The width of the border along each axis in device pixels.  It
         is only obtained when BORDER is {\tt TRUE}.  If only a single value
         is given it is duplicated to the second dimension.  The
         suggested default is the current value.  The widths must lie
         in the range 1.0--20.0. {\tt [4.0]}
      }
      \sstsubsection{
         FONT = LITERAL (Read)
      }{
         The fount to be used for the line graphics.  It can be either
         {\tt "NCAR"} for the NCAR fancy characters and {\tt "GKS"} for the standard
         GKS san-serif fount.   The former is intended for hardcopy
         publication-quality plots, since it is relatively slow; the
         latter is intended for normal interactive graphics requiring
         rapid plotting, and it is clearer on small plots. The
         suggested default is the current value. {\tt ["GKS"]}
      }
      \sstsubsection{
         MAJTIC( 2 ) = \_REAL (Read)
      }{
         The parameter controlling the numbers of major tick marks
         for the $x$ and $y$ axes.  (Number used is between MAJTIC$+$2 and
         5$*$MAJTIC/2$+$4.)   A negative value for an axis makes the
         graphics package decide an appropriate value.  This parameter
         is only used when the axes option is selected. {\tt [3.,3.]}
      }
      \sstsubsection{
         MINTIC( 2 ) = \_REAL (Read)
      }{
         The number of minor tick marks between each major tick mark
         for the $x$ and $y$ axes.  A negative value forces the graphics
         package to compute appropriate values.   This parameter is
         only used when the axes option is selected. {\tt [-1.,-1.]}
      }
      \sstsubsection{
         ORDLAB  =  LITERAL (Read)
      }{
         Label for the plot ordinate, in which NCAR fancy founts
         may be embedded when FONT = {\tt "NCAR"}.   This parameter is only used when the axes
         option is selected.  If axis information is present the
         suggested default is the NDF's axis label followed by the
         units, in parentheses.  If an error occurs obtaining the label
         the suggested default is {\tt "Y"}. {\tt []}
      }
      \sstsubsection{
         OUTTIC = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the axis tick marks are to appear on the outside of
         the axes instead of inside.   This parameter is only used
         when the axes option is selected. {\tt [TRUE]}
      }
      \sstsubsection{
         PLTITL = LITERAL (Read)
      }{
         The title of the plot.  Up to about 40 characters can be
         accommodated, and NCAR fancy founts may be embedded ({\it cf.}
         SUN/90) when FONT = {\tt "NCAR"}.  This parameter is only used
         when the axes option is selected. {\tt [}The NDF title{\tt ]}
      }
      \sstsubsection{
         THICK = \_REAL (Read)
      }{
         The thickness of the axes and annotations in the plot, where
         1.0 is the normal thickness.  It should be between 0.5 and 5.
         This feature is only available on some devices.   This
         parameter is only used when the axes option is selected. {\tt [1.0]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         SCAHIGH = \_DOUBLE (Write)
      }{
         The array value scaled to the maximum colour index for display.
         In Flash mode or when there is no scaling the highest colour
         index is used.  The current display linear-scaling maximum is
         set to this value.
      }
      \sstsubsection{
         SCALOW = \_DOUBLE (Write)
      }{
         The array value scaled to the minimum colour index for display.
         In Flash mode or when there is no scaling the lowest colour
         index is used.  The current display linear-scaling minimum is
         set to this value.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         display ngc6872 mode=p percentiles=[10,90]
      }{
         Displays the NDF called ngc6872 on the current image-display
         device.  The scaling is between the 10 and 90 per cent
         percentiles of the image.
      }
      \sstexamplesubsection{
         display vv256 mode=flash border bwidth=6.0 badcol="Red"
      }{
         Displays the NDF called vv256 on the current image-display
         device.  There is no scaling of the data; instead the modulus of
         each pixel with respect to the number of colour-table indices
         is shown.  Any bad data will be displayed in red.  A coloured
         border, of width six device pixels, is drawn around the image;
         it will have the current border colour.
      }
      \sstexamplesubsection{
         display mode=fa axes clear out=video cosys=d $\backslash$
      }{
         Displays the current NDF data component with annotated axes
         after clearing the current picture on the current image-display
         device.  The axes take the axis labels and title from the NDF,
         and are annotated in data co-ordinates.  The scaling is
         between the $-$1 and $+$7 standard deviations of the image around
         its mean.  The scaled data are stored in an NDF called video.
      }
      \sstexamplesubsection{
         display video noscale $\backslash$
      }{
         Displays the data component of the NDF called video (created
         in the previous example) without scaling within the current
         picture on the current image-display device.
      }
      \sstexamplesubsection{
         display in=cgs4a comp=v mode=sc low=1 high=5.2 device=xwindows
      }{
         Displays the variance component of NDF cgs4a on the xwindows
         device, scaling between 1 and 5.2.
      }
      \sstexamplesubsection{
         display redrectangle xmagn=4 centre=[300,200] $\backslash$
      }{
         Displays the redrectangle NDF with a magnification of four
         times, so that four device pixels corresponds to one image
         pixel, on the current device.  The exact portion of the image
         visible will depend on the size and location of the current
         picture, however the displayed portion will have pixel
         (300,200) at the centre of the current picture.  The current
         scaling is used.
      }
      \sstexamplesubsection{
         display ngc6872 mode=ra device=lj250\_p lut=pizza
      }{
         Displays the NDF called ngc6872 on the LJ250\_P device. The
         lookup table in the NDF called pizza is mapped on the LJ250\_P's
         colour table.  The scaling is between the minimum and maximum
         of the image.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The application stores a number of pictures in the graphics
         database in the following order: a FRAME of the specified size
         containing the title, annotated axes, and the image area
         (provided AXES is {\tt TRUE}) or the border (if BORDER is
         {\tt TRUE}), whose
         world co-ordinates are in device pixels; a DATA picture with
         world co-ordinates in pixel co-ordinates.  The DATA picture also
         may have data co-ordinates derived from the NDF axis components
         provided these are linear and different from pixel co-ordinates;
         the data co-ordinates are stored via a double-precision linear
         transformation.  The NDF associated with the plot is stored by
         reference with the DATA picture.  On exit the current database
         picture for the chosen device reverts to the input picture.

         \sstitem
         When axes are requested the axis annotations are defined by
         their lower and upper bounds, {\it i.e.}\ a regular array is assumed.
         The bounds are derived from the part of NDF being displayed, and
         will be in pixel or data co-ordinates.

         \sstitem
         The data type of the output NDF depends on the number of colour
         indices: \_UBYTE for no more than 256, \_UWORD for 257 to 65535,
         and \_INTEGER otherwise.   The output NDF will not contain any
         extensions, UNITS, QUALITY, and VARIANCE; but LABEL, TITLE,
         and AXIS information are propagated from the input NDF.  The
         output NDF does not become the new current data array.  It is a
         Simple NDF (because the bad-pixel flag is set to false in order to
         access the maximum colour index, and to handle sections),
         therefore only NDF-compliant applications can process it.

         \sstitem
         For images much larger than the current picture size measured in
         device pixels, the resolution of the device will allow only a
         fraction of the detail in the array to be plotted.  Therefore,
         the application compresses the image by block averaging when it
         can do so without loss of resolution when displayed.  This saves
         time scaling the data and transmitting them to the image display.
         Note that the default values for parameters LOW and HIGH are
         the minimum and maximum values in the {\em compressed\/}
         floating-point array.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: GREYPLOT; Figaro: IGREY, IMAGE; SPECDRE: MOVIE.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, and UNITS components of the input NDF.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         This application will handle data in all numeric types, though
         type conversion to integer will occur for unsigned byte and word
         images.  However, when there is no scaling only integer data will
         not be type converted, but this is not expensive for the expected
         byte-type data.
      }
   }
}
\begin{htmlonly}
Click \htmladdnormallink{here}{display_exam1.gif} to see an example
plot (101k).
\end{htmlonly}
\sstroutine{
   DIV
}{
   Divides one NDF data structure by another
}{
   \sstdescription{
      The routine divides one NDF data structure by another
      pixel-by-pixel to produce a new NDF.
   }
   \sstusage{
      div in1 in2 out
   }
   \sstparameters{
      \sstsubsection{
         IN1 = NDF (Read)
      }{
         First NDF, to be divided by the second NDF.
      }
      \sstsubsection{
         IN2 = NDF (Read)
      }{
         Second NDF, to be divided into the first NDF.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF to contain the ratio of the two input NDFs.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the NDF supplied for parameter IN1 to be used
         instead. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         div a b c
      }{
         This divides the NDF called a by the NDF called b, to make the
         NDF called c.  NDF c inherits its title from a.
      }
      \sstexamplesubsection{
         div out=c in1=a in2=b title="Normalised data"
      }{
         This divides the NDF called a by the NDF called b, to make the
         NDF called c.  NDF c has the title {\tt "Normalised data"}.
      }
   }
   \sstnotes{
      If the two input NDFs have different pixel-index bounds, then
      they will be trimmed to match before being divided.  An error will
      result if they have no pixels in common.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ADD, CADD, CDIV, CMULT, CSUB, MATHS, MULT, SUB.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, HISTORY, and VARIANCE components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Units processing is not supported at present and therefore the
         UNITS component is not propagated.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.
         Calculations will be performed using either real or double
         precision arithmetic, whichever is more appropriate.  If the input
         NDF structures contain values with other data types, then
         conversion will be performed as necessary.

      }
   }
}
\sstroutine{
   DRAWSIG
}{
   Draws ${\pm}n$ standard-deviation lines on a line plot
}{
   \sstdescription{
      This routine draws straight lines on an existing plot stored in
      the graphics database, such as produced by LINPLOT or HISTOGRAM.
      The lines are located at arbitrary multiples of the standard
      deviation (NSIGMA) either side of the mean of a given dataset.
      The default dataset is the one used to draw the existing plot.
      You can plot the lines horizontally or vertically as appropriate.
      The lines extend the full width or height of the plot's data
      area.  Up to five different multiples of the standard deviation
      may be presented in this fashion.

      The application also computes statistics for those array values
      that lie between each pair of plotted lines.  In other words it
      finds the statistics between clipping limits defined by each
      2$*$NSIGMA range centred on the unclipped mean.

      The task tabulates NSIGMA, the mean, the standard deviation, and
      the error in the mean after the application of each pair of
      clipping limits.  For comparison purposes the first line of the
      table presents these values without clipping.  The table is
      written at the normal reporting level.
   }
   \sstusage{
      drawsig ndf nsigma [axis] [comp] [sigcol] [linestyle]
   }
   \sstparameters{
      \sstsubsection{
         AXIS = LITERAL (Read)
      }{
         The orientation of the lines, or put another way, the axis
         which represents data value.  Thus the allowed values are
         {\tt "Horizontal"}, {\tt "Vertical"}, {\tt "X"}, or {\tt "Y"}.
         {\tt "Horizontal"} is equivalent to {\tt "Y"} and
         {\tt "Vertical"} is a synonym for {\tt "X"}.  On
         LINPLOT output AXIS would be {\tt "Y"}, but on a plot from HISTOGRAM
         it would be {\tt "X"}.  The suggested default is the current value.
         {\tt ["Y"]}
      }
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The name of the NDF array component from which to derive the
         mean and standard deviation used to draw the lines: {\tt "Data"},
         {\tt "Error"}, {\tt "Quality"} or {\tt "Variance"} (where
         {\tt "Error"} is the
         alternative to {\tt "Variance"} and causes the square root of the
         variance values to be taken before computing the statistics).
         If {\tt "Quality"} is specified, then the quality values are treated
         as numerical values (in the range 0 to 255).  {\tt ["Data"]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device to draw the sigma lines on.
         {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         LINESTYLE = \_INTEGER (Read)
      }{
         Line style to be used.  The allowed values produce the
         following styles.
         \begin{description}
         \item {\tt 1} = solid
         \item {\tt 2} = dashed
         \item {\tt 3} = dotted
         \item {\tt 4} = dot-dashed
         \end{description}
  
         LINESTYLE defaults to the current value, which is initially
         {\tt 3}, giving dotted lines. {\tt []}
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         The NDF structure containing the data array whose error limits
         are to be plotted.  Usually this parameter is not defined
         thereby causing the statistics to be derived from the dataset
         used to draw the plot.  If, however, you had plotted a section
         of a dataset but wanted to plot the statistics from the whole
         dataset, you would specify the full dataset with parameter NDF.
         {\tt [}The dataset used to create the existing plot.{\tt ]}
      }
      \sstsubsection{
         NSIGMA() = \_REAL (Read)
      }{
         Number of standard deviations about the mean at which the
         lines should be drawn.  The null value or {\tt 0.0} causes a line to
         be drawn at the mean value.
      }
      \sstsubsection{
         SIGCOL = \_INTEGER (Read)
      }{
         The colour in which to draw any graphics specified by
         parameter LINESTYLE.  The options are described below.

         \begin{description}
         \item [{\tt "MAX"}] --- The maximum colour index used for the
                            display of the image.
         \item [{\tt "MIN"}] --- The minimum colour index used for the
                            display of the image.
         \item [An integer]  --- The actual colour index.  It is constrained
                            between 0 and the maximum colour index
                            available on the device.
         \item [A named colour] --- Uses the named colour from the palette, and
                            if it is not present, the nearest colour
                            from the palette is selected.
         \end{description}

         If the colour is to remain unaltered as the lookup table is
         manipulated choose an integer between 0 and 15, or a named
         colour.  The suggested default is the current value.
      }
      \sstsubsection{
         THICK = \_REAL (Read)
      }{
         The thickness of the lines in the plot, where 1.0 is the
         normal thickness.  It must take a value in the range
         0.5--10.0.  {\tt [1.0]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         drawsig nsigma=3 linestyle=1
      }{
         This draws solid horizontal lines on the last DATA picture on
         the current graphics device located at plus and minus 3
         standard deviations about the mean.  The statistics come from
         the data array used to draw the DATA picture.
      }
      \sstexamplesubsection{
         drawsig phot 2.5
      }{
         This draws horizontal plus and minus 2.5 standard-deviation
         lines about the mean for the data in the NDF called phot on
         the default graphics device.
      }
      \sstexamplesubsection{
         drawsig cluster [2,3] X Error
      }{
         This draws vertical lines at plus and minus 2 and 3
         standard deviations about the mean for the error data in the
         NDF called cluster on the default graphics device.
      }
      \sstexamplesubsection{
         drawsig device=xwindows phot(20:119) 3 linestyle=3 sigcol=red
      }{
         This draws red dotted horizontal lines on the xwindows device
         at $\pm$ 3 standard deviations using the 100 pixels in NDF
         phot(20:119).
      }
   }
   \sstnotes{
      There must be an existing DATA picture stored within the graphics
      database for the chosen device.  Lines will only be plotted
      within this picture.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: HISTOGRAM, LINPLOT, MLINPLOT, STATS.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the DATA, VARIANCE, and
         QUALITY, components of the NDF.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  The
         statistics are calculated using double-precision floating point.

         \sstitem
         Any number of NDF dimensions is supported.
      }
   }
}
\sstroutine{
   ELPROF
}{
   Creates a radial or azimuthal profile of a 2-dimensional image
}{
   \sstdescription{
      This application will bin the input image into elliptical annuli
      or into a `fan' of adjacent sectors, centred on a specified
      position.  The mean data values in each bin are found, and stored in 
      a 1-dimensional NDF which can be examined using
      LINPLOT, INSPECT, {\it etc.}  Options exist to restrict the area binned
      to a given range of radial distance and/or azimuthal angle.  A
      2-dimensional mask image can optionally be produced indicating
      which bin each input pixel was placed in.

      If radial binning is selected (the default), then each bin is an
      elliptical annulus of shape and size determined by parameters
      RMIN, RMAX, ANGMAJ, RATIO, XC, YC, and WIDTH.  The bins can be
      restricted to a specified sector of these annuli using parameter
      ANGLIM.

      If azimuthal binning is selected, then each bin is a sector
      ({\it i.e.}\ a wedge-shape), with its vertex given by parameters XC and
      YC, and its opening angle given by parameters WIDTH.  The range of
      azimuthal angles to be binned can be specified by parameter
      ANGLIM.  The bins can be restricted to the intersection of these
      sectors with an elliptical annulus by specified values for
      parameters RMIN, RMAX, ANGMAJ and RATIO.
   }
   \sstusage{
      elprof in out nbin xc yc $\left\{ {\begin{tabular}{l}
                                     angmaj=? \\
                                     ratio=? \\
                                     rmin=? \\
                                     rmax=?
                                    \end{tabular} }
                          \right.$
                                    \newline\hspace*{12.5em}
                                    \makebox[0mm][c]{\small radial}

   }
   \sstparameters{
      \sstsubsection{
         ANGLIM( 2 ) = \_REAL (Read)
      }{
         Defines the wedge-shaped sector within which binning is to be
         performed.  The first value should be the azimuthal angle of
         the clockwise boundary of the sector, and the second should be
         the azimuthal angle of the anti-clockwise boundary. The angles
         are measured in degrees from the $x$-axis, and rotation from the
         $x$-axis to the $y$-axis is positive.  If only a single value is
         supplied, or if both values are equal, the sector starts at
         the given angle and extends for 360\dgs. {\tt [0]}
      }
      \sstsubsection{
         ANGMAJ = \_REAL (Read)
      }{
         The angle between the $x$-axis and the major axis of the
         ellipse, in degrees.  Rotation from the $x$-axis to the $y$-axis is
         positive.  If an azimuthal profile (see parameter RADIAL) is
         being produced a run-time default of zero is used, otherwise
         you will be prompted for a value. {\tt []}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF containing the 2-dimensional image from which a
         profile is to be generated.
      }
      \sstsubsection{
         MASK = NDF (Write)
      }{
         An output NDF of the same shape and size as the input NDF
         indicating the bin into which each input pixel was placed.  For
         radial profiles, the bins are identified by a mask value equal
         to the radius (in pixels) measured on the major axis, at the centre
         of the annular bin.  For azimuthal profiles, the bins are
         identified by a mask value equal to the angle from the $x$-axis
         to the centre of the sector-shaped bin (in degrees).  If a null
         value is supplied, then no mask NDF is produced. {\tt [!]}
      }
      \sstsubsection{
         MTITLE = LITERAL (Read)
      }{
         A title for the mask NDF. If a null value is given, the title
         is propagated from the input NDF.  This is only prompted for if
         MASK is given a non-null value. {\tt ["Mask created by KAPPA -
         Elprof"]}
      }
      \sstsubsection{
         NBIN = \_INTEGER (Read)
      }{
         The number of radial or azimuthal bins required.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The output 1-dimensional NDF containing the required profile.
         For radial profiles, it has associated axis information
         describing the radius, in pixels, at the centre of each
         annular bin (the radius is measured on the major axis).  For
         azimuthal profiles, the axis information describes the
         azimuthal angle, in degrees, at the centre of each
         sector-shaped bin.  It will contain associated variance
         information if the input NDF has associated variance
         information.
      }
      \sstsubsection{
         RADIAL = \_LOGICAL (Read)
      }{
         Specifies the sort of profile required.  If RADIAL is {\tt TRUE}, then
         a radial profile is produced in which each bin is an elliptical
         annulus.  Otherwise, an azimuthal profile is produced in which
         each bin is a wedge-shaped sector. {\tt [TRUE]}
      }
      \sstsubsection{
         RATIO = \_REAL (Read)
      }{
         The ratio of the length of the minor axis of the ellipse to
         the length of the major axis. It must be in the range 0.0 to
         1.0.  If an azimuthal profile (see parameter RADIAL) is being
         produced a run-time default of 1.0 is used, otherwise you are
         prompted for a value. {\tt []}
      }
      \sstsubsection{
         RMAX = \_REAL (Read)
      }{
         The radius in pixels, measured on the major axis, at the outer edge
         of the elliptical region to be binned.  If an azimuthal profile
         (see parameter RADIAL) is being produced a large run-time
         default is used which results in the entire image being
         binned, otherwise you are prompted for a value. {\tt []}
      }
      \sstsubsection{
         RMIN = \_REAL (Read)
      }{
         The radius in pixels, measured on the major axis, at the inner edge
         of the elliptical region to be binned.  If an azimuthal profile
         (see parameter RADIAL) is being produced a run-time default of
         0.0 is used, otherwise you are prompted for a value. {\tt []}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         A title for the output profile NDF.  If a null value is supplied
         the title is propagated from the input NDF. {\tt ["KAPPA - Elprof"]}
      }
      \sstsubsection{
         WIDTH = \_REAL (Read)
      }{
         The width of each bin.  If a radial profile is being created
         (see parameter RADIAL) this is the width of each annulus in
         pixels (measured on the major axis).  If an azimuthal profile
         is being created, it is the opening angle of each sector, in
         degrees.  The run-time default is chosen so that there are no
         gaps between adjacent bins.  Smaller values will result in gaps
         appearing between adjacent bins. The supplied value must be
         small enough to ensure that adjacent bins do not overlap.
         {\tt []}
      }
      \sstsubsection{
         XC = \_REAL (Read)
      }{
         The $x$ pixel co-ordinate of the centre of the ellipse, and the
         vertex of the sectors.
      }
      \sstsubsection{
         YC = \_REAL (Read)
      }{
         The $y$ pixel co-ordinate of the centre of the ellipse, and the
         vertex of the sectors.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         elprof galaxy galprof 20 113 210 angmaj=49 rmin=10 rmax=210 ratio=0.5
      }{
         This example will create a 1-dimensional NDF called galprof
         containing a radial profile of the 2-dimensional NDF called
         galaxy.  The profile will contain 20 bins and it will be
         centred on the pixel co-ordinates (113,210).  Each bin will be
         an annulus of an ellipse with axis ratio of 0.5 and inclination
         of 49\dgs\ to the $x$-axis.  The image will be binned between
         radii of 10 pixels, and 210 pixels (measured on the major
         axis), and there will be no gaps between adjacent bins ({\it i.e.}
         each bin will have a width on the major axis of about 10
         pixels).
      }
      \sstexamplesubsection{
         elprof galaxy galprof 10 113 210 radial=f anglim=20 rmin=50 rmax=60
      }{
         This example also creates a 1-dimensional NDF called
         galprof, this time containing an azimuthal profile of the
         2-dimensional NDF called galaxy, containing 10 bins.  Each
         bin will be a wedge-shaped sector with vertex at pixel
         co-ordinates (113,210).  The clockwise edge of the first bin
         will be at an angle of 20\dgs\ to the $x$-axis, and each bin
         will have a width (opening angle) of 36\dgs\ (so that 360\dgs
         are covered in total).  Only the section of each
         sector bounded by radii of 50 and 60 pixels is included in the
         profile.  In this case the default value of 1.0 is accepted for
         parameter RATIO and so the bins will form a circular annulus
         of width 10 pixels.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: INSPECT; ESP: ELLFOU, ELLPRO, SECTOR.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the DATA, VARIANCE, TITLE,
         UNITS, and HISTORY components of the input NDF.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using single-precision floating point.

      }
   }
}
\sstroutine{
   ERASE
}{
   Erases an HDS object
}{
   \sstdescription{
      This routine erases a specified HDS object or container file. If
      the object is a structure, then all the structure's components
      (and sub-components, {\it etc.}) are also erased.  If a slice or cell
      of an array is specified, then the entire array is erased.
   }
   \sstusage{
      erase object
   }
   \sstparameters{
      \sstsubsection{
         OBJECT = UNIV (Write)
      }{
         The HDS object or container file to be erased.
      }
      \sstsubsection{
         OK = \_LOGICAL (Read)
      }{
         This parameter is used to seek confirmation before an object
         is erased.  If a {\tt TRUE} value is given, then the HDS object will
         be erased.  If a {\tt FALSE} value is given, then the object will not
         be erased and a message will be issued to this effect.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         erase horse
      }{
         This erases the HDS container file called {\tt horse.sdf}.
      }
      \sstexamplesubsection{
         erase fig123.axis
      }{
         This erases the AXIS component of the HDS file called
         {\tt fig123.sdf}.
         If AXIS is a structure, all its components are erased too.
      }
      \sstexamplesubsection{
         erase fig123.axis(1).label
      }{
         This erases the LABEL component within the first element of
         the AXIS structure of the HDS file called {\tt fig123.sdf}.
      }
      \sstexamplesubsection{
         erase \$AGI\_USER/agi\_restar.agi\_3200\_1
      }{
         This erases the AGIDEV\_3200\_1 structure of the HDS file called \linebreak
         {\tt \$AGI\_USER/agi\_restar.sdf}.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      Figaro: CREOBJ, DELOBJ, RENOBJ.
   }
}
\sstroutine{
   ERRCLIP  
}{
   Removes pixels with large errors from an NDF
}{
   \sstdescription{
      This application produces a copy of the input NDF in which pixels
      with errors greater than a specified limit are set invalid in
      both DATA and VARIANCE components.  The error limit may be
      specified as the maximum acceptable standard deviation (or
      variance), or the minimum acceptable signal-to-noise ratio.
   }
   \sstusage{
      errclip in out limit [mode]
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF.  An error is reported if it contains no VARIANCE
         component.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The output NDF.
      }
      \sstsubsection{
         LIMIT = \_DOUBLE (Read)
      }{
         Either the maximum acceptable standard deviation or variance
         value, or the minimum acceptable signal-to-noise ratio
         (depending on the value given for MODE).  It must be positive.
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         Determines how the value supplied for LIMIT is to be
         interpreted: {\tt "Sigma"} for a standard deviation, {\tt "Variance"}
         for variance, or {\tt "SNR"} for minimum signal-to-noise ratio.
         {\tt ["Sigma"]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         errclip m51 m51\_good 2.0
      }{
         The NDF m51\_good is created holding a copy of m51 in which all
         pixels with standard deviation greater than 2 are set invalid.
      }
      \sstexamplesubsection{
         errclip m51 m51\_good 2.0 snr
      }{
         The NDF m51\_good is created holding a copy of m51 in which all
         pixels with a signal-to-noise ratio less than 2 are set
         invalid.
      }
      \sstexamplesubsection{
         errclip m51 m51\_good mode=v limit=100
      }{
         The NDF m51\_good is created holding a copy of m51 in which all
         pixels with a variance greater than 100 are set invalid.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The limit and the number of rejected pixels are reported.

         \sstitem
         A pair of output data and variance values are set bad when
         either of the input data or variances values is bad.

         \sstitem
         For MODE={\tt "SNR"} the comparison is with respect to the absolute
         data value.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FFCLEAN, PASTE, SEGMENT, SETMAGIC, THRESH.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF data
         structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  The output
         NDF has the same numeric type as the input NDF.  However, all
         internal calculations are performed in double precision.
      }
   }
}
\manroutine {{\manheadstyle{EXP10}}}{ Takes the base-10 exponential of each pixel of
  a data array.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine takes the exponential to base 10 of each pixel of
  a data array. The result goes into a new output data array.
  Both data arrays are stored in the {\mantt{IMAGE}} format.

  The magic-value method is used for processing bad data.

\manroutineitem {Invocation }{}
  EXP10

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  Input {\mantt{IMAGE}} structure containing the data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{BASE}}  }{{\mantt{\_REAL}}}
  Base of exponential to be taken of each input data array pixel. \mbox{\mantt [10]}
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure holding result of exponentiated array.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title string for the output {\mantt{IMAGE}} structure. \mbox{\mantt ['KAPPA - Exp10']}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\manroutine {{\manheadstyle{EXPE}}}{ Takes the exponential of each pixel of a data
  array (base e).}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine takes the exponential to base e of each pixel of
  a data array. The result goes into a new output data array.
  Both data arrays are stored in the {\mantt{IMAGE}} format.

  The magic-value method is used for processing bad data.

\manroutineitem {Invocation }{}
  EXPE

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  Input {\mantt{IMAGE}} structure containing the data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{BASE}}  }{{\mantt{\_REAL}}}
  Base of exponential to be taken of each input data array pixel.
  \mbox{\mantt [2.718282]}
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure holding result of exponentiated array.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title string for the output {\mantt{IMAGE}} structure. \mbox{\mantt ['KAPPA - Expe']}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\manroutine {{\manheadstyle{EXPON}}}{ Takes the exponential of each pixel
   of a data array.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine takes the exponential to an input base of each pixel
  of a data array. The result goes into a new output data array.
  Both data arrays are stored in the {\mantt{IMAGE}} format.

  The magic-value method is used for processing bad data.

\manroutineitem {Invocation }{}
  EXPON

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  Input {\mantt{IMAGE}} structure containing the data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{BASE}}  }{{\mantt{\_REAL}}}
  Base of exponential to be taken of each input data array pixel.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure holding result of exponentiated array.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title string for the output {\mantt{IMAGE}} structure. \mbox{\mantt ['KAPPA - Expon']}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\sstroutine{
   FFCLEAN
}{
   Removes defects from a substantially flat 1- or 2-dimensional NDF
}{
   \sstdescription{
      This application cleans a 1- or 2-dimensional NDF by removing defects smaller
      than a specified size.  The defects are flagged with the bad
      value. The defects are found by looking for pixels that deviate
      from the image's smoothed version by more than an arbitrary
      number of standard deviations from the local mean, and that lie
      within a specified range of values.  Therefore, the image must be
      substantially flat.  The data variances provide the local noise
      estimate for the threshold, but if these are not available a
      variance for the whole of the image is derived from the mean
      squared deviations of the original and smoothed images.  The
      smoothed version of the image is obtained by block averaging over
      a rectangular box.  An iterative process progressively removes
      the outliers from the image.
   }
   \sstusage{
      ffclean in out clip box [thresh] [wlim] [ilevel]
   }
   \sstparameters{
      \sstsubsection{
         BOX( 2 ) = \_INTEGER (Read)
      }{
         The $x$ and $y$ sizes (in pixels) of the rectangular box to be
         applied to smooth the image. If only a single value is given,
         then it will be duplicated so that a square filter is used
         except where the image is 1-dimensional for which the box size
         along the insignificant dimension is set to 1.  The values
         given will be rounded up to positive odd integers if
         necessary.
      }
      \sstsubsection{
         CLIP( ) = \_REAL (Read)
      }{
         The number of standard deviations for the rejection threshold
         of each iteration.  Pixels that deviate from their counterpart
         in the smoothed image by more than CLIP times the noise are
         made bad.  The number of values given specifies the number of
         iterations.  Values should lie in the range 0.5--100.  Up to
         one hundred values may be given.  {\tt [3.0, 3.0, 3.0]}
      }
      \sstsubsection{
         ILEVEL = \_INTEGER (Read)
      }{
         The interactive level of the routine.  When it is greater or
         equal to two, the application will report the intermediate
         results after each iteration during processing. It should lie
         between 1 and 3. {\tt [2]}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The 1- or 2-dimensional NDF containing the input image to be cleaned.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The NDF to contain the cleaned image.
      }
      \sstsubsection{
         THRESH( 2 ) = \_DOUBLE (Read)
      }{
         The range between which data values must lie if cleaning is to
         occur.  Thus it is possible to clean the background without
         removing the cores of images by a judicious choice of these
         thresholds.  If null, {\tt !}, is given, then there is no limit on
         the data range. {\tt [!]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         The title of the output NDF.  A null ({\tt !}) value means using the
         title of the input NDF. {\tt [!]}
      }
      \sstsubsection{
         WLIM = \_REAL (Read)
      }{
         If the input image contains bad pixels, then this parameter
         may be used to determine the number of good pixels which must
         be present within the smoothing box before a valid output
         pixel is generated.  It can be used, for example, to prevent
         output pixels from being generated in regions where there are
         relatively few good pixels to contribute to the smoothed
         result.

         By default, a null ({\tt !}) value is used for WLIM, which causes
         the pattern of bad pixels to be propagated from the input
         image to the output image unchanged. In this case, smoothed
         output values are only calculated for those pixels which are
         not bad in the input image.

         If a numerical value is given for WLIM, then it specifies the
         minimum fraction of good pixels which must be present in the
         smoothing box in order to generate a good output pixel.  If
         this specified minimum fraction of good input pixels is not
         present, then a bad output pixel will result, otherwise a
         smoothed output value will be calculated.  The value of this
         parameter should lie between 0.0 and 1.0 (the actual number
         used will be rounded up if necessary to correspond to at least
         1 pixel). {\tt [!]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         SIGMA = \_DOUBLE (Write)
      }{
         The estimation of the RMS noise per pixel of the output image.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         ffclean dirty clean $\backslash$
      }{
         The NDF called dirty is filtered such that pixels that
         deviate by more than three standard deviations from the
         smoothed version of dirty are rejected.  Three iterations are performed.
         Each pixel in the smoothed image is the average of the
         neighbouring nine pixels.  The filtered NDF is called clean.
      }
      \sstexamplesubsection{
         ffclean out=clean in=dirty thresh=[-100,200]
      }{
         As above except only those pixels whose values lie between
         $-$100 and 200 can be cleaned.
      }
      \sstexamplesubsection{
         ffclean poxy dazed [2.5,2.8] [5,5]
      }{
         The 2-dimensional NDF called poxy is filtered such that pixels that
         deviate by more than 2.5 then 2.8 standard deviations from the
         smoothed version of poxy are rejected.  The smoothing is an
         average of a 5-by-5-pixel neighbourhood.
         The filtered NDF is called dazed.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CHPIX, FILLBAD, GLITCH, MEDIAN, MSTATS, ZAPLIN; Figaro: BCLEAN,
      COSREJ, CLEAN, ISEDIT, MEDFILT, MEDSKY, TIPPEX.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using single- or double-precision floating point as
         appropriate.
      }
   }
}
\sstroutine{
   FILLBAD
}{
   Removes regions of bad values from a 2-dimensional NDF
}{
   \sstdescription{
      This application replaces bad values in a 2-dimensional NDF with
      a smooth function which matches the surrounding data.  It can fill
      arbitrarily shaped regions of bad values within images.

      It forms a smooth replacement function for the regions of bad
      values by forming successive approximations to a solution of
      Laplace's equation, with the surrounding valid data providing the
      boundary conditions.
   }
   \sstusage{
      fillbad in out [niter] [size]
   }
   \sstparameters{
      \sstsubsection{
         BLOCK = \_INTEGER (Read)
      }{
         The maximum number of pixels along either dimension when the
         array is divided into blocks for processing.  It is ignored
         unless MEMORY={\tt TRUE}.  This must be at least 256.
         {\tt [512]}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The 2-dimensional NDF containing the input image with bad
         values.
      }
      \sstsubsection{
         MEMORY = \_LOGICAL (Read)
      }{
         If this is {\tt FALSE}, the whole array is processed at the same
         time.  If it is {\tt TRUE}, the array is divided into chunks whose
         maximum dimension along an axis is given by parameter BLOCK.
         {\tt [FALSE]}
      }
      \sstsubsection{
         NITER = INTEGER (Given)
      }{
         The number of iterations of the relaxation algorithm.  This
         value cannot be less than two, since this is the minimum
         number required to ensure that all bad values are assigned a
         replacement value.  The more iterations used, the finer the
         detail in the replacement function and the closer it will
         match the surrounding good data.  {\tt [2]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The NDF to contain the image free of bad values.
      }
      \sstsubsection{
         SIZE  = \_REAL (Read)
      }{
         The initial scale length in pixels to be used in the first
         iteration.  For maximum efficiency, it should normally have a
         value about half the `size' of the largest invalid region to
         be replaced.  (See the Notes section for more details.)
         {\tt [5.0]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         The title of the output NDF.  A null ({\tt !}) value means using the
         title of the input NDF. {\tt [!]}
      }
      \sstsubsection{
         VARIANCE = \_LOGICAL (Read)
      }{
         If VARIANCE is {\tt TRUE}, variance information is to be propagated;
         any bad values therein are filled.  Also the variance is used
         to weight the calculation of the replacement data values.  If
         VARIANCE is {\tt FALSE}, there will be no variance processing thus
         requiring two less arrays in memory.  This parameter is only
         accessed if the input NDF contains a VARIANCE component.
         {\tt [TRUE]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         CNGMAX = \_DOUBLE (Write)
      }{
         The maximum absolute change in output values which occurred in
         the final iteration.
      }
      \sstsubsection{
         CNGRMS = \_DOUBLE (Write)
      }{
         The root-mean-squared change in output values which occurred in the last
         iteration.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fillbad aa bb
      }{
         The NDF called aa has its bad pixels replaced by good values
         derived from the surrounding good pixel values using two
         iterations of a relaxation algorithm.  The initial scale length
         is 5 pixels.  The resultant NDF is called bb.
      }
      \sstexamplesubsection{
         fillbad aa bb 6 20 title="Cleaned image"
      }{
         As above except the initial scale length is 20 pixels, 5
         iterations will be performed, and the output title is {\tt "}Cleaned
         image{\tt "} instead of the title of NDF aa.
      }
      \sstexamplesubsection{
         fillbad aa bb memory novariance
      }{
         As in the first example except that processing is performed
         with blocks up to 512 by 512 pixels to reduce the memory
         requirements, and no variance information will be used or
         propagated.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The algorithm is based on the relaxation method of repeatedly
         replacing each bad pixel with the mean of its four nearest
         neighbours.  Such a method converges to the required solution,
         but information about the good regions only propagates at a rate
         of about one pixel per iteration into the bad regions, resulting
         in slow convergence if large areas are to be filled.

         This application speeds convergence to an acceptable function by
         forming the replacement mean from all the pixels in the same
         image row and column, using a weight which decreases
         exponentially with distance and goes to zero after the first good
         pixel is encountered in any direction.  If there is variance
         information, this is included in the weighting so as to give more
         weight to surrounding values with lower variance.  The scale
         length of the exponential weight is initially set large, to allow
         rapid propagation of an approximate `smooth' solution into the
         bad regions---an initially acceptable solution is thus rapidly
         obtained (often in the first one or two iterations).  The scale
         length is subsequently reduced by a factor of 2 whenever the
         maximum absolute change occurring in an iteration has decreased
         by a factor of 4 since the current scale length was first used.
         In this way, later iterations introduce progressively finer
         detail into the solution.  Since this fine detail occurs
         predominantly close to the `crinkly' edges of the bad regions,
         the slower propagation of the solution in the later iterations is
         then less important.

         When there is variance processing the output variance is
         reassigned if either the input variance or data value was bad.
         Where the input value is good but its associated variance is bad,
         the calculation proceeds as if the data value were bad, except
         that only the variance is substituted in the output.  The new
         variance is approximated as twice the inverse of the sum of the
         weights.

         \sstitem
         The price of the above efficiency means that considerable
         workspace is required, typically two or three times the size of
         the input image, but even larger for the one and two-byte integer
         types.  If memory is at a premium, there is an option to process
         in blocks ({\it cf.}\ parameter MEMORY).  However, this may not give as
         good results as processing the array in full, especially when the
         bad-pixel regions span blocks.

         \sstitem
         The value of the parameter SIZE is not critical and the
         default value will normally prove effective.  It primarily
         affects the efficiency of the algorithm on various size scales.
         If the smoothing scale is set to a large value, large scale
         variations in the replacement function are rapidly found, while
         smaller scale variations may require many iterations.
         Conversely, a small value will rapidly produce the small scale
         variations but not the larger scale ones.  The aim is to select
         an initial value SIZE such that during the course of a few
         iterations, the range of size scales in the replacement function
         are all used.  In practice this means that the value of SIZE
         should be about half the size of the largest scale variations
         expected.  Unless the valid pixels are very sparse, this is
         usually determined by the `size' of the largest invalid region to
         be replaced.

         \sstitem
         An error results if the input NDF has no bad values to replace.

         \sstitem
         The progress of the iterations is reported.
      }
   }
   \sstdiytopic{
      Timing
   }{
      The time taken increases in proportion to the value of NITER.
      Adjusting the SIZE parameter to correspond to the largest regions
      of bad values will reduce the processing time.  See the Notes
      section.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CHPIX, GLITCH, MEDIAN, ZAPLIN; Figaro: BCLEAN,
      COSREJ, CLEAN, ISEDIT, MEDFILT, MEDSKY, REMBAD, TIPPEX.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.  The output bad-pixel flag is set to indicate no bad
         values in the data and variance arrays.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using single- or double-precision floating point as
         appropriate.
      }
   }
}
 
\sstroutine{
   FITSDIN
}{
   Reads a FITS disc file composed of simple, group or table objects
}{
   \sstdescription{
      This application reads selected disc-FITS files.  The files may
      be Basic (simple) FITS, and/or have TABLE extensions (Harten
      {\it et al.}\ 1988).

      The programme reads a simple or a random-groups-format FITS file
      (Wells {\it et al.}\ 1981; Greisen \& Harten 1981), and writes the
      data into an NDF, and the headers into the NDF's FITS extension.
      Table-format files (Grosb{\o}l {\it et al.}\ 1988) are read, and the
      application creates two files: a text formatted table/catalogue
      and a FACTS description file (as used by SCAR) based upon the FITS
      header cards.  Composite FITS files can be processed.  You may
      specify a list of files, including wildcards.  A record of the
      FITS headers, and group parameters (for a group-format file) can
      be stored in a text file.

      There is an option to run in automatic mode, where the names of
      output NDF data structures are generated automatically, and you
      can decide whether or not format conversion is to be applied to
      all files (rather than being prompted for each).  This is very
      useful if there is a large number of files to be processed.  Even
      if you want unique file names, format-conversion prompting may be
      switched off globally.
   }
   \sstusage{
      fitsdin files out [auto] fmtcnv [logfile] dscftable=? table=?
   }
   \sstparameters{
      \sstsubsection{
         AUTO = \_LOGICAL (Read)
      }{
         It is {\tt TRUE} if automatic mode is required, where the name of each
         output NDF structure or table file is to be generated by the
         application, and therefore not prompted; and a global
         format-conversion switch may be set.  In manual mode the
         FITS header is reported, but not in automatic.

         In automatic mode the application generates a filename
         beginning with the input filename, less any extension.  For
         example, if the input file was {\tt saturn.fits} the filename of the
         output NDF would be {\tt saturn.sdf}, and an output table would be
         {\tt saturn.dat} with a description file {\tt dscfsaturn.dat}.
         If there are sub-files (more than one FITS object in the file)
         a suffix {\tt \_<subfile>} is appended.  So if {\tt saturn.fits}
         comprised a simple file followed by a table, the table would
         be called {\tt SATURN\_2.DAT} and the description file
         {\tt DSCFSATURN\_2.DAT}.  For group format a suffix
         {\tt G<groupnumber>} is appended.  Thus if {\tt saturn.fits}
         is a group format file, the first NDF created would be
         called {\tt saturn.sdf}, the second would be {\tt saturnG2.sdf}.
         {\tt [FALSE]}
      }
      \sstsubsection{
         DSCFTABLE = FILENAME (Read)
      }{
         Name of the text file to contain the FACTS descriptors, which
         defines the table's format for {\footnotesize SCAR}.  Since {\footnotesize SCAR} is now
         deprecated, this parameter has little use, except perhaps to
         give a summary of the format of the file specified by parameter
         TABLE.  A null value ({\tt !}) means that no description file will
         be created, so this is now the recommended usage.  If your
         FITS file comprises just tables, you should consider other
         tools such as the {\footnotesize CURSA} package, which has facilities for
         examining and processing ASCII and binary tables in FITS files.

         A suggested filename for the description file is reported
         immediately prior to prompting in manual mode.  It is the name
         of the catalogue, as written in the FITS header, with a
         {\tt "dscf"} prefix.
      }
      \sstsubsection{
         FILES() = LITERAL (Read)
      }{
         A list of (optionally wild-carded) file specifications which
         identify the disc-FITS files to be processed.  Up to 10 values
         may be given, but only a single specification such as {\tt "*.fits"}
         is normally required.  Be careful not to include non-FITS files
         in this list.
      }
      \sstsubsection{
         FMTCNV = \_LOGICAL (Read)
      }{
         This specifies whether or not format conversion will occur.
         If {\tt FALSE}, the HDS type of the data array in the NDF will be
         the equivalent of the FITS data format in the file ({\it e.g.}\ BITPIX =
         16 creates a \_WORD array).  If {\tt TRUE}, the data array in the
         current file, or all files in automatic mode, will be
         converted from the FITS data type in the FITS file to \_REAL in the NDF.
         The conversion applies the values of the FITS keywords BSCALE
         and BZERO to the FITS-file data to generate the {\tt "}true{\tt "} data values.
         If BSCALE and BZERO are not given in the FITS header, they are
         taken to be 1.0 and 0.0 respectively.
         The suggested default is {\tt TRUE}.
      }
      \sstsubsection{
         GLOCON  = \_LOGICAL (Read)
      }{
         If {\tt FALSE} a format-conversion query occurs for each FITS file.
         If {\tt TRUE}, the value of parameter FMTCNV is obtained before any file
         numbers and will apply to all data arrays.  It is ignored
         in automatic mode---in effect it becomes true. {\tt [FALSE]}
      }
      \sstsubsection{
         LOGFILE = FILENAME (Read)
      }{
         The file name of the text log of the FITS header cards.
         For group-format data the group parameters are evaluated
         and appended to the full header.  The log includes the names of
         the output files used to store the data array or table. A null
         value ({\tt !}) means that no log file is produced. {\tt [!]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF structure holding the full contents of the FITS
         file.  If the null value ({\tt !}) is given no NDF will be created.
         This offers an opportunity to review the descriptors before
         deciding whether or not the data are to be extracted.
      }
      \sstsubsection{
         TABLE = FILENAME (Read)
      }{
         Name of the text file to contain the table itself, read from
         the file.  In manual mode, the suggested default filename is the name of
         description file less the {\tt "dscf"} prefix, or if there is no
         description file or if the description file does not have the
         {\tt "dscf"} prefix, the suggested name reverts to the catalogue name
         in the FITS header.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitsdin files=*.fit auto nofmtcnv
      }{
         This reads all the files with extension {\tt "fit"} in the default
         directory.  If the files were {\tt sao.fit} and {\tt moimp.fit}
         and each
         contained just an image array, the output NDFs will be sao and
         moimp respectively.  The data will not have format conversion.
      }
      \sstexamplesubsection{
         fitsdin files=ccd.ifits fmtcnv logfile=jkt.log
      }{
         This reads the file {\tt ccd.ifits} and processes all the FITS
         objects within it.  Integer data arrays are converted to real
         using the scale and zero found in the FITS header.  A record
         of the headers and the names of the output files are written
         to the text file {\tt jkt.log}.
      }
      \sstexamplesubsection{
         fitsdin files=[*.*fits,*.mt] glocon fmtcnv
      }{
         This reads the files {\tt *.*fits} and {\tt *.mt} and processes all the
         FITS objects within them.  Integer data arrays are converted
         to real using the scale and zero found in the FITS header.
         Any IEEE-format data will not be converted although the global
         conversion switch is on.
      }
   }
   \sstdiytopic{
      References
   }{
      \begin{refs}
      \item  Wells, D.C., Greisen, E.W. \& Harten, R.H. 1981,
      {\it Astron. Astrophys. Suppl. Ser.} {\bf 44}, 363.

      \item  Greisen, E.W. \& Harten, R.H. 1981,
      {\it Astron. Astrophys. Suppl. Ser.} {\bf 44}, 371.

      \item  Grosb{\o}l, P., Harten, R.H., Greisen, E.W \& Wells, D.C.
      1988 {\it Astron. Astrophys. Suppl. Ser.} {\bf 73}, 359.

      \item  Harten, R.H., Grosb{\o}l, P., Greisen, E.W \& Wells, D.C.
      1988 {\it Astron. Astrophys. Suppl. Ser.} {\bf 73}, 365.

      \end{refs}
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSHEAD, FITSIMP, FITSIN, FITSLIST; CONVERT: FITS2NDF;
      CURSA; Figaro: RDFITS.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         The application processes FITS files blocked at other than an
         integer multiple of 2880 bytes up to a maximum of 28800, provided
         it is a multiple of the number of bytes per data value.

         \sstitem
         For simple or group format FITS:

         \ssthitemlist{

            \sstitem
            IEEE floating point is supported.

            \sstitem
            If BUNIT is present its value will appear as the NDF's units
            component.

            \sstitem
            If OBJECT is present its value will appear as the NDF's title
            component.

            \sstitem
            If the BLANK item is present in the header, undefined pixels
            are converted from the BLANK value to Starlink-standard bad
            value during data conversion.

            \sstitem
            An AXIS component will be stored in the NDF if the CRVAL$n$
            keyword is present.  ($n$ is the number of the dimension.)  If the
            CRPIX$n$ keyword is absent it defaults to 1, and likewise for the
            CDELT$n$ keyword.  The value of CRTYPE$n$ is made the label of the
            axis structure.
         }

         \sstitem
         For groups format, a new NDF is created for each data array.
         The name of the NDF of the second and subsequent data arrays is
         generated by the application as the {\tt <filename>G<number>}, where
         {\tt <filename>} is the name of the first NDF, you supply or
         generated automatically, and {\tt <number>} is the number of the group.

         Each group NDF contains the full header in the FITS extension,
         appended by the set of group parameters.  The group parameters
         are evaluated using their scales and offsets, and made to look
         like FITS cards, whose keywords are derived from the values of
         PTYPE$m$ in the main header.  ($m$ is the number of the group
         parameter.) The same format is used in the log file.

         \sstitem
         If there is no data array in the FITS file, {\it i.e.}\ the FITS file comprises
         header cards only, then a dummy vector data array of dimension
         two is created to make the output a valid NDF.  This data array
         is undefined.
      }
   }
}
\sstroutine{
   FITSEDIT
}{
   Edits the FITS extension of an NDF
}{
   \sstdescription{
      This procedure allows you to use your favourite editor to
      modify the FITS headers stored in an NDF's FITS extension.
      There is limited validation of the FITS headers after editing.
   }
   \sstusage{
      fitsedit ndf
   }
   \sstparameters{
      \sstsubsection{
         NDF = NDF (Read)
      }{
         The name of the NDF whose FITS extension is to be edited.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitsedit m51b
      }{
         This allows editing of the FITS headers in the NDF called m51b.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         This uses the environmental variable, EDITOR, to select
         the editor.  If this variable is undefined vi is assumed.

         \sstitem
         The script lists the headers to a temporary file; allows text
         editing; and then replaces the former FITS extension with the
         modified version, performing some validation at this stage.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSEXP, FITSHEAD, FITSIMP, FITSLIST; Figaro: FITSKEYS.
   }
}
\sstroutine{
   FITSEXIST
}{
   Inquires whether or not a keyword exists in a FITS extension
}{
   \sstdescription{
      This application reports whether or not a keyword exists in an NDF's
      FITS extension.

      It is a synonym for {\tt fitsmod edit=exist mode=interface}.
   }
   \sstusage{
      fitsexist ndf keyword
   }
   \sstparameters{
      \sstsubsection{
         KEYWORD = LITERAL (Given)
      }{
         The name of the keyword whose existence in the FITS extension
         is to be tested.  A name may be compound to handle hierarchical
         keywords, and it has the form keyword1.keyword2.keyword3
         {\em etc.}  The maximum number of keywords per FITS card is 20.
         Each keyword must be no longer than 8 characters, and be a
         valid FITS keyword comprising only alphanumeric characters,
         hyphen, and underscore.  Any lowercase letters are converted to
         uppercase and blanks are removed before comparison with the
         existing keywords.

         KEYWORD may have an occurrence specified in brackets {\tt []}
         following the name.  This enables testing for the existence of
         multiple occurrences.  Note that it is not normal to have
         multiple occurrences of a keyword in a FITS header, unless it
         is blank, COMMENT or HISTORY.  Any text between the brackets
         other than a positive integer is interpreted as the first
         occurrence.

         The suggested value is the current value.
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         The NDF containing the FITS keyword.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitsexist abc bscale
      }{
         This reports {\tt TRUE} or {\tt FALSE} depending on whether or not the
         FITS keyword BSCALE exists in the FITS extension of the NDF
         called abc.
      }
      \sstexamplesubsection{
         fitsexist ndf=abc keyword=date[2]
      }{
         This reports {\tt TRUE} or {\tt FALSE} depending on whether or not the
         FITS there are at least two occurrences of the keyword DATE.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSEDIT, FITSHEAD, FITSLIST, FITSMOD, FITSVAL.
   }
}
\sstroutine{
   FITSEXP
}{
   Exports NDF-extension information into an NDF FITS extension
}{
   \sstdescription{
      This application places the values of components of an NDF
      extension into the FITS extension within the same NDF.  This
      operation is needed if auxiliary data are to appear in the header
      of a FITS file converted from the NDF.  The list of extension
      components whose values are to be copied, their corresponding
      FITS keyword names, optional FITS inline comments, and the
      location of the new FITS header are specified in a {\tt "}keyword
      translation table{\tt "} held in a separate text file.
   }
   \sstusage{
      fitsexp ndf table
   }
   \sstparameters{
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF in which the extension data are to be exported to
         the FITS extension.
      }
      \sstsubsection{
         TABLE = FILE (Read)
      }{
         The text file containing the keyword translation table. The
         format of this file is described under {\tt "}Table Format{\tt "}.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitsexp datafile fitstable.txt
      }{
         This writes new FITS-extension elements for the NDF called
         datafile, creating the FITS extension if it does not exist.
         The selection of auxiliary components to export to the FITS
         extension, their keyword names, locations, and comments
         are under the control of a keyword translation table held in
         the file {\tt fitstable.txt}.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Requests to assign values to the following reserved keywords
         in the FITS extension are ignored: SIMPLE, BITPIX, NAXIS, NAXISn,
         EXTEND, PCOUNT, GCOUNT, XTENSION, BLOCKED, and END.

         \sstitem
         Only scalar or one-element vector components may be
         transferred to the FITS extension.

         \sstitem
         The data type of the component selects the type of the FITS
         value.

         \sstitem
         If the destination keyword exists, the existing value and
         comment are replaced with the new values.

         \sstitem
         If an error is found within a line, processing continues
         to the next line and the error reported.

         \sstitem
         To be sure that the resultant FITS extension is what you
         desired, you should inspect it using the command fitslist before
         exporting the data.  If there is something wrong, you may find it
         convenient to use command fitsedit to make minor corrections.
      }
   }
   \sstdiytopic{
      Timing
   }{
      Approximately proportional to the number of FITS keywords to be
      translated.
   }
   \sstdiytopic{
      Table Format
   }{
      The keyword translation table should be held in a text file, with
      one extension component specified per line.  Each line should
      contain two or three fields, separated by spaces and/or tabs, as
      follows.

      \sstitemlist{

         \sstitem
         Field 1:
            The name of the input extension component whose value is to be
            copied to the FITS extension.  For example, {\tt CCDPACK.FILTER}
            would copy the value of the component called FILTER in the
            extension called CCDPACK; and \linebreak
            {\tt IRAS90.ASTROMETRY.EQUINOX} would
            copy the value of component EQUINOX in the structure
            ASTROMETRY in the extension IRAS90.  The extension may not be
            FITS.

         \sstitem
         Field 2:
            The name of the FITS keyword to which the value is to be
            copied.  Hierarchical keywords are not permissible.  The
            keyword name may be followed by a further keyword name in
            parentheses (and no spaces).  This second keyword defines the
            card before which the new keyword is to be placed.  If this
            second keyword is not present in the FITS extension or is not
            supplied, the new header card is placed at the end of the
            existing cards, but immediately before any END card.  For
            example, {\tt EQUINOX(EPOCH)} would write the keyword EQUINOX
            immediately before the existing card with keyword EPOCH.  FITS
            keywords are limited to 8 characters and may only comprise
            uppercase alphabetic characters, digits, underscore, and
            hyphen.  While it is possible to have multiple occurrences of
            the same keyword in a FITS header, it is regarded as bad
            practice.  For this and efficiency reasons, this programme
            only looks for the first appearance of a keyword when
            substituting the values, and so only the last value inserted
            appears in the final FITS extension.  (See {\tt "}Implementation
            Status{\tt "}.)

         \sstitem
         Field 3:
            The comment to appear in the FITS header card for the chosen
            keyword.  This field is optional.  As much of the comment will
            appear in the header card as the value permits up to a maximum
            of 47 characters.

      }
      Comments may appear at any point in the table and should begin
      with an exclamation mark.  The remainder of the line will then be
      ignored.
   }
   \sstdiytopic{
      References
   }{
      {\tt "}A User's Guide for the Flexible Image Transport System (FITS){\tt "},
      NASA/Science Office of Science and Technology (1994).
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSEDIT, FITSHEAD, FITSLIST, FITSMOD; CONVERT: NDF2FITS.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         The replacements are made in blocks of 32 to reduce the number
         of time-consuming shuffles of the FITS extension.  Thus it is
         possible to locate a new keyword before another keyword, provided
         the latter keyword appears in an earlier block, though reliance
         on this feature is discouraged; instead run the application
         twice.

         \sstitem
         For each block the application inserts new cards or relocates
         old ones, marking each with different tokens, and then sorts the
         FITS extension into the requested order, removing the relocated
         cards.  It then inserts the new values.  If there are multiple
         occurrences of a keyword, this process can leave behind cards
         having the token value {\tt '\{undefined\}'}.
      }
   }
}

\sstroutine{
   FITSHEAD
}{
   Lists the headers of FITS files
}{
   \sstdescription{
      This procedure lists to standard output the headers of the primary
      header and data unit, and any extensions present that are
      contained within a set of input FITS files, or a range of
      specified files on a tape.
   }
   \sstusage{
      fitshead file [block] [start] [finish]
   }
   \sstparameters{
      \sstsubsection{
         FILE  = FILENAME (Read)
      }{
         A space-separated list of FITS files whose headers are to be
         listed, or the name of a single no-rewind tape device.  The list
         of files can include wildcard characters.
      }
      \sstsubsection{
         BLOCK = \_INTEGER (Read)
      }{
         The FITS blocking factor of the tape to list.  This is the tape
         blocksize in bytes divided by the FITS record length of 2880
         bytes.  BLOCK must be a positive integer, between 1 and 12,
         otherwise you will be prompted for a new value.  Should the first
         argument not be a tape device, this argument will be treated as
         a file name. {\tt [1]}
      }
      \sstsubsection{
         START = \_INTEGER (Read)
      }{
         The first file on the tape to list.  This defaults to 1, {\it i.e.}
         the start of the tape.  It must be a positive integer,
         otherwise you will be prompted for a new value.  Should the
         first argument not be a tape device, this argument will be
         treated as a file name. {\tt [1]}
      }
      \sstsubsection{
         FINISH = \_INTEGER (Read)
      }{
         The last file on the tape to list.  This defaults to the end
         of the tape.  It must be a positive integer and at least equal
         to the value of start, otherwise you will be prompted for a new
         value.  Should the first argument not be a tape device, this
         argument will be treated as a file name. {\tt []}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitshead /dev/nrmt1
      }{
         This lists the FITS headers for all the files of the tape mounted
         on device {\tt /dev/nrmt1}.  The tape block size is 2880 bytes.
      }
      \sstexamplesubsection{
         fitshead /dev/nrmt1 10 $>$ tape.lis
      }{
         This lists to file {\tt tape.lis} the FITS headers for all the files of
         the tape mounted on device {\tt /dev/nrmt1}.  The tape blocking factor is
         10, the tape's blocksize is 28800 bytes.
      }
      \sstexamplesubsection{
         fitshead /dev/rmt/0n 2 3 5 $>$$>$ tape.lis
      }{
         This appends the FITS headers for files 3 to 5 of the tape mounted
         on device {\tt /dev/rmt/0n} to the file {\tt tape.lis}.  The tape blocking factor
         is 2, {\it i.e.}\ the tape's blocksize is 5760 bytes.
      }
      \sstexamplesubsection{
         fitshead /dev/nrst2 prompt
      }{
         This lists the FITS headers for files of the tape mounted on
         device {\tt /dev/nrst2}.  The command prompts you for the file limits
         and tape blocking factor.
      }
      \sstexamplesubsection{
         fitshead $\sim$/fits/$*$.fit $\sim$/data/p?.fi$*$ $|$ lpr
      }{
         This prints the FITS headers in the files {\tt $\sim$/fits/$*$.fit}
         and {\tt $\sim$/data/p?.fi$*$}.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Prompting is directed to the standard error, so that the listings
         may be redirected to a file.

         \sstitem
         If the blocking factor is unknown it is possible to obtain only
         a part of the headers and some of the FITS data.  Unless the FITS
         file is small, it is usually safe to set parameter BLOCK higher
         than its true value.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSEDIT, FITSEXP, FITSIMP, FITSLIST; Figaro: FITSKEYS.
   }
}

\sstroutine{
   FITSIMP
}{
   Imports FITS information into an NDF extension
}{
   \sstdescription{
      This application extracts the values of FITS keywords from a FITS
      extension in an NDF and uses them to construct another NDF
      extension.  The list of new extension components required, their
      data types and the names of the FITS keywords from which to
      derive their values are specified in a {\tt "}keyword translation
      table{\tt "} held in a separate text file.
   }
   \sstusage{
      fitsimp ndf table xname xtype
   }
   \sstparameters{
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF in which the new extension is to be created.
      }
      \sstsubsection{
         TABLE = FILENAME (Read)
      }{
         The text file containing the keyword translation table.  The
         format of this file is described under {\tt "}Table Format{\tt "}.
      }
      \sstsubsection{
         XNAME = LITERAL (Read)
      }{
         The name of the NDF extension which is to receive the values
         read from the FITS extension.  If this extension does not
         already exist, then it will be created.  Otherwise, it should
         be a scalar structure extension within which new components
         may be created (existing components of the same name will be
         over-written).  Extension names may contain up to 15
         alpha-numeric characters, beginning with an alphabetic
         character.
      }
      \sstsubsection{
         XTYPE = LITERAL (Read)
      }{
         The HDS data type of the output extension.  This value will
         only be required if the extension does not initially exist and
         must be created.  New extensions will be created as scalar
         structures.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitsimp datafile fitstable ccdinfo ccd\_ext
      }{
         Creates a new extension called CCDINFO (with a data type of
         CCD\_EXT) in the NDF structure called datafile.  Keyword values
         are read from the NDF's FITS extension and written into the new
         extension as separate components under control of a keyword
         translation table held in the file {\tt fitstable}.
      }
      \sstexamplesubsection{
         fitsimp ndf=n1429 table=std\_table xname=std\_extn
      }{
         FITS keyword values are read from the FITS extension in the
         NDF structure n1429 and written into the pre-existing
         extension STD\_EXTN under control of the translation table
         {\tt std\_table}.  Components which already exist within the
         extension may be over-written by this process.
      }
   }
   \sstdiytopic{
      Timing
   }{
      Approximately proportional to the number of FITS keywords to be
      translated.
   }
   \sstdiytopic{
      Table Format
   }{
      The keyword translation table should be held in a text file, with
      one extension component specified per line.  Each line should
      contain 3 fields, separated by spaces and/or tabs, as follows.

      \sstitemlist{

         \sstitem
         Field 1:
            The name of the component in the output extension for which a
            value is to be obtained.

         \sstitem
         Field 2:
            The data type of the output component, to which the keyword
            value will be converted (one of {\tt \_INTEGER}, {\tt \_REAL},
            {\tt \_DOUBLE}, {\tt \_LOGICAL} or {\tt \_CHAR}).

         \sstitem
         Field 3:
            The name of the FITS keyword from which the value is to be
            obtained.  Hierarchical keywords are permissible; the format
            is concatenated keywords joined with full stops and no spaces,
            {\it e.g.}\ {\tt HIERARCH.ESO.NTT.HUMIDITY}, {\tt ING.DETHEAD}.

      }
      Comments may appear at any point in the table and should begin
      with an exclamation mark.  The remainder of the line will then be
      ignored.
   }
   \sstdiytopic{
   Related Applications
   }{
      KAPPA: FITSHEAD, FITSLIST, FITSDIN, FITSIN; CONVERT: FITS2NDF;
      Figaro: RDFITS.
   }
}
\sstroutine{
   FITSIN
}{
   Reads a FITS tape composed of simple, group or table files
}{
   \sstdescription{
      This application reads selected files from a FITS tape.  The
      files may be Basic (simple) FITS, and/or have TABLE extensions
      (Harten {\it et al.}\ 1988).

      The programme reads a simple or a random-groups-format FITS file
      (Wells {\it et al.}\ 1981; Greisen \& Harten 1981), and writes the
      data into an NDF, and the headers into the NDF's FITS extension.
      Table-format files (Grosb{\o}l {\it et al.}\ 1988) are read, and the
      application creates two files: a text formatted table/catalogue
      and a FACTS description file (as used by SCAR) based upon the FITS
      header cards.  Composite FITS files can be processed.  You may
      specify a list of files, including wildcards.  A record of the
      FITS headers, and group parameters (for a group-format file) can
      be stored in a text file.

      There is an option to run in automatic mode, where the names of
      output NDF data structures are generated automatically, and you
      can decide whether or not format conversion is to be applied to
      all files (rather than being prompted for each).  This is very
      useful if there is a large number of files to be processed.  Even
      if you want unique file names, format-conversion prompting may be
      switched off globally.
   }
   \sstusage{
      fitsin mt files out [auto] fmtcnv [logfile] more=? dscftable=? table=?
   }
   \sstparameters{
      \sstsubsection{
         AUTO = \_LOGICAL (Read)
      }{
         It is {\tt TRUE} if automatic mode is required, where the name of
         each output NDF structure or table file is to be generated by the
         application, and therefore not prompted; and a global
         format-conversion switch may be set.  In manual mode the
         FITS header is reported, but not in automatic.

         For simple or group format FITS objects in automatic mode the
         application generates a filename beginning with a defined
         prefix followed by the number of the file on tape.  For
         example, if the prefix was {\tt "XRAY"} and the 25$^{\rm th}$ file of the
         tape was being processed, the filename of the NDF would be
         XRAY25.

         For table-format FITS objects in the automatic mode the
         application generates a filename beginning with a defined
         prefix followed by the number of the file on tape.  For
         example, if the prefix was {\tt "cat"} and the 9$^{\rm th}$ file of the tape
         was being processed, the filename of the table and its
         associated FACTS description file would be {\tt cat9.dat} and
         {\tt dscfcat9.dat} respectively.
         {\tt [FALSE]}
      }
      \sstsubsection{
         DSCFTABLE = FILENAME (Read)
      }{
         Name of the text file to contain the FACTS descriptors, which
         defines the table's format for {\footnotesize SCAR}.  Since {\footnotesize SCAR} is now
         deprecated, this parameter has little use, except perhaps to
         give a summary of the format of the file specified by parameter
         TABLE.  A null value ({\tt !}) means that no description file will
         be created, so this is now the recommended usage.  If your
         FITS file comprises just tables, you should consider other
         tools such as the {\footnotesize CURSA} package, which has facilities for
         examining and processing ASCII and binary tables in FITS files.

         A suggested filename for the description file is reported
         immediately prior to prompting in manual mode.  It is the name
         of the catalogue, as written in the FITS header, with a
         {\tt "dscf"} prefix.
      }
      \sstsubsection{
         FILES()  = \_CHAR (Read)
      }{
         The list of the file numbers to be processed.  Files are
         numbered consecutively from 1 from the start of the tape.
         Single files or a set of adjacent files may be specified,
         {\it e.g.}\ typing {\tt [4,6-9,12,14-16]} will read files
         4,6,7,8,9,12,14,15,16.  (Note that the
         brackets are required to distinguish this array of characters
         from a single string including commas.  The brackets are
         unnecessary when there only one item.)  For efficiency reasons
         it is sensible to give the file numbers in ascending order.

         If you wish to extract all the files enter the wildcard $*$.
         5-$*$ will read from 5 to the last file.  The processing will
         continue until the end of the tape is reached; no error
         will result from this.
      }
      \sstsubsection{
         FMTCNV = \_LOGICAL (Read)
      }{
         This specifies whether or not format conversion will occur.
         If {\tt FALSE}, the HDS type of the data array in the NDF will be
         the equivalent of the FITS data format on tape ({\it e.g.}\ BITPIX =
         16 creates a \_WORD array).  If {\tt TRUE}, the data array in the
         current file, or all files in automatic mode, will be
         converted from the FITS data type on tape to \_REAL in the NDF.
         The conversion applies the values of the FITS keywords BSCALE
         and BZERO to the tape data to generate the `true' data values.
         If BSCALE and BZERO are not given in the FITS header, they are
         taken to be 1.0 and 0.0 respectively.  The suggested default
         is {\tt TRUE}.
      }
      \sstsubsection{
         GLOCON  = \_LOGICAL (Read)
      }{
         If {\tt FALSE}, a format-conversion query occurs for each FITS file.
         If {\tt TRUE}, the value of FMTCNV is obtained before any file
         numbers and will apply to all data arrays.  It is ignored
         in automatic mode---in effect it becomes true. {\tt [FALSE]}
      }
      \sstsubsection{
         LABEL  = \_LOGICAL (Read)
      }{
         It should be {\tt TRUE} if the tape has labelled files.
         Labelled files are non-standard.  If {\tt TRUE}, the application
         skips three file marks per file, rather that one. {\tt [FALSE]}
      }
      \sstsubsection{
         LOGFILE = FILENAME (Read)
      }{
         The file name of the text log of the FITS header cards.
         For group-format data the group parameters are evaluated
         and appended to the full header.  The log includes the names of
         the output files used to store the data array or table. A null
         value ({\tt !}) means that no log file is produced. {\tt [!]}
      }
      \sstsubsection{
         MORE   = \_LOGICAL (Read)
      }{
         A prompt asking if any more files are to be processed once the
         current list has been exhausted.
      }
      \sstsubsection{
         MT = DEVICE (Read)
      }{
         Tape deck containing the data, usually an explicit device,
         though it can be a pre-assigned environment variable.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF structure holding the full contents of the FITS
         file.  If the null value ({\tt !}) is given no NDF will be created.
         This offers an opportunity to review the descriptors before
         deciding whether or not the data are to be extracted.
      }
      \sstsubsection{
         PREFIX = LITERAL (Read)
      }{
         The prefix of the NDF's or table's file name.  It is only used
         in the automatic mode.
      }
      \sstsubsection{
         REWIND = \_LOGICAL (Read)
      }{
         If it is {\tt TRUE}, the tape drive is rewound before the reading of
         the FITS files commences.  If it is {\tt FALSE}, the tape is not
         rewound, and the current tape position is read from file
         {\tt USRDEVDATASET.sdf}.  Note that file numbers are absolute and
         not relative.  REWIND = {\tt FALSE} is useful if you need to read a
         series of files, process them, then read some more, without
         having to remember the tape's position or apply unnecessary
         wear to the tape.  {\tt [TRUE]}
      }
      \sstsubsection{
         TABLE = FILENAME (Read)
      }{
         Name of the text file to contain the table itself, read from
         the file.  In manual mode, the suggested default filename is the name of
         description file less the {\tt "dscf"} prefix, or if there is no
         description file or if the description file does not have the
         {\tt "dscf"} prefix, the suggested name reverts to the catalogue name
         in the FITS header.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitsin mt=/dev/rmt/1n files=[2-4,9] auto prefix=ccd nofmtcnv
      }{
         This reads files 2, 3, 4, and 9 from the FITS tape on
         device {\tt /dev/rmt/1n}.  The output NDF names will be ccd2, ccd3, ccd4,
         and ccd9 (assuming there are no groups).  The data will not
         have format conversion.
      }
      \sstexamplesubsection{
         fitsin mt=\$TAPE files=$*$ auto prefix=ccd fmtcnv logfile=jkt.log
      }{
         This reads all the files from the FITS tape on the device
         assigned to the environment variable TAPE.  The output files
         begin with a prefix {\tt "}ccd{\tt "}.  Integer data
         arrays are converted to real using the scale and zero
         found in the FITS header.  A record of the headers and the
         names of the output files are written to the text file
         {\tt jkt.log}.
      }
   }
   \sstdiytopic{
      References
   }{
      \begin{refs}
      \item  Wells, D.C., Greisen, E.W. \& Harten, R.H. 1981,
      {\it Astron. Astrophys. Suppl. Ser.} {\bf 44}, 363.

      \item  Greisen, E.W. \& Harten, R.H. 1981,
      {\it Astron. Astrophys. Suppl. Ser.} {\bf 44}, 371.

      \item  Grosb{\o}l, P., Harten, R.H., Greisen, E.W \& Wells, D.C.
      1988 {\it Astron. Astrophys. Suppl. Ser.} {\bf 73}, 359.

      \item  Harten, R.H., Grosb{\o}l, P., Greisen, E.W \& Wells, D.C.
      1988 {\it Astron. Astrophys. Suppl. Ser.} {\bf 73}, 365.

      \end{refs}
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSDIN, FITSHEAD, FITSIMP, FITSLIST; CONVERT: FITS2NDF;
      CURSA; Figaro: RDFITS.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         The application processes tapes blocked at other than an
         integer multiple of 2880 bytes up to a maximum of 63360, provided
         it is a multiple of the number of bytes per data value.

         \sstitem
         For simple or group format FITS:

         \ssthitemlist{

            \sstitem
            IEEE floating point is supported.

            \sstitem
            If BUNIT is present its value will appear as the NDF's units
            component.

            \sstitem
            If OBJECT is present its value will appear as the NDF's title
            component.

            \sstitem
            If the BLANK item is present in the header, undefined pixels
            are converted from the BLANK value to Starlink-standard bad
            value during data conversion.

            \sstitem
            An AXIS component will be stored in the NDF if the CRVAL$n$
            keyword is present.  ($n$ is the number of the dimension.)  If the
            CRPIX$n$ keyword is absent it defaults to 1, and likewise for the
            CDELT$n$ keyword.  The value of CRTYPE$n$ is made the label of the
            axis structure.
         }

         \sstitem
         For groups format, a new NDF is created for each data array.
         The name of the NDF of the second and subsequent data arrays is
         generated by the application as the {\tt <filename>G<number>}, where
         {\tt <filename>} is the name of the first NDF, supplied by you or
         generated automatically, and {\tt <number>} is the number of the group.

         Each group NDF contains the full header in the FITS extension,
         appended by the set of group parameters.  The group parameters
         are evaluated using their scales and offsets, and made to look
         like FITS cards, whose keywords are derived from the values of
         PTYPE$m$ in the main header.  ($m$ is the number of the group
         parameter.) The same format is used in the log file.

         \sstitem
         If there is no data array on tape, {\it i.e.}\ the FITS file comprises
         header cards only, then a dummy vector data array of dimension
         two is created to make the output a valid NDF.  This data array
         is undefined.
      }
   }
}
 
\sstroutine{
   FITSLIST
}{
   Lists the FITS extension of an NDF
}{
   \sstdescription{
      This application lists the FITS header stored in an NDF FITS
      extension.  The list may either be reported directly to you,
      or written to a text file.
   }
   \sstusage{
      fitslist in [logfile]
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         The NDF whose FITS extension is to be listed.
      }
      \sstsubsection{
         LOGFILE = FILENAME (Read)
      }{
         The name of the text file to store a list of the FITS
         extension.  If it is null ({\tt !}) the list of the FITS extension
         is reported directly to you. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitslist saturn
      }{
         The contents of the FITS extension in NDF saturn are
         reported to you.
      }
      \sstexamplesubsection{
         fitslist ngc205 logfile=ngcfits.lis
      }{
         The contents of the FITS extension in NDF ngc205 are
         written to the text file {\tt ngcfits.lis}.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         If the NDF does not have a FITS extension the application will
         exit.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSEDIT, FITSHEAD; Figaro: FITSKEYS.
   }
}
\sstroutine{
   FITSMOD
}{
   Edits an NDF FITS extension via a text file or parameters
}{
   \sstdescription{
      This application edits the FITS extension of an NDF file in a
      variety of ways.  It permits insersion of new keywords, including
      comment lines; revision of existing keyword, values, and inline
      comments; relocation of keywords; deletion of keywords; printing
      of keyword values; and it can test whether or not a keyword
      exists.  The occurrence of keywords may be defined, when there
      are more than one cards of the same name.  The location of each
      insertion or move is immediately before some occurrence of a
      corresponding keyword.
 
      Control of the editing can be through parameters, or from a text
      file whose format is described in topic {\tt "}File Format{\tt "}.
   }
   \sstusage{
      fitsmod ndf
        \newline\hspace*{1.5em}
        $\left\{ {\begin{tabular}{l}
                    keyword edit value comment position \\
                    table=?
                   \end{tabular} }
        \right.$
        \newline\hspace*{1.9em}
        \makebox[0mm][c]{\small mode}
   }
   \sstparameters{
      \sstsubsection{
         COMMENT = LITERAL (Given)
      }{
         The comments to be written to the KEYWORD keyword for the
         {\tt "Update"} and {\tt "Write"} editing commands.  A null
         value ({\tt !}) gives a blank comment.  The special value
         {\tt "\$C"} means use the current comment.  In addition
         {\tt "\$C(}keyword{\tt)"} requests that the
         comment of the keyword given between the parentheses be
         assigned to the keyword being edited.  If this positional
         keyword does not exist, the comment is unchanged for {\tt "Update"},
         and is blank for a {\tt "Write"} edit.
      }
      \sstsubsection{
         EDIT = LITERAL (Read)
      }{
         The editing command to apply to the keyword.  The allowed
         options are listed below.

         \begin{description}
            \item {\tt "Delete"} --- removes a named keyword.

            \item {\tt "Exist"} --- reports {\tt TRUE} to standard
            output if the named keyword exists in the header, and
            {\tt FALSE} if the keyword is not present.

            \item {\tt "Move"} --- relocates a named keyword to be
            immediately before a second keyword (see parameter POSITION).
            When this positional keyword is not supplied, it defaults to the END
            card, and if the END card is absent, the new location is at the end
            of the headers.

            \item {\tt "Print"} --- causes the value of a named keyword to be
            displayed to standard output.  This will be a blank for a comment
            card.

            \item {\tt "Rename"} --- renames a keyword, using parameter
            NEWKEY to obtain the new keyword.

            \item {\tt "Update"} --- revises the value and/or the comment.
            If a secondary keyword is defined explicitly (parameter
            POSITION), the card may be relocated at the same time.  If
            the secondary keyword does not exist, the card being edited
            is not moved.  {\tt "Update"} requires that the keyword
            being edited exists.

            \item {\tt "Write"} --- creates a new card given a value and
            an optional comment.  Its location uses the same rules as for
            the {\tt "Move"} command.  The FITS extension is created
            first should it not exist.
         \end{description}
      }
      \sstsubsection{
         KEYWORD = LITERAL (Given)
      }{
         The name of the keyword to be edited in the FITS extension.  A
         name may be compound to handle hierarchical keywords, and it
         has the form keyword1.keyword2.keyword3 {\em etc.}  The maximum
         number of keywords per FITS card is 20.  Each keyword must be
         no longer than 8 characters, and be a valid FITS keyword
         comprising only alphanumeric characters, hyphen, and underscore.
         Any lowercase letters are converted to uppercase and blanks
         are removed before insertion, or comparison with the existing
         keywords.

         The keywords {\tt " "}, {\tt "COMMENT"}, and {\tt "HISTORY"}
         are comment cards and do not have a value.

         The keyword must exist except for the {\tt "Write"} and
         {\tt "Exist"} commands.

         Both KEYWORD and POSITION keywords may have an occurrence
         specified in brackets {\tt []} following the name (the value
         of KEYWORD should then appear in quotes).  This enables editing of
         a keyword that is not the first occurrence of that keyword, or
         locate a edited keyword not at the first occurrence of the
         positional keyword.  Note that it is not normal to have
         multiple occurrences of a keyword in a FITS header, unless it
         is blank, COMMENT or HISTORY.  Any text between the brackets
         other than a positive integer is interpreted as the first
         occurrence.
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The mode by which the editing instructions are supplied.  The
         alternatives are {\tt "File"}, which uses a text file; and
         {\tt "Interface"} which uses parameters. {\tt ["Interface"]}
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF in which the FITS extension is to be edited.
      }
      \sstsubsection{
         NEWKEY = LITERAL (Given)
      }{
         The name of the keyword to replace the KEYWORD keyword.  It is
         only accessed when EDIT={\tt "Rename"}.  A name may be compound to
         handle hierarchical keywords, and it has the form
         keyword1.keyword2.keyword3 {\em etc.}  The maximum number of
         keywords per FITS card is 20.  Each keyword must be no longer
         than 8 characters, and be a valid FITS keyword comprising only
         alphanumeric characters, hyphen, and underscore.
      }
      \sstsubsection{
         POSITION = LITERAL (Given)
      }{
         The position keyword name.  A position name may be compound to
         handle hierarchical keywords, and it has the form
         keyword1.keyword2.keyword3 {\em etc.}  The maximum number of
         keywords per FITS card is 20.  Each keyword must be no longer
         than 8 characters.  When locating the position card,
         comparisons are made in uppercase and with the blanks removed.
         An occurrence may be specified (see parameter KEYWORD for
         details).

         The new keywords are inserted immediately before each
         corresponding position keyword.  If any name in it does not
         exist in FITS array, or the null value ({\tt !}) is supplied the
         consequences will be as follows.  For a {\tt "Write"} or
         {\tt "Move"} edit, the KEYWORD keyword will be inserted just
         before the END card or appended to FITS array when the END card
         does not exist; for an {\tt "Update"} edit, the edit keyword is
         not relocated.

         A positional keyword is only accessed by the {\tt "Move"},
         {\tt "Write"}, and {\tt "Update"} editing commands.
      }
      \sstsubsection{
         STRING = \_LOGICAL (Read)
      }{
         When STRING is {\tt FALSE}, inferred data typing is used for the
         {\tt "Write"} and {\tt "Update"} editing commands.  So for
         instance, if parameter VALUE = {\tt "Y"}, it would appears as logical
         TRUE rather than the string {\tt 'Y~~~~~~~~~'} in the FITS header.
         See topic {\tt "}Value Data Type{\tt "}.  When STRING is {\tt TRUE},
         the value will be treated as a string for the purpose of
         writing the FITS header. {\tt [FALSE]}
      }
      \sstsubsection{
         TABLE = FILENAME (Read)
      }{
         The text file containing the keyword translation table. The
         format of this file is described under {\tt "}File Format{\tt "}.  For
         illustrations, see under {\tt "}Examples of the File Format{\tt "}.
      }
      \sstsubsection{
         VALUE = LITERAL (Given)
      }{
         The new value of the KEYWORD keyword for the {\tt "Update"} and
         {\tt "Write"} editing commands.  The special value {\tt "\$V"}
         means use the
         current value of the KEYWORD keyword.  This makes it possible
         to modify a comment, leaving the value unaltered.  In addition
         {\tt "\$V(}keyword{\tt)"} requests that the value of the
         reference keyword given between the parentheses be assigned
         to the keyword being edited.  This reference keyword must exist
         and have a value for a {\tt "Write"} edit; whereas the FITS-header
         value is unchanged for {\tt "Update"} if there are problems
         with this reference keyword.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitsmod dro42 bscale exist
      }{
         This reports {\tt TRUE} or {\tt FALSE} depending on whether or not the
         FITS keyword BSCALE exists in the FITS extension of the NDF
         called dro42.
      }
      \sstexamplesubsection{
         fitsmod dro42 bscale p
      }{
         This reports the value of the keyword BSCALE stored in the
         FITS extension of the NDF called dro42.  An error message will
         appear if BSCALE does not exist.
      }
      \sstexamplesubsection{
         fitsmod abc edit=move keyword=bscale position=bzero
      }{
         This moves the keyword BSCALE to lie immediately before keyword
         BZERO in the FITS extension of the NDF called abc.  An error
         will result if either BSCALE or BZERO does not exist.
      }
      \sstexamplesubsection{
         fitsmod dro42 airmass dele
      }{
         This deletes the keyword AIRMASS, if it exists, in the FITS
         extension of the NDF called dro42.
      }
      \sstexamplesubsection{
         fitsmod ndf=dro42 edit=d keyword="airmass[2]"
      }{
         This deletes the second occurrence of keyword AIRMASS, if it
         exists, in the FITS extension of the NDF called dro42.
      }
      \sstexamplesubsection{
         fitsmod @100 airmass w 1.456 "Airmass at mid-observation"
      }{
         This creates the keyword AIRMASS in the FITS extension of the
         NDF called 100, assigning the keyword the real value 1.456 and
         comment {\tt "Airmass at mid-observation"}.  The header is
         located just before the end.  The FITS extension is created if
         it does not exist.
      }
      \sstexamplesubsection{
         fitsmod @100 airmass w 1.456 "Airmass at mid-observation" phase
      }{
         As the previous example except that the new keyword is written
         immediately before keyword PHASE.
      }
      \sstexamplesubsection{
         fitsmod obe observer u value="O'Leary" comment=\$C
      }{
         This updates the keyword OBSERVER with value {\tt "O'Leary"},
         retaining its old comment.  The modified FITS extension lies
         within the NDF called obe.
      }
      \sstexamplesubsection{
         fitsmod test filter w position=end value=27 comment=! string
      }{
         This creates the keyword FILTER in the FITS extension of the
         NDF called test, assigning the keyword the string value {\tt "27"}.
         There is no comment.  The keyword is located at the end of the
         headers, but before any END card.  The FITS extension is
         created if it does not exist.
      }
      \sstexamplesubsection{
         fitsmod test edit=w keyword=detector comment=" ~~~ Detector name"
                 value=\$V(ing.dethead) accept
      }{
         This creates the keyword DETECTOR in the FITS extension of the
         NDF called test, assigning the keyword the value of the
         existing hierarchical keyword ING.DETHEAD.  The comment is
         {\tt " ~~~ Detector name"}, the leading spaces are significant.  The
         keyword is located at the current position keyword.  The FITS
         extension is created if it does not exist.
      }
      \sstexamplesubsection{
         fitsmod datafile mode=file table=fitstable.txt
      }{
         This edits the FITS-extension of the NDF called
         datafile, creating the FITS extension if it does not exist.
         The editing instructions are stored in the text file called
         {\tt fitstable.txt}.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Requests to move, assign values or comments, the following
         reserved keywords in the FITS extension are ignored: SIMPLE,
         BITPIX, NAXIS, NAXIS$n$, EXTEND, PCOUNT, GCOUNT, XTENSION, BLOCKED,
         and END.

         \sstitem
         When an error occurs during editing, warning messages are sent
         at the normal reporting level, and processing continues to the
         next editing command.

         \sstitem
         The FITS fixed format is used for writing or updating
         headers, except for double-precision values requiring more space.
         The comment is delineated from the value by the string {\tt " / "}.

         \sstitem
         The comments in comment cards begin one space following the
         keyword or from column 10 whichever is greater.

         \sstitem
         To be sure that the resultant FITS extension is what you
         desired, you should inspect it using the command FITSLIST before
         exporting the data.  If there is something wrong, you may find it
         convenient to use command FITSEDIT to make minor corrections.
      }
   }
   \sstdiytopic{
      Parameter Defaults
   }{
      All the parameters have a suggested default of their current
      value, except NDF, which uses the global current dataset.
   }
   \sstdiytopic{
      Timing
   }{
      Approximately proportional to the number of FITS keywords to be
      edited.  {\tt "Update"} and {\tt "Write"} edits require the most time.
   }
   \sstdiytopic{
      File Format
   }{
      The file consists of a series of lines, one per editing
      instruction, although blank lines and lines beginning with a {\tt !} or
      {\tt \#} are treated as comments.  Note that the order does matter, as
      the edits are performed in the order given.

      The format is summarised below:

       {\tt command keyword\{[occurrence]\}\{(keyword\{[occurrence]\})\} \{value \{comment\}\}}

      where braces indicate optional values, and occur is the
      occurrence of the keyword.  In effect there are four fields
      delineated by spaces that define the edit operation, keyword,
      value and comment.

      \sstitemlist{

         \sstitem
         Field 1:
            This specifies the editing operation.  Allowed values are
            {\tt Delete}, {\tt Exist}, {\tt Move}, {\tt Read}, {\tt Write},
            and {\tt Update}, and can be
            abbreviated to the initial letter.  {\tt Delete} removes a named
            keyword.  {\tt Read} causes the value of a named keyword to be
            displayed to standard output.  {\tt Exist} reports {\tt TRUE}
            to standard output if the named keyword exists in the
            header, and {\tt FALSE} if the keyword is not present.
            {\tt Move} relocates a named keyword to
            be immediately before a second keyword.  When this positional
            keyword is not supplied, it defaults to the END card, and if
            the END card is absent, the new location is at the end of the
            headers.  {\tt Write} creates a new card given a value and an
            optional comment.  Its location uses the same rules as for the
            {\tt Move} command.  {\tt Update} revises the value and/or the comment.
            If a secondary keyword is defined explicitly, the card may be
            relocated at the same time.  {\tt Update} requires that the keyword
            exists.

         \sstitem
         Field 2:
            This specifies the keyword to edit, and optionally the
            position of that keyword in the header after the edit (for
            {\tt Move}, {\tt Write} and {\tt Update} edits).  The new
            position in the header
            is immediately before a positional keyword, whose name is
            given in parentheses concatenated to the edit keyword.  See
            {\tt "}Field 1{\tt "} for defaulting when the position
            parameter is not defined or is null.

            Both the editing keyword and position keyword may be
            compound to handle hierarchical keywords.  In this case the
            form is keyword1.keyword2.keyword3 {\em etc.}  All
            keywords must be valid FITS keywords.  This means they must
            be no more than 8 characters long, and the only permitted
            characters are uppercase alphabetic, numbers, hyphen,
            and underscore. Invalid keywords will be rejected.

            Both the edit and position keyword may have an occurrence
            specified in brackets {\tt []}.  This enables editing of a keyword
            that is not the first occurrence of that keyword, or locate a
            edited keyword not at the first occurrence of the positional
            keyword.  Note that it is not normal to have multiple
            occurrences of a keyword in a FITS header, unless it is blank,
            COMMENT or HISTORY.  Any text between the brackets other than
            a positive integer is interpreted as the first occurrence.

            Use a null value ({\tt ' '} or {\tt " "}) if you want the
            card to be a comment with keyword other than COMMENT or
            HISTORY.  As blank keywords are used for hierarchical
            keywords, to write a comment in a blank keyword you must
            give a null edit keyword.  These have no keyword appearing
            before the left parenthesis or bracket, such as
            {\tt ()}, {\tt []}, {\tt [2]}, or {\tt (EPOCH)}.

         \sstitem
         Field 3:
            This specifies the value to assign to the edited keyword in
            the {\tt Write} and {\tt Update} operations, or the name of the new
            keyword in the {\tt Rename} modification.  If the keyword exists,
            the existing value or keyword is replaced, as appropriate.
            The data type used to store the value is inferred from the
            value itself.  See topic {\tt "}Value Data Type{\tt "}.

            For the {\tt Update} and {\tt Write} modifications there is a special
            value, {\tt \$V}, which means use the current value of the edited
            keyword, provided that keyword exists.  This makes it possible
            to modify a comment, leaving the value unaltered.  In addition
            {\tt \$V(}keyword{\tt)} requests that the value of the keyword given
            between the parentheses be assigned to the keyword being
            edited.

            The value field is ignored when the keyword is COMMENT,
            HISTORY or blank, and the modification is to {\tt Update}
            or {\tt Write}.

         \sstitem
         Field 4:
            This specifies the comment to assign to the edited keyword for
            the {\tt Write} and {\tt Update} operations.  A leading {\tt "/"} should not be
            supplied.

            There is a special value, {\tt \$C}, which means use the current
            comment of the edited keyword, provided that keyword exists.
            This makes it possible to modify a value, leaving the comment
            unaltered.  In addition {\tt \$C(}keyword{\tt)} requests that the comment
            of the keyword given between the parentheses be assigned to
            the edited keyword.

            To obtain leading spaces before some commentary, use a quote
            ({\tt{'}}) or double quote ({\tt{"}}) as the first character
            of the comment.
            There is no need to terminate the comment with a trailing and
            matching quotation character.  Also do not double quotes
            should one form part of the comment.
      }
   }
   \sstdiytopic{
      Value Data Type
   }{
      The data type of a value is determined as follows:
      \ssthitemlist{

         \sstitem
            For the text-file, values enclosed in quotes ({\tt{'}}) or doubled
            quotes ({\tt{"}}) are strings.  Note that numeric or logical string
            values must be quoted to prevent them being converted to a
            numeric or logical value in the FITS extension.

         \sstitem
            For prompting the value is a string when parameter STRING
            is {\tt TRUE}.

         \sstitem
            Otherwise type conversions of the first word after the
            keywords are made to integer, double precision, and logical
            types in turn.  If a conversion is successful, that becomes the
            data type.  In the case of double precision, the type is set
            to real when the number of significant digits only warrants
            single precision.  If all the conversions failed the value
            is deemed to be a string.
      }
   }
   \sstdiytopic{
      Examples of the File Format
   }{
      The best way to illustrate the options is by listing some example
      lines.

      \begin{description}

      \sstexamplesubsection{
         P AIRMASS
      }{
         This reports the value of keyword AIRMASS to standard output.
      }
      \sstexamplesubsection{
         E FILTER
      }{
         This determines whether keyword FILTER exists and reports
         {\tt TRUE} or {\tt FALSE} to standard output.
      }
      \sstexamplesubsection{
         D OFFSET
      }{
         This deletes the keyword OFFSET.
      }
      \sstexamplesubsection{
         Delete OFFSET[2]
      }{
         This deletes any second occurrence of keyword OFFSET.
      }
      \sstexamplesubsection{
         Rename OFFSET1[2] OFFSET2
      }{
         This renames the second occurrence of keyword OFFSET1 to have
         keyword OFFSET2.
      }
      \sstexamplesubsection{
         W AIRMASS 1.379
      }{
         This writes a real value to new keyword AIRMASS, which will be
         located at the end of the FITS extension.
      }
      \sstexamplesubsection{
         W FILTER(AIRMASS) Y
      }{
         This writes a logical true value to new keyword FILTER, which
         will be located just before the AIRMASS keyword, if it exists.
      }
      \sstexamplesubsection{
         Write FILTER(AIRMASS) 'Y'
      }{
         As the preceding example except that this writes a character
         value {\tt "Y"}.
      }
      \sstexamplesubsection{
         W COMMENT(AIRMASS) . Following values apply to mid-observation
      }{
         This writes a COMMENT card immediately before the AIRMASS card,
         the comment being {\tt "Following values apply to mid-observation"}.
         Note the full stop.
      }
      \sstexamplesubsection{
         W DROCOM(AIRMASS) '' Following values apply to mid-observation
      }{
         As the preceding example but this writes to a non-standard
         comment keyword called DROCOM.  Note the need to supply a null
         value.
      }
      \sstexamplesubsection{
         W (AIRMASS) '' Following values apply to mid-observation
      }{
        As the preceding example but this writes to a blank-keyword
        comment.
      }
      \sstexamplesubsection{
         U OBSERVER "Dr.\ Peter O'Leary" Name of principal observer
      }{
         This updates the OBSERVER keyword with the string value
         {\tt "Dr.\ Peter O'Leary"}, and comment {\tt "Name of principal observer"}.
         Note that had the value been enclosed in single quotes ({\tt '}), the
         apostrophe would need to be doubled.
      }
      \sstexamplesubsection{
         M OFFSET
      }{
         This moves the keyword OFFSET to just before the END card.
      }
      \sstexamplesubsection{
         Move OFFSET(SCALE)
      }{
         This moves the keyword OFFSET to just before the SCALE card.
      }
      \sstexamplesubsection{
         Move OFFSET[2](COMMENT[3])
      }{
         This moves the second occurrence of keyword OFFSET to just
         before the third COMMENT card.
      }
      \end{description}
   }
   \sstdiytopic{
      References
   }{
      \begin{refs}
      \item {\tt "A User's Guide for the Flexible Image Transport System (FITS)"},
      NASA/Science Office of Science and Technology (1994).

      \end{refs}
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSEDIT, FITSEXIST, FITSEXP, FITSHEAD, FITSIMP, FITSLIST,
      FITSVAL, FITSWRITE.
   }
}
\sstroutine{
   FITSTEXT
}{
   Creates an NDF FITS extension from a text file
}{
   \sstdescription{
      This application takes a version of a FITS header stored in a
      text file, and inserts it into the FITS extension of an NDF.  The
      header is not copied verbatim as some validation of the headers
      as legal FITS occurs.  An existing FITS extension is removed.
   }
   \sstusage{
      fitstext ndf file
   }
   \sstparameters{
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The name of the NDF to store the FITS header information.
      }
      \sstsubsection{
         FILE = FILENAME (Read)
      }{
         The text file containing the FITS headers.  Each record should
         be the standard 80-character `card image'.  If the file has
         been edited care is needed to ensure that none of the cards
         are wrapped onto a second line.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitstext hh73 headers.lis
      }{
         This places the FITS headers stored in the text file called
         {\tt headers.lis} in the FITS extension of the NDF called hh73.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The validation process performs the following checks on each
         header `card': \\
           a) the length of the header is no more than 80 characters,
           otherwise it is truncated; \\
           b) the keyword only contains uppercase Latin alphabetic
           characters, numbers, underscore, and hyphen (the header will
           not be copied to the extension except when the invalid
           characters are lowercase letters); \\
           c) value cards have an equals sign in column 9 and a space in
           column 10; \\
           d) quotes enclose character values; \\
           e) single quotes inside string values are doubled; \\
           f) character values are left justified to column 11 (retaining
           leading blanks) and contain at least 8 characters (padding with
           spaces if necessary); \\
           g) non-character values are right justified to column 30, except
           for non-mandatory keywords which have a double-precision value
           requiring more than 20 digits; \\
           h) the comment delimiter is in column 32 or two characters
           following the value, whichever is greater; \\
           i) an equals sign in column 9 of a commentary card is replaced
           by a space; and \\
           j) comments begin at least two columns after the end of the
           comment delimiter.

         \sstitem
         The validation issues warning messages at the normal reporting
         level for violations a), b), c), d), and i).

         \sstitem
         The validation can only go so far.  If any of your header lines
         are ambiguous, the resulting entry in the FITS extension may not
         be what you intended.  Therefore, you should inspect the
         resulting FITS extension using the command FITSLIST before
         exporting the data.  If there is something wrong, you may find it
         convenient to use command FITSEDIT to make minor corrections.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSEDIT, FITSEXP, FITSLIST; CONVERT: NDF2FITS.
   }
}
\newpage
\sstroutine{
   FITSURFACE
}{
   Fits a polynomial surface to 2-dimensional data array
}{
   \sstdescription{
      This task fits a surface to a 2-dimensional data array stored
      array within an NDF data structure.  At present it only
      permits a fit with a polynomial, and the coefficients of that
      surface are stored in a POLYNOMIAL structure (SGP/38) as an
      extension to that NDF.

      Unlike SURFIT, neither does it bin the data nor does it reject
      outliers.
   }
   \sstusage{
      fitsurface ndf [fittype] nxpar nypar
   }
   \sstparameters{
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either {\tt "World"}
         or {\tt "Data"}.  If COSYS = {\tt "World"} the co-ordinates used to fits
         the surface are pixel co-ordinates.  If COSYS = {\tt "Data"} the
         data co-ordinates used are used in the fit, provided there are
         axis centres present in the NDF.  COSYS={\tt "World"} is
         recommended.  {\tt [}Current co-ordinate system{\tt ]}
      }
      \sstsubsection{
         FITTYPE = LITERAL (Read)
      }{
         The type of fit.  It must be either {\tt "Polynomial"} for a
         polynomial or {\tt "Spline"} for a bi-cubic spline.
         {\tt ["Polynomial"]}
      }
      \sstsubsection{
         NDF  = NDF (Update)
      }{
         The NDF containing the 2-dimensional data array to be fitted.
      }
      \sstsubsection{
         NXPAR = \_INTEGER (Read)
      }{
         The number of fitting parameters to be used in the $x$
         direction.  It must be in the range 1 to 15 for a polynomial
         fit, and 4 to 15 for a bi-cubic-spline fit.  Thus 1 gives a
         constant, 2 a linear fit, 3 a quadratic {\em etc.}  Increasing this
         parameter increases the flexibility of the surface in the $x$
         direction.  The upper limit of acceptable values will be
         reduced for arrays with an $x$ dimension less than 29.
      }
      \sstsubsection{
         NYPAR = \_INTEGER (Read)
      }{
         The number of fitting parameters to be used in the $y$
         direction.  It must be in the range 1 to 15 for a polynomial
         fit, and 4 to 15 for a bi-cubic-spline fit.  Thus 1 gives a
         constant, 2 a linear fit, 3 a quadratic {\em etc.}  Increasing this
         parameter increases the flexibility of the surface in the $y$
         direction.  The upper limit of acceptable values will be
         reduced for arrays with a $y$ dimension less than 29.
      }
      \sstsubsection{
         OVERWRITE = \_LOGICAL (Read)
      }{
         OVERWRITE={\tt TRUE}, allows an NDF extension containing an existing
         surface fit to be overwritten.  OVERWRITE={\tt FALSE} protects an
         existing surface-fit extension, and should one exist, an error
         condition will result and the task terminated.  {\tt [TRUE]}
      }
      \sstsubsection{
         VARIANCE = \_LOGICAL (Read)
      }{
         A flag indicating whether any variance array present in the
         NDF is used to define the weights for the fit.  If VARIANCE
         is {\tt TRUE} and the NDF contains a variance array this will be
         used to define the weights, otherwise all the weights will be
         set equal.  {\tt [TRUE]}
      }
      \sstsubsection{
         XMAX = \_DOUBLE (Read)
      }{
         The maximum $x$ value to be used in the fit.  This must be
         greater than or equal to the $x$ co-ordinate of the right-hand
         pixel in the data array.  Normally this parameter is
         automatically set to the maximum $x$ co-ordinate found in the
         data, but this mechanism can be overridden by specifying XMAX
         on the command line.  The parameter is provided to allow the
         fit limits to be fine tuned for special purposes.  It should
         not normally be altered. {\tt [}Maximum $x$ co-ordinate of the
         fitted data{\tt ]}
      }
      \sstsubsection{
         XMIN = \_DOUBLE (Read)
      }{
         The minimum $x$ value to be used in the fit.  This must be
         smaller than or equal to the $x$ co-ordinate of the left-hand
         pixel in the data array.  Normally this parameter is
         automatically set to the minimum $x$ co-ordinate found in the
         data, but this mechanism can be overridden by specifying XMIN
         on the command line.  The parameter is provided to allow the
         fit limits to be fine tuned for special purposes.  It should
         not normally be altered.  {\tt [}Minimum $x$ co-ordinate of
         the fitted data{\tt ]}
      }
      \sstsubsection{
         YMAX = \_DOUBLE (Read)
      }{
         The maximum $y$ value to be used in the fit.  This must be
         greater than or equal to the $y$ co-ordinate of the top pixel in
         the data array.  Normally this parameter is automatically set
         to the maximum $y$ co-ordinate found in the data, but this
         mechanism can be overridden by specifying YMAX on the command
         line.  The parameter is provided to allow the fit limits to be
         fine tuned for special purposes.  It should not normally be
         altered. {\tt [}Maximum $y$ co-ordinate of the fitted data{\tt ]}
      }
      \sstsubsection{
         YMIN = \_DOUBLE (Read)
      }{
         The minimum $y$ value to be used in the fit.  This must be
         smaller than or equal to the $y$ co-ordinate of the bottom pixel
         in the data array.  Normally this parameter is automatically
         set to the minimum $y$ co-ordinate found in the data, but this
         mechanism can be overridden by specifying YMIN on the command
         line.  The parameter is provided to allow the fit limits to be
         fine tuned for special purposes.  It should not normally be
         altered. {\tt [}Minimum $y$ co-ordinate of the fitted data{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitsurface virgo nxpar=4 nypar=4 novariance
      }{
         This fits a bi-cubic polynomial surface to the data array
         in the NDF called virgo.  All the data values are given
         equal weight.  The coefficients of the fitted surface are
         stored in an extension of virgo.
      }
      \sstexamplesubsection{
         fitsurface virgo nxpar=4 nypar=4
      }{
         As the first example except the data variance, if present,
         is used to weight the data values.
      }
      \sstexamplesubsection{
         fitsurface mkn231 nxpar=6 nypar=2 cosys=d xmin=-10.0 xmax=8.5
      }{
         This fits a polynomial surface to the data array in the NDF
         called mkn231.  A fifth order is used along the $x$ direction,
         but only a linear fit along the $y$ direction.  The fit is made
         between $x$ data co-ordinates $-$10.0 to 8.5.  The variance
         weights the data values.  The coefficients of the fitted
         surface are stored in an extension of mkn231.
      }
   }
   \sstnotes{
      The polynomial surface fit is stored in SURFACEFIT extension,
      component FIT of type POLYNOMIAL, variant CHEBYSHEV.  This is
      read by MAKESURFACE to create a NDF of the fitted surface.  Also
      stored in the SURFACEFIT extension are the r.m.s. deviation to the
      fit (component RMS), the maximum absolute deviation (component
      RSMAX), and the co-ordinate system (component COSYS).
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: MAKESURFACE, SURFIT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, and HISTORY components of an NDF data structure.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using double-precision floating point.
      }
   }
}
\sstroutine{
   FITSVAL
}{
   Reports the value of a keyword in the FITS extension.
}{
   \sstdescription{
      This application reports the value of a keyword in an NDF's FITS
      extension.

      It is a synonym for {\tt fitsmod edit=print mode=interface}.
   }
   \sstusage{
      fitsval ndf keyword
   }
   \sstparameters{
      \sstsubsection{
         KEYWORD = LITERAL (Given)
      }{
         The name of an existing keyword in the FITS extension whose value
         is to be reported.  A name may be compound to handle hierarchical
         keywords, and it has the form keyword1.keyword2.keyword3
         {\em etc.}  The maximum number of keywords per FITS card is 20.
         Each keyword must be no longer than 8 characters, and be a
         valid FITS keyword comprising only alphanumeric characters,
         hyphen, and underscore.  Any lowercase letters are converted to
         uppercase and blanks are removed before comparison with the
         existing keywords.

         KEYWORD may have an occurrence specified in brackets {\tt []}
         following the name.  This enables the values to be obtained
         for keywords that appear more than once.  Note that it is not
         normal to have multiple occurrences of a keyword in a FITS
         header, unless it is blank, COMMENT or HISTORY.  Any text
         between the brackets other than a positive integer is
         interpreted as the first occurrence.

         The suggested value is the current value.
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         The NDF containing the FITS keyword.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitsval abc bscale
      }{
         This reports the value of the FITS keyword BSCALE, which is
         located within the FITS extension of the NDF called abc.
      }
      \sstexamplesubsection{
         fitsval ndf=abc keyword=date[2]
      }{
         This reports the value of the second occurrence FITS keyword
         DATE, which is located within the FITS extension of the NDF
         called abc.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSEDIT, FITSEXIST, FITSHEAD, FITSLIST, FITSMOD.
   }
}
\sstroutine{
   FITSWRITE
}{
   Writes a new keyword to the FITS extension
}{
   \sstdescription{
      This application writes a new keyword in an NDF's FITS extension
      given a value and an optional inline comment.  It allows the
      location of the new keyword to be specified.  The FITS extension
      is created if it does not exist.

      It is a synonym for {\tt fitsmod edit=write mode=interface position=!}.
   }
   \sstusage{
      fitswrite ndf keyword value=? comment=?
   }
   \sstparameters{
      \sstsubsection{
         COMMENT = LITERAL (Given)
      }{
         The comments to be written to the KEYWORD keyword.  A null value
         ({\tt !}) gives a blank comment.  The special value {\tt "\$C"}
         means use the current comment.  In addition {\tt "\$C(}keyword{\tt)"} requests that the comment of the
         keyword given between the parentheses be assigned to the
         keyword being edited.  If this positional keyword does not exist,
         the comment is is blank.
      }
      \sstsubsection{
         KEYWORD = LITERAL (Given)
      }{
         The name of the new keyword in the FITS extension.  A name may
         be compound to handle hierarchical keywords, and it has the
         form keyword1.keyword2.keyword3 {\em etc.}  The maximum number
         of keywords per FITS card is 20.  Each keyword must be no
         longer than 8 characters, and be a valid FITS keyword comprising
         only alphanumeric characters, hyphen, and underscore.  Any
         lowercase letters are converted to uppercase and blanks are
         removed before comparison with the existing keywords.

         Note that it is not normal to have multiple occurrences of a
         keyword in a FITS header, unless it is blank, COMMENT or HISTORY.

         The suggested value is the current value.
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF containing the FITS extension into which the new FITS
         keyword.
      }
      \sstsubsection{
         POSITION = LITERAL (Given)
      }{
         The position keyword name.  A position name may be compound to
         handle hierarchical keywords, and it has the form
         keyword1.keyword2.keyword3 {\em etc.}  The maximum number of
         keywords per FITS card is 20.  Each keyword must be no longer
         than 8 characters.  When locating the position card,
         comparisons are made in uppercase and with the blanks removed.
         An occurrence may be specified (see parameter KEYWORD for
         details).

         The new keywords are inserted immediately before each
         corresponding position keyword.  If any name in it does not
         exist in FITS array, or the null value ({\tt !}) is supplied,
         the KEYWORD keyword will be inserted just before the END card
         or appended to FITS array when the END card does not exist.
         {\tt [!]}
      }
      \sstsubsection{
         STRING = \_LOGICAL (Read)
      }{
         When STRING is {\tt FALSE}, inferred data typing is used.  So for
         instance if parameter VALUE = {\tt "Y"}, it would appears as logical
         TRUE rather than the string {\tt 'Y~~~~~~~~~'} in the FITS header.
         See topic {\tt "}Value Data Type{\tt "}.  When STRING is {\tt TRUE},
         the value will be treated as a string for the purpose of
         writing the FITS header. {\tt [FALSE]}
      }
      \sstsubsection{
         VALUE = LITERAL (Given)
      }{
         The new value of the KEYWORD keyword.  The special value {\tt "\$V"}
         means use the current value of the KEYWORD keyword.  This makes
         it possible to modify a comment, leaving the value unaltered.
         In addition {\tt "\$V(}keyword{\tt)"} requests that the value
         of the reference keyword given between the parentheses be
         assigned to the keyword being written.  This reference keyword
         must exist and have a value.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fitswrite abc bscale value=1.234
      }{
         This writes the FITS keyword BSCALE just before the end of the
         FITS extension, which is located within the NDF called abc.  It
         assigns BSCALE a value of 1.234.  There is no inline comment.

      }
      \sstexamplesubsection{
         fitswrite @100 airmass value=1.456 comment="Airmass at mid-observation"
      }{
         This creates the keyword AIRMASS in the FITS extension of the
         NDF called 100, assigning the keyword the real value 1.456 and
         comment {\tt "Airmass at mid-observation"}.  The header is
         located just before the end.
      }
      \sstexamplesubsection{
         fitswrite @100 airmass value=1.456 "Airmass at mid-observation"
         position=phase
      }{
         As the previous example except that the new keyword is written
         immediately before keyword PHASE.
      }
      \sstexamplesubsection{
         fitswrite afcyg observer value="O'Leary" comment=\$C(prininv)
      }{
         This writes the keyword OBSERVER with value {\tt "O'Leary"},
         and its comment is copied from keyword PRININV.  The modified
         FITS extension lies within the NDF called afcyg.
      }
      \sstexamplesubsection{
         fitswrite test filter position=end value=27 comment=! string
      }{
         This creates the keyword FILTER in the FITS extension of the
         NDF called test, assigning the keyword the string value {\tt "27"}.
         There is no comment.  The keyword is located at the end of the
         headers, but before any END card.
      }
      \sstexamplesubsection{
         fitswrite ndf=test keyword=detector comment=" ~~~ Detector name"
                 value=\$V(ing.dethead) accept
      }{
         This creates the keyword DETECTOR in the FITS extension of the
         NDF called test, assigning the keyword the value of the
         existing hierarchical keyword ING.DETHEAD.  The comment is
         {\tt " ~~~ Detector name"}, the leading spaces are significant.  The
         keyword is located at the current position keyword.
      }
   }
   \sstdiytopic{
      Value Data Type
   }{
      The data type of a value is determined as follows:
      \ssthitemlist{

         \sstitem
            For the text-file, values enclosed in quotes ({\tt{'}}) or doubled
            quotes ({\tt{"}}) are strings.  Note that numeric or logical string
            values must be quoted to prevent them being converted to a
            numeric or logical value in the FITS extension.

         \sstitem
            For prompting the value is a string when parameter STRING
            is {\tt TRUE}.

         \sstitem
            Otherwise type conversions of the first word after the
            keywords are made to integer, double precision, and logical
            types in turn.  If a conversion is successful, that becomes the
            data type.  In the case of double precision, the type is set
            to real when the number of significant digits only warrants
            single precision.  If all the conversions failed the value
            is deemed to be a string.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSEDIT, FITSEXP, FITSMOD.
   }
}
\sstroutine{
   FLIP
}{
   Reverses an NDF's pixels along a specified dimension
}{
   \sstdescription{
      This application reverses the order of an NDF's pixels along a
      specified dimension, leaving all other aspects of the data
      structure unchanged.
   }
   \sstusage{
      flip in out dim
   }
   \sstparameters{
      \sstsubsection{
         AXIS = \_LOGICAL (Read)
      }{
         If a {\tt TRUE} value is given for this parameter (the default),
         then any axis values associated with the NDF dimension being
         reversed will also be reversed in the same way.  If a {\tt FALSE}
         value is given, then all axis values will be left unchanged.
         {\tt [TRUE]}
      }
      \sstsubsection{
         DIM = \_INTEGER (Read)
      }{
         The number of the dimension along which the NDF's pixels
         should be reversed.  The value should lie between 1 and the
         total number of NDF dimensions.  If the NDF has only a single
         dimension, then this parameter is not used, a value of 1 being
         assumed.
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF data structure whose pixel order is to be
         reversed.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The output NDF data structure.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         A title for the output NDF. A null value will cause the title
         of the NDF supplied for parameter IN to be used instead.
         {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         flip a b 2
      }{
         Reverses the pixels in the NDF called a along its second
         dimension to create the new NDF called b.
      }
      \sstexamplesubsection{
         flip specin specout
      }{
         If specin is a 1-dimensional spectrum, then this example
         reverses the order of its pixels to create a new spectrum
         specout.  Note that no value for the DIM parameter need be
         supplied in this case.
      }
      \sstexamplesubsection{
         flip in=cube out=newcube dim=2 noaxis
      }{
         Reverses the order of the pixels along dimension 2 of the NDF
         called cube to give newcube, but leaves the associated axis
         values in their original order.
      }
   }
   \sstnotes{
      The pixel-index bounds of the NDF are unchanged by this routine.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ROTATE, TRANSFORMER; Figaro: IREVX, IREVY, IROT90.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of the
         input NDF and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  The data
         type of the input pixels is preserved in the output NDF.
      }
   }
}
\sstroutine{
   FOURIER
}{
   Performs forward and inverse Fourier transforms of 1- or 2-dimensional NDFs
}{
   \sstdescription{
      This application performs forward or reverse Fast Fourier
      Transforms (FFTs) of 1- or 2-dimensional NDFs.  The output in the 
      forward transformation (from the space domain to the Fourier) can
      be produced in Hermitian form in a single NDF, or as two NDFs giving
      the real and imaginary parts of the complex transform, or as two
      NDFs giving the power and phase of the complex transform.  Any
      combination of these may also be produced. The inverse procedure
      accepts any of these NDFs and produces a purely real output NDF.

      Any bad pixels in the input NDF may be replaced by a constant value.
      Input NDFs need neither be square, nor be a power of 2 in size in
      either dimension; their shape is arbitrary.

      The Hermitian transform is a single image in which each quadrant
      consisting of a linear combination of the real and imaginary
      parts of the transform.  This form is useful if you just want to
      multiply the Fourier transform by some known purely real mask and
      then invert it to get a filtered image.  However, if you want to
      multiply the Fourier transform by a complex mask ({\it e.g.}\ the
      Fourier transform of another NDF), or do any other operation
      involving combining complex values, then the Hermitian NDF must
      be untangled into separate real and imaginary parts.

      There is an option to swap the quadrants of the input NDF around
      before performing a forward FFT.  This is useful if you want to
      perform convolutions with the FFTs, since the point-spread
      function (PSF) image can be created with the PSF centre at the
      array centre, rather than at pixel (1,1) as is usually required.
   }
   \sstusage{
      fourier in hermout
   }
   \sstparameters{
      \sstsubsection{
         FILLVAL = LITERAL (Read)
      }{
         A value to replace bad pixels before performing the transform.
         The input image is also padded with this value if necessary to
         form an image of acceptable size.  A value of {\tt "Mean"} will cause
         the mean value in the array to be used. {\tt [0.0]}
      }
      \sstsubsection{
         HERMIN = NDF (Read)
      }{
         Hermitian frequency-domain input NDF containing the complex
         transform.  If null is entered no Hermitian NDF is read and
         then the application should be supplied either separate real
         and imaginary NDFs, or the power and phase NDFs.  Prompting
         will not occur if one of the other (inverse) input NDFs has
         been given on the command line, but not HERMIN as well.  This
         parameter is only relevant for an inverse transformation.
      }
      \sstsubsection{
         HERMOUT = NDF (Write)
      }{
         Hermitian output NDF from a forward transform.  If a null value
         is given then this NDF is not produced.
      }
      \sstsubsection{
         HM\_TITLE = LITERAL (Read)
      }{
         Title for the Hermitian Fourier-transform output NDF.
         A null ({\tt !}) value means using the title of the input NDF.
         {\tt ["KAPPA - Fourier - Hermitian"]}
      }
      \sstsubsection{
         IM\_TITLE = LITERAL (Read)
      }{
         Title for the frequency-domain imaginary output NDF.
         A null ({\tt !}) value means using the title of the input NDF.
         {\tt ["KAPPA - Fourier - Imaginary"]}
      }
      \sstsubsection{
         IMAGIN = NDF (Read)
      }{
         Input frequency-domain NDF containing the real part of the
         complex transform.  If a null is given then an image of zeros is
         assumed unless a null is also given for REALIN, in which case
         the input is requested in power and phase form.  This parameter
         is only available if HERMIN is not used.  One way to achieve
         that is to supply IMAGIN, but not HERMIN, on the command
         line.  This parameter is only relevant for an inverse
         transformation.
      }
      \sstsubsection{
         IMAGOUT = NDF (Write)
      }{
         Frequency-domain output NDF containing the imaginary part of
         the complex Fourier transform.  If a null value is given then
         this NDF is not produced. {\tt [!]}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         Real (space-domain) input NDF for a forward transformation.
         There are no restrictions on the size or shape of the input
         NDF, although the it may have to be padded or trimmed before
         being transformed.  This parameter is only used if a forward
         transformation was requested.
      }
      \sstsubsection{
         INVERSE = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, then the inverse transform---frequency domain to
         space domain---is required, otherwise a transform from the
         space to the frequency domain is undertaken. {\tt [FALSE]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Real space-domain output NDF.  This parameter is only used if
         an inverse transformation is requested.
      }
      \sstsubsection{
         PH\_TITLE = LITERAL (Read)
      }{
         Title for the frequency-domain phase output NDF.
         A null ({\tt !}) value means using the title of the input NDF.
         {\tt ["KAPPA - Fourier - Phase"]}
      }
      \sstsubsection{
         PHASEIN = NDF (Read)
      }{
         Input frequency-domain NDF containing the phase of the complex
         transform.  If a null is given then an image of zeros is
         assumed unless a null is also given for PHASEIN, in which
         case the application quits.  This parameter is only available
         if HERMIN, REALIN and IMAGIN are all not used.  One way to
         achieve that is to supply PHASEIN, but none of the
         aforementioned parameters, on the command line.  This
         parameter is only relevant for an inverse transformation.
      }
      \sstsubsection{
         PHASEOUT = NDF (Write)
      }{
         Frequency-domain output NDF containing the phase of the
         complex Fourier transform.  If a null value is given then this
         NDF is not produced. {\tt [!]}
      }
      \sstsubsection{
         POWERIN = NDF (Read)
      }{
         Input frequency-domain NDF containing the modulus of the
         complex transform.  Note, this should be the square root of the
         power rather than the power itself.  If a null is given then an
         image of zeros is assumed unless a null is also given for
         PHASEIN, in which case the application quits.  This parameter
         is only available if HERMIN, REALIN and IMAGIN are all not
         used.  One way to achieve that is to supply POWERIN, but none
         of the aforementioned parameters, on the command line.  This
         parameter is only relevant for an inverse transformation.
      }
      \sstsubsection{
         POWEROUT = NDF (Write)
      }{
         Frequency-domain output NDF containing the modulus of the
         complex Fourier transform. Note, this is the square root of
         the power rather than the power itself. If a null value is
         given then this NDF is not produced. {\tt [!]}
      }
      \sstsubsection{
         PW\_TITLE = LITERAL (Read)
      }{
         Title for the frequency-domain power output NDF.
         A null ({\tt !}) value means using the title of the input NDF.
         {\tt ["KAPPA - Fourier - Power"]}
      }
      \sstsubsection{
         REALIN = NDF (Read)
      }{
         Input frequency-domain NDF containing the real part of the
         complex transform.  If a null is given then an image of zeros is
         assumed unless a null is also given for IMAGIN, in which case
         the input is requested in power and phase form.  This parameter
         is only available if HERMIN is not used.  One way to achieve
         that is to supply REALIN, but not HERMIN, on the command
         line.  This parameter is only relevant for an inverse
         transformation.
      }
      \sstsubsection{
         REALOUT = NDF (Write)
      }{
         Frequency-domain output NDF containing the real part of the
         complex Fourier transform.  If a null value is given then this
         NDF is not produced. {\tt [!]}
      }
      \sstsubsection{
         RL\_TITLE = LITERAL (Read)
      }{
         Title for the frequency-domain real output NDF.
         A null ({\tt !}) value means using the title of the input NDF.
         {\tt ["KAPPA - Fourier - Real"]}
      }
      \sstsubsection{
         SHIFT = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, the transform origin is to be located at the array's
         centre.  This is implemented by swapping bottom-left and
         top-right, and bottom-right and top-left array quadrants,
         before doing the transform. This results in the transformation
         effectively being done about pixel $x$ = INT(NAXIS1/2)$+$1 and
         $y$ = INT(NAXIS2/2)$+$1, where NAXIS$n$ are the padded or trimmed
         dimensions of the NDF. {\tt [FALSE]}
      }
      \sstsubsection{
         TRIM = LOGICAL (Read)
      }{
         If {\tt TRUE}, when the input array dimension cannot be processed by
         the transform, the output arrays will be trimmed rather than
         padded with the fill value. {\tt [FALSE]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the real space-domain output NDF.
         A null ({\tt !}) value means using the title of the input NDF.
         {\tt ["KAPPA - Fourier"]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         fourier galaxy ft\_gal
      }{
         Makes an Hermitian Fourier transform stored in an NDF called
         ft\_gal from the 2-d NDF called galaxy.
      }
      \sstexamplesubsection{
         fourier hermin=ft\_gal out=galaxy inverse
      }{
         Takes an Hermitian Fourier transform stored in an NDF called
         ft\_gal and performs the inverse transformation to yield a
         normal (spatial domain) image in NDF galaxy.
      }
      \sstexamplesubsection{
         fourier in=galaxy powerout=galpow hermout=ft\_gal fillval=mean
      }{
         Makes an Hermitian Fourier transform stored in an NDF called
         ft\_gal from the 2-d NDF called galaxy.  Any bad values in
         galaxy are replaced by the mean data value of galaxy.  In
         addition the power of the transform is written to an NDF
         called galpow.
      }
      \sstexamplesubsection{
         fourier realin=real\_gal out=galaxy inverse
      }{
         Takes the real component of a Fourier transform stored in an
         NDF called real\_gal and performs the inverse transformation to
         yield a normal image in NDF galaxy.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         See the NAG documentation, Chapter C06, and/or KAPPA routine
         {\tt kpg1\_hmltx.gen} for more details of Hermitian Fourier transforms.
       }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CONVOLVE, LUCY, MEM2D, WIENER; Figaro: BFFT, CMPLX$\lsk$,
      COSBELL, FFT, $\lsk$2CMPLX.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         AXIS, VARIANCE and QUALITY are not propagated from the input to
         output NDFs, but the LABEL, TITLE, HISTORY components and all
         extensions are.  Arithmetic is performed using single- or
         double-precision floating point, as appropriate for the type of
         the data array.
      }
   }
}
\sstroutine{
   GAUSMOOTH
}{
   Smooths a 1- or 2-dimensional image using a Gaussian filter
}{
   \sstdescription{
      This application applies a symmetrical filter to a 1- or 2-dimensional
      image so as to convolve it with a Gaussian point spread function
      (PSF) of specified width, or widths and orientation.  The image
      is held in an NDF data structure.
   }
   \sstusage{
      gausmooth in out fwhm
   }
   \sstparameters{
      \sstsubsection{
         BOX() = \_INTEGER (Read)
      }{
         The $x$ and $y$ sizes (in pixels) of the rectangular region over
         which the Gaussian PSF should be applied at each point.  The
         smoothing PSF will be set to zero outside this rectangle,
         which should therefore be sufficiently large not to truncate
         the PSF too early.  A square region is defined should only one
         size be given.  For a 1-dimensional or circular Gaussian a
         second size is ignored.  Two values are expected when an
         elliptical PSF is requested (see the description of parameter
         FWHM).

         The values given will be rounded up to positive odd integers
         if necessary.  A dynamic default value is calculated which is
         just sufficient to accommodate the Gaussian PSF out to a
         radius of 3 standard deviations. Note that the time taken to
         perform the smoothing increases in approximate proportion to
         the value of this parameter for a circular Gaussian, and in
         proportion to the product of the two box sizes for an
         elliptical Gaussian. {\tt []}
      }
      \sstsubsection{
         FWHM() = \_REAL (Read)
      }{
         This specifies whether a circular or elliptical Gaussian
         point-spread function is used in smoothing a 2-dimensional
         image.  If one value is given it is the full-width at
         half-maximum of a 1-dimensional or circular Gaussian PSF.
         (Indeed only one value is permitted for a 1-dimensional
         array.)  If two values are supplied, this parameter becomes the
         full-width at half-maximum of the major and minor axes of an
         elliptical Gaussian PSF.  Values between 0.1 and 100.0 pixels
         should be given.  Note that unless a non-default value is specified
         for the BOX parameter, the time taken to perform the smoothing
         will increase in approximate proportion to the value(s) of
         FWHM.  The suggested default is the current value.
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF containing the 1- or 2-dimensional image to which
         Gaussian smoothing is to be applied.
      }
      \sstsubsection{
         ORIENT = \_REAL (Read)
      }{
         The orientation of the major axis of the elliptical Gaussian
         PSF, measured in degrees in an anti-clockwise direction from
         the $x$ axis of the NDF.  ORIENT is not obtained if FWHM has one
         value, {\it i.e.}\ a circular Gaussian PSF will be used to smooth the
         image, or the input NDF is 1-dimensional.  The suggested
         default is the current value.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The output NDF which is to contain the smoothed image.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the input NDF to be used. {\tt [!]}
      }
      \sstsubsection{
         WLIM = \_DOUBLE (Read)
      }{
         If the input image contains bad pixels, then this parameter
         may be used to determine the number of good pixels which must
         be present within the PSF area before a valid output pixel is
         generated.  It can be used, for example, to prevent output
         pixels from being generated in regions where good pixels are
         only present in the wings of the PSF.

         By default, a null ({\tt !}) value is used for WLIM, which causes
         the pattern of bad pixels to be propagated from the input
         image to the output image unchanged. In this case, smoothed
         output values are only calculated for those pixels which are
         not bad in the input image.

         If a numerical value is given for WLIM, then it specifies the
         minimum PSF-weighted fraction of good pixels which must be
         present in the PSF area ({\it i.e.}\ box) in order to generate a good
         output pixel.  The maximum value, in the absence of bad
         pixels, is unity.  If the specified minimum fraction of good
         input pixels is not present, then a bad output pixel will
         result, otherwise a smoothed output value will be calculated.
         The value of this parameter should lie between 1E-6 and 1.0.
         {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         gausmooth image1 image2 5.0
      }{
         Smooths the 2-dimensional image held in the NDF structure
         image1 using a symmetrical Gaussian PSF with a full-width at
         half-maximum of 5 pixels.  The smoothed image is written to image2.
         If any pixels in the input image are bad, then the
         corresponding pixels in the output image will also be bad.
      }
      \sstexamplesubsection{
         gausmooth spectrum1 spectrum2 5.0 box=9
      }{
         Smooths the 1-dimensional image held in the NDF structure
         spectrum1 using a symmetrical Gaussian PSF with a full-width
         at half-maximum of 5, and is evaluated over a length of 9
         pixels.  The smoothed image is written to spectrum2.  If any
         pixels in the input image are bad, then the corresponding
         pixels in the output image will also be bad.
      }
      \sstexamplesubsection{
         gausmooth in=a out=b fwhm=3.5 box=31
      }{
         Smooths the 2-dimensional image held in the NDF structure a,
         writing the result into the structure b.  The Gaussian
         smoothing PSF has a full-width at half-maximum of 3.5 pixels
         and is evaluated over a large square of size 31$\times$31 pixels.
      }
      \sstexamplesubsection{
         gausmooth in=a out=b fwhm=[4,3] orient=52.7 box=[29,33]
      }{
         Smooths the 2-dimensional image held in the NDF structure a,
         writing the result into the structure b.  The elliptical
         Gaussian smoothing PSF has full-width at half-maximum of 4
         pixels along its major axis and three pixels along its minor
         axis, and is evaluated over a large rectangle of size 29$\times$33
         pixels.  The major axis of the PSF is oriented 52.\udeg7
         anti-clockwise from the $x$ axis of the data array.
      }
      \sstexamplesubsection{
         gausmooth ngc1097 ngc1097s fwhm=7.2 wlim=0.1
      }{
         Smooths the specified image data using a Gaussian PSF with a
         full-width at half-maximum of 7.2.  An output value is
         calculated for any pixel for which the PSF-weighted fraction
         of good input pixels is at least 0.1.  This will cause the
         smoothing operation to fill in moderately sized regions of bad
         pixels.
      }
   }
   \sstdiytopic{
      Timing
   }{
      For a circular PSF, the execution time is approximately
      proportional to the number of pixels in the image to be smoothed
      and to the value given for the BOX parameter.  By default, this
      latter value is proportional to the value given for FWHM.  For an
      elliptical PSF, the execution time is approximately proportional
      to the number of pixels in the image to be smoothed and to the
      product of the values given for the BOX parameter.  By default,
      these latter values are approximately proportional to the values
      given for FWHM.  Execution time will be approximately doubled if
      a variance array is present in the input NDF.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: BLOCK, CONVOLVE, FFCLEAN, MATHS, MEDIAN, PSF; Figaro:
      ICONV3, ISMOOTH, IXSMOOTH, MEDFILT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY, VARIANCE,
         LABEL, TITLE, UNITS, and HISTORY components of the input NDF
         and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.
         The bad-pixel flag is also written for the data and variance arrays.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using single-precision floating point, or double
         precision if appropriate.
      }
   }
}
 
\sstroutine{
   GDCLEAR
}{
   Clears a graphics device and purges its database entries
}{
   \sstdescription{
      This application software resets an SGS graphics device. In effect
      the device is cleared.  It purges the graphics-database entries
      for the device.  Optionally, only the current picture is cleared
      and the database unchanged. (Note the clearing of the current
      picture may not work on some graphics devices.)
   }
   \sstusage{
      gdclear [device] [current]
   }
   \sstparameters{
      \sstsubsection{
         CURRENT = \_LOGICAL (Read)
      }{
         If {\tt TRUE} then only the current picture is cleared. {\tt [FALSE]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device to be cleared.
         {\tt [}Current graphics device{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         gdclear
      }{
         Clears the current graphics device and purges its graphics
         database entries.
      }
      \sstexamplesubsection{
         gdclear current
      }{
         Clears the current picture on the current graphics device.
      }
      \sstexamplesubsection{
         gdclear xw
      }{
         Clears the xw device and purges its graphics database entries.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: GDSET, GDSTATE, IDCLEAR, OVCLEAR.
   }
}

\sstroutine{
   GDNAMES
}{
   Shows which graphics devices are available
}{
   \sstdescription{
      The routine displays a list of the graphics devices available and
      the names which identify them.  Each name is accompanied by a
      brief descriptive comment.
   }
   \sstusage{
      gdnames
   }
}
\sstroutine{
   GDSET
}{
   Selects a current graphics device
}{
   \sstdescription{
      This application selects a current graphics device.  This
      device will be used for all applications requiring an
      image-display until changed explicitly.
   }
   \sstusage{
      gdset device
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device to become the current graphics device.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         gdset xwindows
      }{
         Makes the xwindows device the current graphics device.
      }
   }
}
\newpage
\sstroutine{
   GDSTATE
}{
   Shows the current status of a graphics device
}{
   \sstdescription{
      This application displays the current status of a graphics
      device, including details of the current graphics-database
      picture ({\it e.g.}\ its co-ordinate system and position on the display
      surface).
   }
   \sstusage{
      gdstate [device]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the graphics device about which information is
         required. {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         OUTLINE = \_LOGICAL (Read)
      }{
         If OUTLINE is {\tt TRUE}, then an outline will be drawn around the
         current picture to indicate its position. {\tt [FALSE]}
      }
      \sstsubsection{
         REPORT = \_LOGICAL (Read)
      }{
         If this is {\tt FALSE} details of the graphics device are not
         reported, merely the results are written to the output
         parameters.  It is intended for use within procedures. {\tt [TRUE]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         COMMENT = LITERAL (Write)
      }{
         The comment of the current picture.  Up to 132 characters
         will be written.
      }
      \sstsubsection{
         LABEL = LITERAL (Write)
      }{
         The label of the current picture.  It is blank if there is no
         label.
      }
      \sstsubsection{
         NAME = LITERAL (Write)
      }{
         The name of the current picture.
      }
      \sstsubsection{
         NCX1 = \_REAL (Write)
      }{
         The lower $x$ normalised device co-ordinate of the current
         picture.
      }
      \sstsubsection{
         NCX2 = \_REAL (Write)
      }{
         The upper $x$ normalised device co-ordinate of the current
         picture.
      }
      \sstsubsection{
         NCY1 = \_REAL (Write)
      }{
         The lower $y$ normalised device co-ordinate of the current
         picture.
      }
      \sstsubsection{
         NCY2 = \_REAL (Write)
      }{
         The upper $y$ normalised device co-ordinate of the current
         picture.
      }
      \sstsubsection{
         REFNAM = LITERAL (Write)
      }{
         The reference object associated with the current picture.  It
         is blank if there is no reference object.  Up to 132 characters
         will be written.
      }
      \sstsubsection{
         WCX1 = \_REAL (Write)
      }{
         The lower $x$ world co-ordinate of the current picture.
      }
      \sstsubsection{
         WCX2 = \_REAL (Write)
      }{
         The upper $x$ world co-ordinate of the current picture.
      }
      \sstsubsection{
         WCY1 = \_REAL (Write)
      }{
         The lower $y$ world co-ordinate of the current picture.
      }
      \sstsubsection{
         WCY2 = \_REAL (Write)
      }{
         The upper $y$ world co-ordinate of the current picture.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         gdstate
      }{
         Shows the status of the current graphics device.
      }
      \sstexamplesubsection{
         gdstate ps\_l
      }{
         Shows the status of the ps\_l device.
      }
      \sstexamplesubsection{
         gdstate outline
      }{
         Shows the status of the current graphics device and draws an
         outline around the current database picture.
      }
      \sstexamplesubsection{
         gdstate refnam=(ndfname)
      }{
         Shows the status of the current graphics device.  If there
         is a reference data object, its name is written to the
         {\footnotesize ICL} variable NDFNAME.
      }
      \sstexamplesubsection{
         gdstate ncx1=(x1) ncx2=(x2) ncy1=(y1) ncy2=(y2)
      }{
         Shows the status of the current graphics device.  The bounds
         of the current picture in normalised device co-ordinates
         are written to the {\footnotesize ICL} variables: X1, X2, Y1, Y2.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         If a channel to the graphics device cannot be opened, then
         this application will still execute without error, but a reduced
         amount of information will be displayed and an outline around the
         current picture (if requested) will not be drawn.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: IDSTATE.
   }
}
\manroutine {{\manheadstyle{GLITCH}}}{ Replaces bad pixels in a 2-d data array with
  the local median.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This application removes bad pixels from a 2-d data array, stored
  in the input {\mantt{IMAGE}} structure, and replaces them with the local
  median of the eight (or less at the edges) neighbouring pixels.
  At least three defined pixels must be in the neighbourhood,
  otherwise the resultant pixel becomes bad.

  There are three modes of use:
\begin{manenumerate}
\manenumerateitem {1.}
  The application can be used interactively until the user is
  finished cleaning his array. The user specifies the position of
  `glitches' or bad pixels by their {$x$} and {$y$} pixel indices.
\manenumerateitem {2.}
  The user can give the name of a file which contains a free-format
  list giving the {$x$} and {$y$} positions of the pixels to be
  deglitched. The glitch list file should look like the following
  example:

\begin{verbatim}
        Glitch list for SBRC FPA #005
        22  45
        19  56
        2  30
        .  .
        .  .
        .  .
        <EOF>
\end{verbatim}

  {\it i.e.}\ a header string that is output to the user, followed by
  integer {$x$}-{$y$} pixel position pairs, terminated just by the
  end-of-file marker. The header string is output to the user.
\manenumerateitem {3.}
  The bad (undefined/magic) pixels are automatically deglitched.
  (Edge, especially corner pixels will need checking if the
  density of bad pixels is high, because of the constraints
  mentioned above. Such conditions may require a repeat dose of
  this application, probably with option 1 or 2.)

\end{manenumerate}
  The magic-value method is used for processing bad data.

\manroutineitem {Invocation }{}
  GLITCH

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  {\mantt{IMAGE}} structure containing the 2-d data array to be deglitched.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure containing the deglitched version of
  the data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title for the output {\mantt{IMAGE}} structure.
  \mbox{\mantt ['KAPPA - Glitch']}
\manparameterentry {{\mantt{READ}} }{{\mantt{WHERE}}  }{{\mantt{\_CHAR}}}
  Source of glitch positions - {\mantt Interface} or {\mantt File} or
  {\mantt Bad}.
\manparameterentry {{\mantt{READ}} }{{\mantt{FILENAME}}  }{{\mantt{\_CHAR}}}
  File containing the free-format glitch list.
\manparameterentry {{\mantt{READ}} }{{\mantt{XCOORD}}  }{{\mantt{\_INTEGER}}}
  {$x$} pixel index of the pixel to be deglitched.
\manparameterentry {{\mantt{READ}} }{{\mantt{YCOORD}}  }{{\mantt{\_INTEGER}}}
  {$y$} pixel index of the pixel to be deglitched.
\manparameterentry {{\mantt{READ}} }{{\mantt{AGAIN}}  }{{\mantt{\_LOGICAL}}}
  Whether or not user is prompted for another pixel.
\end{manparametertable}
\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

 
\sstroutine{
   GLOBALS
}{
   Displays the values of the KAPPA global parameters
}{
   \sstdescription{
      This procedure lists the meanings and values of the {\footnotesize KAPPA}
      global parameters.  If a global parameter does not have a value, the
      string {\tt "<undefined>"} is substituted where the value would have been
      written.
   }
   \sstusage{
      globals
   }
}
\sstroutine{
   GREYPLOT
}{
   Produces a greyscale plot of a 1-d or 2-d NDF
}{
   \sstdescription{
      This application produce a greyscale plot of a 1- or 2-dimensional NDF,
      especially for a hardcopy device.  The image is usually the data
      array, but may also be the variance or quality.  The plot appears
      in the current graphics-database picture.

      The greyscale plot resides within optional, annotated and
      enumerated axes. An optional key may be drawn to the right of the
      greyscale plot comprising a title and grey blocks annotated with
      the corresponding array value.  There are a number of scaling
      methods to map array values to grey levels in the plot.

      The time to output to hardcopy devices can be quite lengthy
      and generally depends on the size of the greyscale plot.
      Therefore, there are parameters for controlling the size of
      the plot.
   }
   \sstusage{
      greyplot in [comp] key [device] mode [pxsize] [pysize] [out]
        \newline\hspace*{1.5em}
        $\left\{ {\begin{tabular}{l}
                    white=? black=? \\  
                    percentiles=? \\
                    sigmas=? 
                  \end{tabular} }
        \right.$
        \newline\hspace*{1.9em}
        \makebox[0mm][c]{\small mode}
   }
   \sstparameters{
      \sstsubsection{
         BLACK = \_DOUBLE (Read)
      }{
         The array value that scales to the black in the greyscale
         colour table.  All smaller array values appear black when
         BLACK is less than WHITE, otherwise all array values
         smaller than BLACK appear white.  The dynamic default is the
         minimum data value.   There is an efficiency gain when both
         BLACK and WHITE are given on the command line, because the
         extreme values need not be computed.  (Scale mode)
      }
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The NDF component to be displayed.  It may be {\tt "Data"},
         {\tt "Quality"}, {\tt "Variance"}, or {\tt "Error"} (where
         {\tt "Error"} is the alternative to {\tt "Variance"} and causes
         the square root of the variance values to be displayed).
         If {\tt "Quality"} is specified, then the quality values are
         treated as numerical values (in the range 0 to 255).  {\tt ["Data"]}
      }
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either
         {\tt "World"} or {\tt "Data"}.  {\tt "World"} makes pixel
         co-ordinates to appear on axes. If COSYS = {\tt "Data"} the
         NDF's axis information is used to annotate axes.
         {\tt [}Current co-ordinate system{\tt ]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the graphics device used to display the image.
         The device must be in one of the following GNS categories:
         IMAGE\_DISPLAY, IMAGE\_OVERLAY, MATRIX\_PRINTER, or WINDOW, and
         have at least 24 greyscale intensities.
         {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         FILL = \_LOGICAL (Read)
      }{
         The greyscale plot normally has square pixels, in other words
         a length along each axis corresponds to the same number of
         pixels.  However, for images with markedly different
         dimensions, such as two-dimensional spectra, this default
         behaviour may not be suitable or give the clearest plot.  When
         FILL is {\tt TRUE}, the square-pixel constraint is relaxed and the
         greyscale plot is the largest possible within the current
         picture.  When FILL is {\tt FALSE}, the pixels are square.  The
         suggested default is the current value.  {\tt [FALSE]}
      }
      \sstsubsection{
         FULL = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, the whole greyscale lookup table for the device is
         used including the normally reserved pens.  Thus all but two
         of the available intensities participate in the greyscale,
         which improves the photographic quality of the image.  The
         remaining pens are for the background, and for the key and
         axes.  This option is only available for devices that reset
         their `colour' tables when the device is opened, such as
         Laserprinters. (This restriction prevents problems on devices
         that retain their `colour tables', where using the normally
         reserved pens would mean that either part of the greyscale
         would be emphemeral---departing when the application
         completes if the reserved pens are stored and reinstated on
         completion, or earlier plots drawn by other applications would
         be altered.)

         If FULL = {\tt FALSE}, only non-reserved intensities will form the
         greyscale.  The default is {\tt TRUE} when the non-reserved pens
         is less than 32, and {\tt FALSE} otherwise.  (This figure was chosen
         because it is roughly the number of grey levels at which the
         eye will clearly detect the quantisation in a complex scene.)
         {\tt []}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         Input NDF data structure containing the image to be displayed.
      }
      \sstsubsection{
         KEY = \_LOGICAL (Read)
      }{
         A key of the greyscale versus pixel value and title is to be
         produced when this is {\tt TRUE}. {\tt [TRUE]}
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The type of scaling to be applied to the array.  There are a
         number of options described below.
         \begin{description}
         \item {\tt "Faint"} --- The image is scaled from the mean minus one
                         standard deviation to the mean plus seven
                         standard deviations.   The scaling values are
                         reported so that the faster Scale mode may be
                         utilised later.
         \item {\tt "Flash"} --- The image is flashed onto the screen without
                         any scaling at all.  This is the fastest
                         option.  Since there is no longer a one-to-one
                         mapping between data values and grey levels
                         this scaling mode is not available with a key.
         \item {\tt "Percentiles"} --- The image is scaled between the values
                         corresponding to two percentiles.  The scaling
                         values are reported so that the faster Scale
                         mode may be utilised later.
         \item {\tt "Range"} --- The image is scaled between the minimum and
                         maximum data values.
         \item {\tt "Scale"} --- You define the upper and lower limits
                         between which the image is to be scaled.  The
                         application reports the maximum and the minimum
                         values for reference and makes these defaults
                         respectively.
         \item {\tt "Sigmas"} --- The image is scaled between two
                         standard-deviation limits.  The scaling values
                         used are reported so that the faster Scale mode
                         may be utilised later.
         \end{description}
      }
      \sstsubsection{
         NUMBIN  =  \_INTEGER (Read)
      }{
         The number of histogram bins used to compute percentiles for
         scaling. (Percentiles mode) {\tt [2048]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The scaled NDF that is displayed; it also does not have
         values that equal the reserved portion of the colour table.
         The output NDF is intended to be used as the input data in
         conjunction with SCALE={\tt FALSE}.  It will be vertically
         inverted with respect to the input array because of GKS
         convention.  If it has a null value ({\tt !}) no output NDF will be
         created.  This parameter is ignored when SCALE={\tt FALSE}.
         {\tt [!]}
      }
      \sstsubsection{
         PERCENTILES( 2 ) = \_REAL (Read)
      }{
         The percentiles that define the scaling limits. For example,
         {\tt [75,25]} would scale between the quartile values. (Percentile
         mode)
      }
      \sstsubsection{
         PXSIZE = \_REAL (Read)
      }{
         The length ($x$ axis) of the plot in metres.  {\tt [}Maximum that can
         fit in the current picture whilst preserving square pixels.{\tt ]}
      }
      \sstsubsection{
         PYSIZE = \_REAL (Read)
      }{
         The length ($y$ axis) of the plot in metres.  {\tt [}Maximum that can
         fit in the current picture whilst preserving square pixels.{\tt ]}
      }
      \sstsubsection{
         REDUCT = \_REAL (Read)
      }{
         The reduction factor of the array, and must be in the range
         1/MAX($NX$,$NY$)--1.0, where $NX$ and $NY$ are the number of pixels
         in the image along the $x$ and $y$ directions.  Since the output
         to the hardcopy device may be long, the size of the image
         with respect to the key can be reduced by lowering REDUCT.
         The spooling time goes approximately proportional to the
         square of REDUCT.  1.0 means the array fills the plotting zone.
         {\tt [1.0]}
      }
      \sstsubsection{
         SCALE = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the input array is scaled according to the value of
         parameter MODE.  If it is {\tt FALSE}, MODE is ignored, and the input
         array is displayed as is.  There is no scaling, inversion
         or avoidance of annotation pens.  SCALE = {\tt FALSE} is intended to
         be used with arrays previously scaled by this or similar
         applications which have already performed the scaling,
         inversion and exclusion.  It provides the quickest method of
         image display within this application. {\tt [TRUE]}
      }
      \sstsubsection{
         SIGMAS( 2 ) = \_REAL (Read)
      }{
         The standard-deviation bounds that define the scaling limits.
         To obtain values either side of the mean both a negative and
         a positive value are required.  Thus {\tt [$-$2,3]} would scale
         between the mean minus two and the mean plus three standard
         deviations.  {\tt [3,$-$2]} would give the negative of that. (Sigmas
         mode).
      }
      \sstsubsection{
         WHITE = \_DOUBLE (Read)
      }{
         The array value that scales to white in the greyscale
         colour table.  All larger array values appear white when
         WHITE is greater than BLACK, otherwise all array values
         larger than WHITE appear black.  The dynamic default is the
         maximum data value.   There is an efficiency gain when both
         BLACK and WHITE are given on the command line, because the
         extreme values need not be computed.  (Scale mode)
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         ABSLAB  =  LITERAL (Read)
      }{
         Label for the plot abscissa, in which NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  This parameter is only used
         when the axes option is selected.  If axis information is present
         the suggested default is the NDF's axis label followed by the
         units, in parentheses.  If an error occurs obtaining the label
         the suggested default is {\tt "X"}. {\tt []}
      }
      \sstsubsection{
         AXES = \_LOGICAL (Read)
      }{
         {\tt TRUE} if annotated axes are to be drawn around the displayed
         image.  The annotations are either the data co-ordinates from
         the NDF axis components, provided these are present and linear
         and COSYS = {\tt "Data"}; otherwise pixel co-ordinates are used.
         {\tt [FALSE]}
      }
      \sstsubsection{
         BADCOL = LITERAL (Read)
      }{
         The grey level to give a bad pixel in the display.  There are
         a number of options described below.

         \begin{description}
           \item [{\tt "MAX"}]  - Black
           \item [{\tt "MIN"}]  - White
           \item [An integer] - The actual 'colour' index. It is constrained
                            between 0 and the maximum colour index
                            available on the device, and so gives a shade
                            of grey.  0 gives the background colour.
           \item [A named `colour'] - Uses the named `colour' from the
                            palette, and if it is not present, the nearest
                            colour from the palette is selected.  The
                            palette contains grey levels at percentages
                            from black to white, {\it e.g.}\ GREY50 is
                            midway between black and white.
         \end{description}
         If this application is run on a device that supports colour
         it is possible to mark the bad pixels in colour on the
         greyscale provided BADCOL is an integer between 0 and 15, or
         a named colour.  The bad pixels will remain unaltered if the
         lookup table is manipulated.  The suggested default is the
         current value. {\tt [}The current value, but equals {\tt "MIN"} if
         there is no current value.{\tt ]}
      }
      \sstsubsection{
         FONT = LITERAL (Read)
      }{
         The fount to be used for the line graphics.  It can be either
         {\tt "NCAR"} for the NCAR fancy characters and {\tt "GKS"} for the standard
         GKS san-serif fount.   The former is intended for hardcopy
         publication-quality plots, since it is relatively slow; the
         latter is intended for normal interactive graphics requiring
         rapid plotting, and it is clearer on small plots. The
         suggested default is the current value. {\tt ["GKS"]}
      }
      \sstsubsection{
         MAJTIC( 2 ) = \_REAL (Read)
      }{
         The parameter controlling the numbers of major tick marks
         for the $x$ and $y$ axes.  (Number used is between MAJTIC$+$2 and
         5$*$MAJTIC/2$+$4.)   A negative value for an axis makes the
         graphics package decide an appropriate value.  This parameter
         is only used when the axes option is selected. {\tt [3.,3.]}
      }
      \sstsubsection{
         MINTIC( 2 ) = \_REAL (Read)
      }{
         The number of minor tick marks between each major tick mark
         for the $x$ and $y$ axes.  A negative value forces the graphics
         package to compute appropriate values.   This parameter is
         only used when the axes option is selected. {\tt [-1.,-1.]}
      }
      \sstsubsection{
         ORDLAB  =  LITERAL (Read)
      }{
         Label for the plot ordinate, in which NCAR fancy founts
         may be embedded when FONT = {\tt "NCAR"}.  This parameter is only
         used when the axes option is selected.  If axis information is
         present the suggested default is the NDF's axis label followed by
         the units, in parentheses.  If an error occurs obtaining the label
         the suggested default is {\tt "Y"}. {\tt []}
      }
      \sstsubsection{
         OUTTIC = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the axis tick marks are to appear on the outside of
         the axes instead of inside.   This parameter is only used
         when the axes option is selected. {\tt [TRUE]}
      }
      \sstsubsection{
         PLTITL = LITERAL (Read)
      }{
         The title of the plot.  Up to about 25 characters can be
         accommodated when there are no axes, and about 40 otherwise.
         NCAR fancy founts may be embedded ({\it cf.}\ SUN/90) when
         FONT = {\tt "NCAR"}.  This parameter is only used when either
         the axes or key option is selected. {\tt [}The NDF title{\tt ]}
      }
      \sstsubsection{
         THICK = \_REAL (Read)
      }{
         The thickness of the axes and annotations in the plot, where
         1.0 is the normal thickness.  It should be between 0.5 and 5.
         This feature is only available on some devices.   This
         parameter is only used when the axes option is selected.
         {\tt [1.0]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         SCAHIGH = \_DOUBLE (Write)
      }{
         The array value scaled to white in the greyscale display.
         In Flash mode or when there is no scaling the colour index of
         white is used.  The current display linear-scaling maximum is
         set to this value.
      }
      \sstsubsection{
         SCALOW = \_DOUBLE (Write)
      }{
         The array value scaled to black in the greyscale display.
         In Flash mode or when there is no scaling the colour index of
         black is used.  The current display linear-scaling minimum is
         set to this value.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         greyplot sdor key mode=sc black=1 white=5.2
      }{
         Makes a greyscale display of the data component of the NDF
         called sdor on the current image-display device, scaling
         between 1 and 5.2.  Values up to 1.0 in the data array will
         appear black in the plot, and values larger than 5.2 will be
         white.  Intermediate values will a grey level determined by
         linear interpolation.  A key is drawn to the right of the
         greyscale.
      }
      \sstexamplesubsection{
         greyplot in=sdor nokey mode=p percentiles=[10,90] badcol="Black"
      }{
         This makes a greyscale plot of the NDF called sdor on the
         current image-display device. The scaling is between the 10 and
         90 per cent percentiles of the image.  No key is drawn.  Bad
         data appear black in the plot.
      }
      \sstexamplesubsection{
         greyplot mode=fa axes out=video cosys=d $\backslash$
      }{
         Displays a greyscale of the current NDF data component with
         annotated axes on the current image-display device.  The axes
         take the axis labels and title from the NDF, and are annotated
         in data co-ordinates.  The scaling is between the $-$1 and $+$7
         standard deviations of the image around its mean.  A key is
         drawn.  The scaled data are stored in an NDF called video.
      }
      \sstexamplesubsection{
         greyplot video noscale $\backslash$
      }{
         Displays the data component of the NDF called video (created
         in the previous example) without scaling within the current
         picture on the current image-display device.
      }
      \sstexamplesubsection{
         greyplot cgs4k v key mode=ra device=canon\_l
      }{
         Makes a greyscale display of the variance component of NDF
         cgs4k on the Canon\_l device, scaling between the minimum and
         maximum variance values.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The application stores a number of pictures in the graphics
         database in the following order: a FRAME of the specified size
         containing the title, annotated axes, and the image area
         (provided AXES is {\tt TRUE}), whose world co-ordinates are in pixels;
         a DATA picture with world co-ordinates in units of data pixels;
         and a KEY.  The DATA picture also may have double-precision data
         co-ordinates derived from the NDF axis component provided these
         are linear and different from pixel co-ordinates; the data
         co-ordinates are stored via a linear transformation.  The NDF
         associated with the plot is stored by reference with the DATA
         picture.  On exit the current database picture for the chosen
         device reverts to the input picture.

         \sstitem
         When axes are requested the axis annotations are defined by
         their lower and upper bounds, {\it i.e.}\ a regular array is assumed.
         The bounds are in pixel or data co-ordinates.

         \sstitem
         The data type of the output NDF depends on the number of colour
         indices: \_UBYTE for no more than 256, \_UWORD for 257 to 65535,
         and \_INTEGER otherwise.   The output NDF will not contain any
         extensions, UNITS, QUALITY, and VARIANCE; but LABEL, TITLE,
         and AXIS information are propagated from the input NDF.  The
         output NDF does not become the new current data array.  It is a
         Simple NDF (because the bad-pixel flag is set to false in order to
         access the maximum grey level), therefore only NDF-compliant
         applications can process it.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: DISPLAY; Figaro: IGREY, IMAGE; SPECDRE: MOVIE.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, and UNITS components of the input NDF.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         This application will handle data in all numeric types, though
         type conversion to integer will occur for unsigned byte and word
         images.  However, when there is no scaling only integer data will
         not be type converted, but this is not expensive for the expected
         byte-type data.
      }
   }
}
\sstroutine{
   HISCOM
}{
   Adds commentary to the history of an NDF
}{
   \sstdescription{
      This task allows application-independent commentary to be added
      to the history records of an NDF.  The text may be read from a
      text file or obtained through a parameter.
   }
   \sstusage{
      hiscom ndf [mode] 
        $\left\{ {\begin{tabular}{l}
                    file=? \\
                    comment=?
                   \end{tabular} }
        \right.$
        \newline\hspace*{8.85em}
        \makebox[0mm][c]{\small mode}
   }
   \sstparameters{
      \sstsubsection{
         COMMENT = LITERAL (Read)
      }{
         A line of commentary limited to 72 characters.  If the value is
         supplied on the command line only that line of commentary will
         be written into the history.  Otherwise repeated prompting
         enables a series of commentary lines to be supplied.  A null
         value ({\tt !}) terminates the loop.  Blank lines delimit
         paragraphs.  Paragraph wrapping is enabled by parameter WRAP.
         There is no suggested default to allow more room for entering
         the value.
      }
      \sstsubsection{
         FILE =  FILENAME (Read)
      }{
         Name of the text file containing the commentary.  It is only
         accessed if MODE={\tt "File"}.
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The interaction mode.  The allowed values are described below.

            \begin{description}
            \item {\tt "File"}  ---  The commentary is to be read from a text
                             file.  The formatting and layout of the
                             text is preserved in the history unless
                             WRAP={\tt TRUE} and there are lines longer than
                             the width of the history records.
            \item {\tt "Interface"} --- The commentary is to be supplied through a
                             parameter.  See parameter COMMENT.
            \end{description}

         {\tt ["Interface"]}
      }
      \sstsubsection{
         NDF = (Read and Write)
      }{
         The NDF for which commentary is to be added to the history.
      }
      \sstsubsection{
         WRAP = \_LOGICAL (Read)
      }{
         WRAP={\tt TRUE} requests that the paragraphs of comments are wrapped
         to make as much text fit on to each line of the history record
         as possible.  WRAP={\tt FALSE} means that the commentary text beyond
         the width of the history records (72 characters) is lost.  The
         default is {\tt TRUE} when MODE={\tt "Interface"} and {\tt FALSE} if
         MODE={\tt "File"}.  The suggested default is the current value.
         {\tt []}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         hiscom frame256 comment="This image has a non-uniform background"
      }{
         This adds the comment {\tt "This image has a non-uniform background"}
         to the history records of the NDF called frame256.
      }
      \sstexamplesubsection{
         hiscom ndf=eso146-g14 comment="This galaxy is retarded" mode=i
      }{
         This adds the comment {\tt "This galaxy is retarded"} to the history
         records of the NDF called eso146-g14.
      }
      \sstexamplesubsection{
         hiscom hh14\_k file file=ircam\_info.lis
      }{
         This reads the file {\tt ircam\_info.lis} and places the text
         contained therein into the history records of the NDF called
         hh14\_k.  Any lines longer than 72 characters are truncated to
         that length.
      }
      \sstexamplesubsection{
         hiscom hh14\_k file file=ircam\_info.lis wrap
      }{
         As the previous example except the text in each paragraph is
         wrapped to a width of 72 characters within the history
         records.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         A history component is created if it does not exist within the
         NDF.  The width of the history record is 72 characters.

         \sstitem
         An error will result if the current history update mode of the
         NDF is {\tt "Disabled"}, and no commentary is written.  Otherwise the
         commentary is written at the priority equal to the current
         history update mode.

         \sstitem
         A warning messages (at the normal reporting level) is issued
         if lines in the text file are too long for the history record and
         WRAP={\tt FALSE}, though the first 72 characters are stored.

         \sstitem
         The maximum line length in the file is 200 characters.

         \sstitem
         Paragraphs should have fewer than 33 lines.  Longer ones will
         be divided.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: HISLIST, HISSET, NDFTRACE.
   }
}
\sstroutine{
   HISLIST
}{
   Lists NDF history records
}{
   \sstdescription{
      This lists all the history records in an NDF.  The reported
      information comprises the date, time, and application name,
      and optionally the history text.
   }
   \sstusage{
      hislist ndf
   }
   \sstparameters{
      \sstsubsection{
         BRIEF = \_LOGICAL (Read)
      }{
         This controls whether a summary or the full history information
         is reported.  BRIEF={\tt TRUE} requests that only the date and
         application name in each history record is listed.  BRIEF={\tt FALSE}
         causes the task to report the history text in addition.
         {\tt [FALSE]}
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         The NDF whose history information is to be reported.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         hislist vcc953
      }{
         This lists the full history information for the NDF called
         vcc935.  The information comprises the names of the
         applications and the times they were used, and the associated
         history text.
      }
      \sstexamplesubsection{
         hislist vcc953 brief
      }{
         This gives a summary of the history information for the NDF
         called vcc935.  It comprises the names of the applications
         and the times they were used.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: HISCOM, HISSET, NDFTRACE.
   }
}
\sstroutine{
   HISSET
}{
   Sets the NDF history update mode
}{
   \sstdescription{
      This task controls the level of history recording in an NDF,
      and can also erase the history information.

      The level is called the history update mode and it is a permanent
      attribute of the history component of the NDF, and remains with
      the NDF and any NDF created therefrom until the history is erased
      or the update mode is modified (say by this task).
   }
   \sstusage{
      hisset ndf [mode] ok=?
   }
   \sstparameters{
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The history update mode.  It can take one of the following
         values.
            \begin{description}
            \item {\tt "Disabled"}  ---  No history recording is to take place.
            \item {\tt "Erase"}     ---  Erases the history of the NDF.
            \item {\tt "Normal"}    ---  Normal history recording is required.
            \item {\tt "Quiet"}     ---  Only brief history information is to be
                             recorded.
            \item {\tt "Verbose"}   ---  The fullest-possible history information
                             is to be recorded.
            \end{description}

         The suggested default is {\tt "Normal"}.  {\tt ["Normal"]}
      }
      \sstsubsection{
         NDF = (Read and Write)
      }{
         The NDF whose history update mode to be modified or history
         information erased.
      }
      \sstsubsection{
         OK = \_LOGICAL (Read)
      }{
         This is used to confirm whether or not the history should be
         erased.  OK={\tt TRUE} lets the history records be erased; if
         OK={\tt FALSE} the history is retained and a message will be issued
         to this effect.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         hisset final
      }{
         This sets the history-recording level to be normal for the NDF
         called final.
      }
      \sstexamplesubsection{
         hisset final erase ok
      }{
         This erases the history information from the NDF called final.
      }
      \sstexamplesubsection{
         hisset mode=disabled ndf=spectrum
      }{
         This disables history recording in the NDF called spectrum.
      }
      \sstexamplesubsection{
         hisset test42 v
      }{
         This sets the history-recording level to be verbose for the NDF
         called test42 so that the fullest-possible history is included.
      }
      \sstexamplesubsection{
         hisset ndf=test42 mode=q
      }{
         This sets the history-recording level to be quiet for the NDF
         called test42, so that only brief information is recorded.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         A history component is created if it does not exist within the
         NDF, except for MODE={\tt "Erase"}.

         \sstitem
         The task records the new history update mode within the
         history records, even if MODE= {\tt "Disabled"} provided the mode has
         changed.  Thus the history information will show where there may
         be gaps in the recording.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: HISCOM, HISLIST, NDFTRACE.
   }
}
\sstroutine{
   HISTAT
}{
   Computes ordered statistics for an NDF's pixels using an histogram
}{
   \sstdescription{
      This application computes and displays simple ordered statistics
      for the pixels in an NDF's data, quality, error, or variance
      array.  The statistics available are:
      \ssthitemlist{

         \sstitem
         the pixel sum,

         \sstitem
         the pixel mean,

         \sstitem
         the pixel median,

         \sstitem
         the pixel mode,

         \sstitem
         the pixel value at selected percentiles,

         \sstitem
         the value and position of the minimum- and maximum-valued
         pixels,

         \sstitem
         the total number of pixels in the NDF,

         \sstitem
         the number of pixels used in the statistics, and

         \sstitem
         the number of pixels omitted.
      }
   }
   \sstusage{
      histat ndf [comp] [percentiles] [logfile] [numbin]
   }
   \sstparameters{
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The name of the NDF array component for which statistics are
         required.  The options are limited to the arrays within the
         supplied NDF.  In general the value may {\tt "Data"}, {\tt "Error"},
         {\tt "Quality"} or {\tt "Variance"} (note that {\tt "Error"} is the
         alternative to {\tt "Variance"} and causes the square root of
         the variance values to be taken before computing the statistics).  If
         {\tt "Quality"} is specified, then the quality values are treated as
         numerical values (in the range 0 to 255).  {\tt ["Data"]}
      }
      \sstsubsection{
         LOGFILE = FILENAME (Write)
      }{
         A text file into which the results should be logged.  If a null
         value is supplied (the default), then no logging of results
         will take place. {\tt [!]}
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         The NDF data structure to be analysed.
      }
      \sstsubsection{
         NUMBIN = \_INTEGER (Read)
      }{
         The number of histogram bins to be used.  There is a tradeoff
         between accuracy and processing time depending on the value
         of NUMBIN. Increasing the number of bins yields more accurate
         results, because it reduces quantisation errors biasing
         the computed statistics, but takes longer to compute.  The
         bias is in the same sense as the skewness, so generally it
         will add positive bias.  The default value of NUMBIN is a
         compromise and generally will give satisfactory results.

         It is hard to quantify the tradeoff precisely; testing with a
         typical CCD image of stars and galaxies the increase in time
         went approximately as log of the number of bins, and the
         accuracy of the pixel sum for some values of NUMBIN were: 100,
         2.8\%; 1000, 0.1\% 10000, 0.03\%; and 100000, 0.002\%.

         NUMBIN must be in the range 100 to 100000. {\tt [2048]}
      }
      \sstsubsection{
         PERCENTILES( 100 ) = \_REAL (Read)
      }{
         A list of percentiles to be found.  None are computed if this
         parameter is null ({\tt !}).  The percentiles must be in the range
         0.0 to 100.0. {\tt [!]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         MAXCOORD( ) = \_DOUBLE (Write)
      }{
         A 1-dimensional array of values giving the user co-ordinates of
         the centre of the (first) maximum-valued pixel found in the
         NDF array.  The number of co-ordinates is equal to the number
         of NDF dimensions.
      }
      \sstsubsection{
         MAXIMUM = \_DOUBLE (Write)
      }{
         The maximum pixel value found in the NDF array.
      }
      \sstsubsection{
         MAXPOS( ) = \_INTEGER (Write)
      }{
         A 1-dimensional array of pixel indices identifying the (first)
         maximum-valued pixel found in the NDF array.  The number of
         indices is equal to the number of NDF dimensions.
      }
      \sstsubsection{
         MEAN = \_DOUBLE (Write)
      }{
         The mean value of all the valid pixels in the NDF array.
      }
      \sstsubsection{
         MEDIAN = \_DOUBLE (Write)
      }{
         The median value of all the valid pixels in the NDF array.
      }
      \sstsubsection{
         MINCOORD( ) = \_DOUBLE (Write)
      }{
         A 1-dimensional array of values giving the user co-ordinates of
         the centre of the (first) minimum-valued pixel found in the
         NDF array.  The number of co-ordinates is equal to the number
         of NDF dimensions.
      }
      \sstsubsection{
         MINIMUM = \_DOUBLE (Write)
      }{
         The minimum pixel value found in the NDF array.
      }
      \sstsubsection{
         MINPOS( ) = \_INTEGER (Write)
      }{
         A 1-dimensional array of pixel indices identifying the (first)
         minimum-valued pixel found in the NDF array. The number of
         indices is equal to the number of NDF dimensions.
      }
      \sstsubsection{
         MODE = \_DOUBLE (Write)
      }{
         The modal value of all the valid pixels in the NDF array.  It
         is estimated from 3 $*$ median $-$ 2 $*$ mean.  This is only valid
         for moderately skew distributions.
      }
      \sstsubsection{
         NUMBAD = \_INTEGER (Write)
      }{
         The number of pixels which were either not valid or were
         rejected from the statistics during iterative K-sigma
         clipping.
      }
      \sstsubsection{
         NUMGOOD = \_INTEGER (Write)
      }{
         The number of NDF pixels which actually contributed to the
         computed statistics.
      }
      \sstsubsection{
         NUMPIX = \_INTEGER (Write)
      }{
         The total number of pixels in the NDF (both good and bad).
      }
      \sstsubsection{
         PERVAL() = \_DOUBLE (Write)
      }{
         The values of the percentiles of the good pixels in the NDF
         array.  This parameter is only written when one or more
         percentiles have been requested.
      }
      \sstsubsection{
         TOTAL = \_DOUBLE (Write)
      }{
         The sum of the pixel values in the NDF array.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         histat image
      }{
         Computes and displays simple ordered statistics for the data
         array in the NDF called image.
      }
      \sstexamplesubsection{
         histat ndf=spectrum variance
      }{
         Computes and displays simple ordered statistics for the
         variance array in the NDF called spectrum.
      }
      \sstexamplesubsection{
         histat spectrum error
      }{
         Computes and displays ordered statistics for the variance
         array in the NDF called spectrum, but takes the square root of
         the variance values before doing so.
      }
      \sstexamplesubsection{
         histat halley logfile=stats.dat
      }{
         Computes ordered statistics for the data array in the NDF
         called halley, and writes the results to a logfile called
         {\tt stats.dat}.
      }
      \sstexamplesubsection{
         histat ngc1333 percentiles=[0.25,0.75] numbin=100000
      }{
         Computes ordered statistics for the data array in the NDF
         called ngc1333, including the quartile values.  The highest
         accuracy is used.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         If the array has a few extreme outliers this can bias the
         statistics unless the number of bins in the histogram is large,
         or the outliers are removed (flagged) via application THRESH.

         \sstitem
         There is quantisation bias in the statistics.  See parameter
         NUMBIN.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: HISTOGRAM, INSPECT, MSTATS, NDFTRACE, NUMB, STATS;
      Figaro: ISTAT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, VARIANCE,
         QUALITY, TITLE, and HISTORY components of the NDF.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using single- or double-precision floating point,
         as appropriate.

         \sstitem
         Any number of NDF dimensions is supported.
      }
   }
}

\sstroutine{
   HISTEQ
}{
   Performs an histogram equalisation on an NDF
}{
   \sstdescription{
      This application transforms an NDF via histogram equalisation.
      Histogram equalisation is an image-processing technique in which
      the distribution (between limits) of data values in the input
      array is adjusted so that in the output array there are
      approximately equal numbers of elements in each histogram bin.
      To achieve this the histogram bin size is no longer a constant.
      This technique is commonly known as histogram equalisation.  It
      is useful for displaying features across a wide dynamic range,
      sometimes called a maximum information picture.  The transformed
      array is output to a new NDF.
   }
   \sstusage{
      histeq in out [numbin]
   }
   \sstparameters{
      \sstsubsection{
         IN  = NDF (Read)
      }{
         The NDF structure to be transformed.
      }
      \sstsubsection{
         NUMBIN = INTEGER (Read)
      }{
         The number of histogram bins to be used.  This should be a
         large number, say 2000, to reduce quantisation errors.  It
         must be in the range 100 to 10000. {\tt [2048]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The NDF structure to contain the transformed data array.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the input NDF to the output NDF.
         {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         histeq halley maxinf
      }{
         The data array in the NDF called halley is remapped via
         histogram equalisation to form the new NDF called maxinf.
      }
      \sstexamplesubsection{
         histeq halley maxinf 10000 title="Maximum information of Halley"
      }{
         The data array in the NDF called halley is remapped via
         histogram equalisation to form the new NDF called maxinf.
         Ten thousand bins in the histogram are required rather than
         the default of 2048.  The title of NDF maxinf is
         {\tt "Maximum information of Halley"}.
      }
   }
   \sstnotes{
      If there are a few outliers in the data and most of the points
      concentrated about a value it may be wise to truncate the
      data array via THRESH, or have a large number of histogram bins.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: LAPLACE, LUTABLE, SHADOW, THRESH; Figaro: HOPT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, and HISTORY components of an NDF data structure and
         propagates all extensions.  UNITS and VARIANCE become undefined
         by the transformation, and so are not propagated.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         Any number of NDF dimensions is supported.
      }
   }
}
\begin{htmlonly}
Click \htmladdnormallink{here}{histeq_exam.gif} to see an image
displayed before (left) and after histogram equalisation (67k).  The
same
scaling limits are used for both images. 
\end{htmlonly}
\sstroutine{
   HISTOGRAM
}{
   Computes an histogram of an NDF's values
}{
   \sstdescription{
      This application derives histogram information for an NDF array
      between specified limits.  The histogram is reported, and may
      optionally be written to a text log file, and/or plotted
      graphically.
   }
   \sstusage{
      histogram in numbin range [comp] [logfile]
   }
   \sstparameters{
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The name of the NDF array component to have its histogram
         computed: {\tt "Data"}, {\tt "Error"}, {\tt "Quality"} or
         {\tt "Variance"} (where {\tt "Error"} is the alternative
         to {\tt "Variance"} and causes the square
         root of the variance values to be taken before computing the
         statistics).  If {\tt "Quality"} is specified, then the quality
         values are treated as numerical values (in the range 0 to
         255).  {\tt ["Data"]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation on which to produce the plot. If it
         is null, {\tt !}, there will be no plot made.
         {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The NDF data structure to be analysed.
      }
      \sstsubsection{
         LOGFILE = FILENAME (Write)
      }{
         A text file into which the results should be logged. If a null
         value is supplied (the default), then no logging of results
         will take place. {\tt [!]}
      }
      \sstsubsection{
         NUMBIN = \_INTEGER (Read)
      }{
         The number of histogram bins to be used. This must lie in the
         range 2 to 10000.  The suggested default is the current value.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         Name of the NDF structure to save the histogram in its data
         array.  If null, {\tt !}, is entered the histogram NDF is not
         created. {\tt [!]}
      }
      \sstsubsection{
         RANGE( 2 ) = \_DOUBLE (Write)
      }{
         The range of values for which the histogram is to be computed.
         A null value ({\tt !}) selects the minimum and maximum array values.
         If RANGE is specified on the command line, the extreme values
         are not calculated and reported.  The suggested defaults are
         the current values, or {\tt !} if these do not exist.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the histogram NDF.  {\tt ["KAPPA - Histogram"]}
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         ABSLAB = LITERAL (Read)
      }{
         Label for the plot's abscissa.  NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  Only the first 50 characters are
         used.  If axis information is present in the NDF the suggested
         default is the NDF's axis label followed by the units, in
         parentheses.  If an error occurs obtaining the label or there
         is no axis information, the label takes its current value,
         which initially is {\tt "COMP values"}, where COMP is the value of
         parameter COMP.  {\tt []}
      }
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         Determines if the graphics workstation is to be cleared before
         producing the plot. {\tt [TRUE]}
      }
      \sstsubsection{
         FONT = LITERAL (Read)
      }{
         The fount to be used for the histogram plot.  It can be either
         {\tt "NCAR"} for the NCAR fancy characters and {\tt "GKS"} for the standard
         GKS san-serif fount.   The former is intended for hardcopy
         publication-quality plots, since it is relatively slow; the
         latter is intended for normal interactive graphics requiring
         rapid plotting, and it is clearer on small plots.  The
         suggested default is the current value. {\tt ["GKS"]}
      }
      \sstsubsection{
         MAJTIC( 2 ) = \_REAL (Read)
      }{
         The parameter controlling the numbers of major tick marks
         for the $x$ and $y$ axes.  (Number used is between MAJTIC$+$2 and
         5$*$MAJTIC/2$+$4.) {\tt [4.,4.]}
      }
      \sstsubsection{
         MINTIC( 2 ) = \_REAL (Read)
      }{
         The number of minor tick marks between each major tick mark
         for the $x$ and $y$ axes.  A negative value forces the graphics
         package to compute appropriate values. {\tt [-1.,-1.]}
      }
      \sstsubsection{
         ORDLAB = LITERAL (Read)
      }{
         A label for the plot's ordinate axis.  NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  Only the first 50 characters
         are used.  {\tt ["Number"]}
      }
      \sstsubsection{
         OUTTIC = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the axis tick marks are to appear on the outside of
         the axes instead of inside. {\tt [FALSE]}
      }
      \sstsubsection{
         PLTITL = LITERAL (Read)
      }{
         A title for the histogram plot, in which NCAR fancy founts may
         be embedded.  The default is to use the NDF's title if present
         and no error occurs, otherwise the current value becomes the
         suggested default.  This is initially {\tt "Histogram"}.  Only the
         first 50 characters are used.  {\tt []}
      }
      \sstsubsection{
         PXSIZE = \_REAL (Read)
      }{
         The length ($x$ axis) of the plot in metres. {\tt [}Maximum that can
         fit in the current picture whilst preserving square pixels{\tt ]}
      }
      \sstsubsection{
         PYSIZE = \_REAL (Read)
      }{
         The length ($y$ axis) of the plot in metres. {\tt [}Maximum that can
         fit in the current picture whilst preserving square pixels{\tt ]}
      }
      \sstsubsection{
         THICK = \_REAL (Read)
      }{
         The thickness of the axes and annotations in the histogram
         plots, where 1.0 is the normal thickness.  Currently,
         this is only available on a few devices.  It must take a value
         in the range 0.5--5.0. {\tt [1.0]}
      }
      \sstsubsection{
         XLOG = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the plot abscissa is to be logarithmic.  It is
         unlikely that you would want to do this.  By default, the
         abscissa is linear. {\tt [FALSE]}
      }
      \sstsubsection{
         YLOG = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the plot ordinate is to be logarithmic.  By default,
         the ordinate is linear.  {\tt [FALSE]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         histogram image 100 ! device=!
      }{
         Computes and reports the histogram for the data array in the
         NDF called image.  The histogram has 100 bins and spans the
         full range of data values.
      }
      \sstexamplesubsection{
         histogram ndf=spectrum comp=variance range=[100,200] numbin=20
      }{
         Computes and reports the histogram for the variance array in
         the NDF called spectrum.  The histogram has 20 bins and spans
         the values between 100 and 200.  A plot is made to the current
         graphics device.
      }
      \sstexamplesubsection{
         histogram cube(3,4,) 10 ! out=c3\_4\_hist device=!
      }{
         Computes and reports the histogram for the z-vector at ($x$,$y$)
         element (3,4) of the data array in the 3-dimensional NDF called
         cube.  The histogram has 10 bins and spans the full range of
         data values.  The histogram is written to a one-dimensional
         NDF called c3\_4\_hist.
      }
      \sstexamplesubsection{
         histogram cube numbin=32 ! device=xwindows
      }{
         Computes and reports the histogram for the data array in
         the NDF called cube.  The histogram has 32 bins and spans the
         full range of data values.  A plot of the histogram is made to
         the XWINDOWS device.
      }
      \sstexamplesubsection{
         histogram cube numbin=32 ! device=xwindows ylog pltitl="Taurus 2"
      }{
         As in the previous example except the logarithm of the number
         in each histogram bin is plotted, and the plot title is
         {\tt "Taurus 2"}.
      }
      \sstexamplesubsection{
         histogram halley($\sim$200,$\sim$300) range=[-1000,1000] logfile=hist.dat $\backslash$
      }{
         Computes the histogram for the central 200 by 300 elements of
         the data array in the NDF called halley, and writes the
         results to a logfile called {\tt hist.dat}.  The histogram uses the
         current number of bins, and includes data values between $-$1000
         and 1000.  A plot appears on the current graphics device.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: HISTAT, INSPECT, MSTATS, NUMB, STATS; Figaro: HIST, ISTAT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, VARIANCE,
         QUALITY, LABEL, TITLE, UNITS, and HISTORY components of the input
         NDF.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         Any number of NDF dimensions is supported.
      }
   }
}
\begin{htmlonly}
Click \htmladdnormallink{here}{histogram_exam.gif} to see an example
plot (6k).
\end{htmlonly}

\sstroutine{
   IDCLEAR
}{
   Clears an image display and purges its database entries
}{
   \sstdescription{
      This application software resets an image-display device. In effect
      the device is cleared.  It purges the graphics-database entries
      for the device.  Optionally, only the current picture is cleared
      and the database unchanged.  (Note that the clearing of the current
      picture may not work on some image-display devices.)
   }
   \sstusage{
      idclear [device] [current]
   }
   \sstparameters{
      \sstsubsection{
         CURRENT = \_LOGICAL (Read)
      }{
         If true then only the current picture is cleared. {\tt [FALSE]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device to be cleared. {\tt [}Current image-display
         device{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         idclear
      }{
         Clears the current image display and purges its graphics
         database entries.
      }
      \sstexamplesubsection{
         idclear current
      }{
         Clears the current picture on the current image display.
      }
      \sstexamplesubsection{
         idclear xwindows
      }{
         Clears the xwindows device and purges its graphics-database entries.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: GDCLEAR, IDINVISIBLE, IDSTATE, OVCLEAR.
   }
}
 
\sstroutine{
   IDINVISIBLE
}{
   Makes memory planes of an image-display device invisible
}{
   \sstdescription{
      This routine makes invisible nominated planes of an IDI-supported
      image display, such as X-windows.
   }
   \sstusage{
      idinvisible [planes] [device]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the image-display device whose memory plane is to
         be made invisible.  The name of the base plane should be given
         even if an overlay plane is to be made invisible.
         {\tt [}Current image display{\tt ]}
      }
      \sstsubsection{
         PLANES() = \_INTEGER (Read)
      }{
         The numbers of the memory planes to be made invisible.  All
         unspecified planes become visible.  If it is null the base
         (image) memory is made invisible.  The base memory is 0 and
         overlays are numbered consecutively from 1.  For an Ikon the
         only overlay plane is 1.
         {\tt [0]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         idinvisible [0,1]
      }{
         Makes only planes 0 and 1 invisible on the current image
         display device.
      }
      \sstexamplesubsection{
         idinvisible device=xwindows
      }{
         Makes the base plane invisible on the xwindows device.
      }
      \sstexamplesubsection{
         idinvisible 1 ikon
      }{
         Makes the first overlay plane invisible on the Ikon device.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         On some devices making a memory invisible may have the effect
         of making other memories visible.

         \sstitem
         On the Ikon the visibilities are set to visible on start up,
         so that any set up change introduced by an application calling
         GKS are not lost, therefore all planes to be made invisible must
         be given in one invocation.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: IDCLEAR, IDSTATE.
   }
}
 
\sstroutine{
   IDPAZO
}{
   Pans and zooms an image-display device
}{
   \sstdescription{
      This routine pans all planes of an IDI-supported image display,
      such as X-windows.  The zoom factor is controlled by the mouse
      or trackerball buttons.

      For an X-windows device, pressing the left button of the mouse
      increases the zooming, the centre button reduces the zoom factor, and
      the right-hand button ends the pan and zoom operation.
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the image-display device to be panned and zoomed.
         The name of the base plane should be given.
         {\tt [}Current image display{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         idpazo xwindows
      }{
         Pans and zooms the xwindows device.
      }
      \sstexamplesubsection{
         idpazo
      }{
         Pans and zooms the current image-display device.
      }
   }
}
 
\sstroutine{
   IDSET
}{
   Selects a current image-display device
}{
   \sstdescription{
      This application selects a current image-display device. This
      device will be used for all applications requiring an
      image-display until changed explicitly.
   }
   \sstusage{
      idset device
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The image-display device to become the current image-display
         device.  The device must be in one of the following GNS
         categories: IMAGE\_DISPLAY, IMAGE\_OVERLAY, MATRIX\_PRINTER, or
         WINDOW, and have at least 24 colour indices or greyscale
         intensities.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         idset xwindows
      }{
         Makes the xwindows device the current image-display device.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: GDSET, IDSTATE, OVSET; Figaro: SOFT.
   }
}
\sstroutine{
   IDSTATE
}{
   Shows the current status of an image display
}{
   \sstdescription{
      This application displays the current status of an image-display
      device, including details of the current graphics-database
      picture ({\it e.g.}\ its co-ordinate system and position on the display
      surface), and the reserved colour table.
   }
   \sstusage{
      idstate [device]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image-display device about which information is
         required. {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         OUTLINE = \_LOGICAL (Read)
      }{
         If OUTLINE is {\tt TRUE}, then an outline will be drawn around the
         current picture to indicate its position. {\tt [FALSE]}
      }
      \sstsubsection{
         REPORT = \_LOGICAL (Read)
      }{
         If this is {\tt FALSE}, details of the image-display device are not
         reported, merely the results are written to the output
         parameters.  It is intended for use within procedures. {\tt [TRUE]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         COMMENT = LITERAL (Write)
      }{
         The comment of the current picture.  Up to 132 characters
         will be written.
      }
      \sstsubsection{
         LABEL = LITERAL (Write)
      }{
         The label of the current picture.  It is blank if there is no
         label.
      }
      \sstsubsection{
         NAME = LITERAL (Write)
      }{
         The name of the current picture.
      }
      \sstsubsection{
         NCX1 = \_REAL (Write)
      }{
         The lower $x$ normalised device co-ordinate of the current
         picture.
      }
      \sstsubsection{
         NCX2 = \_REAL (Write)
      }{
         The upper $x$ normalised device co-ordinate of the current
         picture.
      }
      \sstsubsection{
         NCY1 = \_REAL (Write)
      }{
         The lower $y$ normalised device co-ordinate of the current
         picture.
      }
      \sstsubsection{
         NCY2 = \_REAL (Write)
      }{
         The upper $y$ normalised device co-ordinate of the current
         picture.
      }
      \sstsubsection{
         REFNAM = LITERAL (Write)
      }{
         The reference object associated with the current picture.  It
         is blank if there is no reference object.  Up to 132 characters
         will be written.
      }
      \sstsubsection{
         WCX1 = \_REAL (Write)
      }{
         The lower $x$ world co-ordinate of the current picture.
      }
      \sstsubsection{
         WCX2 = \_REAL (Write)
      }{
         The upper $x$ world co-ordinate of the current picture.
      }
      \sstsubsection{
         WCY1 = \_REAL (Write)
      }{
         The lower $y$ world co-ordinate of the current picture.
      }
      \sstsubsection{
         WCY2 = \_REAL (Write)
      }{
         The upper $y$ world co-ordinate of the current picture.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         idstate
      }{
         Shows the status of the current image-display device.
      }
      \sstexamplesubsection{
         idstate xwindows
      }{
         Shows the status of the xwindows device.
      }
      \sstexamplesubsection{
         idstate outline
      }{
         Shows the status of the current image-display device and draws
         an outline around the current database picture.
      }
      \sstexamplesubsection{
         idstate refnam=(ndfname)
      }{
         Shows the status of the current image-display device.  If there
         is a reference data object, its name is written to the
         {\footnotesize ICL} variable NDFNAME.
      }
      \sstexamplesubsection{
         idstate wcx1=(x1) wcx2=(x2) wcy1=(y1) wcy2=(y2)
      }{
         Shows the status of the current image-display device.  The
         bounds of the current picture in world co-ordinates are
         written to the {\footnotesize ICL} variables: X1, X2, Y1, Y2.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         If a channel to the image-display device cannot be opened, then
         this application will still execute without error, but a reduced
         amount of information will be displayed and an outline around the
         current picture (if requested) will not be drawn.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: GDSTATE.
   }
}

\sstroutine{
   INSPECT
}{
   Inspects a 2-d NDF in a variety of ways
}{
   \sstdescription{
      This application provides an interactive facility to inspect the
      details of whole or part of the 2-dimensional data array in an input NDF.
      Briefly, the inspection options permit: a region to be selected
      for which statistics may be calculated, its values written to a
      text file, or an histogram be plotted and saved in an NDF; the
      region itself may be saved in an NDF; the value of a pixel or a
      region of pixels to be viewed; a slice between two pixels may be
      calculated, plotted and saved in an NDF; text files containing
      $x$-$y$-value of selected pixels may be created and extended, and
      chosen pixels marked.

      The application has two modes of interaction: cursor and
      interface.  In cursor mode the selection of pixels, and the
      definition of the region are made by moving a graphics cursor
      over a previously displayed image or contour plot.  Since
      instructional text showing the function of the mouse or
      trackerball buttons is shown, the graphics device providing the
      cursor must be an image-display overlay.  Also the name of the
      NDF used to display the image or contour plot is known and need
      not be entered.  This is the recommended interaction mode.  The
      alternative, interface, means that the pixel indices of pixels
      and regions to be inspected are specified in response to prompts.

      The application is composed of two parts.  First the preliminaries
      obtains the mode, the input NDF and graphics devices.  In cursor
      mode this usually amounts to a single prompt, and but two in
      interface mode.  The second stage is a loop where the inspection
      option is selected and performed.
   }
   \sstusage{
      inspect in [mode] gdevice option [overlay]
        \newline\hspace*{1.5em}
        $\left\{ {\begin{tabular}{l}
                   numbin=? hirep=? histogram=? hititle=? \\
                   filename=? \\
                   peind=? \\
                   lbound=? ubound=? \\
                   out=? \\
                   slstart=? slend=? slice=? sltitle=? \\
                   vaind=? \\
                   xycont=? xyfile=? xytitle=?
                   \end{tabular} }
        \right.$
        \newline\hspace*{1.9em}
        \makebox[0mm][c]{\small option}
   }
   \sstparameters{
      \sstsubsection{
         FILENAME = FILENAME (Write)
      }{
         Name of the text file to contain the Listing of image values.
         The suggested default is {\tt inspect\_list.lis}.  This is only
         required for the {\tt "List"} option.
      }
      \sstsubsection{
         GDEVICE = DEVICE (Read)
      }{
         The name of the graphics device for line plots produced by the
         {\tt "Histogram"} and {\tt "Slice"} options.  The device should
         not be the image display, but it may be the image-display overlay
         plane used in the cursor-interaction mode, {\it i.e.}\ the same
         value as parameter OVERLAY (though this is not advisable for
         X-windows).  In the latter case plotting occurs in the same
         picture as the overlay annotations, namely the current
         picture.  If the existing plot on the base plane of the image
         display has text, {\it e.g.}\ annotated axes, a mess can of confused
         lines can appear.  To avoid this the current picture should be
         made the DATA picture rather than the FRAME around it.  If
         null, {\tt !}, is given no line plots will be drawn unless the
         {\tt "Device"} option is selected.
      }
      \sstsubsection{
         HIREP = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the full Histogram is to be reported to you.  A
         large number of bins may be required for the plot but need not
         be listed in full.  This parameter provides a way of preventing
         unwanted, tedious and long output.  The suggested default is
         {\tt FALSE}.  HIREP is only required for the {\tt "Histogram"} option.
      }
      \sstsubsection{
         HISTOGRAM = NDF (Read)
      }{
         Name of the NDF structure to save the Histogram in its data
         array.  If null, {\tt !}, is entered, the histogram NDF is not
         created.  This parameter is only required for the {\tt "Histogram"}
         option.  The suggested default is {\tt !}.
      }
      \sstsubsection{
         HITITLE = LITERAL (Read)
      }{
         Title for the output NDF containing the Histogram.  For the
         first histogram saved this defaults to
         {\tt "KAPPA - Inspect\_Histogram"}, and subsequently this becomes
         the suggested default.   This parameter is only required for
         the {\tt "Histogram"} option. {\tt []}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         NDF structure containing the 2-dimensional data array to be inspected.
      }
      \sstsubsection{
         LBOUND( 2 ) = \_INTEGER (Read)
      }{
         Lower bounds in pixel indices of the Region.  The chosen pixel
         must be different from that at the lower bound.  It is only
         used in Interface mode with the {\tt "Region"} option.
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The interaction mode.  The options are {\tt "Cursor"} to use a
         graphics cursor to select regions and pixels to inspect,
         or {\tt "Interface"} where prompted ADAM parameters are used to
         define those parts of the image to inspect. {\tt [}Current
         interaction mode{\tt ]}
      }
      \sstsubsection{
         NUMBIN = \_INTEGER (Read)
      }{
         Number of bins needed for the Histogram.  The suggested default
         is the current value, which is 100 initially.  A value is the
         range 2--5000 is required.  This parameter is only required
         for the {\tt "Histogram"} option.
      }
      \sstsubsection{
         OPTION = LITERAL (Read)
      }{
         Current inspection mode.  The options are:
         \begin{description}
         \item {\tt "Device"} --- This allows the selection and opening of a new
                          line-plot graphics device, at the same time
                          closing down the old one, whose last plot is
                          stored in the graphics database.
         \item {\tt "Exit"} --- Exit the application.
         \item {\tt "Histogram"} --- This calculates the histogram of the current
                          region. A summary and plot (if there is a
                          graphics device available) of the histogram
                          is produced.  The style of the plot may be
                          adjusted via several parameters.  The full
                          histogram may also be reported. The histogram
                          data can be stored in a 1-d NDF.
         \item {\tt "List"} --- This produces a formatted and headed listing
                          of the chosen region to a text file.
         \item {\tt "Peep"} --- Obtain a formatted listing of the 7$\times$7 section
                          of the array data, centred on a pixel
                          specified using the cursor or via prompting.
         \item {\tt "Region"} --- To define the region of the array to be used
                          by other options. If the image display
                          is available, then the cursor is used to
                          define the area, otherwise, the pixel bounds
                          of the region come from the environment. Using
                          the cursor, the functions of the choice-device
                          buttons are drawn on the overlay.
         \item {\tt "Save"} --- Writes the current region to a new NDF,
                          propagating all the components.
         \item {\tt "Slice"} --- Two points are defined via the cursor or from
                          parameter prompting between which a slice ({\it i.e.}
                          cross-section) is calculated.  Using the
                          cursor, the functions of the trackerball or
                          mouse buttons are drawn on the overlay. A plot
                          is made to the graphics device if available.
                          The style of the plot may be adjusted via
                          several parameters.  The slice can be stored
                          in a 1-d NDF.  The slice abscissa has units in
                          true pixels (assuming pixels are square), thus
                          a 45\dgs\ slice would have a length $\surd$2
                          times its projected length in $x$ or $y$.
         \item {\tt "Statistics"} --- The key statistical parameters of the current
                          region are determined and reported.
         \item {\tt "Value"} --- Obtain the value of a pixel at a point selected
                          via the cursor or via prompting.
         \item {\tt "XYcur"} --- A list of the co-ordinates and values of pixels
                          selected by the image-display cursor are
                          written to a text file with Fortran carriage
                          control. The functions of the trackerball or
                          mouse buttons are displayed.  Optionally, an
                          existing file in the same format as produced
                          by XYcur can be appended to, for example, when
                          a session has been interrupted.  These stored
                          pixels are displayed on the overlay plane as
                          if there had been no interruption.  XYcur
                          requires cursor mode.
         \end{description}
         The suggested default is {\tt "Region"}.

         If the option is specified on the command line a single
         inspection may be undertaken, {\it i.e.}\  there is no looping.  This
         feature is intended for command procedures.
      }
      \sstsubsection{
         OUT = NDF (Read)
      }{
         Name of the NDF structure to contain the Saved Region.  This
         is only used in the {\tt "Save"} option.
      }
      \sstsubsection{
         OVERLAY = DEVICE (Read)
      }{
         Name of the overlay-plane device used in the cursor interaction
         mode.  It must have class IMAGE\_OVERLAY or WINDOW\_OVERLAY and
         support colour.  It is ignored when MODE is not {\tt "Cursor"}.
         {\tt [}Current image-display-overlay device{\tt ]}
      }
      \sstsubsection{
         PEIND( 2 ) = \_INTEGER (Read)
      }{
         $x$-$y$ pixel index of the pixel about which the Peep is required.
         The values must lie within their respective bounds of the
         input image.  The suggested default is the image centre.  It
         is only used in Interface mode with the {\tt "Peep"} option.
      }
      \sstsubsection{
         SLEND( 2 ) = \_INTEGER (Read)
      }{
         The $x$-$y$ pixel index defining the end of the Slice.  It
         must lie within the bounds of the array and be distinct from
         the start of the slice.  The suggested default is the upper
         bound of the input NDF.  It is only used in Interface mode
         with the {\tt "Slice"} option.
      }
      \sstsubsection{
         SLICE = NDF (Read)
      }{
         Name of the NDF structure to save the Slice in its data array.
         If null, {\tt !}, is entered, the slice NDF is not created.
         It is only required in the {\tt "Slice"} option.  The
         suggested default is {\tt !}. 
      }
      \sstsubsection{
         SLSTART( 2 ) = \_INTEGER (Read)
      }{
         The $x$-$y$ pixel index defining the start of the Slice.  It must
         lie within the bounds of the array.  The suggested default is
         the lower bound of the input NDF.  It is only used in
         Interface mode with the {\tt "Slice"} option.
      }
      \sstsubsection{
         SLTITLE = LITERAL (Read)
      }{
         Title for the Slice NDF.  Title for the Region NDF.  For the
         first region saved this defaults to {\tt "KAPPA - Inspect\_Slice"},
         and subsequently this becomes the suggested default.  It is
         only required in the {\tt "Slice"} option. {\tt []}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the Region NDF.  For the first region saved this
         defaults to {\tt "KAPPA - Inspect"}, and subsequently this becomes
         the suggested default.  It is only used in the {\tt "Save"} option.
         {\tt []}
      }
      \sstsubsection{
         UBOUND( 2 ) = \_INTEGER (Read)
      }{
         Upper bounds in pixel indices of the Region.  The chosen pixel
         must be different from that at the lower bound.  It is only
         used in Interface mode with the {\tt "Region"} option.
      }
      \sstsubsection{
         VAIND( 2 ) = \_INTEGER (Read)
      }{
         $x$-$y$ pixel index of pixel whose Value is required.  The values
         must lie within their respective bounds of the input image.
         The suggested default is the image centre.  It is only used in
         Interface mode with the {\tt "Value"} option.
      }
      \sstsubsection{
         XYCONT = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, an existing file is appended to in the XYcur option.
         The suggested default is {\tt FALSE}.  It is only available in
         cursor mode with the {\tt "XYcur"} option.
      }
      \sstsubsection{
         XYFILE = FILENAME (Update)
      }{
         Name of the text file to which pixel data are written by
         XYcur option.  The suggested default is {\tt xylist.lis}.  It is
         only available in cursor mode with the {\tt "XYcur"} option.
      }
      \sstsubsection{
         XYTITLE = LITERAL (Read)
      }{
         Title for the text file in XYcur option.  For the first file
         created this defaults to {\tt "\# KAPPA - Inspect\_XYcur"}, and
         subsequently this becomes the suggested default. It is not
         accessed if XYCONT is {\tt TRUE}.   It is only available in cursor
         mode with the {\tt "XYcur"} option. {\tt []}
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         ABSLAB  =  LITERAL (Read)
      }{
         Label for the line-plot abscissa, in which NCAR fancy founts
         may be embedded when FONT = {\tt "NCAR"}.  Note Slice and
         Histogram have different defaults and these are stored separately.

         For a slice plot the suggested default is the current value,
         which is initially {\tt "Pixels"}.  If an error occurs obtaining the
         label the default is {\tt "Pixels"}.

         If axis information is present the suggested default for a
         plot of an histogram is the NDF's axis label followed by the
         units, in parentheses.  If an error occurs obtaining the label
         or there is no axis information, the label takes its current
         value, which initially is {\tt "Values"}.

         For the first plot ABSLAB is defaulted to the suggested value
         unless PLOTSTYLE is included on the command line, and
         subsequently will only be obtained whenever PLOTSTYLE is {\tt TRUE}.
         {\tt []}
      }
      \sstsubsection{
         FONT = LITERAL (Read)
      }{
         The fount to be used for the line graphics.  It can be either
         {\tt "NCAR"} for the NCAR fancy characters and {\tt "GKS"} for the standard
         GKS san-serif fount.   The former is intended for hardcopy
         publication-quality plots, since it is relatively slow; the
         latter is intended for normal interactive graphics requiring
         rapid plotting, and it is clearer on small plots. The
         suggested default is the current value.

         For the first plot FONT is defaulted to {\tt "GKS"}, and subsequently
         will only be obtained whenever PLOTSTYLE is {\tt TRUE}. {\tt []}
      }
      \sstsubsection{
         MAJTIC( 2 ) = \_REAL (Read)
      }{
         The parameter controlling the numbers of major tick marks
         for the $x$ and $y$ axes in the slice or histogram plot.
         (The number used is between MAJTIC$+$2 and 5$*$MAJTIC/2$+$4.).

         By default, it is {\tt [4.,4.]}.  For the first plot MAJTIC is
         defaulted, and subsequently will only be obtained whenever
         PLOTSTYLE is {\tt TRUE}.  {\tt []}
      }
      \sstsubsection{
         MINTIC( 2 ) = \_REAL (Read)
      }{
         The number of minor tick marks between each major tick mark
         for linear $x$ and $y$ axes in the slice or histogram plot.  A
         negative value forces the graphics package to compute
         appropriate values.  The number of minor tick marks per major
         tick is fixed (8) for a logarithmic axis.

         By default, it is {\tt [-1.,-1.]}.  For the first plot MINTIC is
         defaulted, and subsequently will only be obtained whenever
         PLOTSTYLE is {\tt TRUE}.  {\tt []}
      }
      \sstsubsection{
         ORDLAB  =  LITERAL (Read)
      }{
         Label for the line-plot ordinate, in which NCAR fancy founts
         may be embedded.  Note Slice and Histogram have different
         defaults and these are stored separately.

         For an histogram plot the suggested default is the current
         value, which is initially {\tt "Number"}.  If an error occurs
         obtaining the label the default is {\tt "Number"}.

         If axis information is present the suggested default for a
         plot of a slice is the NDF's axis label followed by the
         units, in parentheses.  If an error occurs obtaining the label
         or there is no axis information, the label takes its current
         value, which initially is {\tt "Data values"}.

         For the first plot ORDLAB is defaulted to the suggested value
         unless PLOTSTYLE is included on the command line, and
         subsequently will only be obtained whenever PLOTSTYLE is {\tt TRUE}.
         {\tt []}
      }
      \sstsubsection{
         OUTTIC = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the axis tick marks are to appear on the outside of
         the axes instead of inside in the slice or histogram plots.
         This eliminates intersections of ticks with the data locus.

         By default, the tick marks are drawn inside the plot region.
         For the first plot OUTTIC is defaulted, and subsequently will
         only be obtained whenever PLOTSTYLE is {\tt TRUE}.  {\tt []}
      }
      \sstsubsection{
         PLOTSTYLE = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, the plotting style of line plots is to be altered from
         the default for the first plot, or the existing values for
         subsequent graphs.  Initially, it is defaulted to {\tt FALSE}, then
         the suggested value is the current value.  Therefore to
         override the plotting-style parameters on the first plot, new
         values should be given on the command line, and along with the
         PLOTSTYLE keyword for ABSLAB, ORDLAB, and PLTITL.
         Subsequently, the plotting style may be
         retained or modified via prompts.  {\tt []}
      }
      \sstsubsection{
         PLTITL = LITERAL (Read)
      }{
         The title of a line plot, in which NCAR fancy founts may be
         embedded.  Note Slice and Histogram have different defaults
         and these are stored separately.  Both attempt to use the NDF's
         title if present and no error occurs, otherwise the current
         value becomes the suggested default.  For the histogram plot
         this is initially {\tt "Histogram of current region"} and for the
         slice plot it is initially {\tt "Slice plot"}.

         For the first plot PLTITL is defaulted to the suggested value
         unless PLOTSTYLE is included on the command line, and
         subsequently will only be obtained whenever PLOTSTYLE is {\tt TRUE}.
         {\tt []}
      }
      \sstsubsection{
         THICK = \_REAL (Read)
      }{
         The thickness of the axes and annotations in the histogram and
         line plots, where 1.0 is the normal thickness.  Currently,
         this is only available on a few devices.  It must take a value
         in the range 0.5--5.0.

         By default the line thickness is 1.0.  For the first plot
         THICK is defaulted to the suggested value, and subsequently
         will only be obtained whenever PLOTSTYLE is {\tt TRUE}.  {\tt []}
      }
      \sstsubsection{
         XLOG = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the line-plot abscissa is to be logarithmic.  Note, for
         Slice and Histogram options each has its own independent
         switch.  It is unlikely that you would want to do this.  By
         default, the abscissa is linear.  For the first plot XLOG is
         defaulted, and subsequently will only be obtained whenever
         PLOTSTYLE is {\tt TRUE}. {\tt []}
      }
      \sstsubsection{
         YLOG = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the line-plot ordinate is to be logarithmic. Note, for
         Slice and Histogram options each has its own independent
         switch.  By default, the ordinate is linear.  For the first
         plot YLOG is defaulted, and subsequently will only be
         obtained whenever PLOTSTYLE is {\tt TRUE}.  {\tt []}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         Note:
      }{
         Since INSPECT is an interacting, graphical and self-contained
         monolith of applications it is not straightforward to give
         command-line examples.  Generally, the best way to run INSPECT
         is in cursor mode after having displayed an image.  The
         following examples use the prompting mode.
      }
      \sstexamplesubsection{
         inspect rulupi i canon\_l sl slstart=[3,10] slend=[9,42] slice=!
      }{
         Plots a slice from pixel (3,10) to (9,42) of the NDF called
         rulupi to the CANON\_L graphics device.
      }
      \sstexamplesubsection{
         inspect rulupi i gdevice=x2w option=hi numbin=100 histogram=ru\_hg
      }{
         Calculates the histogram of the NDF called rulupi, reporting
         a summary to you, and plots the histogram to the x2w
         device.  The histogram has one hundred bins and is stored in
         an NDF called ru\_hg.
      }
      \sstexamplesubsection{
         inspect rulupi i ! option=hi numbin=100 hirep $\backslash$
      }{
         As above except no plot is made, no NDF is created, and
         the full one hundred histogram values are reported.  In this
         particular example the second parameter could equally well be
         C for cursor mode since no co-ordinate information is
         obtained.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         In cursor mode there must be an existing DATA picture for the
         chosen image display stored in the graphics database.  Valid
         cursor positions are bounded by the DATA picture.

         \sstitem
         On exit the input picture, if there was one, is made the
         current picture on the overlay; and the last graphics plot is
         stored in the database as a FRAME picture.  Also, if the
         {\tt "Device"} option is used a FRAME picture is stored for
         that device.

         \sstitem
         The Histogram NDF has an AXIS component whose the LABEL and
         UNITS are those of the input NDF's data array; its centres are
         in data value of the bin centre.  The NDF LABEL is {\tt "Number"}.

         \sstitem
         The Slice NDF has an AXIS component whose LABEL is {\tt "Pixel"} and
         centres are pixel co-ordinates from 0.5; its LABEL and UNITS are
         propagated from the input NDF.

         \sstitem
         The current palette entries 1 to 4 associated with the OVERLAY
         are used as follows in the cursor mode for IMAGE\_OVERLAY devices.
         A sample slice or region, and the associated button, are drawn
         with palette index 1.  Similarly, index 3 is used to indicate an
         accepted slice or region.  The exit button is drawn in the colour
         of index 2.  Index 4 is used to draw the boxes representing the
         mouse or trackerball buttons.  Use the PAL$\lsk$ commands to select
         suitable complementary colours for the image's colour table,
         especially for palette indices 1 and 3.

         For WINDOW\_OVERLAY devices, all the above are drawn with the
         colour of palette index 1, but the various colours are replaced
         by different dashed-line patterns.  Use PALENTRY to change the
         colour of the lines.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CURSOR, HISTOGRAM, ELPROF, LOOK, NDFCOPY, STATS; ESP:
      SECTOR; Figaro: HIST, ICUR, IGCUR, ILIST, ISTAT, SLICE; GAIA.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         Only real data can be processed directly.  Other data types
         will undergo a type conversion before processing occurs.

         \sstitem
         The routine correctly processes the AXIS, DATA, QUALITY, VARIANCE,
         LABEL, TITLE, UNITS, and HISTORY components of an NDF, and
         propagates all extensions to the output Region NDF.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.  Bad pixels are excluded from statistics and are
         indicated in reports of data values by the word {\tt INVALID}.  In the
         slice plot they appear as gaps, and they do not affect the limits
         of the ordinate.  The same applies to zero or negative data
         values if the plot is to have a logarithmic ordinate.  Similarly,
         for the histogram abscissa.
      }
   }
}
\sstroutine{
   KAPHELP
}{
   Gives help about KAPPA
}{
   \sstdescription{
      Displays help about {\footnotesize KAPPA}.  The help information
      has classified and alphabetical lists of commands, general
      information about {\footnotesize KAPPA} and related material;
      it describes individual commands in detail.

      Here are some of the main options.
      \begin{description}
      \item {\tt kaphelp} \\
         No parameter is given so the introduction and the top-level
         help index is displayed.

      \item {\tt kaphelp application/topic} \\
         This gives help about the specified application or topic.

      \item {\tt kaphelp application/topic subtopic} \\
         This lists help about a subtopic of the specified application
         or topic.  The hierarchy of topics has a maximum of four levels.

      \item {\tt kaphelp Hints} \\
         This gives hints for new and intermediate users.

      \item {\tt kaphelp summary} \\
         This shows a one-line summary of each application.

      \item {\tt kaphelp classified classification} \\
         This lists a one-line summary of each application in the
         given functionality classification.
      \end{description}

      See the Section {\tt "}Navigating the Help Library{\tt "} for details
      how to move around the help information, and to select the topics
      you want to view.
   }
   \sstusage{
      kaphelp [topic] [subtopic] [subsubtopic] [subsubsubtopic]
   }
   \sstparameters{
      \sstsubsection{
         TOPIC = LITERAL (Read)
      }{
         Topic for which help is to be given. {\tt [" "]}
      }
      \sstsubsection{
         SUBTOPIC = LITERAL (Read)
      }{
         Subtopic for which help is to be given. {\tt [" "]}
      }
      \sstsubsection{
         SUBSUBTOPIC = LITERAL (Read)
      }{
         Subsubtopic for which help is to be given. {\tt [" "]}
      }
      \sstsubsection{
         SUBSUBSUBTOPIC = LITERAL (Read)
      }{
         Subsubsubtopic for which help is to be given. {\tt [" "]}
      }
   }
   \sstdiytopic{
      Navigating the Help Library
   }{
      The help information is arranged hierarchically.  You can
      move around the help information whenever KAPHELP prompts.  This
      occurs when it has either presented a screen's worth of text or
      has completed displaying the previously requested help.  The
      information displayed by KAPHELP on a particular topic includes
      a description of the topic and a list of subtopics that further
      describe the topic.

      At a prompt you may enter:

      \sstitemlist{

         \sstitem
         a topic and/or subtopic name(s) to display the help for that
         topic or subtopic, so for example, {\tt block parameters box}
         gives help on {\tt BOX}, which is a subtopic of {\tt Parameters},
         which in turn is a subtopic of {\tt BLOCK};

         \sstitem 
         a {\tt $<$CR$>$} to see more text at a {\tt Press RETURN to
         continue ...} request;

         \sstitem
         a {\tt $<$CR$>$} at topic and subtopic prompts to move up one
         level in the hierarchy, and if you are at the top level it will
         terminate the help session;

         \sstitem
         a {\tt CTRL/D }(pressing the CTRL and D keys simultaneously) in
         response to any prompt will terminate the help session;

         \sstitem
         a question mark {\tt ?} to redisplay the text for the current topic,
         including the list of topic or subtopic names; or
 
         \sstitem
         an ellipsis {\tt ...} to display all the text below the current
         point in the hierarchy.  For example, {\tt BLOCK...} displays
         information on the BLOCK topic as well as information on all the
         subtopics under BLOCK.
      }
 
      You can abbreviate any topic or subtopic using the following rules.

      \sstitemlist{

         \sstitem
         Just give the first few characters, {\it e.g.}\ {\tt PARA} for
         {\tt Parameters}.
 
         \sstitem
         Some topics are composed of several words separated by
         underscores.  Each word of the keyword may be abbreviated,
         {\it e.g.}\ {\tt Colour\_Set} can be shortened to {\tt C\_S}.
 
         \sstitem
         The characters {\tt \%} and {\tt $\lsk$} act as wildcards, where
         the percent sign matches any single character, and asterisk
         matches any sequence of characters.  Thus to display information
         on all available topics, type an asterisk in reply to a prompt.

         \sstitem
         If a word contains, but does end with an asterisk wildcard, it
         must not be truncated.
 
         \sstitem
         The entered string must not contain leading or embedded spaces.
      }

      Ambiguous abbreviations result in all matches being displayed.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         Uses the portable help system.

      }
   }
}
\sstroutine{
   KSTEST
}{
   Compares data sets using the Kolmogorov-Smirnov test
}{
   \sstdescription{
      This routine reads in a data array and performs a two sided
      Kolmogorov-Smirnov test on the vectorised data.  It does this in
      two ways:

      \begin{enumerate}
      \item If only one dataset is to be tested the data array is
           divided into subsamples.  First it compares subsample 1 with
           subsample 2, if they are thought to be from the same sample
           they are concatenated.  This enlarged sample is then
           compared with subsample 3 {\em{etc.}}, concatenating if consistent,
           until no more subsamples remain.

       \item If more than one dataset is specified, the datasets are
           compared to the reference dataset in turn.  If the
           probability the two are from the same sample is greater than
           the specified confidence level, the datasets are
           concatenated, and the next sample is tested against this
           enlarged reference dataset.
       \end{enumerate}

      The probability and maximum separation of the cumulative
      distribution function is written for each comparison (at the
      normal reporting level).  The mean value of the consistent data
      and its error are also reported.  In all cases the consistent
      data can be output to a new dataset.  The statistics and
      probabilities are written to results parameters.

   }
   \sstusage{
      kstest in out [limit]
   }
   \sstparameters{
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The name of the NDF array component to be tested for
         consistency: {\tt "Data"}, {\tt "Error"}, {\tt "Quality"} or
         {\tt "Variance"} (where {\tt "Error"} is the alternative to
         {\tt "Variance"} and causes the square
         root of the variance values to be taken before performing the
         comparisons).  If {\tt "Quality"} is specified, then the quality
         values are treated as numerical values (in the range 0 to
         255).  {\tt ["Data"]}
      }
      \sstsubsection{
         LIMIT = \_REAL (Read)
      }{
         Confidence level at which samples are thought to be
         consistent.  This must lie in the range 0 to 1. {\tt [0.05]}
      }
      \sstsubsection{
         IN = LITERAL (Read)
      }{
         The names of the NDFs to be tested.  If just one dataset is
         supplied, it is divided into subsamples, which are compared
         (see parameter NSAMPLE).  When more than one dataset is
         provided, the first becomes the reference dataset to which all
         the remainder are compared.

         It may be a list of NDF names or direction specifications
         separated by commas.  If a list is supplied on the command
         line, the list must be enclosed in double quotes.  NDF names
         may include the regular expressions ({\tt "$*$"}, {\tt "?"},
         {\tt "[a-z]"} {\em{etc.}}).
         Indirection may occur through text files (nested up to seven
         deep).  The indirection character is {\tt "$\wedge$"}.  If extra prompt
         lines are required, append the continuation character {\tt "-"} to
         the end of the line.  Comments in the indirection file begin
         with the character {\tt "\#"}.
      }
      \sstsubsection{
         NSAMPLE = \_INTEGER (Read)
      }{
         The number of the subsamples into which to divide the reference
         dataset.  This parameter is only requested when a single NDF
         is to be analysed, {\em{i.e.}}\ when only one dataset name is supplied
         via parameter IN.  The allowed range is 2 to 20.  {\tt [3]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output 1-dimensional NDF to which the consistent data is
         written.  A null value ({\tt !})---the suggested default---prevents
         creation of this output dataset.
      }
   }
   \sstresparameters{
      \sstsubsection{
         DIST() = \_REAL (Write)
      }{
         Maximum separation found in the cumulative distributions for
         each comparison subsample.  Note that it excludes the
         reference dataset.
      }
      \sstsubsection{
         ERRMEAN = \_DOUBLE (Write)
      }{
         Error in the mean value of the consistent data.
      }
      \sstsubsection{
         FILES() = LITERAL (Write)
      }{
         The names of the datasets intercompared.  The first is the
         reference dataset.
      }
      \sstsubsection{
         MEAN = \_DOUBLE (Write)
      }{
         Mean value of the consistent data.
      }
      \sstsubsection{
         NKEPT = \_INTEGER (Write)
      }{
         Number of consistent data.
      }
      \sstsubsection{
         PROB() = \_REAL (Write)
      }{
         Probability that each comparison subsample is drawn from the
         same sample.  Note that this excludes the reference sample.
      }
      \sstsubsection{
         SIGMA = \_DOUBLE (Write)
      }{
         Standard deviation of the consistent data.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         kstest arlac accept
      }{
         This tests the NDF called arlac for self-consistency at the 95\%
         confidence level using three subsamples.  No output dataset is
         created.

         The following applies to all the examples.  If the reference
         dataset and a comparison subsample are consistent, the two
         merge to form an expanded reference dataset, which is then
         used for the next comparison.  Details of the comparisons are
         presented.
      }
      \sstexamplesubsection{
         kstest arlac arlac\_filt 0.10 nsample=10
      }{
         As above except data are retained if they exceed the 90\%
         probability level, the comparisons are made with ten
         subsamples, and the consistent data are written to the
         one-dimensional NDF called arlac\_filt.
      }
      \sstexamplesubsection{
         kstest in="ref,obs$*$" comp=v out=master
      }{
         This compares the variance in the NDF called ref with that in
         a series of other NDFs whose names begin {\tt "obs"}.  The variance
         consistent with the reference dataset are written to the data
         array in the NDF called master.  To be consistent, they must be
         the same at 95\% probability.
      }
      \sstexamplesubsection{
         kstest "ref,$\wedge$96lc.lis,obs$*$" master comp=v
      }{
         As the previous example, except the comparison files include
         those listed in the text file {\tt 96lc.lis}.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The COMP array MUST exist in each NDF to be compared.  The
         COMP array becomes the data array in the output dataset.  When
         COMP={\tt "Data"}, the variance values corresponding to
         consistent data are propagated to the output dataset.

         \sstitem
         Pixel bounds are ignored for the comparisons.

         \sstitem
         The internal comparison of a single dataset follows the method
         outlined in Hughes D., 1993, {\em JCMT-UKIRT Newsletter}, \#4, p32.

         \sstitem
         The maximum number of files is 20.
      }
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes DATA, VARIANCE, HISTORY, LABEL,
         TITLE, and UNITS components, and propagates all extensions.  AXIS
         information is lost.  Propagation is from the reference
         dataset.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All numeric data types are supported, however, processing uses
         the \_REAL data type, and the output dataset has this type.
      }
   }
}
\manroutine {{\manheadstyle{LAPLACE}}}{ Performs a Laplacian convolution as an
  edge detector in a 2-d data array.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine works out the Laplacian of the 2-d data array, in the
  input {\mantt{IMAGE}} structure, and subtracts it from the original array
  to create a new data array in the output {\mantt{IMAGE}} structure. The
  subtractions can be done a specified integer number of times.
  This operation can be approximated with a convolution by
\[
\begin{array}{ccc}
  -N & -N & -N \\
  -N & +8N & -N \\
  -N & -N & -N
\end{array}
\]

  where {$N$} is the integer number of times the Laplacian is
  subtracted.  This convolution is used as a uni-directional edge
  detector.  Areas where the input data array is flat become zero
  in the output data array.

  The magic-value method is used for processing bad data.

\manroutineitem {Invocation }{}
  LAPLACE

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}} }{{\mantt{IMAGE}}}
  Input {\mantt{IMAGE}} structure containing the 2-d data array to be
  processed.
\manparameterentry {{\mantt{READ}} }{{\mantt{NUMBER}} }{{\mantt{\_INTEGER}}}
  Number of times the Laplacian is to be subtracted. \mbox{\mantt [1]}
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}} }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure containing the processed data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}} }{{\mantt{\_CHAR}}}
  Title string for output {\mantt{IMAGE}} structure. \mbox{\mantt ['KAPPA - Laplace']}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\sstroutine{
   LINPLOT
}{
   Draws a line plot of a 1-d NDF's data values against their axis
   co-ordinates
}{
   \sstdescription{
      This application creates a line plot of an array versus its
      co-ordinates for a 1-dimensional NDF on the current graphics
      device.  Thus it could be used to display a spectrum.  The array
      may be part or whole of the data array, but also the error,
      variance, or quality can be shown.  The plot is situated within
      the current graphics-database picture.

      The graph of the $x$-$y$ data points can take one of a number of
      forms:

      \begin{itemize}
      \item  straight lines connecting successive points;
      \item  histogram, where the $y$ co-ordinate is the histogram height;
      \item  symbols (from a selection of five) drawn at each point; and
      \item  horizontal lines, whose length is specified by the axis
             width of each pixel.
      \end{itemize}

      Error bars may be plotted too.  There is control of the colour
      of the components of the plot.

      The graph resides within labelled and enumerated axes
      corresponding to pixel or data co-ordinates and data value
      respectively.  The data co-ordinates are derived from the NDF's
      axis component.  A title and axis labels may be specified.
   }
   \sstusage{
      linplot ndf [comp] [mode] [pltitl] [abslab] [ordlab] [device]
   }
   \sstparameters{
      \sstsubsection{
         AXLIM = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, you decide the upper and lower limits of the axes via
         parameters ABSLIM and ORDLIM.  Otherwise, LINPLOT calculates
         sensible limits as described below.

         When CLEAR={\tt TRUE} the limits of the ordinate are derived after
         applying a margin of 3-percent of the data range to the
         maximum combined data value and error, and from the minimum
         combined value and error.  The default abscissa limits are the
         axis co-ordinate limits after correction for pixel width and
         axis error.  There may also be up to a half-pixel enlargement
         if MODE={\tt "Histogram"}.
 
         When CLEAR={\tt FALSE} the plot limits are derived from the previous
         plot (DATA picture), thus allowing composite pictures to be
         formed.
         {\tt [FALSE]}
      }
      \sstsubsection{
         ABSLIM( 2 ) = \_REAL (Read)
      }{
         The abscissa limits when AXLIM is {\tt TRUE}.  The limits may extend
         beyond the extreme axis values present in the dataset, however,
         the values must be distinct, and positive when XLOG={\tt TRUE}.
         The order you supply the limits is unimportant; the first value
         will always have the lower pixel co-ordinate, and the second
         will always have the larger pixel co-ordinate.  (This
         restriction arises because graphics database only permits this
         order.  Use the FLIP command if you want to reverse the axis.)
 
         The suggested defaults are the values that would have been used
         had AXLIM={\tt FALSE}, and depend on the value of parameter CLEAR.
         See parameter AXLIM for details.
      }
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the current picture is to be cleared before the
         line plot is drawn.  If CLEAR is {\tt FALSE} not only is the existing
         plot retained, but also the previous plot (DATA picture) is
         deemed to specify the axis limits when AXLIM={\tt FALSE}, and the
         suggested defaults for parameters ABSLIM and ORDLIM when
         AXLIM={\tt TRUE}.  Thus you can generate a composite plot within
         a single set of axes, say using different colours or modes to
         distinguish data from different datasets.  {\tt [TRUE]}
      }
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The NDF component to be plotted.  It may be {\tt "Data"},
         {\tt "Quality"}, {\tt "Variance"}, or {\tt "Error"} (where
         {\tt "Error"} is the alternative to {\tt "Variance"} and causes
         the square root of the variance values to be displayed).
         If {\tt "Quality"} is specified, then the quality values are
         treated as numerical values (in the range 0 to 255).  {\tt ["Data"]}
      }
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either
         {\tt "World"} or {\tt "Data"}.  {\tt "World"} makes pixel
         co-ordinates to appear on axes and the bounds are obtained in
         pixel indices.  If COSYS = {\tt "Data"} the NDF's axis
         information is used to annotate axes and the bounds are
         specified in that co-ordinate system.  {\tt [}Current
         co-ordinate system{\tt ]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The plotting device. {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         ERRBAR = \_LOGICAL (Read)
      }{
         {\tt TRUE} if error bars are to be drawn.  The error bars can
         comprise either or both of the data and axis-centre errors,
         depending on what is available in the supplied dataset.  See
         parameter SHAPE to control the appearance of the error bars.
         This parameter is ignored unless COMP={\tt "Data"}.
         {\tt [FALSE]}
      }
      \sstsubsection{
         FREQ = \_INTEGER (Read)
      }{
         The frequency at which error bars are to be plotted.  For
         instance, a value of {\tt 2} would mean that alternative points
         have error bars plotted.  This lets some plots be less
         cluttered.  FREQ must lie in the range {\tt 1} to half of
         the number of points to be plotted.  FREQ is only accessed when
         ERRBAR={\tt TRUE}.  {\tt [1]}
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The type of the line plot.  This can be one of the following
         values.

         \begin{description}
            \item {\tt "Histogram"} --- An histogram of the points is
                      plotted (with vertical lines only joining the $y$
                      values and not extending to the base of the plot).
                      The vertical lines are placed midway between
                      adjacent $x$ positions.
            \item {\tt "Line"}  --- The points are joined by straight lines.
            \item {\tt "Point"}  --- A dot is plotted at each point.
            \item {\tt "Step"}  --- Each point is displayed as a horizontal
                      line, whose length is specified by the axis width
                      of the pixel.
            \item {\tt 1} --- A synonym for {\tt "Point"}.
            \item {\tt 2}--{\tt 5} --- These are similar to
                      {\tt "Point"}, but give different symbols.  {\tt 
                      2} gives plus signs,  {\tt 3} generates asterisks,
                      {\tt 4} produces circles, and {\tt 5} creates
                      multiplication crosses.
         \end{description}

         Where colour is available the lines of the {\tt "Histogram"},
         {\tt "Line"}, and {\tt "Step"} options are plotted in the colour
         defined by parameter LINCOL; likewise the symbols of options
         {\tt "Point"} and the integers is plotted in the colour
         specified through parameter SYMCOL.

         MODE is defaulted to the current value, which is initially
         {\tt "Line"}. {\tt []}
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         NDF structure containing the array to be plotted.
      }
      \sstsubsection{
         ORDLIM( 2 ) = \_REAL (Read)
      }{
         The ordinate limits when AXLIM is {\tt TRUE}.  The limits may extend
         beyond the extreme data values present in the dataset, however,
         the values must be distinct, and positive when YLOG={\tt TRUE}.
         It is usually best to allow some margin to prevent the plotted
         values hitting the axes.  The order you supply the limits is
         unimportant; the smaller value will become the lower limit,
         and the larger the upper limit.
 
         The suggested defaults are the values that would have been used
         had AXLIM={\tt FALSE}, and depend on the value of parameter CLEAR.
         See parameter AXLIM for details.
      }
      \sstsubsection{
         PXSIZE = \_REAL (Read)
      }{
         The horizontal size of the display in metres. If a value less
         than the default is requested, the display will appear at
         the bottom left of the current device.  There is an upper
         limit given by the $x$ size of the current picture. {\tt [}Maximum
         that can fit in the current picture{\tt ]}
      }
      \sstsubsection{
         PYSIZE = \_REAL (Read)
      }{
         The vertical size of the display in metres. If a value less
         than the default is requested, then the display will appear at
         the bottom left of the current device.  There is an upper
         limit given by the $y$ size of the current picture. {\tt [}Maximum
         that can fit in the current picture{\tt ]}
      }
      \sstsubsection{
         SHAPE = LITERAL (Read)
      }{
         The way the errors are to be represented graphically.  SHAPE
         can take the following values.

         \begin{description}
         \item {\tt "Bars"}  --- A cross with serifs is plotted joining the
                     $x$ error limits and then the $y$ error limits.
         \item {\tt "Cross"} --- A san-serif cross is plotted joining the 
                     $x$ error limits and then the $y$ error limits.
         \item {\tt "Diamond"}  --- Adjacent error limits are joined to
                     form an error diamond.
         \end{description}

         SHAPE is defaulted to the current value, which is initially
         {\tt "Bars"}.  SHAPE is only accessed when ERRBAR={\tt TRUE}.
         {\tt []}
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         ABSLAB = LITERAL (Read)
      }{
         Label for the plot abscissa, in which NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  If axis information is present the suggested
         default is the NDF's axis label followed by the units, in
         parentheses.  If an error occurs obtaining the label the
         default is {\tt "Pixel co-ordinates"}. {\tt []}
      }
      \sstsubsection{
         ERRCOL = LITERAL (Read)
      }{
         The colour of the error bars (on devices that support colour).
         See parameter LINCOL for the available options and their
         meanings.  ERRCOL is only accessed when ERRBAR={\tt TRUE}.
         {\tt [}The current value, but equals {\tt 1} (the foreground
         colour) if there is no current value.{\tt ]}
      }
      \sstsubsection{
         FONT = LITERAL (Read)
      }{
         The fount to be used for the line graphics.  It can be either
         {\tt "NCAR"} for the NCAR fancy characters and {\tt "GKS"} for the standard
         GKS san-serif fount.   The former is intended for hardcopy
         publication-quality plots, since it is relatively slow; the
         latter is intended for normal interactive graphics requiring
         rapid plotting, and it is clearer on small plots. The
         suggested default is the current value. {\tt ["GKS"]}
      }
      \sstsubsection{
         LINCOL = LITERAL (Read)
      }{
         The colour of the lines showing the data (so this excludes the
         annotated axes) on devices that support colour.  The options
         are described below.

         \begin{description}
         \item {\tt "MAX"}  --- The maximum colour index in the image
                          display colour lookup table.
         \item {\tt "MIN"}  --- The minimum (non-reserved) colour index in
                          the image-display colour lookup table.
         \item {\bf An integer} --- The actual colour index.  It is
                          constrained between 0 and the maximum colour
                          index available on the device.
         \item {\bf A named colour} --- Uses the named colour from the
                          palette, and if it is not present, the nearest
                          colour from the palette is selected.
         \end{description}

         If the colour is to remain unaltered as the lookup table is
         manipulated choose an integer between 0 and 15, or a named
         colour.  This parameter will be ignored if symbols are plotted.
         {\tt [}The current value, but equals {\tt 1} (the foreground
         colour) if there is no current value.{\tt ]}
      }
      \sstsubsection{
         MAJTIC( 2 ) = \_REAL (Read)
      }{
         The parameter controlling the numbers of major tick marks
         for the $x$ and $y$ axes.  (Number used is between MAJTIC$+$2 and
         5$*$MAJTIC/2$+$4.) {\tt [4.,4.]}
      }
      \sstsubsection{
         MINTIC( 2 ) = \_REAL (Read)
      }{
         The number of minor tick marks between each major tick mark
         for the $x$ and $y$ axes.  A negative value forces the graphics
         package to compute appropriate values.  The number of
         minor tick marks per major tick is fixed (8) for a logarithmic
         axis. {\tt [-1.,-1.]}
      }
      \sstsubsection{
         ORDLAB = LITERAL (Read)
      }{
         Label for the plot ordinate, in which NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  The suggested default is the NDF's label followed
         by the units, if present, in parentheses.  If an error occurs
         obtaining the label the default is the component name followed
         by {\tt " values"}. {\tt []}
      }
      \sstsubsection{
         OUTTIC = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the axis tick marks are to appear on the outside of
         the axes instead of inside. By default, the tick marks are
         drawn inside the plot region.  {\tt [FALSE]}
      }
      \sstsubsection{
         PLTITL = LITERAL (Read)
      }{
         The title of the plot.  Up to about 40 characters can be
         accommodated, and NCAR fancy founts may be embedded when
         FONT = {\tt "NCAR"}.  The suggested default is the title of
         the NDF. If an error occurs obtaining the title, it is
         defaulted to {\tt "Line plot"}. {\tt []}
      }
      \sstsubsection{
         SYMCOL = LITERAL (Read)
      }{
         The colour of the plotted symbols (on devices that support
         colour).  See parameter LINCOL for the available options and
         their meanings.  SYMCOL is only accessed when MODE={\tt "Point"}
         or has a integer value.   {\tt [}The current value, but equals
         {\tt 1"} (the foreground colour) if there is no current
         value.{\tt ]}
      }
      \sstsubsection{
         THICK = \_REAL (Read)
      }{
         The thickness of the axes and annotations in the plot, where
         1.0 is the normal thickness.  Currently, this is only available
         on a few devices.  When FONT={\tt "GKS"}, axis annotations are
         unaffected by THICK.  It must take a value in the range
         0.5--10.0.  {\tt [1.0]}
      }
      \sstsubsection{
         XLOG = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the abscissa (pixel number) is to be logarithmic.  It
         is unlikely that you would want to do this. {\tt [FALSE]}
      }
      \sstsubsection{
         YLOG = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the ordinate (data value) is to be logarithmic.  This
         is useful when the data have wide dynamic range. {\tt [FALSE]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         linplot spectrum cosys=d
      }{
         Plots data values versus data co-ordinate for the whole of the
         1-dimensional NDF called spectrum on the current graphics device.  The
         data co-ordinates will be in pixels if spectrum does not have
         an axis component (this remark applies to all the examples save
         the last where world co-ordinates are plotted).
      }
      \sstexamplesubsection{
         linplot spectrum(1:500) device=graphon
      }{
         Plots data values versus the data or pixel co-ordinates
         (whichever is the current system) for the first 500 elements
         of the 1-dimensional NDF called spectrum on the Graphon device.
      }
      \sstexamplesubsection{
         linplot ironarc v pltitl="Fe Arc variance"
      }{
         Plots variance values versus data or pixel co-ordinate for the
         whole of the 1-dimensional NDF called ironarc on the current graphics
         device.  The plot has a title of {\tt "Fe Arc variance"}.
      }
      \sstexamplesubsection{
         linplot rscvn(3000.42:3994.) noclear abslab="Epoch" cosys=d
      }{
         This plots on the current graphics device data values versus
         data co-ordinates for those elements of the 1-dimensional NDF called
         rscvn whose axis values lie between 3000.42 and 3994.0.  If
         the current co-ordinate system is already {\tt "Data"}, the COSYS
         parameter may be dispensed with.  The device is not cleared so
         the plot will overlay the existing picture.  The abscissa has
         label {\tt "Epoch"}.
      }
      \sstexamplesubsection{
         linplot frequencies mode=h lincol=red
      }{
         This draws a red histogram of the 1-dimensional dataset called
         frequencies.  The data values will be plotted logarithmically.
         The red colour will continue to be used in subsequent plots.
      }
      \sstexamplesubsection{
         linplot xspec mode=p errbar shape=d errcol=pink symcol=1 xlog
      }{
         This plots the data values versus the logarithm of the axis
         centres for the dataset called xspec.  Each pixel is plotted
         as a point surrounded by diamond-shaped error bars.  The
         points are drawn in the foreground colour and the error
         diamonds are displayed in pink.
      }
      \sstexamplesubsection{
         linplot ndf=spectrum axlim ordlim=[100,250] cosys=w accept
      }{
         Plots data values versus pixel co-ordinate for the whole of the
         1-dimensional NDF called spectrum on the current graphics device.  The
         limits of the ordinate axis are 100 and 250, so data values
         outside this range will be clipped.
      }
      \sstexamplesubsection{
         linplot ndf=spectrum2 lincol=yellow noclear
      }{
         This plots data values versus pixel co-ordinate, overlaid on
         the previous example, for the 1-dimensional NDF called
         spectrum2.  The new locus is plotted in yellow.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The application stores a number of pictures in the graphics
         database in the following order: a FRAME of the specified size
         containing the title, annotated axes, and line plot; and a DATA
         picture, which has world co-ordinates for linear axes measured in
         pixels along the $x$ axis and data values along $y$, and their
         logarithms if a logarithmic axis is selected.  The DATA picture
         also has data co-ordinates stored; for a linear axis this
         requires that the NDF's axis units are not pixel co-ordinates;
         for a logarithmic axis the actual data co-ordinate or value is
         recorded.  If there is no NDF axis information and a logarithmic
         abscissa, the DATA co-ordinates are pixel co-ordinates.  The NDF
         associated with the plot is stored by reference with the DATA
         picture.  On exit the current database picture for the chosen
         device reverts to the input picture.

         \sstitem
         In a logarithmic plot only positive data along each
         logarithmic axis can be displayed, therefore negative data are
         excluded.  Likewise any error bar which has a non-positive
         limit is not plotted, even though the data point itself can
         appear if it has positive co-ordinates.  A logarithmic axis will
         always increase from left to right, and from bottom to top.

         \sstitem
         Bad pixels appear as gaps in the plot, and they do not affect
         the limits of the ordinate.  The same applies to zero or negative
         data values if the plot is to have a logarithmic ordinate.

         \sstitem
         When COSYS={\tt "World"}, default axis errors and widths are used,
         if needed.  The defaults are the constants 0 and 1 respectively.

         \sstitem
         If you wish to make a composite plot, ensure that parameters
         ABSLAB, ORDLAB, COSYS, XLOG, and YLOG do not change between plots.
         For COSYS={\tt "Data"} and XLOG={\tt FALSE}, the data co-ordinates must
         be linear.

      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: INSPECT, MLINPLOT; Figaro: ESPLOT, IPLOTS, MSPLOT, SPLOT;
      SPECDRE: SPECGRID.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, VARIANCE,
         QUALITY, LABEL, TITLE, and UNITS components of the NDF.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  Only
         single-precision floating-point data can be processed directly.
         Other non-complex data types will undergo a type conversion
         before the line plot is drawn.
      }
   }
}
\begin{htmlonly}
Click \htmladdnormallink{here}{linplot_exam.gif} to see an example
plot (10k).
\end{htmlonly}
\manroutine {{\manheadstyle{LOG10}}}{ Takes the base-10 logarithm of each pixel of a
  data array.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine takes the logarithm to base 10 of each pixel of
  a data array. The result goes into a new output data array.
  Both data arrays are stored in the {\mantt{IMAGE}} format.

  The magic-value method is used for processing bad data.

\manroutineitem {Invocation }{}
  LOG10

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  Input {\mantt{IMAGE}} structure containing the data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{BASE}}  }{{\mantt{\_REAL}}}
  Base of logarithm to be taken of each input data array pixel.
  {\mantt [10]}
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure holding result of processed data.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title string for the output {\mantt{IMAGE}} structure. \mbox{\mantt ['KAPPA - Log10']}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\manroutine {{\manheadstyle{LOGAR}}}{ Takes the logarithm of each pixel of a data
  array (specified base).}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine takes the logarithm to a specified base of each
  pixel of a data array. The result goes into a new output data
  array. Both data arrays are stored in the {\mantt{IMAGE}} format.

  The magic-value method is used for processing bad data.

\manroutineitem {Invocation }{}
  LOGAR

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  Input {\mantt{IMAGE}} structure containing the data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{BASE}}  }{{\mantt{\_REAL}}}
  Base of logarithm to be taken of each input data array pixel.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure holding result of processed data.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title string for the output {\mantt{IMAGE}} structure. \mbox{\mantt ['KAPPA - Logar']}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\manroutine {{\manheadstyle{LOGE}}}{ Takes the natural logarithm of each pixel
  of a data array.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine takes the logarithm to base e of each pixel of
  a data array. The result goes into a new output data array.
  Both data arrays are stored in the {\mantt{IMAGE}} format.

  The magic-value method is used for processing bad data.

\manroutineitem {Invocation }{}
  LOGE

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  Input {\mantt{IMAGE}} structure containing the data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{BASE}}  }{{\mantt{\_REAL}}}
  Base of logarithm to be taken of each input data array pixel. {\mantt [2.718282]}
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure holding result of processed data.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title string for the output {\mantt{IMAGE}} structure. \mbox{\mantt ['KAPPA - Loge']}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\manroutine {{\manheadstyle{LOOK}}}{ Outputs the values of a sub-array of a 2-d
  data array to the screen or a text file.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine reports to user or writes to a text file the values
  in a specified sub-array of the 2-d data array in the input {\mantt{IMAGE}}
  structure. Three options are allowed:
\begin{mandescription}
\mandescriptionitem {Peep}  - gives a fixed 7{$\times$}7 box on the screen, centred
  on a given pixel;
\mandescriptionitem {Examine} - gives an {$n\times m$} box on the screen, with the
  lower-left pixel as specified;
\mandescriptionitem {List}  - outputs the specified sub-array to an
  ASCII text file (maximum width 132 characters) of a defined
  name.  Beware that the List option can produce very
  large files if it is not used sensibly.

\end{mandescription}
  The magic-value method is used for processing bad data. Bad pixels
  are denoted in the display or file by {\mantt{INVALID}} rather than a
  numerical value.

\manroutineitem {Invocation }{}
  LOOK

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  {\mantt{IMAGE}} structure containing the 2-d data array to be inspected.
\manparameterentry {{\mantt{READ}} }{{\mantt{CHOICE}} }{{\mantt{LITERAL}}}
  The means by which the data array is examined.  The options
  are {\mantt{Peep}} --- reports the values in a 7{$\times$}7 pixel region;
  {\mantt{Examine}} --- reports the values of a region whose size is defined
  by the user; and {\mantt{List}} is similar to {\mantt{Examine}}, but it
  generates a listing to a text file.
\manparameterentry {{\mantt{READ}} }{{\mantt{XCEN}}  }{{\mantt{\_INTEGER}}}
  {$x$} centre of box to be examined on the screen.
\manparameterentry {{\mantt{READ}} }{{\mantt{YCEN}}  }{{\mantt{\_INTEGER}}}
  {$y$} centre of box to be examined on the screen.
\manparameterentry {{\mantt{READ}} }{{\mantt{XLOW}}  }{{\mantt{\_INTEGER}}}
  {$x$} pixel index of the lower left of the sub-array to be output.
\manparameterentry {{\mantt{READ}} }{{\mantt{YLOW}}  }{{\mantt{\_INTEGER}}}
  {$y$} pixel index of the lower left of the sub-array to be output.
\manparameterentry {{\mantt{READ}} }{{\mantt{XSIZE}}  }{{\mantt{\_INTEGER}}}
  {$x$} size of the sub-array to be output.
\manparameterentry {{\mantt{READ}} }{{\mantt{YSIZE}}  }{{\mantt{\_INTEGER}}}
  {$y$} size of the sub-array to be output.
\manparameterentry {{\mantt{READ}} }{{\mantt{FILENAME}}  }{{\mantt{FILENAME}}}
  Name of the file to be used for the output of the sub-array listing.
\manparameterentry {{\mantt{READ}} }{{\mantt{ANOTHER}} }{{\mantt{\_LOGICAL}}}
  If true, another look at the data is required.
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\sstroutine{
   LUCY
}{
   Performs a Richardson-Lucy deconvolution of a 1- or 2-dimensional array
}{
   \sstdescription{
      This application deconvolves the supplied 1- or 2-dimensional array
      using the Richardson-Lucy (R-L) algorithm.  It takes an array holding
      observed data and another holding a Point-Spread Function (PSF) as
      input and produces an output array with higher resolution.  The
      algorithm is iterative, each iteration producing a new estimate
      of the restored array which (usually) fits the observed data more
      closely than the previous estimate (in the sense that simulated
      data generated from the restored array is closer to the observed
      data).  The closeness of the fit is indicated after each iteration
      by a normalised $\chi^{2}$ value ({\it i.e.}\ the $\chi^{2}$ per
      pixel).  The algorithm terminates when the normalised $\chi^{2}$ 
      given by parameter AIM is reached, or the maximum number of
      iterations given by parameter NITER have been performed.  The
      current estimate of the restored array is then written to the
      output NDF.

      Before the first iteration, the restored array is initialised
      either to the array given by parameter START, or, if no array is
      given, to the difference between the mean value in the input data
      array and the mean value in the background (specified by
      parameters BACK and BACKVAL).  Simulated data is then created from
      this trial array by smoothing it with the supplied PSF, and then
      adding the background on.  The $\chi^{2}$ value describing the
      deviation of this simulated data from the observed data is then
      found and displayed.  If the required $\chi^{2}$ is not reached
      by this simulated data, the first iteration commences, which
      consists of creating a new version of the restored array and then
      creating new simulated data from this new restored array (the
      corresponding $\chi^{2}$ value is displayed).  Repeated
      iterations are performed until the required $\chi^{2}$ is
      reached, or the iteration limit is reached.  The new version of
      the restored array is created as follows.

      \begin{enumerate}

         \item A correction factor is found for each data value.  This is
         the ratio of the observed data value to the simulated data
         value.  An option exists to use the Snyder modification as
         used by the LUCY program in the {\footnotesize STSDAS} package within
         {\footnotesize IRAF}.  With this option selected, the variance
         of the observed
         data value is added to both the numerator and the denominator
         when finding the correction factors.

         \item These correction factors are mapped into an array by
         smoothing the array of correction factors with the transposed
         PSF.

         \item The current version of the restored array is multiplied by
         this correction factor array to produce the new version of the
         restored array.

      \end{enumerate}

      For further background to the algorithm, see L.B. Lucy, {\it
      Astron.J.}\ 1974, Vol 79, No. 6.
   }
   \sstusage{
      lucy in psf out [aim]
   }
   \sstparameters{
      \sstsubsection{
         AIM = \_REAL (Read)
      }{
         The $\chi^{2}$ value at which the algorithm should terminate.
         Smaller values of AIM will result in higher apparent resolution
         in the output array but will also cause noise in the observed
         data to be interpreted as real structure.  Small values will
         require larger number of iterations, so NITER may need to be
         given a larger value.  Very-small values may be completely
         un-achievable, indicated by $\chi^{2}$ not decreasing (or
         sometimes increasing) between iterations.  Larger values will
         result in smoother output arrays with less noise.  {\tt [1.0]}
      }
      \sstsubsection{
         BACK = NDF (Read)
      }{
         An NDF holding the background value for each observed data
         value.  If a null value is supplied, a constant background
         value given by parameter BACKVAL is used. {\tt [!]}
      }
      \sstsubsection{
         BACKVAL = \_REAL (Read)
      }{
         The constant background value to use if BACK is given a null
         value. {\tt [0.0]}
      }
      \sstsubsection{
         CHIFAC = \_REAL (Read)
      }{
         The normalised $\chi^{2}$ value which is used to determine if
         the algorithm should terminate is defined as follows:

         \begin{Large}
         \vspace{5mm}
         \hspace{20mm} $\chi^{2} = \frac{1}{N}.\sum \frac{ ( d - s )^{2}}{( CHIFAC.s - \sigma^{2} )}$
         \vspace{5mm}
         \end{Large}

         where the sum is taken over the entire input array (excluding
         the margins used to pad the input array), $N$ is the number of
         values summed, $d$ is the observed data value, $s$ is the simulated
	 data value based on the current version of the restored array,
         $\sigma^{2}$ is the variance of the error associated with $d$, 
         and $CHIFAC$ is the value of parameter CHIFAC.  Using 0 for CHIFAC 
         results in the standard expression for $\chi^{2}$.  However, the
         algorithm sometimes has difficulty fitting bright features and so 
         may not reach the required normalised $\chi^{2}$ value.  Setting
         CHIFAC to 1 (as is done by the LUCY program in the STSDAS
         package within IRAF) causes larger data values to be given
         less weight in the $\chi^{2}$ calculation, and so encourages
         lower $\chi^{2}$ values. {\tt [1.0]}
      }
      \sstsubsection{
         IN= NDF (Read)
      }{
         The input NDF containing the observed data.
      }
      \sstsubsection{
         NITER = \_INTEGER (Read)
      }{
         The maximum number of iterations to perform. {\tt [50]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The restored output array.  The background specified by
         parameters BACK and BACKVAL will have been removed from this
         array.  The output is the same size as the input.  There is no
         VARIANCE component in the output, but any QUALITY values are
         propagated from the input to the output.
      }
      \sstsubsection{
         PSF = NDF (Read)
      }{
         An NDF holding an estimate of the Point-Spread Function (PSF)
         of the input array.  This could, for instance, be produced
         using the KAPPA application `PSF'.  There should be no bad
         pixels in the PSF otherwise an error will be reported.  The PSF
         can be centred anywhere within the array, but the location of
         the centre must be specified using parameters XCENTRE and
         YCENTRE.  The PSF is assumed to have the value zero outside the
         supplied NDF.
      }
      \sstsubsection{
         SIGMA = \_REAL (Read)
      }{
         The standard deviation of the noise in the observed data.
         This is only used if parameter VARIANCE is given the value
         FALSE.  The run-time default is an estimate of the noise
         based on the difference between adjacent pixel values in the
         observed data. {\tt []}
      }
      \sstsubsection{
         START = NDF (Read)
      }{
         An NDF containing an initial guess at the restored array.
         This could, for instance, be the output from a previous run of
         LUCY, in which case the deconvolution would continue from the
         point it had previously reached.  If a null value is given,
         then the restored array is initialised to a constant value
         equal to the difference between the mean observed data value
         and the mean background value. {\tt [!]}
      }
      \sstsubsection{
         SNYDER = \_LOGICAL (Read)
      }{
         If TRUE then the variance of the observed data sample is added
         to both the numerator and denominator when evaluating the
         correction factor for each data sample.  This is the modified
         form of the R-L algorithm used by the LUCY program in the
         STSDAS package within IRAF.  {\tt [TRUE]}
      }
      \sstsubsection{
         THRESH = \_REAL (Read)
      }{
         The fraction of the PSF peak amplitude at which the extents of
         the PSF are determined.  These extents are used to determine
         the size of the margins used to pad the supplied input array.
         Lower values of THRESH will result in larger margins being
         used.  THRESH must be positive and less than 0.5.  {\tt [0.0625]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         A title for the output NDF.  A null ({\tt !}) value means using the
         title of the input NDF. {\tt [!]}
      }
      \sstsubsection{
         VARIANCE = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, then the variance of each input data sample will be
         obtained from the VARIANCE component of the input NDF.  An
         error is reported if this option is selected and the NDF has
         no VARIANCE component.  If {\tt FALSE}, then a constant variance
         equal to the square of the value given for parameter SIGMA is
         used for all data samples.  The run-time default is {\tt TRUE} if the
         input NDF has a VARIANCE component, and {\tt FALSE} otherwise. {\tt []}
      }
      \sstsubsection{
         WLIM = \_REAL (Read)
      }{
         If the input array contains bad pixels, then this parameter
         may be used to determine the number of good data values which
         must contribute to an output pixel before a valid value is
         stored in the restored array. It can be used, for example, to
         prevent output pixels from being generated in regions where
         there are relatively few good data values to contribute to the
         restored result. It can also be used to `fill in' small areas
         ({\it i.e.}\ smaller than the PSF) of bad pixels.

         The numerical value given for WLIM specifies the minimum total
         weight associated with the good pixels in a smoothing box
         required to generate a good output pixel (weights for each
         pixel are defined by the normalised PSF).  If this specified
         minimum weight is not present, then a bad output pixel will
         result, otherwise a smoothed output value will be calculated.
         The value of this parameter should lie between 0.0 and
         1.0.  WLIM=0 causes a good output value to be created even if
         there is only one good input value, whereas WLIM=1 causes a
         good output value to be created only if all input values are
         good. Values less than 0.5 will tend to reduce the number of
         bad pixels, whereas values larger than 0.5 will tend to
         increase the number of bad pixels.

         This threshold is applied each time a smoothing operation is
         performed. Many smoothing operations are typically performed
         in a run of LUCY, and if WLIM is larger than 0.5 the effects
         of bad pixels will propagate further through the array at each
         iteration.  After several iterations this could result in there
         being no good data left. An error is reported if this
         happens. {\tt [0.001]}
      }
      \sstsubsection{
         XCENTRE = \_INTEGER (Read)
      }{
         The $x$ pixel index of the centre of the PSF within the supplied
         PSF array.  The run-time default is the middle pixel (rounded
         down if there are an even number of pixels per line). {\tt []}
      }
      \sstsubsection{
         YCENTRE = \_INTEGER (Read)
      }{
         The $y$ pixel index of the centre of the PSF within the supplied
         PSF array. The run-time default is the middle line (rounded
         down if there are an even number of lines). {\tt []}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         lucy m51 star m51\_hires
      }{
         This example deconvolves the array in the NDF called m51,
         putting the resulting array in the NDF called m51\_hires.  The
         PSF is defined by the array in NDF star (the centre of the
         PSF is assumed to be at the central pixel).  The deconvolution
         terminates when a normalised $\chi^{2}$ value of 1.0 is
         reached.
      }
      \sstexamplesubsection{
         lucy m51 star m51\_hires 0.5 niter=60
      }{
         This example performs the same function as the previous
         example, except that the deconvolution terminates when a
         normalised $\chi^{2}$ value of 0.5 is reached, giving higher
         apparent resolution at the expense of extra spurious
         noise-based structure.  The maximum number of iterations is
         increased to 60 to give the algorithm greater opportunity to
         reach the reduced $\chi^{2}$ value.
      }
      \sstexamplesubsection{
         lucy m51 star m51\_hires2 0.1 start=m51\_hires
      }{
         This example continues the deconvolution started by the
         previous example in order to achieve a normalised $\chi^{2}$
         of 0.1.  The output array from the previous example is used to
         initialise the restored array.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The convolutions required by the R-L algorithm are performed by
         the multiplication of Fourier transforms.  The supplied input
         array is extended by a margin along each edge to avoid problems
         of wrap-around between opposite edges of the array.  The width of
         this margin is about equal to the width of the significant part
         of the PSF (as determined by parameter THRESH). The application
         displays the width of these margins.  The margins are filled by
         replicating the edge pixels from the supplied input NDFs.

         \sstitem
         The R-L algorithm works best for arrays which have zero
         background.  Non-zero backgrounds cause dark rings to appear
         around bright, compact sources.  To avoid this a background array
         should be created before running LUCY and assigned to the
         parameter BACK.  The SEGMENT and SURFIT applications within
         {\footnotesize KAPPA} can be used to create such a background array.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FOURIER, MEM2D, WIENER.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of the
         input NDF and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using single-precision floating point.
      }
   }
}
\sstroutine{
   LUTABLE
}{
   Manipulates an image-display colour table
}{
   \sstdescription{
      This application allows manipulation of the colour table of an
      image-display device provided some data are, according to the
      graphics database, already displayed upon the device.  A
      2-dimensional data array, stored in the input NDF structure, may
      be nominated to assist in defining the colour table via an histogram
      equalisation.  There are two stages to the running of this
      application.
      \begin{enumerate}
      \item The way in which the lookup table (LUT) is to distributed
      amongst the pens (colour indices) of the colour table is
      required.  Some pens are reserved by {\footnotesize KAPPA} as a palette, particularly
      for annotation.  This application only modifies the unreserved
      portion of the colour table.

      \item The lookup table is now chosen from a programmed selection or
      read from an NDF.
      \end{enumerate}

      The two stages may be repeated cyclically if desired.  To exit the
      loop give the null response, {\tt !}, to a prompt.  Looping will not
      occur if the lookup table and the distribution method are supplied
      on the command line.
   }
   \sstusage{
      lutable mapping coltab lut [device] ndf percentiles shade
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display to be used.  The device must be in
         one of the following GNS categories: IMAGE\_DISPLAY,
         IMAGE\_OVERLAY, MATRIX\_PRINTER, or WINDOW, and have at least 24
         greyscale intensities or colour indices when parameter FULL is
         {\tt FALSE}, and at least 8 when FULL is {\tt TRUE}.  It must
         also not reset when the device is opened (since the new colour
         table would be lost).  {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         COLTAB = LITERAL (Read)
      }{
         The lookup table required.  The options are listed below.
         \begin{description}
            \item {\tt "Negative"} --- This is negative grey scale with black assigned
                         to the highest pen, and white assigned to the
                         lowest available pen.
            \item {\tt "Colour"} --- This consists of eighteen standard colour
                         blocks.
            \item {\tt "Grey"} --- This a standard grey scale.
            \item {\tt "External"} --- Obtain a lookup table stored in an NDF's data
                         array.  If the table cannot be found in the
                         specified NDF or if it is not a LUT then a
                         grey scale is used.
         \end{description}
      }
      \sstsubsection{
         FULL = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the whole colour-table for the device is stored
         including the reserved pens.  This is necessary to save a
         colour table written by another package that does not reserve
         colour indices.  For colour tables produced by
         {\footnotesize KAPPA} this should be {\tt FALSE}. {\tt [FALSE]}
      }
      \sstsubsection{
         LUT = NDF (Read)
      }{
         Name of the NDF containing the lookup table as its data
         array.  The LUT must be 2-dimensional, the first dimension
         being 3, and the second being arbitrary.  The method used to
         compress or expand the colour table if the second dimension is
         different from the number of unreserved colour indices is
         controlled by parameter NN.  Also the LUT's values must lie in
         the range 0.0--1.0.
      }
      \sstsubsection{
         MAPPING = LITERAL (Read)
      }{
         The way in which the colours are to be distributed among
         the pens.  If NINTS is the number of unreserved colour indices
         the mapping options are described below.
         \begin{description}
            \item {\tt "Histogram"} --- The colours are fitted to the pens using
                           histogram equalisation of an NDF, given by
                           parameter IN, so that the colours
                           approximately have an even distribution. In
                           other words each pen is used approximately
                           an equal number of times to display the
                           2-dimensional NDF array.  There must be an existing
                           image displayed.  This is determined by
                           looking for a DATA picture in the database.
                           This is not foolproof as this may be a line
                           plot rather an image.
            \item {\tt "Linear"} --- The colours are fitted directly to the pens.
            \item {\tt "Logarithmic"} --- The colours are fitted
                           logarithmically to the pens, with colour 1
                           given to the first available pen and colour
                           NINTS given to the last pen.
         \end{description}
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         The input NDF structure containing the 2-dimensional data array to be
         used for the histogram-equalisation mapping of the pens.  The
         the data object referenced by the last DATA picture in the
         graphics database is reported.  This assumes that the
         displayed data picture was derived from the nominated NDF data
         array.
      }
      \sstsubsection{
         NN = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the input lookup table is mapped to the colour table by
         using the nearest-neighbour method.  This preserves sharp
         edges and is better for lookup tables with blocks of colour.
         If NN is {\tt FALSE} linear interpolation is used, and this is
         suitable for smoothly varying colour tables. {\tt [FALSE]}
      }
      \sstsubsection{
         PERCENTILES( 2 ) = \_REAL (Read)
      }{
         The percentiles that define the range of the histogram to be
         equalised.  For example, {\tt [25,75]} would scale between the
         quartile values. It is advisable not to choose the limits
         less than 3 per cent and greater than 97.  The percentiles are
         only required for histogram mapping.  All values in the NDF's
         data array less than the value corresponding to the lower
         percentile will have the colour of the first unreserved pen.
         All values greater than the value corresponding to the upper
         percentile will have the colour of the last unreserved pen.
      }
      \sstsubsection{
         SHADE = \_REAL (Read)
      }{
         The type of shading.  This only required for the histogram
         mapping.  A value of {\tt $-$1} emphasises low values;
         {\tt $+$1} emphasises
         high values; {\tt 0} is neutral, all values have equal weight.  The
         shade must lie in the range $-$1 to $+$1.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         lutable lo co
      }{
         Changes the colour table on the current image-display device
         to a series of coloured blocks whose size increase
         logarithmically with the table index number.
      }
      \sstexamplesubsection{
         lutable li ex rococo
      }{
         This maps the lookup table stored in the NDF called rococo
         linearly to the colour table on the current image-display
         device.
      }
      \sstexamplesubsection{
         lutable li ex rococo full
      }{
         This maps the lookup table stored in the NDF called rococo
         linearly to the full colour table on the current image-display
         device, {\it i.e.}\ ignoring the reserved pens.
      }
      \sstexamplesubsection{
         lutable hi gr in=nebula shade=0 percentiles=[5,90]
      }{
         This maps the greyscale lookup table via histogram
         equalisation between the 5 and 90 percentiles of an NDF called
         nebula to the colour table on the current image-display
         device.  There is no bias or shading to white or black.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CRELUT, LUTFLIP, LUTHILITE, LUTREAD, LUTROT, LUTSAVE,
      LUTTWEAK, LUTVIEW; Figaro: COLOUR.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported for the image NDF

         \sstitem
         All non-complex numeric data types can be handled.  Processing
         is performed using single- or double-precision floating point,
         as appropriate.
      }
   }
}
\sstroutine{
   LUTBGYRW
}{
   Loads the {\it BGYRW} lookup table
}{
   \sstdescription{
      This procedure loads the {\it BGYRW\/} lookup table with linear scaling
      into the current image-display device.  It is a continuous LUT
      starting with blue, followed by green, yellow, red and a splash of
      white.
   }
   \sstusage{
      lutbgyrw
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display whose colour table is to be changed.
         {\tt [}Current image-display device{\tt ]}
      }
   }
   \sstnotes{
      This is a procedure that calls LUTABLE.  Therefore, the
      parameter cannot be specified on the command line.  You will 
      only be prompted for the DEVICE parameter if the current
      image display is not suitable or not available.
   }
}
\sstroutine{
   LUTCOL
}{
   Loads the standard colour lookup table
}{
   \sstdescription{
      Procedure for loading the standard colour lookup table into
      the current image-display device with linear scaling.
   }
   \sstusage{
      lutcol
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display whose colour table is to be changed.
         {\tt [}Current image-display device{\tt ]}
      }
   }
   \sstnotes{
      This is a procedure that calls LUTABLE.  Therefore, the parameter
      cannot be specified on the command line.  You will only be
      prompted for the DEVICE parameter if the current image display
      is not suitable or not available.
   }
}
\sstroutine{
   LUTCONT
}{
   Loads a lookup table to give the display the appearance of a
   contour plot
}{
   \sstdescription{
      This procedure loads a lookup table that gives a contour-plot
      appearance into the current image-display device.  The lookup table
      is mainly black with a set of white stripes and it is loaded with
      linear scaling.
   }
   \sstusage{
      lutcont
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display whose colour table is to be changed.
         {\tt [}Current image-display device{\tt ]}
      }
   }
   \sstnotes{
      This is a procedure that calls LUTABLE.  Therefore, the parameter
      cannot be specified on the command line.  You will only be
      prompted for the DEVICE parameter if the current image display
      is not suitable or not available.
   }
}
\sstroutine{
   LUTFC
}{
   Loads the standard false-colour lookup table
}{
   \sstdescription{
      This procedure loads the standard false-colour lookup table with
      linear scaling into the current image-display device.
   }
   \sstusage{
      lutfc
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display whose colour table is to be changed.
         {\tt [}Current image-display device{\tt ]}
      }
   }
   \sstnotes{
      This is a procedure that calls LUTABLE.  Therefore, the parameter
      cannot be specified on the command line.  You will only be
      prompted for the DEVICE parameter if the current image display
      is not suitable or not available.
   }
}
 
\sstroutine{
   LUTFLIP
}{
   Flips the colour table of an image-display device
}{
   \sstdescription{
      This routine `flips' the colour table of a nominated plane of
      an IDI-supported image display, such as X-windows.  The flip
      reverses the order of the colours, so that the first colour
      becomes the last and vice versa, {\it etc.}
   }
   \sstusage{
      lutflip [device] [plane]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the image-display device whose colour table is to
         be flipped. The name of the base plane should be given even if
         the overlay colour table is to be flipped.
         {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         PLANE = \_INTEGER (Read)
      }{
         The number of the memory plane whose colour table is to be
         flipped.  If it is null the base (image) memory's colour table
         is reversed. The base memory is 0 and overlays are numbered
         consecutively from 1.  For an Ikon the only overlay plane is 1.
         PLANE is only permitted to have a value in the range 0 to the
         number of planes minus one. {\tt [0]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         lutflip
      }{
         This reverses the colour table on the current image-display
         device.
      }
      \sstexamplesubsection{
         lutflip xwindows
      }{
         This reverses the colour table on the xwindows device.
      }
      \sstexamplesubsection{
         lutflip ikon 1
      }{
         This flips the colour table on the Ikon overlay plane.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Only Ikons and X-windows are supported.

         \sstitem
         Reserved pens are not flipped.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CRELUT, LUTHILITE, LUTROT, LUTTWEAK.
   }
}
\sstroutine{
   LUTGREY
}{
   Loads the standard greyscale lookup table
}{
   \sstdescription{
      Procedure for loading the standard greyscale lookup table into
      the current image-display device with linear scaling.
   }
   \sstusage{
      lutgrey
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display whose colour table is to be changed.
         {\tt [}Current image-display device{\tt ]}
      }
   }
   \sstnotes{
      This is a procedure that calls LUTABLE.  Therefore, the parameter
      cannot be specified on the command line.  You will only be
      prompted for the DEVICE parameter if the current image display
      is not suitable or not available.
   }
}
\sstroutine{
   LUTHEAT
}{
   Loads the {\it heat} lookup table
}{
   \sstdescription{
      This procedure loads the {\it heat} lookup table with linear scaling
      into the current image-display device.
   }
   \sstusage{
      lutheat
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display whose colour table is to be changed.
         {\tt [}Current image-display device{\tt ]}
      }
   }
   \sstnotes{
      This is a procedure that calls LUTABLE.  Therefore, the parameter
      cannot be specified on the command line.  You will only be
      prompted for the DEVICE parameter if the current image display
      is not suitable or not available.
   }

}
\sstroutine{
   LUTHILITE
}{
   Highlights a colour table of an image-display device
}{
   \sstdescription{
      This routine adjusts the colour table of a nominated plane of
      an IDI-supported image display, such as X-windows.  The adjustment
      is like a highlight pen, only here it can traverse the colour
      table, widen or thin is under mouse, joystick or trackerball
      button control; and the colour of the highlight is arbitrary.
      Thus particular features in an image may readily become visible.

      For an Ikon or X-windows, moving the mouse left or right shifts
      the highlight in the colour table towards lower and higher colour
      indices respectively.  The highlight does not rotate around the
      colour table.  Pressing the left button of the mouse reduces the
      width of the highlight by one colour index.  Pressing the centre
      button increases the width of the highlight by one colour index.
      Hitting the right-hand button ends the modification of the colour
      table.

      The colour table may be viewed during its manipulation without
      permanently altering the display memory.  The colour-table
      display is situated via the cursor, and will disappear once the
      highlighting is complete.
   }
   \sstusage{
      luthilite colour [device] [plane] [view]
   }
   \sstparameters{
      \sstsubsection{
         COLOUR() = LITERAL (Read)
      }{
         The colour to be used as a highlight.  It is either of these
         alternatives.
         \begin{itemize}
           \item  A named colour from the standard colour set, which may
           be abbreviated.  If the abbreviated name is ambiguous the
           first match is selected.  The case of the name is ignored.
           Some examples are {\tt "Seagreen"}, {\tt "Violet"}, and
           {\tt "Orchid"}.

           \item Normalised red, green, and blue intensities separated by
           commas or spaces.  Each value must lie in the range 0.0--1.0.
           For example, {\tt 1.0,1.0,0.5} would give a pale yellow.
         \end{itemize}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the image-display device whose lookup table is to
         be adjusted.  The name of the base plane should be given even
         if the overlay lookup table is to be adjusted.
         {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         FULL = \_LOGICAL (Read)
      }{
         If FULL is {\tt TRUE}, the whole colour-table for the device is
         may be highlighted, including the reserved pens.  When FULL
         is {\tt FALSE}, the reserved pens in the palette are unaltered.
         {\tt [FALSE]}
      }
      \sstsubsection{
         PLANE = \_INTEGER (Read)
      }{
         The number of the memory plane whose lookup table is to be
         manipulated. If it is null the base (image) memory's lookup
         table is adjusted.  The base memory is 0 and overlays are
         numbered consecutively from 1.  For an Ikon the only overlay
         plane is 1.  PLANE is only permitted to have a value in the
         range 0 to the number of planes minus one. {\tt [0]}
      }
      \sstsubsection{
         VIEW = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the colour table is displayed during its manipulation.
         {\tt [FALSE]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         luthilite red
      }{
         Highlights the colour table on the current image-display
         device with a red marker.
      }
      \sstexamplesubsection{
         luthilite red full
      }{
         Highlights the colour table and palette on the current
         image-display device with a red marker.
      }
      \sstexamplesubsection{
         luthilite skyblue xwindows
      }{
         Highlights the colour table on the xwindows device with a
         sky-blue marker.
      }
      \sstexamplesubsection{
         luthilite [1.0,1.0,0.3] ikon 1
      }{
         Highlights the colour table on the Ikon overlay plane in a
         pale yellow.
      }
      \sstexamplesubsection{
         luthilite red view
      }{
         Highlights the colour table on the current image-display
         device with a red marker.  The colour table is displayed
         during the highlighting.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Only Ikons and X-windows are supported.

         \sstitem
         Initially, the highlight has a width of two colour indices,
         and it is located at the second lowest colour index.  The maximum
         width of the highlight is the larger of six and a quarter of the
         colour table, but may be narrower when there are less than 12
         colour indices.  Should the highlight prove to be unsuitable, it
         may be made invisible by reducing the width to zero.

         \sstitem
         The rate of motion of the highlight is a function of the
         speed of cursor movement in addition to the cursor position.
         For a given cursor displacement slow motion moves the highlight
         more slowly, and faster motion moves it more rapidly.  This
         permits both fine control and swift change in the highlight's
         location.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CRELUT, LUTFLIP, LUTROT, LUTTWEAK.
   }
}
\sstroutine{
   LUTIKON
}{
   Loads the default {\it Ikon} lookup table
}{
   \sstdescription{
      This procedure loads the default {\it Ikon\/} lookup table with linear
      scaling into the current image-display device.
   }
   \sstusage{
      lutikon
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display whose colour table is to be changed.
         {\tt [}Current image-display device{\tt ]}
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         This is a procedure that calls LUTABLE.  Therefore, the parameter
         cannot be specified on the command line.  You will only be
         prompted for the DEVICE parameter if the current image display
         is not suitable or not available.

         \sstitem
         The device need not be an Ikon.
      }
   }
}
\sstroutine{
   LUTNEG
}{
   Loads the standard negative greyscale lookup table
}{
   \sstdescription{
      Procedure for loading the standard greyscale lookup table into
      the current image-display device with negative linear scaling.
   }
   \sstusage{
      lutneg
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display whose colour table is to be changed.
         {\tt [}Current image-display device{\tt ]}
      }
   }
   \sstnotes{
      This is a procedure that calls LUTABLE.  Therefore, the parameter
      cannot be specified on the command line.  You will only be
      prompted for the DEVICE parameter if the current image display
      is not suitable or not available.
   }
}
\sstroutine{
   LUTRAMPS
}{
   Loads the coloured-ramps lookup table
}{
   \sstdescription{
      This procedure loads the coloured-ramps lookup table with linear
      scaling into the current image-display device.
   }
   \sstusage{
      lutramps
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display whose colour table is to be changed.
         {\tt [}Current image-display device{\tt ]}
      }
   }
   \sstnotes{
      This is a procedure that calls LUTABLE.  Therefore, the parameter
      cannot be specified on the command line.  You will only be
      prompted for the DEVICE parameter if the current image display
      is not suitable or not available.
   }
}
\sstroutine{
   LUTREAD
}{
   Loads an image-display lookup table from an NDF
}{
   \sstdescription{
      This application reads a lookup table stored in an NDF with
      the standard format, and loads it into the current image-display
      device.
   }
   \sstusage{
      lutread lut
   }
   \sstarguments{
      \sstsubsection{
        LUT = LITERAL (Given)
      }{
           The file containing the lookup table.  It is passed to the
           parameter LUT but not validated.
      }
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display whose colour table is to be changed.
         [Current image-display device]
      }
      \sstsubsection{
         LUT = NDF (Read)
      }{
         Name of the NDF containing the lookup table as its data
         array.  The LUT must be 2-dimensional, the first dimension
         being 3, and the second being arbitrary.  Linear interpolation
         is used to compress or expand the colour table if the second
         dimension is different from the number of unreserved colour
         indices.  Also the LUT's values must lie in the range 0.0--1.0.
      }
   }
   \sstnotes{
      This is a procedure that calls LUTABLE.  Therefore, the parameters
      cannot be specified on the command line.  You will only be
      prompted for the parameters if the device is not suitable or not
      available, and/or the lookup table file could not be accessed.
   }
}
\sstroutine{
   LUTROT
}{
   Rotates the colour table of an image-display device
}{
   \sstdescription{
      This routine rotates the colour table of a nominated plane of
      an IDI-supported image display, such as X-windows. The rotation
      is under mouse, joystick or trackerball button control.

      For an Ikon or X-windows, moving the mouse left or right
      rotates the colour table towards lower and higher pen numbers
      respectively.  Pressing the left button of the mouse resets the
      colour table to its input state, and hitting the right-hand
      button ends the rotation.
   }
   \sstusage{
      lutrot [device] [plane]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the image-display device whose colour table is to
         be rotated.  The name of the base plane should be given even if
         the overlay colour table is to be rotated.
         {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         FULL = \_LOGICAL (Read)
      }{
         If FULL is {\tt TRUE}, the whole colour-table for the device is
         rotated, including the reserved pens.  When FULL is {\tt FALSE},
         the reserved pens in the palette are unaltered. 
         {\tt [FALSE]}
      }
      \sstsubsection{
         PLANE = \_INTEGER (Read)
      }{
         The number of the memory plane whose colour table is to be
         rotated. If it is null the base (image) memory's colour table
         is rotated. The base memory is 0 and overlays are numbered
         consecutively from 1.  For an Ikon the only overlay plane is 1.
         PLANE is only permitted to have a value in the range 0 to the
         number of planes minus one. {\tt [0]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         lutrot
      }{
         This enables rotation of the colour table on the current
         image-display device.
      }
      \sstexamplesubsection{
         lutrot xwindows
      }{
         This enables rotation of the colour table on the xwindows
         device.
      }
      \sstexamplesubsection{
         lutrot full
      }{
         This enables rotation of the colour table and palette on the
         current image-display device.
      }
      \sstexamplesubsection{
         lutrot ikon 1
      }{
         This enables rotation of the colour table on the Ikon overlay
         plane.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Only Ikons and X-windows are supported.

         \sstitem
         The rate of motion of the colour table is a function of the
         speed of cursor movement in addition to the cursor position.
         For a given cursor displacement slow motion rotates the colour
         table more slowly, and faster motion moves it more rapidly.  This
         permits both fine control and swift rotation.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CRELUT, LUTFLIP, LUTHILITE, LUTTWEAK.
   }
}

 
\sstroutine{
   LUTSAVE
}{
   Saves the current colour table of an image-display device in an NDF
}{
   \sstdescription{
      This routine reads the colour table of a nominated plane of
      an IDI-supported image display, such as X-windows, and then copies
      it to an NDF LUT file.
   }
   \sstusage{
      lutsave lut [device] [plane]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the image-display device whose colour table is to
         be saved.  The name of the base plane should be given even if
         the overlay colour table is to be saved.
         {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         FULL = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the whole colour-table for the device is stored
         including the reserved pens.  This is necessary to save a
         colour table written by another package that does not reserve
         colour indices.  For colour tables produced by KAPPA this
         should be {\tt FALSE}. {\tt [FALSE]}
      }
      \sstsubsection{
         LUT = NDF (Write)
      }{
         The output NDF into which the colour table is to be stored.
         Its second dimension equals the number of colour-table
         entries that are stored.  This will be less than the
         total number of colour indices on the device if FULL is {\tt FALSE}.
      }
      \sstsubsection{
         PLANE = \_INTEGER (Read)
      }{
         The number of the memory plane whose colour table is to be
         saved.  If it is null the base (image) memory's colour table
         is reversed.  The base memory is 0 and overlays are numbered
         consecutively from 1.  For an Ikon the only overlay plane is 1.
         PLANE is only permitted to have a value in the range 0 to the
         number of planes minus one. {\tt [0]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         The title for the output NDF. {\tt ["KAPPA - Lutsave"]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         lutsave pizza
      }{
         This saves the current colour table on the current
         image-display device to an NDF called pizza.
      }
      \sstexamplesubsection{
         lutsave ramps ikon 1
      }{
         This saves the current colour table on the Ikon overlay plane
         an NDF called ramps.
      }
      \sstexamplesubsection{
         lutsave redshift full
      }{
         This saves in full the current colour table on the current
         image-display device to an NDF called redshift.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Only Ikons and X-windows are supported.

         \sstitem
         Only the non-reserved portion of the colour table is saved.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CRELUT, LUTFLIP, LUTHILITE, LUTREAD, LUTROT, LUTTWEAK.
   }
}

\sstroutine{
   LUTSPEC
}{
   Loads a spectrum-like lookup table
}{
   \sstdescription{
      This procedure loads an optical-spectrum-like lookup table with linear
      scaling into the current image-display device.
   }
   \sstusage{
      lutspec
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display whose colour table is to be changed.
         {\tt [}Current image-display device{\tt ]}
      }
   }
   \sstnotes{
      This is a procedure that calls LUTABLE.  Therefore, the parameter
      cannot be specified on the command line.  You will only be
      prompted for the DEVICE parameter if the current image display
      is not suitable or not available.
   }
}
 
\sstroutine{
   LUTTWEAK
}{
   Tweaks a colour table of an image-display device
}{
   \sstdescription{
      This routine adjusts the colour table of a nominated plane of
      an IDI-supported image display, such as X-windows.  The adjustment
      is under mouse, joystick or trackerball button control.

      For an Ikon or X-windows, moving the mouse left or right shifts
      the colour table towards lower and higher colour indices
      respectively.  Moving the mouse up stretches the lookup table,
      and moving it down squashes the lookup table until it disappears,
      then the lookup table is flipped.  If the lookup table is
      reversed, moving down stretches, and moving up squashes.
      Pressing the left button of the mouse resets the colour table to
      its input state.  Pressing the centre button alters the way in
      which a squashed lookup table is padded.  The two states are
      white or to use the first and last colours of the input lookup
      table, the sense depending on whether the lookup table is
      flipped.  Hitting the right-hand button ends the modification of
      the colour table.

      The colour table may be viewed during its manipulation without
      permanently altering the display memory.  The colour-table
      display is situated via the cursor, and will disappear once the
      tweaking is complete.
   }
   \sstusage{
      luttweak [device] [plane] [view]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the image-display device whose lookup table is to
         be adjusted.  The name of the base plane should be given even
         if the overlay lookup table is to be adjusted.
         {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         FULL = \_LOGICAL (Read)
      }{
         If FULL is {\tt TRUE}, the whole colour-table for the device is
         may be modified, including the reserved pens.  When FULL
         is {\tt FALSE}, the reserved pens in the palette are unaltered.
         {\tt [FALSE]}
      }
      \sstsubsection{
         PLANE = \_INTEGER (Read)
      }{
         The number of the memory plane whose lookup table is to be
         manipulated.  If it is null the base (image) memory's lookup
         table is adjusted.  The base memory is 0 and overlays are
         numbered consecutively from 1.  For an Ikon the only overlay
         plane is 1.  PLANE is only permitted to have a value in the
         range 0 to the number of planes minus one. {\tt [0]}
      }
      \sstsubsection{
         VIEW = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the colour table is displayed during its manipulation.
         {\tt [FALSE]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         luttweak
      }{
         Tweaks the colour table on the current image-display device.
      }
      \sstexamplesubsection{
         luttweak xwindows
      }{
         Tweaks the colour table on the xwindows device.
      }
      \sstexamplesubsection{
         luttweak xwindows full
      }{
         Tweaks the colour table and palette on the xwindows device.
      }
      \sstexamplesubsection{
         luttweak ikon 1
      }{
         Tweaks the colour table on the Ikon overlay plane.
      }
      \sstexamplesubsection{
         luttweak view
      }{
         Tweaks the colour table on the current image-display device.
         The colour table is displayed during the tweaking.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Only Ikons and X-windows are supported.

         \sstitem
         The speed of the colour-table rotation is not linearly
         proportional to the mouse displacement; the speed of displacement
         tunes the effect so that slow motion makes a small change than
         a faster motion.  The squashing and stretching factors are also
         non-linear.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CRELUT, LUTFLIP, LUTHILITE, LUTROT.
   }
}
\sstroutine{
   LUTVIEW
}{
   Draws a colour-table key
}{
   \sstdescription{
      This application draws a key of the colour table at a location
      you select.  You can constrain the key to lie within either the
      current or the BASE picture.  The key may be annotated, in which
      case you must allow sufficient room for the annotations.  For
      oblate regions colour index increases from left to right, and for
      prolate it increases from bottom to top; the annotations
      appearing to the top and right respectively.  The annotations
      scale linearly between the values corresponding to the lower and
      upper indices of the colour table.

      The situation of the key is defined by the co-ordinates of a pair 
      of opposite corners of a rectangular region.  You may specify
      these using one of the following methods:
      \begin{enumerate}
      \item moving a cursor and pressing a button on the mouse or
            trackerball;
      \item obtaining bounds from the environment (in normalised
            co-ordinates of the reference picture).
      \end{enumerate}
   }
   \sstusage{
      lutview [mode] [low] [high] [annota] [curpic] [device] lbound=?
        ubound=?
   }
   \sstparameters{
      \sstsubsection{
         ANNOTA = \_LOGICAL (Read)
      }{
         ANNOTA is {\tt TRUE} if the colour table is to be annotated with the pen
         numbers.  Note a squarer picture should be created so that the
         annotations are legible. {\tt [FALSE]}
      }
      \sstsubsection{
         CURPIC = \_LOGICAL (Read)
      }{
         If CURPIC is {\tt TRUE}, the colour table key is to lie within the current
         picture, otherwise the new picture can lie anywhere within
         the BASE picture.  It is ignored when the current-picture mode
         is selected. {\tt [FALSE]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The image-display device on which the colour table is to be
         drawn.   The device must be in one of the following GNS
         categories: IMAGE\_DISPLAY, IMAGE\_OVERLAY, MATRIX\_PRINTER, or
         WINDOW, and have at least 24 greyscale intensities or colour
         indices.  It must also not reset when the device is opened
         (since the colour table would be lost).
         {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         HIGH = \_REAL (Read)
      }{
         The value corresponding to the maximum colour index.  It is
         used to calculate the annotation scale for the key.  If it
         is null ({\tt !}) the maximum colour index is used.
         {\tt [}Current display linear-scaling maximum{\tt ]}
      }
      \sstsubsection{
         LBOUND( 2 ) = \_REAL (Read)
      }{
         Co-ordinates of the lower bound that defines the location of
         the colour-table plot. These are in the world system of the
         base or current picture. (XY mode)
      }
      \sstsubsection{
         LOW = \_REAL (Read)
      }{
         The value corresponding to the minimum colour index.  It is
         used to calculate the annotation scale for the key.  If it
         is null ({\tt !}) the minimum colour index is used.
         {\tt [}Current display linear-scaling minimum{\tt ]}
      }
      \sstsubsection{
         LUT = NDF (Read)
      }{
         Name of the NDF containing a lookup table as its data array;
         the lookup table is written to the image-display's colour
         table.  The purpose of this parameter is to provide a means of
         controlling the appearance of the image on certain devices,
         such as colour printers, that do not have a dynamic colour
         table, {\it i.e.}\ the colour table is reset when the device is
         opened.  If used with dynamic devices, such as windows or
         Ikons, the new colour table remains after this application has
         completed.  A null, {\tt !}, means that the existing colour table
         will be used.

         The LUT must be two-dimensional, the first dimension
         being 3, and the second being arbitrary.  The method used to
         compress or expand the colour table if the second dimension is
         different from the number of unreserved colour indices is
         controlled by parameter NN.  Also the LUT's values must lie in
         the range 0.0--1.0. {\tt [!]}
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         Method for defining the position, size and shape of the
         colour-table key.  The options are {\tt "Cursor"} for cursor mode
         (provided the graphics device has one), {\tt "XY"} to select $x$-$y$
         limits via the parameter system, and {\tt "Picture"} where the
         whole of the current picture is used.  Additional positioning
         options are available by using other {\footnotesize KAPPA}
         applications to create new pictures and then specifying the
         picture mode.  {\tt ["Cursor"]}
      }
      \sstsubsection{
         NN = \_LOGICAL (Read)
      }{
         If NN is {\tt TRUE}, the input lookup table is mapped to the colour
         table by using the nearest-neighbour method.  This preserves sharp
         edges and is better for lookup tables with blocks of colour.
         If NN is {\tt FALSE}, linear interpolation is used, and this is
         suitable for smoothly varying colour tables.  NN is ignored
         unless LUT is not null. {\tt [FALSE]}
      }
      \sstsubsection{
         OUTLINE = \_LOGICAL (Read)
      }{
         If OUTLINE is {\tt TRUE}, a box that delimits the key is drawn.  A box
         is always drawn when there are annotations. {\tt [TRUE]}
      }
      \sstsubsection{
         UBOUND( 2 ) = \_REAL (Read)
      }{
         Co-ordinates of the upper bound that defines the location
         of the colour-table plot.  These are in the world system of the
         BASE or current picture. (XY mode)
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         lutview annota
      }{
         Draws an annotated colour table at a position selected via
         the cursor on the current image-display device.
      }
      \sstexamplesubsection{
         lutview p
      }{
         Draws a colour table that fills the current picture on the
         current image-display device.
      }
      \sstexamplesubsection{
         lutview curpic
      }{
         Draws a colour table within the current picture positioned
         via the cursor.
      }
      \sstexamplesubsection{
         lutview mode=xy outline device=ikon $\backslash$
      }{
         Draws a outlined colour table within the BASE picture
         on the Ikon, defined by $x$-$y$ extents.
      }
      \sstexamplesubsection{
         lutview xy lut=my\_lut device=lj250\_p lbound=[0.92,0.2] ubound=[0.98,0.8]
      }{
         Draws the colour table in the NDF called my\_lut with an
         outline within the BASE picture on the device lj250\_p, defined
         by the $x$-$y$ bounds (0.92,0.2) and (0.98,0.8).  In other words
         the plot is to the right-hand side with increasing colour
         index with increasing y position.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         When annotations are selected their precise height and the
         width of the colour table, depend on the largest number of
         significant figures in an annotation.  The colour table will have
         an aspect ratio in the range 0.17--0.45, and the text height is
         adjusted to fit the characters within the available room.  The
         default aspect ratio is 0.275.

         \sstitem
         The text has a maximum height as a fraction of width (if
         oblate) or height of the initial picture (BASE or current) so
         that ridiculously large characters are drawn for big keys.
         However, this can result in characters which are too small if for
         example a highly oblate colour table is plotted within a strongly
         prolate current picture.

         \sstitem
         A FRAME picture (when there are annotations) and the
         colour-table picture are stored in the graphics database. These
         have names FRAME and KEY respectively.  On completion the
         current picture is unchanged.

         \sstitem
         Parameters LOW and HIGH are single precision because they are
         also required to define world co-ordinates of the graphics.  Thus
         this application is not suitable for double-precision data that
         have been scaled over a range near the precision of real values.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: DISPLAY, LUTABLE; Figaro: COLOUR.
   }
}
\sstroutine{
   LUTZEBRA
}{
   Loads a pseudo-contour lookup table
}{
   \sstdescription{
      This procedure loads a pseudo-contour lookup table with linear
      scaling into the current image-display device. The lookup table
      is mainly black with a set of white stripes.
   }
   \sstusage{
      lutzebra
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display whose colour table is to be changed.
         {\tt [}Current image-display device{\tt ]}
      }
   }
   \sstnotes{
      This is a procedure that calls LUTABLE.  Therefore, the parameter
      cannot be specified on the command line.  You will only be
      prompted for the DEVICE parameter if the current image display
      is not suitable or not available.
   }
}
\sstroutine{
   MAKESURFACE
}{
   Creates a 2-dimensional NDF from the coefficients of a polynomial
   surface
}{
   \sstdescription{
      The coefficients describing a 2-dimensional polynomial surface
      are read from a SURFACEFIT extension in an NDF (written by
      FITSURFACE), and are used to create a 2-dimensional surface of
      specified size and extent.  The surface is written to a new NDF.

      The size and extent of the surface may be obtained from a template
      NDF or given explicitly.

      Elements in the new NDF outside the defined range of the
      polynomial will be set to bad values.
   }
   \sstusage{
      makesurface in out [like] type=? lbound=? ubound=? xlimit=? ylimit=?
   }
   \sstparameters{
      \sstsubsection{
         IN  = NDF (Read)
      }{
         The NDF containing the SURFACEFIT extension.
      }
      \sstsubsection{
         LBOUND( 2 ) = \_INTEGER (Read)
      }{
         Lower bounds of new NDF (if LIKE={\tt !}).  The suggested defaults
         are the lower bounds of the IN NDF.
      }
      \sstsubsection{
         LIKE = NDF (Read)
      }{
         An optional template NDF which, if specified, will be used to
         define the labels, size, shape, data type and axis range of
         the new NDF.  If a null response ({\tt !}) is given, the label,
         units, axis labels, and axis units are taken from the IN NDF.
         The task prompts for the data type and bounds, using those of
         the IN NDF as defaults, and the axis ranges. {\tt [!]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The new NDF to contain the surface fit.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         A title for the new NDF.  If a null response ({\tt !}) is given,
         the title will be propagated either from LIKE, or from IN
         if LIKE={\tt !}. {\tt [!]}
      }
      \sstsubsection{
         TYPE = LITERAL (Read)
      }{
         Data type for the new NDF (if LIKE={\tt !}).  It must be one of
         the following: {\tt "\_DOUBLE"}, {\tt "\_REAL"}, {\tt "\_INTEGER"},
         {\tt "\_WORD"}, {\tt "\_BYTE"}, {\tt "\_UBYTE"}.  The suggested default is the data type of
         the data array in the IN NDF.
      }
      \sstsubsection{
         UBOUND( 2 ) = \_INTEGER (Read)
      }{
         Upper bounds of new NDF (if LIKE={\tt !}).  The suggested defaults
         are the upper bounds of the IN NDF.
      }
      \sstsubsection{
         VARIANCE = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, a variance array is created in the output NDF
         provided the SURFACEFIT.FIT structure contains variance
         information. {\tt [FALSE]}
      }
      \sstsubsection{
         XLIMIT( 2 ) = \_DOUBLE (Read)
      }{
         Co-ordinates of the left then right edges of the $x$ axis (if
         LIKE={\tt !}).  The suggested defaults are respectively the
         minimum and maximum $x$ co-ordinates of the IN NDF.
      }
      \sstsubsection{
         YLIMIT( 2 ) = \_DOUBLE (Read)
      }{
         Co-ordinates of the bottom then top edges of the $y$ axis (if
         LIKE={\tt !}).  The suggested defaults are respectively the
         minimum and maximum $y$ co-ordinates of the IN NDF.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         makesurface flatin flatout $\backslash$
      }{
         This generates a 2-dimensional image in the NDF called flatout
         using the surface fit stored in the 2-dimensional NDF flatin.
         The created image has the same data type, bounds, and
         co-ordinate limits as the data array of flatin.
      }
      \sstexamplesubsection{
         makesurface flatin flatout type=\_wo lbound=[1,1] ubound=[320,512]
      }{
         As the previous example, except that the data array in flatout
         has data type \_WORD, and the bounds of flatout are 1:320,
         1:512.
      }
      \sstexamplesubsection{
         makesurface flatin flatout like=flatin
      }{
         This has the same effect as the first example, except it has
         an advantage.  If the current co-ordinate system is {\tt "Data"} and
         either or both of the axes are inverted (values decrease with
         increasing pixel index), the output image will be correctly
         oriented.
      }
      \sstexamplesubsection{
         makesurface flatin flatout like=template title="Surface fit"
      }{
         This generates a 2-dimensional image in the NDF called flatout
         using the surface fit stored in the 2-dimensional NDF flatin.
         The created image inherits the attributes of the NDF called
         template.  The title of flatout is {\tt "Surface fit"}.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The polynomial surface fit is stored in SURFACEFIT extension,
         component FIT of type POLYNOMIAL, variant CHEBYSHEV.  This
         extension is created by FITSURFACE.    Also read from the
         SURFACEFIT extension is the co-ordinate system (component COSYS).

         \sstitem
         When LIKE={\tt !}, COSYS={\tt "Data"} and the original NDF had an axis that
         decreased with increasing pixel index, you may want to flip the
         co-ordinate limits (via parameters XLIMIT or YLIMIT) to match
         the original sense of the axis, otherwise the created surface will
         be flipped with respect to the image from which it was fitted.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSURFACE, SURFIT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF
         data structure and propagates all extensions.  However, neither
         QUALITY nor a SURFACEFIT extension is propagated when LIKE is not
         null.

         \sstitem
         All non-complex numeric data types can be handled.  Processing
         is performed in single- or double-precision floating point, as
         appropriate.
      }
   }
}

\manroutine {{\manheadstyle{MANIC}}}{ Converts all or part of a data array from one
  dimensionality to another.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This application copies or converts all or part of a 1, 2 or
  3-dimensional data array to one or more output data arrays, each
  of 1, 2 or 3 dimensions. All data arrays are stored in {\mantt{IMAGE}}
  structures. Windows may be set in any of the dimensions of the
  input data array. All or part of the input array may be projected
  on to any of the rectangular planes or axes of the input before
  being written to an output array; or a 1- or 2-dimensional data
  array may be grown to more dimensions to fill an output data
  array. Many output data arrays, each of a different configuration
  if required, may be extracted from a single input data array with
  one call to the routine.

\manroutineitem {Invocation }{}
  MANIC

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPUT}}  }{{\mantt{IMAGE}}}
  {\mantt{IMAGE}} structure holding the input data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{ONDIM}}  }{{\mantt{\_INTEGER}}}
  Dimensionality of an output data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{XLIMITS}}(2) }{{\mantt{\_INTEGER}}}
  The {$x$}-axis window on the input data array to be used in
  forming an output data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{YLIMITS}}(2) }{{\mantt{\_INTEGER}}}
  The {$y$}-axis window on the input data array to be used in
  forming an output data array.
\end{manparametertable}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{ZLIMITS}}(2) }{{\mantt{\_INTEGER}}}
  The {$z$}-axis window on the input data array to be used in
  forming an output data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{XRANGE}}(2)  }{{\mantt{\_INTEGER}}}
  The {$x$}-axis range for summation in the input data array in
  forming an output data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{YRANGE}}(2)  }{{\mantt{\_INTEGER}}}
  The {$y$}-axis range for summation in the input data array in
  forming an output data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{ZRANGE}}(2)  }{{\mantt{\_INTEGER}}}
  The {$z$}-axis range for summation in the input data array in
  forming an output data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{EPLANE}}  }{{\mantt{\_CHAR}}}
  Plane to be extracted from the input 3-d data array.   The options are
  {\mantt 'XY'}, {\mantt 'YZ'}, {\mantt 'ZX'}, {\mantt 'YX'},
  {\mantt 'ZY'}, {\mantt 'XZ'}.
\manparameterentry {{\mantt{READ}} }{{\mantt{GPLANE}}  }{{\mantt{\_CHAR}}}
  Input 2-d data array forms this plane when being grown into
  a 3-d data array.   The options are {\mantt 'XY'}, {\mantt 'YZ'},
  {\mantt 'ZX'}, {\mantt 'YX'}, {\mantt 'ZY'}, {\mantt 'XZ'}.
\manparameterentry {{\mantt{READ}} }{{\mantt{ELINE1}}  }{{\mantt{\_CHAR}}}
  Axis of input 2-d data array to be extracted to form an output
  1-d data array.   The alternatives are {\mantt 'X'} or {\mantt 'Y']}.
\manparameterentry {{\mantt{READ}} }{{\mantt{ELINE2}}  }{{\mantt{\_CHAR}}}
  Axis of input 3-d data array to be extracted to form an output
  1-d data array.  The options are {\mantt 'X'}, {\mantt 'Y'}, {\mantt 'Z'}.
\manparameterentry {{\mantt{READ}} }{{\mantt{GLINE1}}  }{{\mantt{\_CHAR}}}
  Input 1-d data array will form this axis of an output 2-d
  data array.   The alternatives are {\mantt 'X'} or {\mantt 'Y'}.
\manparameterentry {{\mantt{READ}} }{{\mantt{GLINE2}}  }{{\mantt{\_CHAR}}}
  Input 1-d data array will form this axis of an output 3-d
  data array.  The options are {\mantt 'X'}, {\mantt 'Y'}, {\mantt 'Z'}.
\manparameterentry {{\mantt{READ}} }{{\mantt{XDIM}}  }{{\mantt{\_INTEGER}}}
  {$x$}-dimension of output 2-d or 3-d data array grown from input
  1-d or 2-d data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{YDIM}}  }{{\mantt{\_INTEGER}}}
  {$y$}-dimension of output 2-d or 3-d data array grown from input
  1-d or 2-d data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{ZDIM}}  }{{\mantt{\_INTEGER}}}
  {$z$}-dimension of output 2-d or 3-d data array grown from input
  1-d or 2-d data array.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPUT}}  }{{\mantt{IMAGE}}}
  {\mantt{IMAGE}} structure to hold an output data array.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title for {\mantt{IMAGE}} structure holding an output data array.
  \mbox{{\mantt ['KAPPA - Manic']}}
\manparameterentry {{\mantt{READ}} }{{\mantt{LOOP}}  }{{\mantt{\_LOGICAL}}}
  Extract or grow further output data arrays from the same input
  data array.
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
  Dave Baines ({\mantt{ROE}}::{\mantt{ASOC5}})
  Dave Pike ({\mantt{RGO}}::{\mantt{CDP}})
  Roger Wood ({\mantt{RGO}}::{\mantt{RW}})
\end{manroutinedescription}

\sstroutine{
   MATHS
}{
   Evaluates mathematical expressions applied to NDF data structures
}{
   \sstdescription{
      This application allows arithmetic and mathematical functions to
      be applied pixel-by-pixel to a number of NDF data structures and
      constants so as to produce a new NDF. The operations to be
      performed are specified using a Fortran-like mathematical
      expression.  Up to 26 each input NDF data and variance arrays, 26
      parameterised `constants', and pixel and data co-ordinates along
      up to 7 dimensions may be combined in wide variety of ways using
      this application.  The task can also calculate variance estimates
      for the result when there is at least one input NDF array.
   }
   \sstusage{
      maths exp out ia-iz=? va-vz=? fa-fz=? pa-pz=? lbound=? ubound=?
   }
   \sstparameters{
      \sstsubsection{
         EXP = LITERAL (Read)
      }{
         The mathematical expression to be evaluated for each NDF
         pixel, {\it e.g.}\ {\tt "(IA-IB$+$2)$*$PX"}.  In this expression, input NDFs are
         denoted by the variables IA, IB, \ldots IZ, while constants may
         either be given literally or represented by the variables PA,
         PB, \ldots  PZ.  Values for those NDFs and constants which appear
         in the expression will be requested via the application's
         parameter of the same name.

         Fortran-77 syntax is used for specifying the expression, which
         may contain the usual intrinsic functions, plus a few extra
         ones. An appendix in SUN/61 gives a full description of the
         syntax used and an up to date list of the functions available.
         The expression may be up to 132 characters long and is case
         insensitive.
      }
      \sstsubsection{
         FA-FZ = LITERAL (Read)
      }{
         These parameters supply the values of `sub-expressions' used
         in the expression EXP.  Any of the 26 (FA, FB, \ldots FZ) may
         appear; there is no restriction on order.  These parameters
         should be used when repeated expressions are present in
         complex expressions, or to shorten the value of EXP to fit
         within the 132-character limit.  Sub-expressions may contain
         references to other sub-expressions and constants (PA-PZ).  An
         example of using sub-expressions is:
         \begin{description}
         \item EXP $>$ {\tt PA$*$ASIND(FA/PA)$*$XA/FA}
         \item FA $>$ {\tt SQRT(XA$*$XA$+$XB$*$XB)}
         \item PA $>$ {\tt 10.1}
         \end{description}
         where the parameter name is to the left of $>$ and its value is
         to the right of the $>$.
      }
      \sstsubsection{
         IA-IZ = NDF (Read)
      }{
         The set of 26 parameters named IA, IB, \ldots IZ is used to
         obtain the input NDF data structure(s) to which the
         mathematical expression is to be applied.  Only those
         parameters which actually appear in the expression are used,
         and their values are obtained in alphabetical order.  For
         instance, if the expression were {\tt "SQRT(IB$+$IA)"}, then the
         parameters IA and IB would be used (in this order) to obtain
         the two input NDF data structures.
      }
      \sstsubsection{
         LBOUND( ) = \_INTEGER (Read)
      }{
         Lower bounds of new NDF, if LIKE={\tt !} and there is no input NDF
         referenced in the expression.  The number of values required
         is the number of pixel co-ordinate axes in the expression.
      }
      \sstsubsection{
         LIKE = NDF (Read)
      }{
         An optional template NDF which, if specified, will be used to
         define bounds and data type of the new NDF, when the expression
         does not contain a reference to an NDF.  If a null response
         ({\tt !}) is given the bounds are obtained via parameters LBOUND
         and UBOUND, and the data type through parameter TYPE. {\tt [!]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF to contain the result of evaluating the expression
         at each pixel.
      }
      \sstsubsection{
         PA-PZ = \_DOUBLE (Read)
      }{
         The set of 26 parameters named PA, PB, \ldots PZ is used to
         obtain the numerical values of any parameterised `constants'
         which appear in the expression being evaluated.  Only those
         parameters which actually appear in the expression are used,
         and their values are obtained in alphabetical order.  For
         instance, if the expression were {\tt "PT$*$SIN(IA/PS)"}, then the
         parameters PS and PT (in this order) would be used to obtain
         numerical values for substitution into the expression at the
         appropriate points.

         These parameters are particularly useful for supplying the
         values of constants when writing procedures, where the
         constant may be determined by a command-language variable, or
         when the constant is stored in a data structure such as a
         global parameter.  In other cases, constants should normally be
         given literally as part of the expression, as in {\tt "IZ$*$$*$2.77"}.
      }
      \sstsubsection{
         QUICK = \_LOGICAL (Read)
      }{
         Specifies the method by which values for the variance
         component of the output NDF are calculated. The algorithm used
         to determine these values involves perturbing each of the
         input NDF data arrays in turn by an appropriate amount, and
         then combining the resulting output perturbations.  If QUICK
         is set to {\tt TRUE}, then each input data array will be perturbed
         once, in the positive direction only.  If QUICK is set to
         {\tt FALSE}, then each will be perturbed twice, in the positive and
         negative directions, and the maximum resultant output
         perturbation will be used to calculate the output variance.
         The former approach (the normal default) executes more
         quickly, but the latter is likely to be more accurate in cases
         where the function being evaluated is highly non-linear,
         and/or the errors on the data are large. This parameter is
         ignored if the expression does not contain a token to at least
         one input NDF structure. {\tt [TRUE]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the (alphabetically) first input NDF to be used
         instead. {\tt [!]}
      }
      \sstsubsection{
         TYPE = LITERAL (Read)
      }{
         Data type for the new NDF, if LIKE={\tt !} and no input NDFs are
         referenced in the expression.  It must be one either
         {\tt "\_DOUBLE"} or {\tt "\_REAL"}. {\tt ["\_REAL"]}
      }
      \sstsubsection{
         UBOUND( ) = \_INTEGER (Read)
      }{
         Upper bounds of new NDF, if LIKE={\tt !} and there is no input NDF
         referenced in the expression.  These must not be smaller
         than the corresponding LBOUND.  The number of values required
         is the number of pixel co-ordinate axes in the expression.
      }
      \sstsubsection{
         UNITS = \_LOGICAL (Read)
      }{
         Specifies whether the units component of the (alphabetically)
         first input NDF or the template NDF will be propagated to the
         output NDF.  By default this component is not propagated since,
         in most cases, the units of the output data will differ from
         those of any of the input data structures.  In simple cases,
         however, the units may be unchanged, and this parameter then
         allows the units component to be preserved.  This parameter is
         ignored if the expression does not contain a token to at least
         one input NDF structure and LIKE={\tt !}.  {\tt [FALSE]}
      }
      \sstsubsection{
         VA-VZ = NDF (Read)
      }{
         The set of 26 parameters named VA, VB, \ldots VZ is used to
         obtain the input NDF variance array(s) to which the
         mathematical expression is to be applied.  The variance VA
         corresponds to the data array specified by parameter IA, and
         so on.  Only those parameters which actually appear in the
         expression, and do not have their corresponding data-array
         parameter IA-IZ present, have their values obtained in
         alphabetical order.  For instance, if the expression were
         {\tt "IB$+$SQRT(VB$+$VA)"}, then the parameters VA and IB would be used
         (in this order) to obtain the two input NDF data structures.
         The first would use just the variance array, whilst the second
         would read both data and variance arrays.
      }
      \sstsubsection{
         VARIANCE = \_LOGICAL (Read)
      }{
         Specifies whether values for the variance component of the
         output NDF should be calculated.  If this parameter is set to
         {\tt TRUE} (the normal default), then output variance values will be
         calculated if any of the input NDFs contain variance
         information.  Any which do not are regarded as having zero
         variance.  Variance calculations will normally be omitted only
         if none of the input NDFs contain variance information.
         However, if VARIANCE is set to {\tt FALSE}, then calculation of
         output variance values will be disabled under all
         circumstances, with a consequent saving in execution time.
         This parameter is ignored if the expression does not contain
         at least one token to an input NDF structure.
         {\tt [TRUE]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         maths "ia-1" dat2 ia=dat1
      }{
         The expression {\tt "ia-1"} is evaluated to subtract 1 from each
         pixel of the input NDF referred to as IA, whose values reside
         in the data structure dat1.  The result is written to the NDF
         structure dat2.
      }
      \sstexamplesubsection{
         maths "(ia-ib)/ic" ia=data ib=back ic=flat out=result units
      }{
         The expression {\tt "(ia-ib)/ic"} is evaluated to remove a
         background from an image and to divide it by a flat-field.
         All the images are held in NDF data structures, the input
         image being obtained from the data structure data, the
         background image from back and the flat-field from flat.  The
         result is written to the NDF structure result.  The data units
         are unchanged and are therefore propagated to the output NDF.
      }
      \sstexamplesubsection{
         maths "-2.5$*$log10(ii)$+$25.7" ii=file1 out=file2
      }{
         The expression {\tt "-2.5$*$log10(ii)$+$25.7"} is evaluated to convert
         intensity measurements into magnitudes, including a zero
         point.  Token II represents the input measurements held in the
         NDF structure file1.  The result is written to the NDF
         structure file2.  If file1 contains variance values, then
         corresponding variance values will also be calculated for
         file2.
      }
      \sstexamplesubsection{
         maths exp="pa$*$exp(ia$+$pb)" out=outfile pb=13.7 novariance
      }{
         The expression {\tt "pa$*$exp(ia$+$pb)"} is evaluated with a value of
         13.7 for the constant PB, and output is written to the NDF
         structure outfile.  The input NDF structure to be used for
         token IA and the value of the other numerical constant PA will
         be prompted for.  {\tt NOVARIANCE} has been specified so that output
         variance values will not be calculated.
      }
      \sstexamplesubsection{
         maths exp="mod(XA,32)$+$mod(XB,64)" out=outfile like=comwest
      }{
         The expression {\tt "mod(XA,32)$+$mod(XB,64)"} is evaluated, and
         output is written to the NDF structure outfile.  The output
         NDF inherits the shape, bounds, and other properties (except the
         variance) of the NDF called comwest.  The data type of outfile
         is \_REAL unless comwest has type \_DOUBLE.  XA and XB represent
         the pixel co-ordinates along the $x$ and $y$ axes respectively.
      }
      \sstexamplesubsection{
         maths "xf$*$xf$+$0$*$xa" ord2 lbound=[-20,10] ubound=[20,50]
      }{
         The expression {\tt "xf$*$xf$+$0$*$xa"} is evaluated, and output is
         written to the NDF structure ord2.  The output NDF has data
         type \_REAL, is two-dimensional with bounds $-$20:20, 10:50.  The
         XA is needed to indicate that XF represents pixel co-ordinates
         along the $y$ axis.
      }
      \sstexamplesubsection{
         maths "xa/max(1,xb)$+$sqrt(va)" ord2 va=fuzz title="Fuzz correction"
      }{
         The expression {\tt "xa/max(1,xb)$+$sqrt(va)"} is evaluated, and output
         is written to the NDF structure ord2.  Token VA represents the
         input variance array held in the NDF structure fuzz.  The
         output NDF inherits the shape, bounds, and other properties of
         fuzz.  The title of ord2 is {\tt "Fuzz correction"}.  The data type
         of ord2 is \_REAL unless fuzz has type \_DOUBLE.  XA and XB
         represent the pixel co-ordinates along the $x$ and $y$ axes
         respectively.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The alphabetically first input NDF is regarded as the primary
         input dataset. NDF components whose values are not changed by
         this application will be propagated from this NDF to the output.
         The same propagation rules apply to the LIKE template NDF,
         except that the output NDF does have inherit any variance
         information.

         \sstitem
         There are additional tokens which can appear in the expression.

         The set of 7 tokens named CA, CB, \ldots CG is used to obtain the
         data co-ordinates from the primary input NDF data structure.  Any
         of the 7 parameters may appear in the expression.  The order
         defines which axis is which, so for example, {\tt "2$*$CF$+$CB$*$CB"} means
         the first-axis data co-ordinates squared, plus twice the
         co-ordinates along the second axis.  There must be at least one
         input NDF in the expression to use the CA-CG tokens, and it must
         have dimensionality of at least the number of CA-CG tokens given.

         The set of 7 tokens named XA, XB, \ldots XG is used to obtain the
         pixel co-ordinates from the primary input NDF data structure.  Any
         of the 7 parameters may appear in the expression.  The order
         defines which axis is which, so for example, {\tt "SQRT(XE)$+$XC"} means
         the first-axis pixel co-ordinates plus the square root of the
         co-ordinates along the second axis.  Here no input NDF need be
         supplied.  In this case the dimensionality of the output NDF is equal to the
         number of XA-XG tokens in the expression.  However, if there is
         at least one NDF in the expression, there should not be more
         XA-XG tokens than the dimensionality of the output NDF (given
         as the intersection of the bounds of the input NDFs).

         \sstitem
         If illegal arithmetic operations ({\it e.g.}\ division by zero, or
         square root of a negative number) are attempted, then a bad pixel
         will be generated as a result.  (However, the infrastructure
         software that detects this currently does not work on OSF/1
         systems, and therefore MATHS will crash in this circumstance.)

         \sstitem
         All arithmetic performed by this application is floating
         point.  Single-precision will normally be used, but
         double-precision will be employed if any of the input NDF arrays
         has a numeric type of \_DOUBLE.

      }
   }
   \sstdiytopic{
      Calculating Variance
   }{
      The algorithm used to calculate output variance values is
      general-purpose and will give correct results for any reasonably
      well-behaved mathematical expression.  However, this application
      as a whole, and the variance calculations in particular, are
      likely to be less efficient than a more specialised application
      written knowing the form of the mathematical expression in
      advance.  For simple operations (addition, subtraction, {\it etc.}) the
      use of other applications (ADD, SUB, {\it etc.}) is therefore
      recommended, particularly if variance calculations are required.

      The main value of the variance estimation algorithm used here
      arises when the expression to be evaluated is too complicated, or
      too infrequently used, to justify the work of deriving a direct
      formula for the variance.  It is also of value when the data
      errors are especially large, so that the linear approximation
      normally used in error analysis breaks down.

      There is no variance processing when there are no tokens for
      input NDF structures.
   }
   \sstdiytopic{
      Timing
   }{
      If variance calculations are not being performed, then the time
      taken is approximately proportional to the number of NDF pixels
      being processed.  The execution time also increases with the
      complexity of the expression being evaluated, depending in the
      usual way on the nature of any arithmetic operations and
      intrinsic functions used.  If certain parts of the expression will
      often give rise to illegal operations (resulting in bad pixels),
      then execution time may be minimised by placing these operations
      near the beginning of the expression, so that later parts may not
      need to be evaluated.

      If output variance values are being calculated and the QUICK
      parameter is set to {\tt TRUE}, then the execution time will be
      multiplied by an approximate factor ($N+$1), where $N$ is the number
      of input NDFs which contain a variance component.  If QUICK is set
      to {\tt FALSE}, then the execution time will be multiplied by an
      approximate factor ($2N+$1).
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CREFRAME, SETAXIS, and numerous arithmetic tasks; Figaro:
      numerous arithmetic tasks.
   }
   \sstimplementationstatus{

      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of the
         input NDFs.  HISTORY and extensions are propagated from both the
         primary NDF and template NDF.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         NDFs with any number of dimensions can be processed.  The NDFs
         supplied as input need not all be the same shape.
      }
   }
}
\sstroutine{
   MEDIAN
}{
   Smooths a 2-dimensional data array using a weighted median filter
}{
   \sstdescription{
      This task filters the 2-dimensional data array in the input NDF
      structure with a Weighted Median Filter (WMF) in a 3-by-3-pixel
      kernel to create a new NDF.  There are a number of predefined
      weighting functions and parameters that permit other symmetric
      weighting functions.  See parameter MODE and the topic
      {\tt "}User-defined Weighting Functions{\tt "}.

      A threshold for replacement of a value by the median can be set.
      If the absolute value of the difference between the actual value
      and the median is less than the threshold, the replacement will
      not occur.  The array boundary is dealt by either pixel
      replication or a reflection about the edge pixels of the array.

      The WMF can be repeated iteratively a specified number of times,
      or it can be left to iterate continuously until convergence is
      achieved and no further changes are made to the data.  In the
      latter case a damping algorithm is used if the number of
      iterations exceeds some critical value, which prevents the result
      oscillating between two solutions (which can sometimes happen).
      When damping is switched on data values are replaced not by the
      median value, but by a value midway between the original and the
      median.

      Bad pixels are not included in the calculation of the median.
      There is a defined threshold which specifies minimum-allowable
      median position as a fraction of the median position when there
      are no bad pixels.  For neighbourhoods with too many bad pixels,
      and so the median position is too small, the resulting output
      pixel is bad.
   }
   \sstusage{
      median in out [mode] [diff] [bound] [numit] corner side centre
   }
   \sstparameters{
      \sstsubsection{
         BOUND = LITERAL (Read)
      }{
         Determines the type of padding required at the array edges
         before the filtering starts.  The alternatives are described
         below.

         \begin{description}
         \item {\tt "Replication"} --- The values at the edge of the data array
                           are replicated into the padded area.  For
                           example, with STEP={\tt 2} one corner of the
                           original and padded arrays would appear
                           as follows:
                                           
                \parbox{29mm}{corner of original array:}
                $
                \begin{array}{ccccc}
                1 & 1 & 1 & 1 & 1 \\  
                1 & 2 & 2 & 2 & 2 \\
                1 & 2 & 3 & 3 & 3 \\
                1 & 2 & 3 & 4 & 4 \\
                1 & 2 & 3 & 4 & 5 
                \end{array}
                $
                \hspace{1em}\parbox{35mm}{corresponding corner of padded array:}
                $
                \begin{array}{ccccccc}
                1 & 1 & 1 & 1 & 1 & 1 & 1 \\
                1 & 1 & 1 & 1 & 1 & 1 & 1 \\
                1 & 1 & 1 & 1 & 1 & 1 & 1 \\
                1 & 1 & 1 & 1 & 1 & 1 & 1 \\
                1 & 1 & 1 & 2 & 2 & 2 & 2 \\
                1 & 1 & 1 & 2 & 3 & 3 & 3 \\
                1 & 1 & 1 & 2 & 3 & 4 & 4 \\
                1 & 1 & 1 & 2 & 3 & 4 & 5
                \end {array}
                $

         \item {\tt "Reflection"} --- The values near the edge of the data array
                           are reflected about the array's edge pixels.
                           For example, with STEP={\tt 2} one corner of the
                           original and padded arrays would appear as
                           follows: 

                \parbox{29mm}{corner of original array:}
                $
                \begin{array}{ccccc}
                1 & 1 & 1 & 1 & 1 \\  
                1 & 2 & 2 & 2 & 2 \\
                1 & 2 & 3 & 3 & 3 \\
                1 & 2 & 3 & 4 & 4 \\
                1 & 2 & 3 & 4 & 5 
                \end{array}
                $
                \hspace{1em}\parbox{35mm}{corresponding corner of padded array:}
                $
                \begin{array}{ccccccc}
                3 & 2 & 1 & 2 & 3 & 3 & 3 \\
                2 & 2 & 1 & 2 & 2 & 2 & 2 \\
                1 & 1 & 1 & 1 & 1 & 1 & 1 \\
                2 & 2 & 1 & 2 & 2 & 2 & 2 \\
                3 & 2 & 1 & 2 & 3 & 3 & 3 \\
                3 & 2 & 1 & 2 & 3 & 4 & 4 \\
                3 & 2 & 1 & 2 & 3 & 4 & 5 \\
                \end {array}
                $
         \end{description}

          {\tt ["Replication"]}
      }
      \sstsubsection{
         CENTRE = \_INTEGER (Read)
      }{
         Central value for weighting function, required if MODE = {\tt $-$1}.
         It must be an odd value in the range 1 to 21. {\tt [1]}
      }
      \sstsubsection{
         CORNER = \_INTEGER (Read)
      }{
         Corner value for weighting function, required if MODE = {\tt $-$1}.
         It must be in the range 0 to 10. {\tt [1]}
      }
      \sstsubsection{
         DIFF  = \_DOUBLE (Read)
      }{
         Replacement of a value by the median occurs if the absolute
         difference of the value and the median is greater than DIFF.
         {\tt [0.0]}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         NDF structure containing the 2-dimensional data array to be
         filtered.
      }
      \sstsubsection{
         ITERATE = LITERAL (Read)
      }{
         Determines the type of iteration used.  The alternatives are
         described below.

         \begin{description}
         \item {\tt "Specified"} --- You specify the number of iterations
                           at each step size in the parameter NUMIT.

         \item {\tt "Continuous"} --- The filter iterates continuously until
                           convergence is achieved and the array is no
                           longer changed by the filter.  A damping
                           algorithm comes into play after MAXIT
                           iterations, and the filter will give up
                           altogether after MAXIT $\times$ 1.5 iterations
                           (rounded up to the next highest integer).
         \end{description}

         {\tt "Continuous"} mode is recommended only for images which are
         substantially smooth to start with (such as a sky background
         frame from a measuring machine).  Complex images may take many
         iterations, and a great deal of time, to converge.
         {\tt ["Specified"]}
      }
      \sstsubsection{
         MAXIT = \_INTEGER (Read)
      }{
         The maximum number of iterations of the filter before the
         damping algorithm comes into play, when ITERATE =
         {\tt "Continuous"}.  It must lie in the range 1 to 30.  {\tt [10]}
      }
      \sstsubsection{
         MEDTHR = \_REAL (Read)
      }{
         Minimum-allowable actual median position as a fraction of the
         median position when there are no bad pixels, for the
         computation of the median at a given pixel. {\tt [0.8]}
      }
      \sstsubsection{
         MODE = \_INTEGER (Read)
      }{
         Determines type of weighting used, {\tt $-$1} allows you to define the
         weighting, and {\tt 0} to {\tt 7} the predefined filters.  The predefined
         modes have the following weighting functions:

\[
\begin{array}{lccclccclccclccc}
{\bf 0:}&1&1&1\hspace{4ex}&{\bf 1:}&0&1&0\hspace{4ex}&{\bf 2:}&1&0&1\hspace{4ex}&{\bf 3:}&1&1&1 \\
  &  1&1&1\hspace{4ex}&&  1&1&1\hspace{4ex}&&  0&1&0\hspace{4ex}&&  1&3&1 \\
  &  1&1&1\hspace{4ex}&&  0&1&0\hspace{4ex}&&  1&0&1\hspace{4ex}&&  1&1&1 \\
\\
\\
{\bf 4:}&0&1&0\hspace{4ex}&{\bf 5:}&1&0&1\hspace{4ex}&{\bf 6:}&1&2&1\hspace{4ex}&{\bf 7:}&1&3&1 \\
  &  1&3&1\hspace{4ex}&&  0&3&0\hspace{4ex}&&  2&3&2\hspace{4ex}&&  3&3&3 \\
  &  0&1&0\hspace{4ex}&&  1&0&1\hspace{4ex}&&  1&2&1\hspace{4ex}&&  1&3&1
\end{array}
\]

         {\tt [0]}
      }
      \sstsubsection{
         NUMIT = \_INTEGER (Read)
      }{
         The specified number of iterations of the filter, when ITERATE
         = {\tt "Specified"}.  {\tt [1]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         NDF structure to contain the 2-dimensional data array after
         filtering.
      }
      \sstsubsection{
         SIDE = \_INTEGER (Read)
      }{
         Side value for weighting function, required if MODE = {\tt $-$1}.
         It must be in the range 0 to 10. {\tt [1]}
      }
      \sstsubsection{
         STEP() = \_INTEGER (Read)
      }{
         The spacings between the median filter elements to be used.
         The data may be filtered at one particular spacing by
         specifying a single value, such as STEP={\tt 4}, or may be filtered
         at a whole series of spacings in turn by specifying a list of
         values, such as STEP={\tt [4,3,2,1]}.  There is a limit of 32 values.
         {\tt [1]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the NDF supplied for parameter IN to be used
         instead. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         median a100 a100med
      }{
         This applies an equally weighted median filter to the NDF
         called a100 and writes the result to the NDF a100med.  It uses
         the default settings, which are a single step size of one
         pixel, and a difference threshold of 0.0.  The task pads the
         array by replication to deals with the edge pixels, and runs
         the filter once only.
      }
      \sstexamplesubsection{
         median a100 a100med bound=ref
      }{
         As in the previous example except that it uses reflection
         rather than replication when padding the array.
      }
      \sstexamplesubsection{
         median abc sabc mode=3 step=4 diff=1.0 numit=2
      }{
         This applies a median filter to the NDF called abc with a
         $
         \begin{array}{ccc}
         1 & 1 & 1 \\
         1 & 3 & 1 \\
         1 & 1 & 1
         \end{array}
         $
         weighting mask (MODE={\tt 3}), a step size of 4 pixels
         (STEP={\tt 4}) and a difference threshold of 1.0 (DIFF={\tt 1.0}).  It
         runs the filter twice (NUMIT={\tt 2}) and writes the result to
         the NDF called sabc.
      }
      \sstexamplesubsection{
         median abc sabc mode=3 step=[4,3,2,1] diff=1.0 numit=2
      }{
         This applies a median filter as in the previous example,
         only this time run the filter at step sizes of 4, 3, 2,
         and 1 pixels, in that order (STEP={\tt [4,3,2,1]}).  It runs the
         filter twice at each step size (NUMIT={\tt 2}).  Note that the
         filter will be run a total of {\em eight\/} times (number of step
         sizes times the number of iterations).
      }
      \sstexamplesubsection{
         median in=spotty step=[4,3,2,1] iterate=cont maxit=6 out=clean
      }{
         This applies a median filter to the NDF called spotty with
         the default settings for the mode and difference threshold.
         It runs the filter at step sizes of 4, 3, 2 and 1 pixels,
         operating continuously at each step size until the result
         converges (ITERATE={\tt CONT}).  Damping will begin after 6
         iterations (MAXIT={\tt 6}), and the filtering will stop regardless
         after 10 iterations (1 $+$ INT(1.5 $*$ MAXIT)).  Note that the
         filter will run an indeterminate number of times, up to a
         maximum of 40 (number of step sizes $\times$ maximum number of
         iterations), and may take a long time.  The resultant data
         array are written to the NDF called clean.
      }
   }
   \sstdiytopic{
      User-defined Weighting Functions
   }{
      Parameters CORNER, SIDE, and CENTRE allow other symmetric
      functions in addition to those offered by MODE={\tt 0} to {\tt 7}.  A step
      size has to be specified too; this determines the spacing of the
      elements of the weighting function. The data can be filtered at
      one step size only, or using a whole series of step sizes in
      sequence.  The weighting function has the form:
\[
\begin{array}{ccccc}

  \%\-{\tt CORNER} & . &  \%\-{\tt SIDE}  & . &  \%\-{\tt CORNER} \\
  . &  & . &  & . \\
  \%\-{\tt SIDE}  & . &  \%\-{\tt CENTRE} & . &  \%\-{\tt SIDE} \\
  . &  & . &  & . \\
  \%\-{\tt CORNER} & . &  \%\-{\tt SIDE}  & . &  \%\-{\tt CORNER}

\end{array}
\]

      The . indicates that the weights are separated by the
      stepsize-minus-one zeros.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: BLOCK, CONVOLVE, FFCLEAN, GAUSMOOTH; Figaro: ICONV3, \linebreak
      ISMOOTH, IXSMOOTH, MEDFILT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, LABEL, TITLE,
         UNITS, and HISTORY components of an NDF data structure and
         propagates all extensions.  VARIANCE is not used to weight the
         median filter and is not propagated.  QUALITY is also lost.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.
      }
   }
}
\sstroutine{
   MEM2D
}{
   Performs a Maximum-Entropy deconvolution of a 2-dimensional NDF
}{
   \sstdescription{
      MEM2D is based on the Gull and Skilling Maximum Entropy package
      {\footnotesize MEMSYS3}.  It takes an image and a Point-Spread Function as input
      and produces an equal-sized image as output with higher
      resolution.  Facilities are provided to `analyse' the resulting
      deconvolved image, {\it i.e.}\ to calculate an integrated flux in some
      area of the deconvolved image and also an estimate of the
      uncertainty in the integrated flux.  This allows the significance
      of structure visible in the deconvolution to be checked.

      For a detailed description of the algorithm, and further
      references, see the {\footnotesize MEMSYS} users manual, and SUN/117.
   }
   \sstusage{
      mem2d in out mask=? $\left\{ {\begin{tabular}{l}
                                     fwhmpsf=? \\
                                     psf=?
                                    \end{tabular} }
                          \right.$
                                    \newline\hspace*{10.3em}
                                    \makebox[0mm][c]{\small psftype}
   }
   \sstparameters{
      \sstsubsection{
         ANALYSE = \_LOGICAL (Read)
      }{
         ANALYSE should be given a {\tt TRUE} value if an analysis of a
         previously generated deconvolution is to be performed, instead
         of a whole new deconvolution being started.  An analysis
         returns the integrated flux in some area of the deconvolved
         image you specify, together with the standard deviation on the
         integrated flux value.  The area to be integrated over is
         specified by an image associated with parameter MASK.  This
         facility can, for instance, be used to assess the significance
         of structure seen in the deconvolution.  An analysis can only
         be performed if the input NDF (see parameter IN) contains a
         MEM2D extension (see parameter EXTEND).  If the input does
         contain such an extension, and if the extension shows that the
         deconvolution was completed, then ANALYSE is defaulted to
         {\tt TRUE}, otherwise it is defaulted to {\tt FALSE}. {\tt []}
      }
      \sstsubsection{
         DEF = \_REAL (Read)
      }{
         This is the value to which the output image will default in
         areas for which there is no valid data in the input.  The `zero
         entropy' image is defined to be a flat surface with value
         given by parameter DEF.  Any deviation of the output image away
         from this image will cause its entropy to become negative.
         Thus a maximum-entropy criterion causes the output image to be
         as similar as possible to a flat surface with value DEF
         (within the constraints of the data).  DEF is defaulted to the
         mean data value in the input image and must always be strictly
         positive. {\tt []}
      }
      \sstsubsection{
         EXTEND = \_LOGICAL (Read)
      }{
         If EXTEND has a {\tt TRUE} value, then the output NDF will contain
         an extension called MEM2D which will contain all the
         information required to either restart or analyse the
         deconvolution.  Note, including this extension makes the output
         file much bigger (by about a factor of seven).  {\tt [TRUE]}
      }
      \sstsubsection{
         FWHMICF = \_REAL (Read)
      }{
         This is the Full Width at Half Maximum (in pixels) of a
         Gaussian Intrinsic Correlation Function (ICF) to be used in
         the deconvolution.  The ICF can be used to encode prior
         knowledge of pixel-to-pixel correlations in the output image.
         A value of {\tt 0} for FWHMICF causes no ICF to be used, and so
         no correlations are expected in the output.  Larger values
         encourage smoothness in the output on the scale of the ICF.  If
         a non-zero ICF is used, the image entropy which is maximised
         is not the output image, but a `hidden' image.  This hidden
         image is the deconvolution of the output image with the ICF,
         and is assumed to have no pixel-to-pixel correlations. {\tt [2]}
      }
      \sstsubsection{
         FWHMPSF = \_REAL (Read)
      }{
         This is the Full Width at Half Maximum (in pixels) of a
         Gaussian Point Spread Function (PSF).  This PSF is used to
         deconvolve the input only if parameter PSFTYPE has the value
         {\tt "Gaussian"}.
      }
      \sstsubsection{
         ILEVEL = \_INTEGER (Read)
      }{
         ILEVEL controls the amount of information displayed as MEM2D
         runs.  If set to {\tt 0} then no information is displayed.  Larger
         values up to a maximum of 3, give larger amounts of
         information.  A value of {\tt 3} gives full {\footnotesize MEMSYS3} diagnostics
         after each iteration. {\tt [1]}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF.  This can either contain an image to be
         deconvolved, or the output from a previous run of MEM2D.  The
         NDF is considered to be an output from MEM2D if it contains an
         extension called MEM2D (see parameter EXTEND).  If such an
         extension is found, a check is made to see if the NDF contains
         a completed deconvolution or a partial deconvolution.  If the
         deconvolution is complete, the ANALYSE parameter is defaulted
         to {\tt TRUE}, and unless you override this default, an
         analysis of the deconvolution contained in the input NDF is
         performed.  If the input deconvolution is not complete, then
         the deconvolution process is restarted from where it left off.
         If no MEM2D extension is found, then a new deconvolution
         is started from scratch.
      }
      \sstsubsection{
         MASK = NDF (Read)
      }{
         An image to use as a mask to define the areas to be integrated
         when performing an analysis (see parameter ANALYSE).  The
         integrated-flux value calculated by the analysis is actually
         the total data sum in the product of the mask and the
         deconvolved image.  Mask pixel values can be positive or
         negative (or zero) and so, for instance, masks can be arranged
         which subtract off a background brightness from a source
         before returning the integrated source flux.
      }
      \sstsubsection{
         MODEL = NDF (Read)
      }{
         An image to use as the default model for the reconstruction.
         If a null value is given, then a constant value given by the
         parameter DEF is used to define a flat default model.  The
         section of the given image which matches the bounds of the
         input image is used.  Any bad pixels in the image cause the
         corresponding pixels in the input image to be ignored.  Such
         pixels are set bad in the output.  The model image should
         contain no pixels with a value of zero or less.  The default
         model is defined to have zero entropy.  The hidden image will
         tend to the default model in the absence of data.  It should be
         noted that this model applies to the `hidden' image, not the
         actually required reconstructed image.  The reconstructed image
         is obtained from the hidden image by blurring the hidden image
         with the ICF. {\tt [!]}
      }
      \sstsubsection{
         MODELOUT = NDF (Write)
      }{
         An image which can be used for the default model in a further
         run of MEM2D.  Each pixel value in the created image is a
         linear combination of the model value at the corresponding
         pixel in the current reconstruction, and the hidden image
         pixel value.  Pixels for which the hidden image is well away
         from the current model, tend towards the value of the hidden
         image; pixels for which the hidden image is close to the
         current model tend towards the model.  Running MEM2D several
         times, using the new model created on the previous run as the
         model for the current run, can reduce the `mottling' often
         seen in MEM2D reconstructions. {\tt [!]}
      }
      \sstsubsection{
         NITER = \_INTEGER (Read)
      }{
         The maximum number of maximum-entropy iterations to perform.  MEM2D
         continues the deconvolution until either {\footnotesize MEMSYS3} indicates
         that the termination criterion ($\Omega=1.0$) has been reached,
         or the maximum number of iterations is reached.  If a
         deconvolution requires more iterations than was allowed by
         NITER, then you can choose to continue the deconvolution
         by giving the prematurely terminated output from MEM2D as the
         input to another run of MEM2D, specifying a larger value for
         NITER. {\tt [50]}
      }
      \sstsubsection{
         NOISE = LITERAL (Read)
      }{
         NOISE defines the noise statistics within the input image.  It
         can take the value {\tt "Gaussian"} or {\tt "Poisson"}.  If Gaussian noise
         is selected, the data variances are set initially to the
         values stored in the VARIANCE component of the input NDF.  If
         no such component exists, then the data variances are set to a
         constant value equal to the RMS difference between adjacent
         pixels in the $x$ direction.  {\footnotesize MEMSYS3} scales these initial noise
         estimates to maximise the data `evidence'.  The evidence is
         displayed as {\tt "LOG(PROB)"} and the noise scaling factor as
         {\tt "SIGMA"}, if parameter ILEVEL is set to {\tt 2} or more.  If Poisson
         statistics are selected the uncertainty in each data value is,
         as usual, of the order of the square root of the data value.
         When using Poisson statistics, there is no equivalent to the
         noise scaling performed when using Gaussian statistics.  Any
         input VARIANCE component is ignored. {\tt ["Gaussian"]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The output image in a `primitive' NDF.  The output is the same
         size as the input.  Any pixels which were flagged as bad in the
         input will also be bad in the output.  If parameter EXTEND is
         {\tt TRUE}, then the output NDF contains an extension called
         MEM2D containing information which allows the deconvolution to
         be either continued or analysed.  There is no VARIANCE
         component in the output, but any QUALITY values are propagated
         from the input to the output.  If parameter UPDATE is {\tt TRUE},
         then the output NDF is created after the first iteration and is
         updated after each subsequent iteration.
      }
      \sstsubsection{
         PSF = NDF (Read)
      }{
         An NDF holding an estimate of the Point Spread Function (PSF)
         of the input image.  This PSF is used to deconvolve the input
         only if parameter PSFTYPE has the value {\tt "NDF"}.  The PSF can be
         centred anywhere within the image, the location of the centre
         is specified using parameters XCENTRE and YCENTRE.  The
         extent of the PSF actually used is controlled by parameter
         THRESH.
      }
      \sstsubsection{
         PSFTYPE = LITERAL (Read)
      }{
         PSFTYPE determines if the Point Spread Function used in the
         deconvolution is to be Gaussian (if PSFTYPE = {\tt "Gaussian"}), or
         is to be defined by an image you supply (if PSFTYPE = {\tt "NDF"}).
         {\tt ["NDF"]}
      }
      \sstsubsection{
         RATE = \_REAL (Read)
      }{
         This is the value to use for the {\footnotesize MEMSYS3} RATE parameter.  It
         determines the rate at which the convergence is allowed to
         proceed.  If RATE is high, each maximum-entropy iteration is allowed to
         make a big change to the current reconstruction.  This can
         cause numeric problems within {\footnotesize MEMSYS3} resulting in MEM2D
         crashing with a {\tt "}floating overflow{\tt "} error.  If this happens,
         try reducing RATE.  Useful values will normally be of the order
         of unity, and must lie in the interval 0.0001 to 100.  {\tt [0.5]}
      }
      \sstsubsection{
         THRESH = \_REAL (Read)
      }{
         The fraction of the PSF peak amplitude at which the extents of
         the NDF PSF are determined.  It must be positive and less than
         0.5.  This parameter is only used when PSFTYPE = {\tt "NDF"}.  An
         error will result if the input PSF is truncated above this
         threshold. {\tt [0.0625]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         A title for the output NDF.  A null (!) value means using the
         title of the input NDF. {\tt [!]}
      }
      \sstsubsection{
         UPDATE = \_LOGICAL (Read)
      }{
         If UPDATE is given a {\tt TRUE} value, then the output NDF will be
         created after the first iteration, and will then be updated
         after each subsequent iteration.  This means that the current
         reconstruction can be examined without aborting the
         application.  Also, if parameter EXTEND is {\tt TRUE}, then
         if the job aborts for any reason, it can be restarted from the
         last completed iteration (see parameter IN). {\tt [TRUE]}
      }
      \sstsubsection{
         XCENTRE = \_INTEGER (Read)
      }{
         The $x$ pixel index of the centre of the PSF within the supplied
         PSF image.  This is only required if PSFTYPE is {\tt "NDF"}.  XCENTRE
         is defaulted to the middle pixel (rounded down if there are an
         even number of pixels per line). {\tt []}
      }
      \sstsubsection{
         YCENTRE = \_INTEGER (Read)
      }{
         The $y$ pixel index (line number) of the centre of the PSF
         within the supplied PSF image.  This is only required if
         PSFTYPE is {\tt "NDF"}.  YCENTRE is defaulted to the middle line
         (rounded down if there are an even number of lines). {\tt []}
      }
   }
   \sstresparameters{
      \sstsubsection{
         DSUM = \_REAL (Write)
      }{
         This is an output parameter to which is written the standard
         deviation of the integrated-flux value calculated if an
         analysis is performed (see parameter ANALYSE).
      }
      \sstsubsection{
         SUM = \_REAL (Write)
      }{
         This is an output parameter to which is written the
         integrated-flux value calculated if an analysis is performed
         (see parameter ANALYSE).
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         mem2d m51 m51\_hires psftype=gaussian fwhmpsf=3
      }{
         This example deconvolves the data array in the NDF called m51,
         putting the resulting image in the data array of the NDF called
         m51\_hires.  A circular Gaussian Point-Spread Function is used
         with a Full Width at Half Maximum of 3 pixels.
      }
      \sstexamplesubsection{
         mem2d m51 m51\_hires psf=star xcentre=20 ycentre=20
      }{
         This example performs the same function as the previous
         example, but the PSF is defined by the data array of the NDF
         called star, instead of being defined to be Gaussian.  This
         allows the PSF to be any arbitrary 2-dimensional function.  NDF
         star could be produced for example, by the {\footnotesize KAPPA}
         application called PSF.
         Parameters XCENTRE and YCENTRE give the pixel indices of
         the centre of the beam defined by the PSF in star.  The PSF is
         truncated to one sixteenth of its peak amplitude.
      }
      \sstexamplesubsection{
         mem2d m51\_hires m51\_hires niter=70 psf=star
      }{
         If the previous example failed to converge within the default
         50 iterations, the deconvolution can be started again from
         its current state, rather than having to start again from
         scratch.  Here NITER gives the upper limit on the total number
         of iterations which can be performed (including those performed
         in the previous run of MEM2D), {\bf not} just the number performed in
         this single run of MEM2D.  This facility can also be used if a
         MEM2D run is interrupted for any reason, such as the host
         computer going down, or a batch-queue CPU limit being reached.
         To use this facility the parameters EXTEND and UPDATE should
         have the default values of {\tt TRUE}.
      }
      \sstexamplesubsection{
         mem2d m51\_hires mask=nucleus
      }{
         Once a deconvolved image has been produced, the significance
         of features seen in the deconvolution can be assessed.  This
         example takes in the NDF m51\_hires produced by a previous run
         of MEM2D.  If this is a completed deconvolution then the
         parameter ANALYSE will be defaulted to {\tt TRUE}, and an analysis
         will be performed.  This effectively results in the
         deconvolution being multiplied by the data array of the NDF
         called nucleus, and the total data sum in the resulting image
         being displayed, together with the standard deviation on the
         total data sum.  The image in m51\_hires is the most probable
         deconvolution, but there may be other deconvolutions only
         slightly less probable than m51\_hires.  The standard deviation
         produced by an analysis takes account of the spread between
         such deconvolutions.  If the total data sum is not significantly
         greater than the standard deviation, then the feature selected
         by the mask image (called nucleus in this case) may well be
         spurious.  The mask image itself may for instance consist of an
         area of uniform value $+$1 covering some feature of interest,
         and the bad value (or equivalently the value zero) everywhere
         else.  The analysis would then give the integrated flux in the
         feature, assuming that the background is known to be zero.  If
         the background is not zero, then the mask may contain a
         background region containing the value $-$1, of equal area to
         the region containing the value $+$1.  The resulting integrated
         flux would then be the total flux in the source minus the flux
         in a background region of equal area.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         MEM2D requires a large quantity of memory---almost as much as
         the rest of {\footnotesize KAPPA}.  In order for the {\footnotesize KAPPA} monolith to
         load without you having to increase your memory or datasize
         resources, and because MEM2D is batch oriented (see Timing) it
         is only available as a separate application.

         \sstitem
         Memory is required to store several intermediate images while
         the deconvolution is in progress.  If the input image is small
         enough, these images are stored in a statically declared, internal
         array.  Otherwise, they are stored in dynamically mapped external
         arrays.  There is no limit on the size of image which can be
         processed by MEM2D (other than those imposed by limited resources
         on the host computer).

         \sstitem
         It is sometimes desirable for the pixels in the output image
         to be smaller than those in the input image.  For instance, if the
         input data are critically sampled (two samples per PSF), the output
         image may not be a very good deconvolution.  In such cases
         sub-dividing the output pixels would give better results.  At the
         moment MEM2D cannot do this.  Be warned that sub-dividing the
         input pixels and then running the current version of MEM2D will not
         have the same effect, since the noise in the input image will then
         have pixel-to-pixel correlations, and be interpreted as real structure.
      }
   }
   \sstdiytopic{
      Timing
   }{
      MEM deconvolution is extremely CPU intensive.  The total CPU time
      taken depends partly on the size of the image, and partly on the
      complexity of the structure within the image.  As a typical
      example, a 100$\times$100 image containing 20 Gaussians on a flat
      background took about 34 minutes of elapsed time on an unloaded
      DEC Alpha 2000.  Deconvolution jobs should therefore always be done
      in batch.  To perform an analysis on a deconvolution takes about the
      same length of time as a single deconvolution iteration.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FOURIER, LUCY, WIENER.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported, though only to remove them by the DEF value.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic is
         performed using single-precision floating point.
      }
   }
}
 
\sstroutine{
   MLINPLOT
}{
   Draws a multi-line plot of a 2-d NDF's data values against their
   axis co-ordinates
}{
   \sstdescription{
      This application takes one dimension of a 2-dimensional NDF as a
      line index and draws a multi-line plot of the NDF's data values against
      selected line indices from its other dimension.  Thus one obvious
      application is the display of 2-dimensional spectra.

      By default, this application selects the first dimension of the
      NDF as the abscissa of the plot and the second dimension of the
      NDF as the line index.  However, you can choose the opposite.
      The vertical axis of the plot is the value of the data lines
      after offsetting.  The horizontal axis of the plot is the axis
      co-ordinates of the selected dimension of the NDF.  If the axis
      co-ordinates are not defined in the NDF, the pixel co-ordinates
      of that dimension will be used in the plot.  The plot is situated
      within the current picture on the current graphics device.

      To separate the data lines from each other, there is a choice of
      three methods by which to offset the lines.  By default, each
      line is annotated with its line index (the indices of the
      dimension are selected for this purpose), and the offsets of the
      lines display in a table.
   }
   \sstusage{
      mlinplot ndf [comp] lnindx ylimit [pltitl] [abslab] [ordlab]
        \newline\hspace*{1.5em} [device]
   }
   \sstparameters{
      \sstsubsection{
         ABSAXS = \_INTEGER (Read)
      }{
         If it is 1, the first significant dimension of the input NDF
         will be taken as the abscissa of the plot.  If it is 2, the
         second significant dimension will be taken as the abscissa. 
         {\tt [1]}
      }
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the current picture is to be cleared before the line
         plot is drawn. {\tt [TRUE]}
      }
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The NDF component to be plotted.  It may be {\tt "Data"},
         {\tt "Quality"}, {\tt "Variance"}, or {\tt "Error"} (where
         {\tt "Error"} is the alternative to {\tt "Variance"} and causes
         the square root of the variance values to be displayed).
         If {\tt "Quality"} is specified, then the quality values are
         treated as numerical values (in the range 0 to 255).  {\tt ["Data"]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The plotting device. {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         KEY = \_LOGICAL (Read)
      }{
         When KEY is {\tt TRUE} a key of the line offsets will be drawn to
         the right of the line plots.  A maximum of 50 values are shown.
         When there are more than 50 lines displayed, the frequency of
         the offsets in the table decreases.  KEY set to {\tt TRUE} also
         causes the line numbers to be drawn to the right of the main
         plot and adjacent to their corresponding lines.  These are
         present to identify the lines when LINLAB is {\tt FALSE} or there are
         more than 26 lines plotted.  They too decrease in frequency
         when there is insufficient room to accommodate them all.  Their
         size is controlled by parameter LBSIZE; they are plotted with
         palette entry 3 when the chosen device supports at least four
         colours.

         When KEY is {\tt FALSE} there will be no key or annotations, enabling
         the plots to be seen at about 40 per cent greater resolution.
         The value of KEY is ignored when parameter YLOG is {\tt TRUE}.
         {\tt [TRUE]}
      }
      \sstsubsection{
         LNINDX = LITERAL (Read)
      }{
         A comma-separated number string specifies the line-index number
         to be displayed.  It can take any of the following values:
         \begin{description}
            \item {\tt "ALL"} or {\tt "$*$"} --- All lines

            \item {{\tt "}$xx$,$yy$,$zz${\tt "}} --- A list of line indices.

            \item {{\tt "}$xx$-$yy${\tt "}} --- Line indices between
                   $xx$ and $yy$ inclusively.  When $xx$ is omitted the
                   range begins from the lower bound of the line
                   dimension; when $yy$ is omitted the range ends
                   with the maximum value it can take, that is the
                   upper bound of the line dimension or the maximum
                   number of lines this routine can plot.
         \end{description}
            Any reasonable combination of above values separated by
            commas.  A maximum of 100 lines may be selected.  The
            suggested default is the current value, initially {\tt "1-5"}.
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         NDF structure containing the array to be plotted.
      }
      \sstsubsection{
         OFFSET() = \_REAL (Read)
      }{
         When the offset method is specified as {\tt "Free"}, this
         parameter is used to get the offset values for each locus of
         data values.
      }
      \sstsubsection{
         PXSIZE = \_REAL (Read)
      }{
         The horizontal size of the display in metres.  If a value less
         than the default is requested, the display will appear at
         the bottom left of the current device.  There is an upper
         limit given by the $x$ size of the current picture. {\tt [}Maximum
         that can fit in the current picture{\tt ]}
      }
      \sstsubsection{
         PYSIZE = \_REAL (Read)
      }{
         The vertical size of the display in metres.  If a value less
         than the default is requested, then the display will appear at
         the bottom left of the current device.  There is an upper
         limit given by the $y$ size of the current picture. {\tt [}Maximum
         that can fit in the current picture{\tt ]}
      }
      \sstsubsection{
         SPACE = LITERAL (Read)
      }{
         The value of this parameter specifies the method by which
         the data lines or loci in the plot are offset.  It can be
         given the values:
         \begin{description}
         \item {\tt "Free"} ---
           The offset of each data locus is specified by you.

         \item {\tt "Constant"} ---
           The base lines of the curves are evenly spaced between upper
           and lower limits of the plotting box.  The width of any line-
           to-line strip is constant, which could result in the loci
           becoming confused when the biases of some loci from their
           base lines are so large that these loci lie totally in the
           strips of other curves.

         \item {\tt "Average"} ---
           This method uses an average data value for each locus and
           produces offsets which ensure that these average data
           values are equally spaced over the plotting area.  Any line-
           to-line striping is thus hidden and the amount of overlap of
           adjacent traces is minimised.
         \end{description}
         The input can be abbreviated to an unambiguous length and
         is case insensitive. {\tt ["Average"]}
      }
      \sstsubsection{
         YLIMIT( 2 ) = \_REAL (Read)
      }{
         Used to get the lower and upper vertical display limits.  The
         suggest default lower limit is the minimum value of the bottom
         line in the display.  The default upper limit is such that
         no line will ever overlap.
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         ABSLAB = LITERAL (Read)
      }{
         Label for the plot abscissa.  If axis information is present
         the suggested default is the NDF's axis label followed by
         the units, in parentheses.  If an error occurs obtaining the
         label the default is {\tt "Pixel co-ordinates"}. {\tt []}
      }
      \sstsubsection{
         FONT = LITERAL (Read)
      }{
         The fount to be used for the line graphics.  It can be either
         {\tt "NCAR"} for the NCAR fancy characters and {\tt "GKS"} for the standard
         GKS san-serif fount.   The former is intended for hardcopy
         publication-quality plots, since it is relatively slow; the
         latter is intended for normal interactive graphics requiring
         rapid plotting, and it is clearer on small plots.  The
         suggested default is the current value. {\tt ["GKS"]}
      }
      \sstsubsection{
         LBSIZE = \_REAL (Read)
      }{
         The text width of the horizontal and vertical axis labels
         given as a fraction of the smaller dimension of the display
         window.  The value less than or equal to zero means using NCAR
         default setting.  The title of the display will have the text
         width of 1.2 $*$ LBSIZE, and the numerical label of the axes
         will have the text width of 0.8 $*$ LBSIZE for mantissa, and
         0.55 $*$ LBSIZE for exponent.  The permitted range is 0.0--0.05.
         {\tt [0.025]}
      }
      \sstsubsection{
         LINLAB = \_LOGICAL (Read)
      }{
         If LINLAB is {\tt TRUE}, the lines in the plot will be interrupted
         and be labelled by their line indices.  If LINLAB is {\tt FALSE}
         the lines will be solid.  There is a maximum of 26 annotated
         lines.  When there are more lines than 26, all the lines are
         solid regardless of the value of LINLAB.  The annotations are
         plotted with palette entry 2 when using a device that supports
         at least four colours. {\tt [TRUE]}
      }
      \sstsubsection{
         MAJTIC( 2 ) = \_REAL (Read)
      }{
         The parameter controlling the number of major tick marks for
         the $x$ and $y$ axes.  (Number used is between MAJTIC$+$2 and
         5$*$MAJTIC / 2 $+$ 4 ) {\tt [4.0, 4.0]}
      }
      \sstsubsection{
         MINTIC( 2 ) = \_REAL (Read)
      }{
         The number of minor tick marks between each major tick mark for
         the $x$ and $y$ axes.  A negative value forces the graphics package
         to compute appropriate values.  The number of minor tick marks
         per major tick is fixed ( 8 ) for a logarithmic axis.
         {\tt [-1.0, -1.0]}
      }
      \sstsubsection{
         ORDLAB = LITERAL (Read)
      }{
         Label for the vertical axis of the plot.  The suggested default
         is the NDF's label followed by the units, if present, in
         parentheses.  If an error occurs obtaining the label the
         default, is the component name followed by {\tt " values"}. {\tt []}
      }
      \sstsubsection{
         OUTTIC = \_LOGICAL (Read)
      }{
         True if the axis tick marks are to appear on the outside of
         the axes instead of inside.  By default, the tick marks are
         drawn inside the plot region. {\tt [FALSE]}
      }
      \sstsubsection{
         PLTITL = LITERAL (Read)
      }{
         The title of the plot.  Up to about 40 characters can be
         accommodated, and NCAR fancy founts may be embedded.  The
         suggested default is the title of the NDF.  If an error occurs
         obtaining the title, it is defaulted to {\tt "Lines plot"}. {\tt []}
      }
      \sstsubsection{
         TICLN = \_REAL (Read)
      }{
         The length of the major tick marks given in the fraction of the
         small dimension of the plot box.  Its value should be within
         range 0.0--0.05.  A value outside this range means using NCAR
         default.  The minor tick marks will have the length 0.66$*$TICLN.
         {\tt [0.015]}
      }
      \sstsubsection{
         XLOG = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the abscissa is to be logarithmic.  It is unlikely that
         you would want to do this. {\tt [FALSE]}
      }
      \sstsubsection{
         YLOG = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the vertical axis is to be logarithmic.  This is useful
         when the data have a wide dynamic range.  In order to
         discriminate between the lines, the lines are plotted using
         the first four pens in a cyclic fashion.  Note that no key is
         drawn when YLOG is {\tt TRUE}. {\tt [FALSE]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         mlinplot rcw3\_b1 reset $\backslash$
      }{
         Plot the first five lines of the 2-dimensional NDF file, rcw3\_b1,
         against its first significant dimension on the current
         graphics device.  The data co-ordinate will be in pixels if
         rcw3\_b1 does not have an axis component.  The lines are
         offset such that the averages of the lines are evenly
         separated in the direction of the vertical axis.
      }
      \sstexamplesubsection{
         mlinplot rcw3\_b1 lnindx="1,3,5,7-10" $\backslash$
      }{
         Plot the lines 1, 3, 5, 7, 8, 9 and 10 of the 2-dimensional NDF file,
         rcw3\_b1, against its first significant dimension on the
         current graphics device.
      }
      \sstexamplesubsection{
         mlinplot rcw3\_b1 lnindx=$*$ $\backslash$
      }{
         Plot all lines of the 2-dimensional NDF file, rcw3\_b1, against its
         first significant dimension on the current graphics device.
      }
      \sstexamplesubsection{
         mlinplot rcw3\_b1 absaxs=2 lnindx="20-25,30,31" $\backslash$
      }{
         Plot lines 20, 21, 22, 23, 24, 25, 30 and 31 of the 2-dimensional NDF
         file, rcw3\_b1, against its second significant dimension on
         the current graphics device.
      }
      \sstexamplesubsection{
         mlinplot rcw3\_b1 pltitl="CRDD rcw3\_b1" $\backslash$
      }{
         Plot the currently selected lines of the 2-dimensional NDF file, rcw3\_b1,
         against its first significant dimension on the current
         graphics device.  The plot has a title of {\tt "CRDD rcw3\_b1"}.
      }
      \sstexamplesubsection{
         mlinplot rcw3\_b1(100.0:500.0,) ylimit=[0.0,1.0E-3] $\backslash$
      }{
         Plot the currently selected lines of the 2-dimensional NDF file, rcw3\_b1,
         against its first significant dimension within co-ordinates
         100.0 to 500.0.  The vertical display range is from 0.0 to
         1.0E-3.
      }
      \sstexamplesubsection{
         mlinplot rcw3\_b1 space=constant device=ps\_p $\backslash$
      }{
         Plot the currently selected lines of the 2-dimensional NDF file, rcw3\_b1,
         against its first significant dimension on the ps\_p device.
         The base lines of them are evenly distributed in the range of
         vertical axis.
      }
      \sstexamplesubsection{
         mlinplot rcw3\_b1 space=free offset=[0.0,2.0e-4,4.0e-4,6.0e-4,0.1] $\backslash$
      }{
         Plot the currently selected lines of the 2-dimensional NDF file, rcw3\_b1,
         against its first significant dimension.  The base lines are
         set at 0.0 for the first line, 2.0E-12 for the second,
         4.0E-2 for the third, 6.0E-12 for the fourth, and 0.1 for
         the fifth.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The application stores a number of pictures in the graphics
         database in the following order: a FRAME of the specified size
         containing the title, annotated axes, and line plot; and a DATA
         picture, which has world co-ordinates for linear axes measured in
         pixels along the $x$ axis and data values along $y$, and their
         logarithms if a logarithmic axis is selected.  The DATA picture
         also has data co-ordinates stored; for a linear axis this
         requires that the NDF's axis units are not pixel co-ordinates;
         for a logarithmic axis the actual data co-ordinate or value is
         recorded.  If there is no NDF axis information and a logarithmic
         abscissa, the DATA co-ordinates are pixel co-ordinates.  The NDF
         associated with the plot is stored by reference with the DATA
         picture.  On exit the current database picture for the chosen
         device reverts to the input picture.

         \sstitem
         In a logarithmic plot only positive data along each
         logarithmic axis can be displayed, therefore non-positive data
         are excluded.  A logarithmic axis will always increase from
         left to right, and from bottom to top.

         \sstitem
         Bad pixels appear as gaps in the plot, and they do not affect
         the limits of the ordinate.  The same applies to zero or negative
         data values if the plot is to have a logarithmic ordinate.

         \sstitem
         On colour graphics devices the actual colours used for
         different portions of the plot may be adjusted using the PAL$\lsk$
         commands.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: INSPECT, LINPLOT; Figaro: ESPLOT, IPLOTS, MSPLOT, SPLOT;
      SPECDRE: SPECGRID.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, VARIANCE,
         QUALITY, LABEL, TITLE, and UNITS components of the NDF.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  Only
         single-precision floating-point data can be processed directly.
         Other non-complex data types will undergo a type conversion
         before the line plot is drawn.
      }
   }
}
\begin{htmlonly}
Click \htmladdnormallink{here}{mlinplot_exam.gif} to see an example
plot (9k).
\end{htmlonly}

\manroutine {{\manheadstyle{MOSAIC}}}{ Merges several non-congruent
  2-d data arrays into one output data array.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  Up to 20 non-congruent 2-d data arrays may be input, along with
  their relative offsets from the first data array, and these are
  then made into a mosaic into one (usually larger) output 2-d data
  array. Where the frames overlap, either the mean value or just the
  sum is inserted into the output data array. Normally averaging is
  performed. All data arrays are stored in {\mantt{IMAGE}} structures.

  The magic-value method is used for processing bad data.  Bad
  pixels are excluded from the averaging in overlap areas. Output
  pixels that have been mapped or correspond to one or more input
  arrays, yet have no good pixels contributing, are set to bad.
  Pixels in the output data array not mapped by any of the input
  arrays are set to zero.

\manroutineitem {Invocation }{}
  MOSAIC

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{NUMBER}}  }{{\mantt{\_INTEGER}}}
  Number of data arrays to be merged.
\manparameterentry {{\mantt{READ}} }{{\mantt{AVERAGE}}  }{{\mantt{\_LOGICAL}}}
  If true overlap regions are averaged, alternatively, they are
  summed.  \mbox{{\mantt [TRUE]}}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}{$n$}  }{{\mantt{IMAGE}}}
  {$n^{\rm th}$} {\mantt{IMAGE}} structure containing a data array to be a
  constituent of a mosaic.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure containing the merged data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title for the output {\mantt{IMAGE}} structure. \mbox{{\mantt ['KAPPA - Mosaic']}}
\end{manparametertable}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{XOFFSET}} }{{\mantt{\_INTEGER}}}
  {$x$} offset of {$n^{\rm th}$} data array from the first, in the sense of the
  {$x$} origin of the {$n^{\rm th}$} data array minus the {$x$} origin of the
  first.
\manparameterentry {{\mantt{READ}} }{{\mantt{YOFFSET}} }{{\mantt{\_INTEGER}}}
  {$y$} offset of {$n^{\rm th}$} data array from the first, in the sense of the
  {$y$} origin of the {$n^{\rm th}$} data array minus the {$y$} origin of the
  first.
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\manroutine {{\manheadstyle{MSTATS}}}{ Does cumulative statistics on
  a 2-d sub-array over a sequence of data arrays.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine is used for the statistics of multiple 2-d data
  arrays. The data arrays must have the same dimensions and reside
  in {\mantt{IMAGE}} structures. The user is asked to specify a number of
  data arrays (up to a fixed limit) either by naming each file
  or defining a sequence of {\mantt{IMAGE}} structures (frames). If the
  latter option is chosen the files must adopt the following naming scheme:
  {\mantt{groupnamennnn}}, where {\mantt{nnnn}} is a four-digit number,
  and {\mantt{groupname}} is the collective name for the set of arrays,
  {\it e.g.}\ {\mantt{ORION0001}}. Missing
  container files, data arrays, or data arrays of the wrong
  dimensions are skipped. The maximum number of data files is 1000.

  Then either Single pixel or Box mode is chosen. In the former
  case the pixel of interest is specified, and in the latter, the
  sub-array of interest. In the Single pixel mode, the value for the
  same pixel is pulled out of each array in sequence, and this
  sequence of values is then statistically analysed over the
  sequence of input frames. The resultant values (mean, median
  and standard deviation) are reported directly to the user.

  In Box mode, a choice of statistics is selected.  The alternatives
  are mean and standard deviation (the default), or median. The
  statistic(s) are formed over the sequence of arrays at each pixel
  position in the box. The output is in the form of one or two 2-d
  data arrays, each being the size of the defined sub-array and
  contains a chosen statistic (mean, standard deviation or median)
  in each pixel. Each output data array is stored in an {\mantt{IMAGE}}
  structure.

  The magic-value method is used for processing bad data.

\manroutineitem {Invocation }{}
  MSTATS

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INMODE}}  }{{\mantt{\_CHAR}}}
  Mode of data array input, the alternatives being {\mantt{Sequential}}
  or {\mantt{Random}}.
\manparameterentry {{\mantt{READ}} }{{\mantt{PIXMODE}}  }{{\mantt{\_CHAR}}}
  Mode of calculation, the alternatives being a {\mantt{Single}} pixel or
  a {\mantt{Box}} of pixels.
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  One of the sequence of input {\mantt{IMAGE}} structures.
\manparameterentry {{\mantt{READ}} }{{\mantt{FIRSTFILE}}  }{{\mantt{\_CHAR}}}
  Name of first container filename in the sequence.
\manparameterentry {{\mantt{READ}} }{{\mantt{NUMSEQ}}  }{{\mantt{\_INTEGER}}}
  Number of sequential frames to be processed.
\manparameterentry {{\mantt{READ}} }{{\mantt{NUMRAN}}  }{{\mantt{\_INTEGER}}}
  Number of random frames to be processed.
\manparameterentry {{\mantt{READ}} }{{\mantt{XPIX}}  }{{\mantt{\_INTEGER}}}
  {$x$} pixel index of the pixel to be used in Single mode.
\manparameterentry {{\mantt{READ}} }{{\mantt{YPIX}}  }{{\mantt{\_INTEGER}}}
  {$y$} pixel index of the pixel to be used in Single mode.
\manparameterentry {{\mantt{READ}} }{{\mantt{ORDRST}}  }{{\mantt{\_LOGICAL}}}
  If true ordered statistics will be computed in Box mode,
  currently only the median, otherwise the mean and standard
  deviation are derived. \mbox{{\mantt [FALSE]}}
\manparameterentry {{\mantt{READ}} }{{\mantt{XSTART}}  }{{\mantt{\_INTEGER}}}
  {$x$} start pixel index of the sub-arrays to be analysed.
\manparameterentry {{\mantt{READ}} }{{\mantt{YSTART}}  }{{\mantt{\_INTEGER}}}
  {$y$} start pixel index of the sub-arrays to be analysed.
\manparameterentry {{\mantt{READ}} }{{\mantt{XSIZE}}  }{{\mantt{\_INTEGER}}}
  {$x$} size of the sub-array to be analysed.
\manparameterentry {{\mantt{READ}} }{{\mantt{YSIZE}}  }{{\mantt{\_INTEGER}}}
  {$y$} size of the sub-array to be analysed.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{MEDIAN}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure containing array of medians for Box
  option.
\manparameterentry {{\mantt{READ}} }{{\mantt{OMTITLE}}  }{{\mantt{\_CHAR}}}
  Title string for {\mantt{IMAGE}} structure containing the median array.
  \mbox{{\mantt ['KAPPA - Mstats - Medians']}}
\manparameterentry {{\mantt{WRITE}} }{{\mantt{MEAN}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure containing array of means for Box
  option.
\manparameterentry {{\mantt{READ}} }{{\mantt{MTITLE}}  }{{\mantt{\_CHAR}}}
  Title string for {\mantt{IMAGE}} structure containing the mean array.
  \mbox{{\mantt ['KAPPA - Mstats - Means']}}
\manparameterentry {{\mantt{WRITE}} }{{\mantt{STDDEV}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure containing standard deviations for Box
  option.
\manparameterentry {{\mantt{READ}} }{{\mantt{STITLE}}  }{{\mantt{\_CHAR}}}
  Title string for {\mantt{IMAGE}} structure containing the
  standard-deviation array. \mbox{{\mantt ['KAPPA - Mstats - Stddevs']}}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\sstroutine{
   MULT
}{
   Multiplies two NDF data structures
}{
   \sstdescription{
      The routine multiplies two NDF data structures pixel-by-pixel to
      produce a new NDF.
   }
   \sstusage{
      mult in1 in2 out
   }
   \sstparameters{
      \sstsubsection{
         IN1 = NDF (Read)
      }{
         First NDF to be multiplied.
      }
      \sstsubsection{
         IN2 = NDF (Read)
      }{
         Second NDF to be multiplied.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF to contain the product of the two input NDFs.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the NDF supplied for parameter IN1 to be used
         instead. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         mult a b c
      }{
         This multiplies the NDF called a by the NDF called b, to make
         the NDF called c.  NDF c inherits its title from a.
      }
      \sstexamplesubsection{
         mult out=c in1=a in2=b title="Normalised spectrum"
      }{
         This multiplies the NDF called a by the NDF called b, to make
         the NDF called c.  NDF c has the title {\tt "Normalised spectrum"}.
      }
   }
   \sstnotes{
      If the two input NDFs have different pixel-index bounds, then
      they will be trimmed to match before being multiplied.  An error
      will result if they have no pixels in common.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ADD, CADD, CDIV, CMULT, CSUB, DIV, MATHS, SUB.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, HISTORY, and VARIANCE components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Units processing is not supported at present and therefore the
         UNITS component is not propagated.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.
         Calculations are performed using the most appropriate of the
         data types integer, real or double precision.  If the input NDF
         structures contain values with other data types, then conversion
         will be performed as necessary.

      }
   }
}
\sstroutine{
   NATIVE
}{
   Converts an HDS object to native machine data representation
}{
   \sstdescription{
      This application converts an HDS object (or structure) so that
      all primitive data values within it are represented using the
      appropriate native data representation for the machine in use
      (this includes the appropriate number format and byte ordering).
      This may typically be required after moving HDS files from
      another machine which uses a different number format and/or byte
      order, and will minimise the subsequent access time on the new
      machine.  Conversion is performed by modifying the data {\it in situ}.
      No separate output file is produced.
   }
   \sstusage{
      native object
   }
   \sstparameters{
      \sstsubsection{
         OBJECT = UNIVERSAL (Read and Write)
      }{
         The HDS structure to be converted; either an entire container
         file or a particular object or structure within the file may
         be specified. If a structure is given, all components (and
         sub-components, {\it etc.}) within it will also be converted.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         native myfile
      }{
         Converts all the primitive data in the HDS container file
         myfile to be held using the appropriate native machine
         representation for faster subsequent access.
      }
      \sstexamplesubsection{
         native yourfile.data\_array
      }{
         Converts just the DATA\_ARRAY component (and its contents, if a
         structure) in the container file yourfile to the appropriate
         native machine data representation. Other file contents remain
         unchanged.
      }
   }
}

\sstroutine{
   NDFCOPY
}{
   Copies an NDF (or NDF section) to a new location
}{
   \sstdescription{
      This application copies an NDF to a new location. By supplying an
      NDF section as input it may be used to extract a subset, or to
      change the size or dimensionality of an NDF. A second NDF may
      also be supplied to act as a shape template, and hence to define
      the region of the first NDF which is to be copied.

      Any unused space will be eliminated by the copying operation
      performed by this routine, so it may be used as a way of
      compressing NDF structures from which components have been
      deleted.  This ability also makes NDFCOPY a useful alternative to
      SETBOUND in cases where an NDF's size is to be reduced.
   }
   \sstusage{
      ndfcopy in out
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF (or section) which is to be copied.
      }
      \sstsubsection{
         LIKE = NDF (Read)
      }{
         This parameter may be used to supply an NDF to be used as a
         shape template during the copying operation. If such a
         template is supplied, then its shape will be used to select a
         matching section from the input NDF before copying takes
         place.  By default, no template will be used and the shape of
         the output NDF will therefore match that of the input NDF (or
         NDF section). {\tt [!]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The output NDF data structure.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         A title for the output NDF. A null value (the default) will
         cause the title of the NDF supplied for parameter IN to be
         used instead. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         ndfcopy infile outfile
      }{
         Copies the contents of the NDF structure infile to the new
         structure outfile. Any unused space will be eliminated during
         the copying operation.
      }
      \sstexamplesubsection{
         ndfcopy in=data1(3:40,-3:17) out=data2 title="Extracted section"
      }{
         Copies the section (3:40,-3:17) of the NDF called data1 to a
         new NDF called data2. The output NDF is assigned the new title
         {\tt "Extracted section"}, which replaces the title derived from the
         input NDF.
      }
      \sstexamplesubsection{
         ndfcopy galaxy newgalaxy like=oldgalaxy
      }{
         Copies a section of the NDF called galaxy to form a new NDF
         called newgalaxy. The section which is copied will correspond
         in shape with the template oldgalaxy. Thus, after the copying
         operation, both newgalaxy and oldgalaxy will have the same
         pixel-index bounds.
      }
      \sstexamplesubsection{
         ndfcopy aa(20$\sim$11,20$\sim$11) bb like=aa
      }{
         Copies from the NDF section consisting of an 11$\times$11-pixel
         region of aa centred on pixel (20,20), into a new NDF called bb.
         The shape of the region copied is made to match the original
         shape of aa.  The effect is to extract the selected square
         region of pixels into a new NDF of the same shape as the
         original, setting the surrounding region to the bad-pixel
         value.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: SETBOUND; Figaro: ISUBSET.
   }
   \sstimplementationstatus{
      If present, an NDF's TITLE, LABEL, UNITS, DATA, VARIANCE,
      QUALITY, AXIS and HISTORY components are copied by this routine,
      together with all extensions. The output NDF's title may be
      modified, if required, by specifying a new value via the TITLE
      parameter.
   }
}
\sstroutine{
   NDFTRACE
}{
   Displays the attributes of an NDF data structure
}{
   \sstdescription{
      This routine displays the attributes of an NDF data structure
      including:
      \ssthitemlist{

         \sstitem
         its name;

         \sstitem
         the values of its character components (title, label and
         units);

         \sstitem
         its shape (pixel bounds, dimension sizes, number of dimensions
         and total number of pixels);

         \sstitem
         axis co-ordinate information (axis labels, units and extents);

         \sstitem
         optionally, axis array attributes (type and storage form) and
         the values of the axis normalisation flags;

         \sstitem
         attributes of the main data array and any other array
         components present (including the type and storage form and an
         indication of whether `bad' pixels may be present);

         \sstitem
         a list of any NDF extensions present, together with their data
         types; and

         \sstitem
         history information (creation and last-updated dates, the
         update mode and the number of history records).

      }
      Most of this information is output to parameters.
   }
   \sstusage{
      ndftrace ndf
   }
   \sstparameters{
      \sstsubsection{
         FULLAXIS = \_LOGICAL (Read)
      }{
         If the NDF being examined has an axis co-ordinate system
         defined, then by default only the label, units and extent of
         each axis will be displayed.  However, if a {\tt TRUE} value is given
         for this parameter, full details of the attributes of all the
         axis arrays will also be given. {\tt [FALSE]}
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         The NDF data structure whose attributes are to be displayed.
      }
      \sstsubsection{
         QUIET = \_LOGICAL (Read)
      }{
         A {\tt TRUE} value suppresses the reporting of the NDF's attributes.
         It is intended for procedures and scripts where only the
         output parameters are needed. {\tt [FALSE]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         AEND( ) = \_DOUBLE (Write)
      }{
         The axis upper extents of the NDF.  For non-monotonic axes,
         zero is used.  See parameter AMONO.  This is not assigned if
         AXIS is {\tt FALSE}.
      }
      \sstsubsection{
         AFORM( ) = LITERAL (Write)
      }{
         The storage forms of the axis centres of the NDF.  This is
         only written when FULLAXIS is {\tt TRUE} and AXIS is {\tt TRUE}.
      }
      \sstsubsection{
         ALABEL( ) = LITERAL (Write)
      }{
         The axis labels of the NDF.  This is not assigned if AXIS is
         {\tt FALSE}.
      }
      \sstsubsection{
         AMONO( ) = \_LOGICAL (Write)
      }{
         These are {\tt TRUE} when the axis centres are monotonic, and {\tt FALSE}
         otherwise.  This is not assigned if AXIS is {\tt FALSE}.
      }
      \sstsubsection{
         ANORM( ) = \_LOGICAL (Write)
      }{
         The axis normalisation flags of the NDF.  This is only written
         when FULLAXIS is {\tt TRUE} and AXIS is {\tt TRUE}.
      }
      \sstsubsection{
         ASTART( ) = \_DOUBLE (Write)
      }{
         The axis lower extents of the NDF.  For non-monotonic axes,
         zero is used.  See parameter AMONO.  This is not assigned if
         AXIS is {\tt FALSE}.
      }
      \sstsubsection{
         ATYPE( ) = LITERAL (Write)
      }{
         The data types of the axis centres of the NDF.  This is only
         written when FULLAXIS is {\tt TRUE} and AXIS is {\tt TRUE}.
      }
      \sstsubsection{
         AUNITS( ) = LITERAL (Write)
      }{
         The axis units of the NDF.  This is not assigned if AXIS is
         {\tt FALSE}.
      }
      \sstsubsection{
         AVARIANCE( ) = \_LOGICAL (Write)
      }{
         Whether or not there are axis variance arrays present in the
         NDF.  This is only written when FULLAXIS is {\tt TRUE} and AXIS is
         {\tt TRUE}.
      }
      \sstsubsection{
         AXIS = \_LOGICAL (Write)
      }{
         Whether or not the NDF has an axis system.
      }
      \sstsubsection{
         BAD = \_LOGICAL (Write)
      }{
         If {\tt TRUE}, the NDF's data array may contain bad values.
      }
      \sstsubsection{
         BADBITS = LITERAL (Write)
      }{
         The BADBITS mask.  This is only valid when QUALITY is {\tt TRUE}.
      }
      \sstsubsection{
         DIMS( ) = \_INTEGER (Write)
      }{
         The dimensions of the NDF.
      }
      \sstsubsection{
         EXTNAME( ) = LITERAL (Write)
      }{
         The names of the extensions in the NDF.  It is only written
         when NEXTN is positive.
      }
      \sstsubsection{
         EXTTYPE( ) = LITERAL (Write)
      }{
         The types of the extensions in the NDF.  Their order
         corresponds to the names in EXTNAME.  It is only written when
         NEXTN is positive.
      }
      \sstsubsection{
         FORM = LITERAL (Write)
      }{
         The storage form of the NDF's data array.
      }
      \sstsubsection{
         HISTORY = \_LOGICAL (Write)
      }{
         Whether or not the NDF contains HISTORY records.
      }
      \sstsubsection{
         LABEL = LITERAL (Write)
      }{
         The label of the NDF.
      }
      \sstsubsection{
         LBOUND( ) = \_INTEGER (Write)
      }{
         The lower bounds of the NDF.
      }
      \sstsubsection{
         NDIM = \_INTEGER (Write)
      }{
         The number of dimensions of the NDF.
      }
      \sstsubsection{
         NEXTN = \_INTEGER (Write)
      }{
         The number of extensions in the NDF.
      }
      \sstsubsection{
         QUALITY = \_LOGICAL (Write)
      }{
         Whether or not the NDF contains a QUALITY array.
      }
      \sstsubsection{
         TITLE = LITERAL (Write)
      }{
         The title of the NDF.
      }
      \sstsubsection{
         TYPE = LITERAL (Write)
      }{
         The data type of the NDF's data array.
      }
      \sstsubsection{
         UBOUND( ) = \_INTEGER (Write)
      }{
         The upper bounds of the NDF.
      }
      \sstsubsection{
         UNITS = LITERAL (Write)
      }{
         The units of the NDF.
      }
      \sstsubsection{
         VARIANCE = \_LOGICAL (Write)
      }{
         Whether or not the NDF contains a VARIANCE array.
      }
      \sstsubsection{
         WIDTH( ) = \_LOGICAL (Write)
      }{
         Whether or not there are axis width arrays present in the NDF.
         This is only written when FULLAXIS is {\tt TRUE} and AXIS is {\tt TRUE}.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         ndftrace mydata
      }{
         Displays information about the attributes of the NDF structure
         called mydata.
      }
      \sstexamplesubsection{
         ndftrace ndf=r106 fullaxis
      }{
         Displays information about the NDF structure r106, including
         full details of any axis arrays present.
      }
      \sstexamplesubsection{
         ndftrace mydata quiet ndim=(mdim)
      }{
         Passes the number of dimensions of the NDF called mydata
         into the {\footnotesize ICL} variable mdim.  No information is displayed.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      HDSTRACE.
   }
}
\sstroutine{
   NOGLOBALS
}{
   Resets the KAPPA global parameters
}{
   \sstdescription{
      This application resets the KAPPA global parameters, and so makes
      their values undefined.
   }
   \sstusage{
      noglobals
   }
}
\sstroutine{
   NOMAGIC
}{
   Replaces all occurrences of magic value pixels in an NDF array
   with a new value
}{
   \sstdescription{
      This function replaces the standard `magic value' assigned to bad
      pixels in an NDF with an alternative value, or with random
      samples taken from a Normal distribution.  Input pixels which do
      not have the magic value are left unchanged.  The number of
      replacements is reported.  NOMAGIC's applications include the
      export of data to software that has different magic values or
      does not support bad values.

      If a constant value is used to replace magic values (which will
      be the case if parameter SIGMA is given the value zero), then the
      same replacement value is used for both the data and variance
      arrays when COMP={\tt "All"}.  If the variance is being processed, the
      replacement value is constrained to be non-negative.

      Magic values are replaced by random values if the parameter SIGMA
      is given a non-zero value. If both Data and Variance components
      are being processed, then the random values are only stored in
      the Data component; a constant value equal to SIGMA squared is
      used to replace all magic values in the variance component.  If
      only a single component is being processed (whether it be Data,
      Variance, or Error), then the random values are used to replace
      the magic values.  If random values are generated which will not
      fit into the allowed numeric range of the output NDF, then they
      are discarded and new random values are obtained instead.  This
      continues until a usable value is obtained. This could introduce
      some statistical bias if many such re-tries are performed.  For
      this reason SIGMA is restricted so that there are at least 4
      standard deviations between the mean (given by REPVAL) and the
      nearest limit.  NOMAGIC notifies of any re-tries that are
      required.
   }
   \sstusage{
      nomagic in out repval sigma [comp]
   }
   \sstparameters{
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The components whose flagged values are to be substituted.  It
         may be:

         \begin{itemize}
            \item {\tt "Data"}
            \item {\tt "Error"}
            \item {\tt "Variance"}
            \item {\tt "All"}
         \end{itemize}

         The last of the
         options forces substitution of bad pixels in both the data and
         variance arrays.  This parameter is ignored if the data array
         is the only array component within the NDF.  {\tt ["Data"]}
      }
      \sstsubsection{
         IN = NDF  (Read)
      }{
         Input NDF structure containing the data and/or variance array
         to have its elements flagged with the magic value replaced by
         another value.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF structure containing the data and/or variance array
         without any elements flagged with the magic value.
      }
      \sstsubsection{
         REPVAL = \_DOUBLE (Read)
      }{
         The constant value to substitute for the magic values, or (if
         parameter SIGMA is given a non-zero value) the mean of the
         distribution from which replacement values are obtained.  It
         must lie within the minimum and maximum values of the data
         type of the array with higher precision, except when variance
         is being processed, in which case the minimum is constrained
         to be non-negative.  The replacement value is converted to the
         data type of the array being converted.  The suggested default
         is the current value.
      }
      \sstsubsection{
         SIGMA = \_DOUBLE (Read)
      }{
         The standard deviation of the random values used to replace
         magic values in the input NDF.  If this is zero (or if a null
         value is given), then a constant replacement value is
         used.  The supplied value must be positive and must be small
         enough to allow at least 4 standard deviations between the
         mean value (given by REPVAL) and the closest limit. {\tt [!]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the input NDF to the output NDF.
         {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         nomagic aitoff irasmap repval=$-$2000000
      }{
         This copies the NDF called aitoff to the NDF irasmap, except
         that any bad values in the data array are replaced with the
         IPAC blank value, $-$2000000, in the NDF called irasmap.
      }
      \sstexamplesubsection{
         nomagic saturnb saturn 9999.0 comp=all
      }{
         This copies the NDF called saturnb to the NDF saturn, except
         that any bad values in the data and variance arrays are
         replaced with 9999 in the NDF called saturn.
      }
      \sstexamplesubsection{
         nomagic in=cleaned out=filled repval=0 sigma=10 comp=all
      }{
         This copies the NDF called cleaned to the NDF filled, except
         that any bad values in the data array are replaced by random
         samples taken from a Normal distribution of mean zero and
         standard deviation 10.  Bad values in the variance array are
         replaced by the constant value 100.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         If the NDF arrays have no bad pixels the application will
         abort.

         \sstitem
         Use GLITCH if a neighbourhood context is required to remove
         the bad values.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CHPIX, FILLBAD, GLITCH, SEGMENT, SETMAGIC, SUBSTITUTE,
      ZAPLIN; SPECDRE: GOODVAR.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         Any number of NDF dimensions is supported.
      }
   }
}
\sstroutine{
   NORMALIZE
}{
   Normalises one NDF to a similar NDF by calculating a scale factor
   and zero-point difference
}{
   \sstdescription{
      This application compares the data values in one NDF against the
      corresponding values in the other NDF. A least-squares
      straight-line is then fitted to the relationship between the two
      sets of data values in order to determine the relative scale
      factor and any zero-level offset between the NDFs.  To reduce
      computation time, the data points are binned according to the
      data value in the first NDF. The mean data value within each bin
      is used to find the fit and weights are applied according to the
      number of pixels which contribute to each bin.

      To guard against erroneous data values, which can corrupt the fit
      obtained, the application then performs a number of iterations. It
      calculates a noise estimate for each bin according to the rms
      deviation of data values in the bin from the straight-line fit
      obtained previously. It then re-bins the data, omitting values
      which lie more than a specified number of standard deviations
      from the expected value in each bin.  The straight-line fit is
      then re-calculated.  You can specify the number of standard
      deviations and the number of iterations used.

      A plot is produced after the final iteration showing the bin
      centres, with error bars representing the spread of values in each
      bin. This plot is produced within the current AGI picture and is
      of a size you specify.

      Optionally, an output NDF can be created containing a normalised
      version of the data array from the first input NDF.
   }
   \sstusage{
      normalize in1 in2 out
   }
   \sstparameters{
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         Determines if the graphics workstation is to be cleared before
         producing the plot. {\tt [TRUE]}
      }
      \sstsubsection{
         DATARANGE( 2 ) = \_REAL (Read)
      }{
         This parameter may be used to override the plot auto-scaling
         feature.  If given, two real numbers should be supplied
         specifying the lower and upper data values in IN2, between
         which data will be used.  If not given, the default is to use
         the auto-scaled values, calculated according to the value of
         PCRANGE. {\tt [,]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation on which to produce the plot.  If it
         is null, {\tt !}, there will be no plot made.
         {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         IN1 = NDF (Read)
      }{
         The NDF to be normalised.
      }
      \sstsubsection{
         IN2 = NDF (Read)
      }{
         The NDF to which IN1 will be normalised.
      }
      \sstsubsection{
         MINPIX = \_INTEGER (Read)
      }{
         The minimum number of good pixels required in a bin before it
         contributes to the fitted line.  It must be in the range 1 to
         the number of pixels per bin. {\tt [2]}
      }
      \sstsubsection{
         NBIN = \_INTEGER (Read)
      }{
         The number of bins to use when binning the scatter plot prior
         to fitting a straight line, in the range 2 to 10000. {\tt [50]}
      }
      \sstsubsection{
         NITER = \_INTEGER (Read)
      }{
         The number of iterations performed to reject bad data values
         in the range 0 to 100. {\tt [2]}
      }
      \sstsubsection{
         NSIGMA = \_REAL (Read)
      }{
         The number of standard deviations at which bad data is
         rejected.  It must lie in the range 0.1 to 1.0E6. {\tt [3.0]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         An optional output NDF to hold a version of IN1 which is
         normalised to IN2.  A null ({\tt !}) value indicates that an output
         NDF is not required.
      }
      \sstsubsection{
         PCRANGE( 2 ) = \_REAL (Read)
      }{
         This parameter takes two real values in the range 0 to 100 and
         is used to modify the action of the auto-scaling algorithm
         which scales the plots. The two values correspond to the
         percentage points in the histogram of IN2 at which the lower
         and upper cuts on data value are placed. With the default
         value, the plots will omit those pixels which lie in the lower
         and upper 2 percent intensity range of IN2. {\tt [2,98]}
      }
      \sstsubsection{
         PXSIZE = \_REAL (Read)
      }{
         The horizontal size of the plot in metres. If a value less
         than the default is requested, then the plot will appear at
         the bottom left of the current picture. {\tt [}The size of the
         current picture{\tt ]}
      }
      \sstsubsection{
         PYSIZE = \_REAL (Read)
      }{
         The vertical size of the plot in metres. If a value less than
         the default is requested, then the plot will appear at the
         bottom left of the current picture. {\tt [}The size of the current
         picture{\tt ]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the NDF supplied for parameter IN1 to be used
         instead.  {\tt [!]}
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         ABSLAB = LITERAL (Read)
      }{
         A title for the plot's $x$ axis. Only the first 50 characters
         are used. The default is {\tt "Data value in $\wedge$NDF"} where $\wedge$NDF is
         replaced by the name of the NDF associated with IN2. {\tt []}
      }
      \sstsubsection{
         FONT = LITERAL (Read)
      }{
         The fount to be used for the line graphics.  It can be either
         {\tt "NCAR"} for the NCAR fancy characters and {\tt "GKS"} for the standard
         GKS san-serif fount.   The former is intended for hardcopy
         publication-quality plots, since it is relatively slow; the
         latter is intended for normal interactive graphics requiring
         rapid plotting, and it is clearer on small plots. The
         suggested default is the current value. {\tt ["GKS"]}
      }
      \sstsubsection{
         MAJTIC( 2 ) = \_REAL (Read)
      }{
         The parameter controlling the numbers of major tick marks
         for the $x$ and $y$ axes.  (Number used is between MAJTIC$+$2 and
         5$*$MAJTIC/2$+$4.) {\tt [3.,3.]}
      }
      \sstsubsection{
         MINTIC( 2 ) = \_REAL (Read)
      }{
         The number of minor tick marks between each major tick mark
         for the $x$ and $y$ axes.  A negative value forces the graphics
         package to compute appropriate values. {\tt [-1.,-1.]}
      }
      \sstsubsection{
         ORDLAB = LITERAL (Read)
      }{
         A title for the plots $y$ axis. Only the first 50 characters
         are used. The default is {\tt "Data value in $\wedge$NDF"} where
         $\wedge$NDF is replaced by the name of the NDF associated with IN1.
         {\tt []}
      }
      \sstsubsection{
         OUTTIC = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the axis tick marks are to appear on the outside of
         the axes instead of inside. {\tt [FALSE]}
      }
      \sstsubsection{
         PTITLE = LITERAL (Read)
      }{
         A title for the top of the plot. Only the first 50 characters
         are used. {\tt ["Normalization plot"]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         OFFSET = \_REAL (Write)
      }{
         An output parameter giving the offset in the linear
         normalisation expression: IN1 = SLOPE $*$ IN2 $+$ OFFSET.
      }
      \sstsubsection{
         SLOPE = \_REAL (Write)
      }{
         An output parameter giving the slope of the linear
         normalisation expression: IN1 = SLOPE $*$ IN2 $+$ OFFSET.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         normalize cl123a cl123b cl123c
      }{
         This normalises NDF cl123a to the NDF cl123b.  A plot of the
         fit is made on the current graphics device, and the resulting
         normalisation scale and offset are written only to the
         {\tt normalize.sdf} parameter file (as in the all the examples
         below except where noted).  The NDF cl123c is the normalised
         version of the input cl123a.
      }
      \sstexamplesubsection{
         normalize cl123a cl123b cl123c TITLE="Gain calibration"
      }{
         This normalises NDF cl123a to the NDF cl123b.  A plot of the
         fit is made on the current graphics device with the title
         {\tt "Gain calibration"}.  The NDF cl123c is the normalised
         version of the input cl123a. 
      }
      \sstexamplesubsection{
         normalize cl123a cl123b cl123c offset=(shift) slope=(scale)
      }{
         This normalises NDF cl123a to the NDF cl123b.  A plot of the
         fit is made on the current graphics device.  The resulting
         normalisation scale and offset are written to the {\footnotesize ICL}
         variables SCALE and SHIFT respectively, where they could be
         passed to another application via an {\footnotesize ICL} procedure.
         The NDF cl123c is the normalised version of the input cl123a.
      }
      \sstexamplesubsection{
         normalize in2=old in1=new out=! device=xwindows
      }{
         This normalises NDF new to the NDF old.  A plot of the fit is
         made on the xwindows device.  No output NDF is produced.
      }
      \sstexamplesubsection{
         normalize in1=new in2=old out=young niter=5 pcrange=[3,98.5]
      }{
         This normalises NDF new to the NDF old.  It has five iterations
         to reject outliers from the linear regression, and forms the
         regression using pixels in old whose data values lie between
         the 3 and 98.5 percentiles, comparing with the corresponding
         pixels in new.  A plot of the fit is made on the current
         graphics device.  The NDF young is the normalised version of
         the input new.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Provided the application does not fail two pictures are stored
         in the graphics database: a FRAME of the specified size containing
         the title, annotated axes and the plot; and a DATA picture with
         each world co-ordinate being the pixel value of each NDF.  Both
         pictures have comment {\tt "}KAPPA - Normalize{\tt "}.  The associated NDFs
         are not stored in the database.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      CCDPACK: MAKEMOS.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         The routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF,
         and propagates all extensions to the output NDF.  All propagated
         components come from the NDF to be normalised.

         \sstitem
         At the moment, variance values are not used in the fitting
         algorithm but are modified in the output NDF to take account of
         the scaling introduced by the normalisation. (A later version may
         take account of variances in the fitting algorithm.)

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         Only \_REAL data can be processed directly.  Other non-complex
         numeric data types will undergo a type conversion before processing
         occurs. \_DOUBLE data cannot be processed due to a loss of precision.

         \sstitem
         The pixel bounds of the two input NDFs are matched by trimming
         before calculating the normalisation constants, and are mapped as
         vectors to allow processing of NDFs of any dimensionality. An
         output NDF may optionally be produced which is based on the
         first input NDF (IN1) by applying the calculated normalisation
         constants to IN1.
      }
   }
}
\sstroutine{
   NUMB
}{
   Counts the number of elements of an NDF with values or absolute
   values above or below a threshold
}{
   \sstdescription{
      This routine counts and reports the number of elements of an
      array within an input NDF structure that have a value or absolute
      value greater or less than a specified threshold.  This statistic
      is also shown as a percentage of the total number of array
      elements.
   }
   \sstusage{
      numb in value [comp]
   }
   \sstparameters{
      \sstsubsection{
         ABS = \_LOGICAL (Read)
      }{
         If ABS = {\tt TRUE}, the criterion is a comparison of the absolute value
         with the threshold; if {\tt FALSE}, the criterion is a comparison of
         the actual value with the threshold.  The current value is the
         suggested default. {\tt [FALSE]}
      }
      \sstsubsection{
         ABOVE = \_LOGICAL (Read)
      }{
         If ABOVE = {\tt TRUE} the criterion tests whether values are greater
         than the threshold; if {\tt FALSE} the criterion tests whether values
         are less than the threshold.  The current value of ABOVE is the
         suggested default. {\tt [TRUE]}
      }
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The components whose flagged values are to be substituted.
         It may be {\tt "Data"}, {\tt "Error"}, {\tt "Variance"}, or
         {\tt "Quality"}.  If {\tt "Quality"} is specified, then the quality values are treated as
         numerical values in the range 0 to 255. {\tt ["Data"]}
      }
      \sstsubsection{
         IN  = NDF (Read)
      }{
         Input NDF structure containing the array to be tested.
      }
      \sstsubsection{
         VALUE  = \_DOUBLE (Read)
      }{
         Threshold against which the values of the array elements will
         be tested.  It must lie in within the minimum and maximum
         values of the data type of the array being processed, unless
         ABS = {\tt TRUE} or the component is the variance or quality
         array, in which case the minimum is zero.  The suggested
         default is the current value.
      }
   }
   \sstresparameters{
      \sstsubsection{
         NUMBER = \_INTEGER (Write)
      }{
         The number of elements that satisfied the criterion.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         numb image 100
      }{
         This counts the number of elements in the data array of the NDF
         called image that exceed 100.
      }
      \sstexamplesubsection{
         numb spectrum 100 noabove
      }{
         This counts the number of elements in the data array of the NDF
         called spectrum that are less than 100.
      }
      \sstexamplesubsection{
         numb cube 100 abs
      }{
         This counts the number of elements in the data array of the NDF
         called cube whose absolute values exceed 100.
      }
      \sstexamplesubsection{
         numb image $-$100 number=(count)
      }{
         This counts the number of elements in the data array of the NDF
         called image that exceed $-$100 and write the number to
         {\footnotesize ICL} variable COUNT.
      }
      \sstexamplesubsection{
         numb image 200 v
      }{
         This counts the number of elements in the variance array of
         the NDF called image that exceed 200.
      }
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the DATA, QUALITY,
         TITLE, and VARIANCE components of an NDF data structure.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         Any number of NDF dimensions is supported.
      }
   }
}
\manroutine {{\manheadstyle{OUTSET}}}{ Sets pixels outside a specified
  circle in a 2-d data array to a specified value.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  A circle of a given centre and diameter within the 2-d data array
  of the input {\mantt{IMAGE}} structure is specified.  The data array
  written to the output {\mantt{IMAGE}} structure, is a copy of the input
  data array except pixels outside the circle are set to a specified value.

\manroutineitem {Invocation }{}
  OUTSET

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  {\mantt{IMAGE}} structure containing the array to be modified.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure containing the modified array.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title for the output {\mantt{IMAGE}} structure.
  \mbox{{\mantt ['KAPPA - Outset']}}
\manparameterentry {{\mantt{READ}} }{{\mantt{XCENTRE}}  }{{\mantt{\_REAL}}}
  {$x$} co-ordinate of the centre of the circle to be used.
\manparameterentry {{\mantt{READ}} }{{\mantt{YCENTRE}}  }{{\mantt{\_REAL}}}
  {$y$} co-ordinate of the centre of the circle to be used.
\manparameterentry {{\mantt{READ}} }{{\mantt{DIAMETER}}  }{{\mantt{\_REAL}}}
  Diameter of the circle to be used.
\manparameterentry {{\mantt{READ}} }{{\mantt{NEWVAL}}  }{{\mantt{\_REAL}}}
  Value to replace old values in the pixels outside the circle.
  If this is set to 'Bad' the magic-value is used.
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\sstroutine{
   OVCLEAR
}{
   Clears an image-display overlay
}{
   \sstdescription{
      This application clears an overlay device, but without purging
      the graphics-database entries for the device.  Optionally, only
      the current picture is cleared.
   }
   \sstusage{
      ovclear [device] [current]
   }
   \sstparameters{
      \sstsubsection{
         CURRENT = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, then only the current picture is cleared. {\tt [FALSE]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device to be cleared.  It must be in GNS classes
         IMAGE\_OVERLAY or WINDOW\_OVERLAY.
         {\tt [}Current image-display-overlay device{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         ovclear
      }{
         Clears the current image-overlay device.
      }
      \sstexamplesubsection{
         ovclear current
      }{
         Clears the current picture on the current image-overlay device.
      }
      \sstexamplesubsection{
         ovclear xoverlay
      }{
         Clears the xoverlay device.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: GDCLEAR, IDCLEAR, OVSET.
   }
}
 
\sstroutine{
   OVSET
}{
   Selects a current image-display overlay
}{
   \sstdescription{
      This application selects a current image-display overlay. This
      device will be used for all applications requiring an
      image-display overlay until changed explicitly.
   }
   \sstusage{
      ovset device
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The image-display overlay to become the current image-display
         overlay device.  The device must be in GNS categories
         IMAGE\_OVERLAY or WINDOW\_OVERLAY.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         ovset xov
      }{
         Makes the xov device the current image-display overlay.
      }
   }
}
 
\sstroutine{
   PALDEF
}{
   Loads the default palette to a colour table
}{
   \sstdescription{
      This application loads the standard palette of colours to fill
      the portion of the current image display's colour table which is
      reserved for the palette.  The palette comprises 16 colours and
      is intended to provide coloured annotations, borders, axes,
      graphs {\it etc.}\ that are unaffected by changes to the lookup table
      used for images.

      The standard palette is as follows
        0: Black     1: White     2: Red       3: Green     4: Blue
        5: Yellow    6: Magenta   7: Cyan      8--15: Black
   }
   \sstusage{
      paldef [device]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display to be used.  The device must be in
         one of the following GNS categories: IMAGE\_DISPLAY,
         IMAGE\_OVERLAY, MATRIX\_PRINTER or WINDOW, and have at least 24
         colour indices. {\tt [}Current image-display device{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         paldef
      }{
         This loads the standard palette into the reserved portion of
         the colour table of the current image display.
      }
      \sstexamplesubsection{
         paldef xwindows
      }{
         This loads the standard palette into the reserved portion of
         the colour table of the xwindows device.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PALENTRY, PALREAD, PALSAVE.
   }
}
 
\sstroutine{
   PALENTRY
}{
   Enters a colour into an image display's palette
}{
   \sstdescription{
      This application obtains a colour and enters it into the palette
      portion of the current image display's colour table.  The palette
      comprises up to 16 colours and is intended to provide coloured
      annotations, borders, axes, graphs {\it etc.}\ that are unaffected by
      changes to the lookup table used for images.

      A colour is specified either by the giving the red, green, blue
      intensities; or named colours.
   }
   \sstusage{
      palentry palnum colour [device]
   }
   \sstparameters{
      \sstsubsection{
         COLOUR() = LITERAL (Read)
      }{
         A colour to be added to the palette at the entry given by
         parameter PALNUM.  It is either:

           o  A named colour from the standard colour set, which may
           be abbreviated.  If the abbreviated name is ambiguous the
           first match (in alphabetical order) is selected.  The case
           of the name is ignored.  Some examples are {\tt "Pink"},
           {\tt "Yellow"}, {\tt "Aquamarine}", and {\tt "Orchid"}.

           o  Normalised red, green, and blue intensities separated by
           commas or spaces.  Each value must lie in the range 0.0--1.0.
           For example, {\tt "1.0,1.0,0.5"} would give a pale yellow.
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display to be used.  The device must be in
         one of the following GNS categories: IMAGE\_DISPLAY,
         IMAGE\_OVERLAY, WINDOW, WINDOW\_OVERLAY, or MATRIX\_PRINTER and
         have at least 2 colour indices. {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         PALNUM = \_INTEGER (Read)
      }{
         The number of the palette entry whose colour is to be
         modified.  PALNUM must lie in the range zero to the minimum
         of 15 or the number of colour indices minus one.  The
         suggested default is 1.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         palentry 5 gold
      }{
         This makes palette entry number 5 have the colour gold in the
         reserved portion of the colour table of the current image
         display.
      }
      \sstexamplesubsection{
         palentry 12 [1.0,1.0,0.3] xwindows
      }{
         This makes the xwindows device's palette entry number 12 have a
         pale-yellow colour.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PALDEF, PALREAD, PALSAVE.
   }
}
 
\sstroutine{
   PALREAD
}{
   Fills the palette of a colour table from an NDF
}{
   \sstdescription{
      This application reads a palette of colours from an NDF, stored as
      red, green and blue intensities, to fill the portion of
      the current image display's colour table which is reserved for
      the palette.  The palette comprises 16 colours and is intended
      to provide coloured annotations, borders, axes, graphs {\it etc.}\ that
      are unaffected by changes to the lookup table used for images.
   }
   \sstusage{
      palread palette [device]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display to be used.  The device must be in
         one of the following GNS categories: IMAGE\_DISPLAY,
         IMAGE\_OVERLAY, or WINDOW, and have at least 24 colour indices.
         The device must also not reset when the device is opened
         (since the existing colour table would be lost).  {\tt [}Current
         image-display device{\tt ]}
      }
      \sstsubsection{
         PALETTE = NDF (Read)
      }{
         The name of the NDF containing the palette of reserved colours
         as its data array.  The palette must be 2-dimensional, the
         first dimension being 3, and the second 16.  If the second
         dimension is greater than 16 only the first 16 colours are
         used; if it has less than 16 just fill as much of the palette
         as is possible starting from the first colour.  The palette's
         values must lie in the range 0.0--1.0.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         palread rustic
      }{
         This loads the palette stored in the NDF called rustic into
         the reserved portion of the colour table of the current
         image display.
      }
      \sstexamplesubsection{
         palread rustic xwindows
      }{
         This loads the palette stored in the NDF called rustic into
         the reserved portion of the colour table of the xwindows
         device.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PALDEF, PALENTRY, PALSAVE.
   }
}
 
\sstroutine{
   PALSAVE
}{
   Saves the current palette of a colour table to an NDF
}{
   \sstdescription{
      This application reads the palette portion of the current image
      display's colour table and saves it in an NDF.  The palette
      comprises 16 colours and is intended to provide coloured
      annotations, borders, axes, graphs {\it etc.}\ that are unaffected by
      changes to the lookup table used for images.  Thus once you have
      established a palette of colours you prefer, it is straightforward
      to recover the palette at a future time.
   }
   \sstusage{
      palsave palette [device] [title]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the image display to be used.  The device must be in
         one of the following GNS categories: IMAGE\_DISPLAY,
         IMAGE\_OVERLAY, or WINDOW, and have at least 24 colour indices.
         The device must also not reset when the device is opened
         (since the existing colour table would be lost).  {\tt [}Current
         image-display device{\tt ]}
      }
      \sstsubsection{
         PALETTE = NDF (Write)
      }{
         The NDF in which the current colour-table reserved pens are
         to be stored.  Thus if you have created non-standard colours
         for annotation, doodling, colour of axes {\it etc.}\ they may be
         stored for future use.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF. {\tt ["KAPPA - Palsave"]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         palsave rustic
      }{
         This saves the palette of the colour table of the current
         image display into the NDF called rustic.
      }
      \sstexamplesubsection{
         palsave hitec xwindows title="Hi-tech-look palette"
      }{
         This saves the palette of the colour table of the xwindows
         device in the NDF called hitec.  The NDF has a title called
         {\tt "Hi-tech-look palette"}.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PALDEF, PALENTRY, PALREAD.
   }
}
\newpage
\sstroutine{
   PARGET
}{
   Obtains the value or values of an application parameter
}{
   \sstdescription{
      This application reports the value or values of a parameter from
      a named task.  It does this by searching the parameter file of
      the task.  The purpose is to offer an easier-to-use interface for
      passing values (especially output parameters) between tasks in
      shell scripts.  The values are formatted in lines with as many
      values as can be accommodated across the screen up to a maximum of
      132 characters; values are space separated.  However, in scripts
      the values are likely to be written to a script variable.  Thus
      for example in the C-shell:

         \hspace{2.1em}{\tt set med = `parget median histat`}

      would redirect the output to shell variable med, and thus a
      reference to \$med would substitute the median value obtained the
      last time application HISTAT was invoked.  If the parameter
      comprises a vector of values these can be stored in a C-shell
      array.  For instance,

         \hspace{2.1em}{\tt set perval = `parget perval histat`}

      would assign elements of the shell array perval[1], perval[2],
      {\em etc.}\ to the last-computed percentile values of HISTAT.
   }
   \sstusage{
      parget parname applic
   }
   \sstparameters{
      \sstsubsection{
         APPLIC = LITERAL (Read)
      }{
         The name of the application from which the parameter comes.
      }
      \sstsubsection{
         PARNAME = LITERAL (Read)
      }{
         The parameter whose value or values are to be reported.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         parget mean stats
      }{
         Report the value of parameter MEAN for the application STATS.
      }
      \sstexamplesubsection{
         parget mincoord $\backslash$
      }{
         This reports the values of parameter MINCOORD of the current
         application, in this case STATS.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The parameter file is located in the {\tt \$ADAM\_USER} directory, if
         defined, otherwise in the {\tt adam} subdirectory of {\tt \$HOME}.  If it
         cannot be located there, the task reports an error.

         \sstitem
         The parameter must exist in the selected application parameter
         file and not be a structure, except one of type ADAM\_PARNAME.

         \sstitem
         This task is not designed for use with {\footnotesize ICL},
         where passing parameter values is quite straightforward.  It
         does not operate with monolith parameter files.
      }
   }
}

\sstroutine{
   PASTE
}{
   Pastes a series of NDFs upon each other
}{
   \sstdescription{
      This application copies a series of NDFs, in the order supplied
      and taking account of origin information, on to a `base' NDF to
      produce an output NDF.  The output NDF is therefore a copy of the
      base NDF obscured wholly or partially by the other input NDFs.
      This operation is analogous to pasting in publishing.  It is
      intended for image editing and the creation of insets.

      The dimensions of the NDFs may be different, and indeed so may
      their dimensionalities.  The output NDF can be constrained to
      have the dimensions of the base NDF, so the pasted NDFs are
      clipped.  Normally, the output NDF will have dimensions such
      that all the input NDFs are accommodated in full.

      Bad values in the pasted NDFs are by default transparent, so the
      underlying data are not replaced during the copying.
   }
   \sstusage{
      paste in p1 [p2] ... [p25] out=?
   }
   \sstparameters{
      \sstsubsection{
         CONFINE = \_LOGICAL (Read)
      }{
         This parameter controls the dimensions of the output NDF.  If
         CONFINE is {\tt FALSE} the output NDF just accommodates all the input
         NDFs.  If CONFINE is {\tt TRUE}, the output NDF's dimensions matches
         those of the base NDF. {\tt [FALSE]}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The base NDF on to which the other input NDFs will be pasted.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The NDF resulting from pasting of the input NDFs onto the base
         NDF.  Its dimensions may be different from the base NDF.  See
         parameter CONFINE.
      }
      \sstsubsection{
         P1-P25 = NDF (Read)
      }{
         The NDFs to be pasted on to the base NDF.  The NDFs are pasted
         in the order P1, P2, ... P25.  There can be no missing NDFs,
         {\it e.g.}\ in order for P3 to be processed there must be a P2 given
         as well.  A null value ({\tt !}) indicates that there is no NDF.
         NDFs P2 to P25 are defaulted to {\tt !}.  At least one NDF must be
         pasted, therefore P1 may not be null.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the base NDF to the output NDF.
         {\tt [!]}
      }
      \sstsubsection{
         TRANSP = \_LOGICAL (Read)
      }{
         If TRANSP is {\tt TRUE}, bad values within the pasted NDFs are not
         copied to the output NDF as the bad values were transparent.
         If TRANSP is {\tt FALSE}, all values are copied during the paste
         and a bad value will obscured an underlying value.  {\tt [TRUE]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         paste aa inset out=bb
      }{
         This pastes the NDF called inset on to the arrays in the NDF
         called aa to produce the NDF bb.  Bad values are transparent.
         The bounds and dimensionality of bb may be larger than those of
         aa.
      }
      \sstexamplesubsection{
         paste aa inset out=bb notransp
      }{
         As above except that bad values are copied from the NDF inset
         to NDF bb.
      }
      \sstexamplesubsection{
         paste aa inset out=bb confine
      }{
         As the first example except that the bounds of NDF bb match
         those of NDF aa.
      }
      \sstexamplesubsection{
         paste ccd fudge inset out=ccdc
      }{
         This pastes the NDF called fudge, followed by NDF inset on to
         the arrays in the NDF called ccd to produce the NDF ccdc.  Bad
         values are transparent.  The bounds and dimensionality of ccd
         may be larger than those of ccdc.
      }
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY, components of an NDF
         data structure and propagates all extensions.  Propagation is from
         the base NDF.
 
         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.
 
         \sstitem
         All non-complex numeric data types can be handled.
 
         \sstitem
         Any number of NDF dimensions is supported.
       }
   }
}

\sstroutine{
   PICBASE
}{
   Selects the BASE picture from the graphics database.
}{
   \sstdescription{
      This command selects the BASE picture.  Subsequent plotting for
      the chosen device will be in this new current picture.  The BASE
      picture is the largest picture available, and encompasses all
      other pictures.  By default the chosen device is the current one.

      This command is a synonym for {\tt piclist picnum=1}.
   }
   \sstusage{
      picbase
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation. {\tt [}The current graphics device{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         picbase
      }{
         This selects the BASE picture for the current graphics device.
      }
      \sstexamplesubsection{
         picbase device=x2w
      }{
         This selects the BASE picture for the x2w device.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICCUR, PICDATA, PICFRAME, PICLAST, PICLIST, PICSEL.
   }
}
 
\sstroutine{
   PICCUR
}{
   Uses a cursor to select the current picture and to report the
   co-ordinates of points.
}{
   \sstdescription{
      This application allows you to select a new current picture in
      the graphics database using the cursor.  The picture associated
      with the last-selected point becomes the new current picture.

      This task also uses the cursor to read Cartesian co-ordinates
      from the chosen graphics device and displays them on your
      terminal.  In addition if the co-ordinate frame changes between
      selected positions, the comment, name, and any label associated
      with the new picture are appended to the message.  There is an
      option to let you store the co-ordinates, and their picture names
      and labels in a text file.

      There are three modes of operation to define which co-ordinate
      system/picture is to be used.  These are ANCHOR, CURRENT and
      DYNAMIC.  See the parameter MODE for details.

      In ANCHOR or DYNAMIC modes there is an option to select only
      pictures of a certain name in the database.  This is most useful
      when DATA pictures are covered by transparent FRAME pictures.
   }
   \sstusage{
      piccur [mode] [name] [logfile] [device]
   }
   \sstparameters{
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either
         {\tt "World"} or {\tt "Data"}.  {\tt "World"} makes the world
         co-ordinates of the cursor position to be reported.  World
         co-ordinates that relate to a location in a data array will be
         in array pixels.  If COSYS = {\tt "Data"} the graphics database
         is examined for data co-ordinates stored via a transformation. 
         Data co-ordinates are arbitrary but most often they will be a
         linear or logarithmic transformation of the world co-ordinates.
         For example, the $x$ co-ordinate of a spectrum would be given in
         pixels if COSYS = {\tt "World"}, but if COSYS = {\tt "Data"} the
         $x$ co-ordinate could be in wavelength units, such as
         {\AA}ngstroms.  If the database does not
         have a world-to-data transformation for a given picture, the
         value of this parameter is irrelevant and world co-ordinates
         will be reported for that picture. {\tt [}Current
         co-ordinate system{\tt ]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation. {\tt [}The current graphics device{\tt ]}
      }
      \sstsubsection{
         DOUBLE = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, co-ordinates will be reported, written to the output
         parameters, and stored in the text file in double precision,
         otherwise single precision is used.  {\tt [FALSE]}
      }
      \sstsubsection{
         LOGFILE = FILENAME (Write)
      }{
         The name of the text file in which the co-ordinates of points
         selected with the cursor may be stored.  A null string ({\tt !})
         means that no file is created. The suggested default is the
         current value. {\tt [!]}
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The mode defining the co-ordinate system/picture in which
         cursor positions are returned. There are three options.
         \begin{description}
            \item {\tt "Current"} selects the current picture in the AGI
            database and reports the position of a point selected by
            the cursor. If the point does not lie within the picture,
            an extrapolated position is reported.

            \item {\tt "Dynamic"} selects the topmost picture in the AGI
            database which encompasses that position selected. Thus
            the second and subsequent cursor hits may result in the
            selection of a new picture. On exit the last picture
            selected becomes the current picture.

            \item {\tt "Anchor"} lets the first cursor hit select a
            picture which remains current throughout the running of
            the application. If subsequent cursor hits fall outside
            the extent of this picture, a position extrapolated from
            the picture's co-ordinate system is reported. On exit the
            anchor picture becomes the current picture.
         \end{description}
         {\tt ["Dynamic"]}
      }
      \sstsubsection{
         NAME = LITERAL (Read)
      }{
         Only pictures of this name are to be selected.  A null string
         ({\tt !}) or blanks means that pictures of all names may be selected.
         {\tt [!]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         XC = \_DOUBLE (Write)
      }{
         The $x$ co-ordinate of the last point selected with the cursor.
      }
      \sstsubsection{
         YC = \_DOUBLE (Write)
      }{
         The $y$ co-ordinate of the last point selected with the cursor.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         piccur
      }{
         This obtains the co-ordinates of any visible picture for the
         current graphics device by use of the cursor.  In this and all
         the examples, the picture containing the last-selected point
         becomes the new picture.
      }
      \sstexamplesubsection{
         piccur cosys=w
      }{
         This obtains the world co-ordinates of any visible picture for
         the current graphics device by use of the cursor.
      }
      \sstexamplesubsection{
         piccur current device=graphon
      }{
         This obtains the co-ordinates of any visible picture in the
         reference frame of the current picture of the Graphon device.
      }
      \sstexamplesubsection{
         piccur logfile=stars.dat name=data
      }{
         This obtains the co-ordinates of any visible DATA picture
         the current graphics device.  The $x$-$y$ co-ordinates, and their
         picture names and labels are stored in the text file called
         {\tt stars.dat}.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Should an error occur trying to obtain the base picture for
         ANCHOR or DYNAMIC modes, the current picture is unchanged.

         \sstitem
         In DYNAMIC and ANCHOR modes, if the cursor is situated at a
         point where there are no pictures of the selected name, the
         co-ordinates in the base picture are reported.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CURSOR, PICBASE, PICDATA, PICEMPTY, PICENTIRE, PICFRAME,
      PICLIST, PICSEL, PICVIS.
   }
}
\sstroutine{
   PICDATA
}{
   Selects the last DATA picture from the graphics database.
}{
   \sstdescription{
      This command selects the last-created DATA picture.  Subsequent
      plotting for the chosen device will be in this new current
      picture.  By default the chosen device is the current one.

      This command is a synonym for {\tt piclist name=data picnum=last}.
   }
   \sstusage{
      picdata
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation. {\tt [}The current graphics device{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         picdata
      }{
         This selects the last DATA picture for the current graphics
         device.
      }
      \sstexamplesubsection{
         picdata device=xw
      }{
         This selects the last DATA picture for the xw device.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICCUR, PICBASE, PICFRAME, PICLAST, PICLIST, PICSEL.
   }
}
 
\sstroutine{
   PICDEF
}{
   Defines a new graphics-database FRAME picture or an array of FRAME pictures
}{
   \sstdescription{
      This application creates either one new graphics-database FRAME picture
      or a grid of new FRAME pictures.  It offers a variety of ways by which
      you can define a new picture's location and extent.  You may
      constrain a new picture to lie within either the current or the
      BASE picture, and the new picture adopts the world co-ordinate
      system of that reference picture.

      You may specify a single new picture using one of three methods:
      \begin{enumerate}
      \item moving a cursor to define the lower and upper bounds via
            pressing choice number 1 (the application will instruct what
            to do for the specific graphics device), provided a cursor
            is available on the chosen graphics workstation;
      \item obtaining the bounds from the environment (in world
            co-ordinates of the reference picture);
      \item or by giving a position code for the new picture, and
            specifying its linear fractional size along each axis in
            terms of the reference picture, and/or its aspect ratio.
            The position code comprises two characters.  The first
            controls the vertical location, and may be {\tt T}, {\tt B},
            or {\tt C} to create the new picture at the top, bottom, or in
            the centre respectively.  The second defines the horizontal
            situation, and may be {\tt L}, {\tt R}, or {\tt C} to define a
            new picture to the left, right, or in the centre respectively.
            Thus a code of {\tt BR} will make a new picture in the
            bottom-right corner.
      \end{enumerate}

      The picture created becomes the current picture on exit.

      Alternatively, you can create an array of $n$-by-$m$ equal-sized
      pictures by giving the number of pictures in the horizontal and
      vertical directions.  These may or may not be abutted.  For easy
      reference in later processing the pictures may be labelled
      automatically.  The label consists of a prefix you define,
      followed by the number of the picture.  The numbering starts at a
      defined value, usually one, and increments by one for each new
      picture starting from the bottom-left corner and moving from left
      to right to the end of the line.  This is repeated in each line
      until the top-right picture.  Thus if the prefix were {\tt "GALAXY"},
      the start number is one and the array comprises three pictures
      horizontally and two vertically, the top-left picture would have
      the label {\tt "GALAXY4"}.  On completion the bottom-left picture in
      the array becomes the current picture.
   }
   \sstusage{
      picdef [mode] [fraction]
        $\left\{ {\begin{tabular}{l}
                    xpic ypic prefix=? \\
                    lbound ubound
                  \end{tabular} }
        \right.$
        \newline\hspace*{12.4em}
        \makebox[0mm][c]{\small mode}
   }
   \sstparameters{
      \sstsubsection{
         ASPECT = \_REAL (Read)
      }{
         The aspect ratio ($x/y$) of the picture to be created in modes
         other than Cursor, Array, and XY.  The new picture is the
         largest possible with the chosen aspect ratio that will fit
         within the part of the reference picture defined by the
         fractional sizes (see parameter FRACTION).  The justification
         comes from the value of MODE.  Thus to obtain the largest
         picture parameter set FRACTION=1.0.  A null value ({\tt !}) does
         not apply an aspect-ratio constraint, and therefore the new
         picture fills the part of the reference picture defined by the
         fractional sizes. {\tt [!]}
      }
      \sstsubsection{
         CURRENT = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the new picture is to lie within the current picture,
         otherwise the new picture can lie anywhere within the BASE
         picture.  In other words, when it is {\tt TRUE} the current picture
         is the reference picture, and when {\tt FALSE} the base is the
         reference picture. {\tt [FALSE]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device. {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         FILL = \_REAL (Read)
      }{
         The linear filling factor for the Array mode.  In other words
         the fractional size (applied to both co-ordinates) of the new
         picture within each of the XPIC $*$ YPIC abutted sections of
         the picture being sub-divided.  Each new picture is located
         centrally within the section.  A filling factor of 1.0 means
         that the pictures in the array abut.  Smaller factors permit a
         gap between the pictures.  For example, FILL = 0.9 would give
         a gap between the created pictures of 10 per cent of the
         height and width of each picture, with exterior borders of 5
         per cent.  FILL must lie between 0.1 and 1.0. {\tt [1.0]}
      }
      \sstsubsection{
         FRACTION() = \_REAL (Read)
      }{
         The fractional size of the new picture along each axis,
         applicable for modes other than Array, Cursor, and XY.  Thus
         FRACTION controls the relative shape as well as the size of
         the new picture.  If only a single value is given then it is
         applied to both $x$ and $y$ axes, whereupon the new picture has
         the shape of the reference picture.  So a value of 0.5 would
         create a picture 0.25 the area of the current or BASE picture.
         The default is 0.5, unless parameter ASPECT is not null, when
         the default is 1.0. {\tt []}
      }
      \sstsubsection{
         LABELNO = \_INTEGER (Read)
      }{
         The number used to form the label for the first (bottom-left)
         picture in Array mode.  It cannot be negative. {\tt [1]}
      }
      \sstsubsection{
         LBOUND( 2 ) = \_REAL (Read)
      }{
         Co-ordinates of the lower bound that defines the new picture.
         The suggested default is the bottom-left of the current
         picture.  (XY mode)
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         Method for selecting the new picture. The options are {\tt "Cursor"}
         for cursor mode (provided the graphics device has one), {\tt "XY"}
         to select $x$-$y$ limits, and {\tt "Array"} to create a grid of
         equal-sized FRAME pictures.  The remainder are locations specified
         by two characters, the first corresponding to the vertical
         position and the second the horizontal.  For the vertical,
         valid positions are {\tt T}(op), {\tt B}(ottom), or
         {\tt C}(entre); and for the horizontal the options are
         {\tt L}(eft), {\tt R}(ight), or {\tt C}(entre). (It
         is the same as the disposition code in SGS). {\tt ["Cursor"]}
      }
      \sstsubsection{
         OUTLINE = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, a box that delimits the new picture is drawn. {\tt [TRUE]}
      }
      \sstsubsection{
         PREFIX = LITERAL (Read)
      }{
         The prefix to be given to the labels of picture created in
         Array mode.  It should contain no more than twelve characters.
         If the empty string {\tt ""} is given, the pictures will have
         enumerated labels.  Note that the database can contain only
         one picture with a given label, and so merely numbering labels
         increases the chance of losing existing labels.  A {\tt !} response
         means no labelling is required.  The suggested default is the
         last-used prefix.
      }
      \sstsubsection{
         UBOUND( 2 ) = \_REAL (Read)
      }{
         Co-ordinates of the upper bound that defines the new picture.
         The suggested default is the top-right of the current picture.
         (XY mode)
      }
      \sstsubsection{
         XPIC = \_INTEGER (Read)
      }{
         The number of new pictures to be formed horizontally in the
         BASE or current picture in Array mode.  The total number of
         new pictures is XPIC $*$ YPIC.    The value must lie in the
         range 1--20.  The suggested default is 2.
      }
      \sstsubsection{
         YPIC = \_INTEGER (Read)
      }{
         The number of new pictures to be formed vertically in the BASE
         or current picture in Array mode.  The value must lie in the
         range 1--20.  The suggested default is the value of parameter
         XPIC.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         picdef tr
      }{
         Creates a new FRAME picture in the top-right quarter of the full
         display area on the current graphics device, and an outline
         is drawn around the new picture.  This picture becomes the
         current picture.
      }
      \sstexamplesubsection{
         picdef bl aspect=1.0
      }{
         Creates a new FRAME picture within the full display area on the
         current graphics device, and an outline is drawn around the
         new picture.  This picture is the largest square possible, and
         it is justified to the bottom-left corner.  It becomes the
         current picture.
      }
      \sstexamplesubsection{
         picdef cc 0.7 current nooutline
      }{
         Creates a new FRAME picture situated in the centre of the current
         picture on the current graphics device.  The new picture has
         the same shape as the current one, but it is linearly reduced
         by a factor of 0.7.  No outline is drawn around it.  The new
         picture becomes the current picture.
      }
      \sstexamplesubsection{
         picdef cc [0.8,0.5] current nooutline
      }{
         As above except that its height is half that of the current
         picture, and its width is 0.8 of the current picture.
      }
      \sstexamplesubsection{
         picdef cu device=graphon
      }{
         Creates a new FRAME picture within the full display area of the
         Graphon device.  The bounds of the new picture are defined by
         cursor interaction.  An outline is drawn around the new picture
         which becomes the current picture.
      }
      \sstexamplesubsection{
         picdef mode=a prefix=M xpic=3 ypic=2
      }{
         Creates six new equally sized and abutting FRAME pictures within the
         full display area of the current graphics device. All are outlined.
         They have labels M1, M2, M3, M4, M5, and M6.  The bottom-left
         picture (M1) becomes the current picture.
      }
      \sstexamplesubsection{
         picdef mode=a prefix="" xpic=3 ypic=2 fill=0.8
      }{
         As above except that the pictures do not abut since each is
         0.8 times smaller in both dimensions, and the labels are 1,
         2, 3, 4, 5, and 6.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICBASE, PICCUR, PICDATA, PICFRAME, PICGRID, PICLABEL,
             PICLIST, PICSEL, PICXY.
   }
}
\sstroutine{
   PICEMPTY
}{
   Finds the first empty FRAME picture in the graphics database
}{
   \sstdescription{
      This application selects the first, {\it i.e.}\ oldest, empty FRAME
      picture in the graphics database for a graphics device.  Empty
      means that there is no additional picture lying completely with
      its bounds.  This implies that the FRAME is clear for plotting.
      This task is probably most useful for plotting data in a grid of
      FRAME pictures.
   }
   \sstusage{
      picempty [device]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation. {\tt [}The current graphics device{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         picempty
      }{
         This selects the first empty FRAME picture for the current
         graphics device.
      }
      \sstexamplesubsection{
         picempty xwindows
      }{
         This selects the first empty FRAME picture for the xwindows
         graphics device.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         An error is returned if there is no empty FRAME picture, and
         the current picture remains unchanged.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICENTIRE, PICGRID, PICLAST, PICLIST, PICSEL, PICVIS.
   }
   \sstdiytopic{
      Timing
   }{
      The execution time is approximately proportional to the number of
      pictures in the database before the first empty FRAME picture is
      identified.
   }
}

\sstroutine{
   PICENTIRE
}{
   Finds the first unobscured and unobscuring FRAME picture in the
   graphics database
}{
   \sstdescription{
      This application selects the first, {\it i.e.}\ oldest, FRAME picture in
      the graphics database for a graphics device, subject to the
      following criterion.  The picture must not obstruct any other
      picture except the BASE, and must itself not be obstructed.
      Unobstructed means that there is no younger picture overlying it
      either wholly or in part.  This means that plotting can occur
      within the selected FRAME picture without overwriting or
      obscuring earlier plots.
   }
   \sstusage{
      picentire [device]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation. {\tt [}The current graphics device{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         picentire
      }{
         This selects the first unobscured and unobscuring FRAME
         picture for the current graphics device.
      }
      \sstexamplesubsection{
         picentire xwindows
      }{
         This selects the first unobscured and unobscuring FRAME picture
         for the xwindows graphics device.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         An error is returned if there is no suitable FRAME picture,
         and the current picture remains unchanged.

         \sstitem
         This routine cannot know whether or a picture has been cleared,
         and hence is safe to reuse, as such information is not stored in
         the graphics database.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICEMPTY, PICGRID, PICLAST, PICLIST, PICSEL, PICVIS.
   }
   \sstdiytopic{
      Timing
   }{
      The execution time is approximately proportional to a linear
      combination of the number of pictures in the database before the
      unobstructed FRAME picture is found, and the square of the number
      of pictures in the database.
   }
}

\
\sstroutine{
   PICFRAME
}{
   Selects the last FRAME picture from the graphics database.
}{
   \sstdescription{
      This command selects the last-created FRAME picture.  Subsequent
      plotting for the chosen device will be in this new current
      picture.  By default the chosen device is the current one.

      This command is a synonym for {\tt piclist name=frame picnum=last}.
   }
   \sstusage{
      picframe
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation. {\tt [}The current graphics device{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         picframe
      }{
         This selects the last FRAME picture for the current graphics
         device.
      }
      \sstexamplesubsection{
         picframe device=xw
      }{
         This selects the last FRAME picture for the xw device.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICBASE, PICCUR, PICDATA, PICLAST, PICLIST, PICSEL.
   }
}
 
\sstroutine{
   PICGRID
}{
   Creates an array of FRAME pictures
}{
   \sstdescription{
      This command creates a two-dimensional grid of equally sized
      new FRAME pictures in the graphics database.  The array of pictures
      do not have to abut, but abutting is the default.  The new pictures
      are formed within either the current or BASE picture, and they
      adopt the world co-ordinate system of that enclosing picture.  On
      completion, the bottom-left picture in the array becomes the
      current picture.

      For easy reference in later processing the pictures have integer
      labels.  The numbering starts at a defined value, usually one,
      and increments by one for each new picture starting from the
      bottom-left corner and moving from left to right to the end of
      the line.  This is repeated in each line until the top-right
      picture.

      This command is a synonym for {\tt picdef array 1.0 prefix=""}.
   }
   \sstusage{
      picgrid xpic ypic
   }
   \sstparameters{
      \sstsubsection{
         CURRENT = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the new pictures are to lie within the current picture,
         otherwise the new pictures can lie anywhere within the BASE
         picture.  In other words, when CURRENT is {\tt TRUE} the
         current picture is the reference picture, and when it is
         {\tt FALSE} the BASE is the reference picture. {\tt [FALSE]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device. {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         FILL = \_REAL (Read)
      }{
         The linear filling factor for the array.  In other words
         the fractional size (applied to both co-ordinates) of the new
         picture within each of the XPIC $*$ YPIC abutted sections of
         the picture being sub-divided.  Each new picture is located
         centrally within the section.  A filling factor of 1.0 means
         that the pictures in the array abut.  Smaller factors permit a
         gap between the pictures.  For example, FILL = 0.9 would give
         a gap between the created pictures of 10 per cent of the
         height and width of each picture, with exterior borders of 5
         per cent.  FILL must lie between 0.1 and 1.0. {\tt [1.0]}
      }
      \sstsubsection{
         LABELNO = \_INTEGER (Read)
      }{
         The number used to form the label for the first (bottom-left)
         picture.  It cannot be negative. {\tt [1]}
      }
      \sstsubsection{
         OUTLINE = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, a box that delimits the new picture is drawn. {\tt [TRUE]}
      }
      \sstsubsection{
         XPIC = \_INTEGER (Read)
      }{
         The number of new pictures to be formed horizontally in the
         BASE picture.  The total number of new pictures is XPIC $*$ YPIC.
         The value must lie in the range 1--20.  The suggested default
         is 2.
      }
      \sstsubsection{
         YPIC = \_INTEGER (Read)
      }{
         The number of new pictures to be formed vertically in the BASE
         picture.  The total number of new pictures is XPIC $*$ YPIC.
         The value must lie in the range 1--20.  The suggested default
         is the value of parameter XPIC.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         picgrid 3 2
      }{
         Creates six new equally sized and abutting FRAME pictures within the
         full display area of the current graphics device.  All the
         pictures are outlined.  They have labels 1, 2, 3, 4, 5, and 6.
         The bottom-left picture (1) becomes the current picture.
      }
      \sstexamplesubsection{
         picgrid xpic=3 ypic=2 fill=0.8 labelno=11 nooutline
      }{
         As above except that the pictures do not abut since each is
         0.8 times smaller in both dimensions, the labels are 11 to
         16, and there are no picture outlines drawn.
      }
      \sstexamplesubsection{
         picgrid device=xw current \
      }{
         This creates a 2-by-2 grid of new FRAME pictures within the current
         picture on device xw.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICCUR, PICDEF, PICGRID, PICSEL.
   }
}
\sstroutine{
   PICIN
}{
   Finds the attributes of a picture interior to the current picture
}{
   \sstdescription{
      This application finds the attributes of a picture, selected by
      name, which was created since the current picture and lies within
      the bounds of the current picture.  The search proceeds backwards
      from the most-recent picture, unless the current picture is
      included, whereupon the current picture is tested before the
      most-recent picture.

      The attributes reported are the name, comment, label, name of the
      reference data object, the bounds in world, raster, and normalised
      device co-ordinates.
   }
   \sstusage{
      picin [name] [device]
   }
   \sstparameters{
      \sstsubsection{
         CURRENT = \_LOGICAL (Read)
      }{
         If this is {\tt TRUE}, the current picture is compared against the
         chosen name before searching from the most-recent picture
         within the current picture. {\tt [FALSE]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Name of the graphics device about which information is
         required. {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         NAME = LITERAL (Read)
      }{
         The name of the picture to be found within the current picture.
         If it is null (!), the first interior picture is selected.
         {\tt ["DATA"]}
      }
      \sstsubsection{
         REPORT = \_LOGICAL (Read)
      }{
         If this is {\tt FALSE} details of the interior picture are not
         reported, merely the results are written to the output
         parameters.  It is intended for use within procedures.
         {\tt [TRUE]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         COMMENT = LITERAL (Write)
      }{
         The comment of the current picture.  Up to 132 characters
         will be written.
      }
      \sstsubsection{
         LABEL = LITERAL (Write)
      }{
         The label of the current picture.  It is blank if there is no
         label.
      }
      \sstsubsection{
         NCX1 = \_REAL (Write)
      }{
         The lower $x$ normalised device co-ordinate of the current
         picture.
      }
      \sstsubsection{
         NCX2 = \_REAL (Write)
      }{
         The upper $x$ normalised device co-ordinate of the current
         picture.
      }
      \sstsubsection{
         NCY1 = \_REAL (Write)
      }{
         The lower $y$ normalised device co-ordinate of the current
         picture.
      }
      \sstsubsection{
         NCY2 = \_REAL (Write)
      }{
         The upper $y$ normalised device co-ordinate of the current
         picture.
      }
      \sstsubsection{
         PNAME = LITERAL (Write)
      }{
         The name of the current picture.
      }
      \sstsubsection{
         RCX1 = \_REAL (Write)
      }{
         The lower $x$ raster co-ordinate of the current picture.  A
         value of $-$1 signifies that the value could not be determined
         because the device is not of the raster type.
      }
      \sstsubsection{
         RCX2 = \_REAL (Write)
      }{
         The upper $x$ raster co-ordinate of the current picture.  A
         value of $-$1 signifies that the value could not be determined
         because the device is not of the raster type.
      }
      \sstsubsection{
         RCY1 = \_REAL (Write)
      }{
         The lower $y$ raster co-ordinate of the current picture.  A
         value of $-$1 signifies that the value could not be determined
         because the device is not of the raster type.
      }
      \sstsubsection{
         RCY2 = \_REAL (Write)
      }{
         The upper $y$ raster co-ordinate of the current picture.  A
         value of $-$1 signifies that the value could not be determined
         because the device is not of the raster type.
      }
      \sstsubsection{
         REFNAM = LITERAL (Write)
      }{
         The reference object associated with the current picture.  It
         is blank if there is no reference object.  Up to 132 characters
         will be written.
      }
      \sstsubsection{
         WCX1 = \_REAL (Write)
      }{
         The lower $x$ world co-ordinate of the current picture.
      }
      \sstsubsection{
         WCX2 = \_REAL (Write)
      }{
         The upper $x$ world co-ordinate of the current picture.
      }
      \sstsubsection{
         WCY1 = \_REAL (Write)
      }{
         The lower $y$ world co-ordinate of the current picture.
      }
      \sstsubsection{
         WCY2 = \_REAL (Write)
      }{
         The upper $y$ world co-ordinate of the current picture.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         picin
      }{
         This reports the attributes of the last DATA picture within
         the current picture for the current graphics device.
      }
      \sstexamplesubsection{
         picin frame graphon
      }{
         This reports the attributes of the last FRAME picture within
         the current picture for the Graphon device.
      }
      \sstexamplesubsection{
         picin refnam=(object) current
      }{
         This reports the attributes of the last data picture within
         the current picture for the current graphics device.  If there
         is a reference data object, its name is written to the
         {\footnotesize ICL} variable OBJECT.  The search includes
         the current picture.
      }
      \sstexamplesubsection{
         picin ncx1=(x1) ncx2=(x2) ncy1=(y1) ncy2=(y2)
      }{
         This reports the attributes of the last DATA picture within
         the current picture for the current graphics device.  The bounds
         of the current picture in normalised device co-ordinates
         are written to the {\footnotesize ICL} variables: X1, X2, Y1, Y2.
      }
   }
   \sstnotes{
      This application is intended for use within procedures.  Also if
      a DATA picture is selected and the current picture is included in
      the search, this application informs about the same picture that
      an application that works in a cursor interaction mode would
      select, and so acts as a check that the correct picture will be
      accessed.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: GDSTATE, PICDEF, PICLIST, PICTRANS, PICXY.
   }
}
 
\sstroutine{
   PICLABEL
}{
   Labels the current graphics-database picture
}{
   \sstdescription{
      This application annotates the current graphics-database picture
      of a specified device with a label you define.  This provides an
      easy-to-remember handle for selecting pictures in subsequent
      processing.
   }
   \sstusage{
      piclabel label [device]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device. {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         LABEL = LITERAL (Read)
      }{
         The label to be given to the current picture.  It is limited
         to 15 characters, but may be in mixed case.  If it is null ({\tt !})
         a blank label is inserted in the database.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         piclabel GALAXY
      }{
         This makes the current picture of the current graphics device
         have a label of {\tt "GALAXY"}.
      }
      \sstexamplesubsection{
         piclabel A3 ikon
      }{
         This labels the current Ikon picture {\tt "A3"}.
      }
   }
   \sstnotes{
      The label must be unique for the chosen device.  If the new label
      clashes with an existing one, then the existing label is deleted.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICDEF, PICLIST, PICSEL.
   }
}

\sstroutine{
   PICLAST
}{
   Selects the last picture from the graphics database.
}{
   \sstdescription{
      This command selects the last-created picture.  Subsequent
      plotting for the chosen device will be in this new current picture.
      By default the chosen device is the current one.

      This command is a synonym for {\tt piclist picnum=last}.
   }
   \sstusage{
      piclast
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation. {\tt [}The current graphics device{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         piclast
      }{
         This selects the last picture for the current graphics device.
      }
      \sstexamplesubsection{
         piclast device=x2w
      }{
         This selects the last picture for the x2w device.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICBASE, PICCUR, PICDATA, PICFRAME, PICLIST, PICSEL.
   }
}

\sstroutine{
   PICLIST
}{
   Lists the pictures in the graphics database for a device
}{
   \sstdescription{
      This application produces a summary of the contents of the
      graphics database for a graphics device, and/or permits a picture
      specified by its order in the list to be made the new current
      picture.  The list may either be reported or written to a text file.

      The headed list has one line per picture.  Each line comprises
      a reference number; the picture's name, comment (up to 24
      characters), and label; and a flag to indicate whether or not
      there is a reference data object associated with the picture.  A
      `C' in the first column indicates that the picture that was
      current when this application was invoked.  In the text file,
      because there is more room, the name of the reference object is
      given (up to 64 characters) instead of the reference flag.
      Pictures are listed in chronological order of their creation.
   }
   \sstusage{
      piclist [name] [logfile] [device] picnum=?
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation. {\tt [}The current graphics device{\tt ]}
      }
      \sstsubsection{
         LOGFILE = FILENAME (Write)
      }{
         The name of the text file in which the list of pictures will
         be made.  A null string ({\tt !}) means the list will be reported
         to you.  The suggested default is the current value. {\tt [!]}
      }
      \sstsubsection{
         NAME = LITERAL (Read)
      }{
         Only pictures of this name are to be selected.  A null string
         ({\tt !}) or blanks means that pictures of all names may be selected.
         {\tt [!]}
      }
      \sstsubsection{
         PICNUM = LITERAL (Read)
      }{
         The reference number of the picture to be made the current
         picture when the application exits.  PICNUM={\tt "Last"} selects the
         last picture in the database.  Parameter PICNUM is not accessed
         if the list is written to the text file.  A null ({\tt !}) or any
         other error causes the current picture on entry to be current
         again on exit.  The suggested default is null.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         piclist
      }{
         This reports all the pictures in the graphics database for the
         current graphics device.
      }
      \sstexamplesubsection{
         piclist device=ps\_l
      }{
         This reports all the pictures in the graphics database for the
         ps\_l device.
      }
      \sstexamplesubsection{
         piclist data
      }{
         This reports all the DATA pictures in the graphics database for
         the current graphics device.
      }
      \sstexamplesubsection{
         piclist data logfile=datapic.dat
      }{
         This lists all the DATA pictures in the graphics database for
         the current graphics device into the text file {\tt datapic.dat}.
      }
      \sstexamplesubsection{
         piclist frame picnum=5
      }{
         This selects the fifth most ancient FRAME picture (in the
         graphics database for the current graphics device) as the
         current picture.  The pictures are not listed.
      }
      \sstexamplesubsection{
         piclist picnum=last
      }{
         This makes the last picture in the graphics database for the
         current graphics device current.  The pictures are not listed.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICBASE, PICDATA, PICEMPTY, PICENTIRE, PICFRAME, PICIN,
      PICLAST, PICSEL, PICVIS.
   }
   \sstdiytopic{
      Timing
   }{
      The execution time is approximately proportional to the number of
      pictures in the database for the chosen graphics device.
      Selecting only a subset by name is slightly faster.
   }
}
 
\sstroutine{
   PICSEL
}{
   Selects a graphics-database picture by its label
}{
   \sstdescription{
      This application selects by label a graphics-database picture of a
      specified device.  If such a picture is found then it becomes the
      current picture on exit, otherwise the input picture remains
      current.  Labels in the database are stored in the case supplied
      when they were created.  However, the comparisons of the label you
      supply with the labels in the database are made in uppercase, and
      leading spaces are ignored.

      Should the label not be found the current picture is unchanged.
   }
   \sstusage{
      picsel label [device]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device. {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         LABEL = LITERAL (Read)
      }{
         The label of the picture to be selected.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         picsel GALAXY
      }{
         This makes the picture labelled {\tt "GALAXY"} the current picture on
         the current graphics device.  Should there be no picture of
         this name, the current picture is unchanged.
      }
      \sstexamplesubsection{
         picsel A3 xwindows
      }{
         This makes the picture labelled {\tt "A3"} the current picture on the
         xwindows device.  Should there be no picture of this name, the current
         picture is unchanged.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICDATA, PICDEF, PICEMPTY, PICENTIRE, PICFRAME, PICLABEL,
      PICLAST, PICVIS.
   }
}
\sstroutine{
   PICTRANS
}{
   Transforms co-ordinates between the current and BASE pictures
}{
   \sstdescription{
      This application converts a position's co-ordinates from the
      current picture to their equivalent in the BASE picture, or
      vice versa.
   }
   \sstusage{
      pictrans inxy [outx] [outy] [bound]
   }
   \sstparameters{
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either {\tt "World"}
         or {\tt "Data"}.  {\tt "World"} makes the application convert between the
         world co-ordinates of the position in the two pictures.  World
         co-ordinates that relate to a location in a data array will be
         in array pixels.  For COSYS = {\tt "Data"} the conversion is between
         data co-ordinates, stored via a transformation.  The BASE
         picture will not normally have data co-ordinates, so the value
         of COSYS usually selects in which co-ordinate system positions
         in the current picture are specified.

         Data co-ordinates are arbitrary but most often they will be a
         linear or logarithmic transformation of the world
         co-ordinates.  For example, the $x$ co-ordinate of a spectrum
         would be given in pixels if COSYS = {\tt "World"}, but if COSYS =
         {\tt "Data"} the $x$ co-ordinate could be in wavelength units, such as
         {\AA}ngstroms.  If the database does not have a world-to-data
         transformation for a given picture, the value of this
         parameter is ignored for that picture, and supplied or
         computed positions in that picture will be in world
         co-ordinates. {\tt [}Current co-ordinate system{\tt ]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation. {\tt [}The current graphics device{\tt ]}
      }
      \sstsubsection{
         INXY( 2 ) = \_DOUBLE (Read)
      }{
         The $x$-$y$ co-ordinates to be transformed.  These need not lie
         within the physical bounds of their associated picture.  The
         suggested value is the current value.
      }
      \sstsubsection{
         TOBASE = \_LOGICAL (Read)
      }{
         This decides the direction of the transformation.  If TOBASE
         is {\tt TRUE}, the conversion is from the current to the BASE
         picture.  If TOBASE is {\tt FALSE}, BASE-picture co-ordinates are
         converted to a position within the current picture.
         The suggested value is the current value.  {\tt [TRUE]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         BOUND = \_LOGICAL (Write)
      }{
         BOUND is {\tt TRUE} when the transformed co-ordinates lie within the
         bounds of their associated picture.
      }
      \sstsubsection{
         OUTX = \_DOUBLE (Write)
      }{
         The transformed $x$ co-ordinate.
      }
      \sstsubsection{
         OUTY = \_DOUBLE (Write)
      }{
         The transformed $y$ co-ordinate.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         pictrans [100.3,-20.1] outx=(dx) outy=(dy) cosys=w
      }{
         This converts the position (100.3,~$-$20.1), in world co-ordinates
         of the current picture of the current graphics device, to the
         world co-ordinates of that point in the BASE picture.  The base
         co-ordinates are written to {\footnotesize ICL} variables DX
         and DY (as well as the application's parameter file).
      }
      \sstexamplesubsection{
         pictrans [-1.e4,2.56] outy=(dy) device=xwindows
      }{
         This converts the position ($-$10000.0,~2.56), in the current
         picture of the xwindows device, to the co-ordinates of that
         point in the BASE picture.  All positions use the current
         co-ordinate system.  The base $y$ co-ordinate is written to
         {\footnotesize ICL} variable DY.
      }
      \sstexamplesubsection{
         pictrans [0.314,0.137] (dx) (dy) (within) cosys=d notobase
      }{
         This converts the position (0.314,~0.137), in the data
         co-ordinates of the BASE picture of the current graphics
         device, to the data co-ordinates of that point in the current
         picture.  The transformed co-ordinates are written to
         {\footnotesize ICL} variables DX and DY.  {\footnotesize ICL}
         variable WITHIN contains a flag to indicate whether or not
         the point lies within the current picture.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: GDSTATE, PICIN, PICXY.
   }
}
\sstroutine{
   PICVIS
}{
   Finds the first unobscured FRAME picture in the graphics database
}{
   \sstdescription{
      This application selects the first, {\it i.e.}\ oldest, unobstructed
      FRAME picture in the graphics database for a graphics device.
      Unobstructed means that there is no younger picture overlying it
      either wholly or in part.
   }
   \sstusage{
      picvis [device]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation. {\tt [}The current graphics device{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         picvis
      }{
         This selects the first unobscured FRAME picture for the
         current graphics device.
      }
      \sstexamplesubsection{
         picvis xwindows
      }{
         This selects the first unobscured FRAME picture for the
         xwindows graphics device.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         An error is returned if there is no unobscured FRAME picture,
         and the current picture remains unchanged.

         \sstitem
         This routine cannot know whether or a picture has been cleared,
         and hence is safe to reuse, as such information is not stored in
         the graphics database.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICEMPTY, PICENTIRE, PICGRID, PICLAST, PICLIST, PICSEL.
   }
   \sstdiytopic{
      Timing
   }{
      The execution time is approximately proportional to a linear
      combination of the number of pictures in the database before the
      unobstructed FRAME picture is found, and the square of the number
      of pictures in the database.
   }
}
 
\sstroutine{
   PICXY
}{
   Creates a new FRAME picture defined by co-ordinate bounds
}{
   \sstdescription{
      This command creates a new FRAME picture in the graphics database.
      The bounds of the new picture are defined through two parameters.
      The new picture is formed within either the current or BASE
      picture, and it adopts the world co-ordinate system of that
      reference picture.  On completion the new picture becomes the
      current picture.

      This command is a synonym for {\tt picdef xy 1.0}.
   }
   \sstusage{
      picxy lbound ubound
   }
   \sstparameters{
      \sstsubsection{
         CURRENT = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the new picture is to lie within the current picture,
         otherwise the new picture can lie anywhere within the BASE
         picture.  In other words, when CURRENT is {\tt TRUE} the
         current picture is the reference picture, and when it is
         {\tt FALSE} the base is the reference picture. {\tt [FALSE]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device. {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         LBOUND( 2 ) = \_REAL (Read)
      }{
         Co-ordinates of the lower bound that defines the new picture.
         The suggested default is the bottom-left of the current
         picture.
      }
      \sstsubsection{
         OUTLINE = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, a box that delimits the new picture is drawn. {\tt [TRUE]}
      }
      \sstsubsection{
         UBOUND( 2 ) = \_REAL (Read)
      }{
         Co-ordinates of the upper bound that defines the new picture.
         The suggested default is the top-right of the current picture.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         picxy [0.1,0.2] [0.9,0.6]
      }{
         This creates a new FRAME picture in the BASE picture extending from
         (0.1,~0.2) to (0.9,~0.6), which becomes the new current picture.
         An outline is drawn around the picture.
      }
      \sstexamplesubsection{
         picxy ubound=[1.1,0.9] lbound=[0.1,0.2] current nooutline
      }{
         This creates a new FRAME picture in the current picture extending
         from (0.1,~0.2) to (1.1,~0.9), which becomes the new current
         picture.  No outline is drawn.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: PICCUR, PICDEF, PICGRID, PICSEL.
   }
}
\sstroutine{
   PIXDUPE
}{
   Expands an NDF by pixel duplication
}{
   \sstdescription{
      This routine expands the size of an NDF structure by duplicating
      each input pixel a specified number of times along each
      dimension, to create a new NDF structure.
   }
   \sstusage{
      pixdupe in out expand
   }
   \sstparameters{
      \sstsubsection{
         EXPAND() = \_INTEGER (Read)
      }{
         Linear expansion factors to be used to create the new data
         array.  The number of factors should equal the number of
         dimensions in the input NDF.  If fewer are supplied the last
         value in the list of expansion factors is given to the
         remaining dimensions.  Thus if a uniform expansion is required
         in all dimensions, just one value need be entered.  If the net
         expansion is one, an error results.  The suggested default is
         the current value.
      }
      \sstsubsection{
         IN  = NDF (Read)
      }{
         Input NDF structure to be expanded.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF structure.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the input NDF to the output NDF. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         pixdupe aa bb 2
      }{
         This expands the NDF called aa duplicating pixels along each
         dimension, and stores the enlarged data in the NDF called bb.
         Thus if aa is 2-dimensional, this command would result in a
         four-fold increase in the array components.
      }
      \sstexamplesubsection{
         pixdupe cosmos galaxy [2,1]
      }{
         This expands the NDF called cosmos by duplicating along the
         first axis, and stores the enlarged data in the NDF called
         galaxy.
      }
      \sstexamplesubsection{
         pixdupe cube1 cube2 [3,1,2] title="Reconfigured cube"
      }{
         This expands the NDF called cube1 by having three pixels for
         each pixel along the first axis and duplicating along the
         third axis, and stores the enlarged data in the NDF called
         cube2.  The title of cube2 is {\tt "Reconfigured cube"}.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: COMPADD, COMPAVE, COMPICK.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY, components of an NDF
         data structure and propagates all extensions.  Origin information
         becomes undefined by the duplication and so is not propagated.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         Any number of NDF dimensions is supported.
      }
   }
}
\manroutine {{\manheadstyle{POW}}}{ Takes the specified power of each
  pixel of a data array.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine takes the specified power of each pixel of the
  data array in the input {\mantt{IMAGE}} structure. The result goes into a
  new output array, also in an {\mantt{IMAGE}} structure.

  The magic-value method is used for processing bad data.  Output
  pixels become bad if the result raising to the specified power is
  undefined or out of range.  Negative pixel values will only
  generate good output pixels when the power is an even integer.

\manroutineitem {Invocation }{}
  POW

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  Input {\mantt{IMAGE}} structure containing the data array to be
  processed.
\manparameterentry {{\mantt{READ}} }{{\mantt{POWER}}  }{{\mantt{\_REAL}}}
  Power to be taken of each input data-array pixel.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure holding result of the processed data
  array.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title string for the output {\mantt{IMAGE}} structure.
 \mbox{{\mantt ['KAPPA - Pow']}}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\sstroutine{
   PSF
}{
   Determines the parameters of a model star profile by fitting star
   images in a two-dimensional NDF
}{
   \sstdescription{
      This application finds a set of parameters to describe a model
      Gaussian star image.  It can be used for profile-fitting stellar
      photometry, to evaluate correction terms to aperture
      photometry, or for filtering.

      The model has a radial profile:
      {\Large
      \[   D =  A \exp^{-0.5\,(r/\sigma)^{\gamma}} \]
      }where $r$ is calculated from the true radial distance from the star
      centre allowing for image ellipticity, $\sigma$ is the Gaussian
      precision constant or profile width.  The application combines a
      number of star images you specify  and determines a mean
      seeing-disc size, radial fall-off parameter ($\gamma$), axis ratio,
      and orientation of a model star image.

      A table, giving details of the seeing and ellipticity of each
      star image used can be reported to an output text file.  This
      table indicates if any star could not be used.  Reasons for
      rejecting stars are too-many bad pixels present in the image,
      the star is too close to the edge of the data array, the
      `star' is a poor fit to model or it could not be located.

      An optional plot of the mean profile and the fitted function may
      be produced.  The point-spread function may be stored in an NDF
      for later use.
   }
   \sstusage{
      psf in cofile device out [cut] [range] [isize] [poscols] [clear]
   }
   \sstparameters{
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         Determines if the graphics workstation is to be cleared before
         producing the plot.  It is ignored if no plotting is required
         defined by DEVICE. {\tt [TRUE]}
      }
      \sstsubsection{
         COFILE = FILENAME (Read)
      }{
         Text file containing the $x$ and $y$ co-ordinates.  The data
         should be in columns separated by spaces or tabs, however
         precise alignment is not necessary.
      }
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either
         {\tt "World"} or {\tt "Data"}.  If COSYS = {\tt "Data"} the
         input co-ordinates are to be expressed in data co-ordinates,
         otherwise pixel (world) co-ordinates are used.  In order to
         compute the point-spread function data co-ordinates are
         converted to pixel indices via the NDF's axis values; if there
         is no axis information within the NDF, world co-ordinates are
         then used.  If COSYS = {\tt "World"} pixel co-ordinates are
         used throughout.  {\tt [}Current co-ordinate system{\tt ]}
      }
      \sstsubsection{
         CUT = \_REAL (Read)
      }{
         This parameter controls the size of the output NDF.  If it is
         null, {\tt !}, the dimension of the square NDF will be the size of
         the region used to calculate the radial profile, which usually
         is given by RANGE $*$ width in pixels $*$ AXISR, unless truncated.
         If CUT has a value it is the threshold which must be included
         in the PSF NDF, and it is given as the fraction of the peak
         amplitude of the PSF.  For example, if CUT=0.5 the NDF would
         contain the point-spread function to half maximum.  CUT must
         be greater than 0 and less than 1.  The suggested default is
         0.0001 {\tt [!]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics workstation on which to produce a plot of the
         mean radial profile of the stars and the fitted function.  A
         null ({\tt !}) name indicates that no plot is required.  The
         suggested default is the current graphics device.
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The NDF containing the star images to be fitted.
      }
      \sstsubsection{
         ISIZE = \_INTEGER (Read)
      }{
         The side of the square area to be used when forming the
         marginal profiles for a star image.  It should be sufficiently
         large to contain the entire star image.  It should be an odd
         number and must lie in the range from 3 to 101.  If an even
         value is given, the next largest odd number is used instead.
         {\tt [15]}
      }
      \sstsubsection{
         LOGFILE = FILENAME (Read)
      }{
         Text file to contain the table of parameters for each star.  A
         null ({\tt !}) name indicates that no log file is required. {\tt [!]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The NDF containing the fitted point-spread function evaluated
         at each pixel. Its dimensions are always odd numbered and
         the centre of the PSF is located at the centre of the image.
         If null, {\tt !}, is entered no output NDF will be created.  The
         dimensions of the array are controlled by parameter CUT. {\tt [!]}
      }
      \sstsubsection{
         POSCOLS = \_INTEGER (Read)
      }{
         Column positions of the co-ordinates in an input record of the
         text file, $x$ then $y$.  The columns must be different amongst
         themselves. If there is duplication new values will be
         requested.  {\tt [1,2]}
      }
      \sstsubsection{
         PXSIZE = \_REAL (Read)
      }{
         The horizontal size of the plot in metres. If a value less
         than the default is requested, then the plot will appear at
         the bottom left of the current picture. {\tt [}The size of the
         current picture{\tt ]}
      }
      \sstsubsection{
         PYSIZE = \_REAL (Read)
      }{
         The vertical size of the plot in metres. If a value less than
         the default is requested, then the plot will appear at the
         bottom left of the current picture. {\tt [}The size of the current
         picture{\tt ]}
      }
      \sstsubsection{
         RADUNITS = LITERAL (Read)
      }{
         The units of the radial profile after applying parameter SCALE
         to the pixels.  RADUNITS defaults to {\tt "pixels"} when SCALE is
         1.0. {\tt []}
      }
      \sstsubsection{
         RANGE = \_REAL (Read)
      }{
         The number of image profile widths out to which the radial
         star profile is to be fitted.  (There is an upper limit of 100
         pixels to the radius at which data are actually used.) {\tt [4.0]}
      }
      \sstsubsection{
         SCALE = \_REAL (Read)
      }{
         A scale factor to convert pixels to some physical units such
         as arcseconds.  This factor is applied to the reported FWHM
         and to the radial distances in the plotted profile. {\tt [1.0]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         The title for the NDF to contain the fitted point-spread
         function.  If null, {\tt !}, is entered the NDF will not contain a
         title.  {\tt ["KAPPA - PSF"]}
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         ABSLAB = LITERAL (Read)
      }{
         Label for the plot abscissa, in which NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  The default is
         {\tt "$\cal A$-axis Distance ($\cal U$)"} where $\cal U$ is
         replaced by the value of parameter RADUNITS, and $\cal A$ is
         {\tt "Minor"} when MINOR={\tt TRUE} and is {\tt "Major"} when
         MINOR={\tt FALSE}.
      }
      \sstsubsection{
         FONT = LITERAL (Read)
      }{
         The fount to be used for the line graphics.  It can be either
         {\tt "NCAR"} for the NCAR fancy characters and {\tt "GKS"} for the standard
         GKS san-serif fount.   The former is intended for hardcopy
         publication-quality plots, since it is relatively slow; the
         latter is intended for normal interactive graphics requiring
         rapid plotting, and it is clearer on small plots. The
         suggested default is the current value. {\tt ["GKS"]}
      }
      \sstsubsection{
         MAJTIC( 2 ) = \_REAL (Read)
      }{
         The parameter controlling the numbers of major tick marks
         for the $x$ and $y$ axes.  (Number used is between MAJTIC$+$2 and
         5$*$MAJTIC/2$+$4.) {\tt [3.,3.]}
      }
      \sstsubsection{
         MINOR = \_LOGICAL (Read)
      }{
         If MINOR is {\tt TRUE} the plot abscissa is the distance along the
         minor axis from the centre of the PSF.  If MINOR is {\tt FALSE}, the
         major axis is plotted.  {\tt [TRUE]}
      }
      \sstsubsection{
         MINTIC( 2 ) = \_REAL (Read)
      }{
         The number of minor tick marks between each major tick mark
         for the $x$ and $y$ axes.  A negative value forces the graphics
         package to compute appropriate values. {\tt [-1.,-1.]}
      }
      \sstsubsection{
         ORDLAB = LITERAL (Read)
      }{
         Label for the plot ordinate, in which NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  {\tt ["Intensity"]}
      }
      \sstsubsection{
         OUTTIC = \_LOGICAL (Read)
      }{
         OUTTIC is {\tt TRUE} if the axis tick marks are to appear on the outside of
         the axes instead of inside. {\tt [FALSE]}
      }
      \sstsubsection{
         PLTITL = LITERAL (Read)
      }{
         The title of the plot.  Up to about 40 characters can be
         accommodated, and NCAR fancy founts may be embedded when
         FONT = {\tt "NCAR"}.  {\tt ["Mean Star Profile"]}
      }
   }
   \sstresparameters{
      \sstsubsection{
         AXISR = \_REAL (Write)
      }{
         The axis ratio of the star images: the ratio of the major
         axis length to that of the minor axis.
      }
      \sstsubsection{
         FWHM = \_REAL (Write)
      }{
         The seeing-disc size: the full width at half maximum across the
         minor axis of the stars.  It is in the units defined by
         parameters SCALE and RADUNITS.  By default this will be in
         pixels.
      }
      \sstsubsection{
         GAMMA = \_REAL (Write)
      }{
         The radial fall-off parameter, $\gamma$, of the star images. See
         the description for more details.  $\gamma = 2$ would be a
         Gaussian.
      }
      \sstsubsection{
         ORIENT = \_REAL (Write)
      }{
         The orientation of the major axis of the star images to the $x$
         axis (increasing pixel-index direction).  This value is in
         degrees, $x$ through $y$ being considered positive.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         psf ngc6405i starlist $\backslash$
      }{
         Derives the mean point-spread function for the stars images
         in the NDF called ngc6405i that are situated near the $x$-$y$ co-ordinates
         given in the first two columns of file {\tt starlist}.  A plot
         of the profile is drawn on the current graphics device.  The
         results are stored in the parameter file {\tt psf.sdf}.
      }
      \sstexamplesubsection{
         psf ngc6405i starlist device=!
      }{
         As above but there is no graphical output.
      }
      \sstexamplesubsection{
         psf cofile=starlist in=ngc6405i logfile=fit.log fwhm=(seeing) $\backslash$
      }{
         As the first example, but the results, including the fits to
         each star, are written to the text file {\tt fit.log}.  The
         full-width half-maximum is written to the {\footnotesize ICL}
         variable SEEING rather than the parameter file.
      }
      \sstexamplesubsection{
         psf ngc6405i starlist isize=31 $\backslash$
      }{
         As the first example, but the area including a star image is
         31 pixels square, say because the seeing is poor or the pixels
         are smaller than normal.
      }
      \sstexamplesubsection{
         psf ngc6405i starlist out=starpsf cut=1.0e-3 $\backslash$
      }{
         As the first example, but the resultant point-spread function
         is stored in the NDF called starpsf, and will contain signals
         as low as 1.0E$-$3.
      }
      \sstexamplesubsection{
         psf ngc6405i starlist out=starpsf cut=1.0e-3 scale=0.52 radunits="arcsec"
      }{
         As the first example, but the resultant point-spread function
         is stored in the NDF called starpsf, and will contain signals
         as low as 1.0E$-$3.  The FWHM and plot abscissa are scaled to
         arcseconds, where a pixel corresponds to 0.52 arcseconds.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The stars used to determine the mean image parameters should
         be chosen to represent those whose magnitudes are to be found
         using a stellar photometry application, and to be sufficiently
         bright, uncrowded, and noise-free to allow an accurate fit to be
         made.

         \sstitem
         The method to calculate the fit is as follows.
         \begin{itemize}
            \item  Marginal profiles of each star image are formed in four
            directions, inclined at 45\dgs\ intervals.  A Gaussian curve
            and background is fitted to each profile.  Using the resulting
            four Gaussian centres, a mean centre is found for each star.
            \item The four Gaussian widths of all the stars are combined,
            using a weighted average with rejection of erroneous data, and
            from the four average widths the seeing-disc size, axis ratio
            and axis inclination are calculated.
            \item The data surrounding each star is then binned into
            isophotal zones which are elliptical annuli centred on the
            star---the ellipse parameters being those just calculated.
            The data in each zone is processed to remove erroneous points
            and to find an average value.  A Gaussian profile is fitted to
            these average values and the derived amplitude is used to
            normalise the values to an amplitude of unity.  The normalised
            values are put into bins together with the corresponding data
            from all other stars and this binned data represents a
            weighted average radial profile for the set of stars, with the
            image ellipticity removed.  Finally a radial profile is fitted
            to these data, giving the radial profile parameter $\gamma$ and a
            final re-estimate of the seeing-disc size.
         \end{itemize}

         \sstitem
         If a plot was requested the application stores two pictures in
         the graphics database in the following order: a FRAME of the
         specified size containing the title, annotated axes, and line
         plot; and a DATA picture, which has world co-ordinates measured
         in pixels along the $x$ axis and normalized intensity values along
         $y$.  The NDF associated with the plot is not stored by reference
         with the DATA picture.  On exit the current database picture for
         the chosen device reverts to the input picture.
      }
   }
   \sstdiytopic{
      Timing
   }{
      Approximately proportional to the number of stars used and the
      image area which each occupies.
   }
   \sstdiytopic{
      Related Applications
   }{
      PHOTOM; Starman.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, and TITLE components of an NDF data structure.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  The output
         point-spread-function NDF has type \_REAL.
      }
   }
}
\begin{htmlonly}
Click \htmladdnormallink{here}{psf_exam.gif} to see an example plot
(6k).
\end{htmlonly}

\manroutine {{\manheadstyle{QUILT}}}{ Generates a mosaic from equally sized
  2-d data arrays, optionally specified from an ASCII file.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine provides a more-sophisticated version of the {\mantt{MOSAIC}}
  application for combining many 2-d data arrays into one large
  output data array. All the data arrays are stored in {\mantt{IMAGE}}
  structures.  The pixels in overlapping regions may be averaged or
  summed.

  The names of {\mantt{IMAGE}} structures to be concatenated and their
  respective offsets of their data arrays from a central
  data array can be input either one by one from the interface, or
  all at once from a free-format file.  The format of the file is
  as follows:  \mbox {}

  \begin{tabular}{@{\hspace{11mm}}l@{\hspace{24mm}}l}
        \\
        {\tt Mosaic title}     & ! header \\
        {\tt central\_image}   & ! name of central {\mantt{IMAGE}} \\
        {\tt 125}              & ! total number of frames \\
        {\tt 345  229}         & ! maximum {$x$}-{$y$} offsets \\
        {\tt -356  -232}       & ! minimum {$x$}-{$y$} offsets \\
        {\tt image\_2}         & ! subsequent {\mantt{IMAGE}} and \\ 
        {\tt 35  34}           & ! its {$x$}-{$y$} offsets \\
        {\tt image\_3}         & \\
        {\tt 36  -33}          & \\
        {\tt .}                & \\
        {\tt .}                & \\
        {\tt .}                & \\
        \\
  \end{tabular}

  Only like-sized data arrays may be input. The reason for this is
  that it is difficult to work out how big the output data array
  needs to be until all the input data arrays and their offsets have
  been read in. By confining the data arrays to be the same size,
  only the maximum and minimum {$x$} and {$y$} offsets from the central data
  array need be input by the user, then the output image size can be
  worked out from these numbers along with the size of the central
  data array.

  Bad pixels are processed by the magic-value method.

\manroutineitem {Invocation }{}
  QUILT

\newpage
\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{WHERE}}  }{{\mantt{\_CHAR}}}
  Whether input comes from an ASCII {\mantt{'File'}} or from the
  {\mantt{'Interface'}}.
\manparameterentry {{\mantt{READ}} }{{\mantt{FNAME}}  }{{\mantt{\_CHAR}}}
  Name of the ASCII file holding the input information to define 
  the mosaic.
\manparameterentry {{\mantt{READ}} }{{\mantt{NUMBER}}  }{{\mantt{\_INTEGER}}}
  Number of data arrays to form the mosaic.
\manparameterentry {{\mantt{READ}} }{{\mantt{INPICI}}  }{{\mantt{IMAGE}}}
  {\mantt{IMAGE}} structure containing the central data array (offset 0, 0).
\manparameterentry {{\mantt{READ}} }{{\mantt{MAXX}}  }{{\mantt{\_INTEGER}}}
  Maximum {$x$} offset of any data array from the central data array
  in {\mantt Interface} mode (must be {$\geq$}0).
\manparameterentry {{\mantt{READ}} }{{\mantt{MAXY}}  }{{\mantt{\_INTEGER}}}
  Maximum {$y$} offset of any data array from the central data array
  in {\mantt Interface} mode (must be {$\geq$}0).
\manparameterentry {{\mantt{READ}} }{{\mantt{MINX}}  }{{\mantt{\_INTEGER}}}
  Minimum {$x$} offset of any data array from the central data array
  in {\mantt Interface} mode (must be {$\leq$}0).
\manparameterentry {{\mantt{READ}} }{{\mantt{MINY}}  }{{\mantt{\_INTEGER}}}
  Minimum {$y$} offset of any data array from the central data array
  in {\mantt Interface} mode (must be {$\leq$}0).
\manparameterentry {{\mantt{READ}} }{{\mantt{AVERAGE}}  }{{\mantt{\_LOGICAL}}}
  If true overlap regions are averaged, alternatively, they are
  summed.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}}  }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}}  }{{\mantt{\_CHAR}}}
  Title string for output {\mantt{IMAGE}} structure.
 \mbox{{\mantt ['KAPPA - Quilt']}}
\manparameterentry {{\mantt{READ}} }{{\mantt{CURPIC}}  }{{\mantt{IMAGE}}}
  {\mantt{IMAGE}} containing the current data array being concatenated to
  the mosaic.
\manparameterentry {{\mantt{READ}} }{{\mantt{OFFSETX}}  }{{\mantt{\_INTEGER}}}
  {$x$} offset of current data array from the central one
  ({\mantt{Interface}} mode).
\manparameterentry {{\mantt{READ}} }{{\mantt{OFFSETY}}  }{{\mantt{\_INTEGER}}}
  {$y$} offset of current data array from the central one
  ({\mantt{Interface}} mode).
\end{manparametertable}
\manroutineitem {Deficiencies }{}
  Works with like-sized images only and uses Fortran i/o for getting
  stuff from a file. 

\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

\sstroutine{
   RIFT
}{
   Adds a scalar to a section of an NDF data structure to correct
   rift-valley defects
}{
   \sstdescription{
      The routine adds a scalar ({\it i.e.}\ constant) value to each pixel of
      an NDF's data array within a sub-section to produce a new NDF
      data structure.
   }
   \sstusage{
      rift in scalar out section
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         Input NDF data structure, to which the value is to be added.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF data structure.
      }
      \sstsubsection{
         SCALAR = \_DOUBLE (Read)
      }{
         The value to be added to the NDF's data array within the
         section.
      }
      \sstsubsection{
         SECTION = LITERAL (Read)
      }{
         The pixels to which a scalar is to be added.  This is defined
         as an NDF section, so that ranges can be defined along any
         axis, and be given as pixel indices or axis (data)
         co-ordinates.  So for example {\tt "3,4,5"} would select the pixel
         at (3,4,5); {\tt "3:5,"} would select all elements in columns 3 to
         5; {\tt ",4"} selects line 4.  See {\tt "}NDF Sections{\tt "} in SUN/95, or the
         online documentation for details.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the NDF supplied for parameter IN to be used
         instead. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         rift aa 10.7 bb "100:105" 20
      }{
         This adds 10 in the columns 100 to 105 in the data array of
         the NDF called aa and stores the result in the NDF called bb.
         In other respects bb is a copy of aa.
      }
      \sstexamplesubsection{
         rift cubein -100 cubeout ",,4"
      }{
         This adds $-$100 to all values in the fourth plane of the data
         array of the NDF called cubein and stores the result in the
         NDF called cubeout.  In other respects cubeout is a copy of
         cubeout.
      }
      \sstexamplesubsection{
         rift in=aa scalar=2 out=bb section="-10:5,200$\sim$9"
      }{
         This adds 2 to the rectangular section between columns $-$10 to
         5 and lines 196 to 204 of the data array of the NDF called aa
         and stores the result in the NDF called bb.  In other respects
         bb is a copy of aa.
      }
   }
   \sstnotes{
      For similar operations performed on a subset, use the appropriate
      application to process the relevant section and then run PASTE to
      paste the result back into the full array.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CADD, CHPIX, GLITCH, PASTE, SEGMENT, ZAPLIN; Figaro: CSET,
      ICSET, NCSET, TIPPEX.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported. 

         \sstitem
         The bad-pixel flag is set to TRUE if undefined values are
         created during the arithmetic.

         \sstitem
         All non-complex numeric data types can be handled.
      }
   }
}
\sstroutine{
   ROTATE
}{
   Rotates a 2-dimensional NDF about its centre through any angle
}{
   \sstdescription{
      This rotates a 2-dimensional array stored in an NDF data
      structure by an arbitrary angle.  The origin of the rotation is
      the centre of the array.  The output array dimensions just
      accommodate the rotated array.  Output pixels can be generated
      from the input array by one of two methods: nearest-neighbour
      substitution or by bi-linear interpolation.  The latter is slower,
      but gives better results.  Output pixels not corresponding to
      input pixels take the bad value.
   }
   \sstusage{
      rotate in out angle
   }
   \sstparameters{
      \sstsubsection{
         ANGLE  = \_REAL (Read)
      }{
         Number of clockwise degrees by which the data array is to be
         rotated.  It must lie between 0\dgs\ and 360\dgs.  The suggested
         default is the current value.
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         NDF structure containing the 2-dimensional array to be rotated.
      }
      \sstsubsection{
         NNMETH = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, the nearest-neighbour method will be used to evaluate
         the output data-array pixels.  This is only accessed when the
         rotation is not a multiple of 90\dgs.  {\tt [FALSE]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF to contain the rotated arrays.
      }
      \sstsubsection{
         QUALITY = \_LOGICAL (Read)
      }{
         This parameter is only accessed when NNMETH is {\tt FALSE} and ANGLE
         is not a multiple of 90\dgs.  Strictly, the quality values
         are undefined by the bi-linear interpolation and hence cannot
         be propagated.  However, QUALITY = {\tt TRUE} offers an
         approximation to the quality array by propagating the
         nearest-neighbour quality to the output NDF. {\tt [FALSE]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         A title for the output NDF.  A null value will cause the title
         of the NDF supplied for parameter IN to be used instead. {\tt [!]}
      }
      \sstsubsection{
         VARIANCE = \_LOGICAL (Read)
      }{
         VARIANCE = {\tt TRUE} instructs that variance values weight the
         pixels in the bi-linear interpolation and that output variance
         is derived from the neighbouring pixels' variance values,
         otherwise the data values are given equal weight.  This
         parameter is ignored if ANGLE is a multiple of 90\dgs\ or
         NNMETH = {\tt TRUE}; in these cases the variance array is merely
         propagated.  The run-time default is {\tt TRUE} if the input NDF has
         a VARIANCE component, and {\tt FALSE} otherwise.  Note that following
         this operation the errors are no longer independent. {\tt []}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         rotate ns ew 90
      }{
         This rotates the array components in the NDF called ns by
         90\dgs\ clockwise, and stores the result in the NDF called
         ew.  The former $x$ axis becomes the new $y$ axis, and the former
         $y$ axis becomes the new $x$ axis.  The former $y$-axis arrays are
         also reversed in the process.
      }
      \sstexamplesubsection{
         rotate angle=180 out=sn in=ns
      }{
         This rotates the array components in the NDF called ns by
         180\dgs\ clockwise, and stores the result in the NDF called
         sn.  The axis arrays are flipped in the output NDF.
      }
      \sstexamplesubsection{
         rotate f1 f1r 37.2 novariance
      }{
         This rotates the array components in the NDF called f1 by
         37.\udeg2 clockwise, and stores the result in the NDF called
         f1r.  The original axis information is lost.  Bi-linear
         interpolation is used without variance information.  No
         quality or variance information is propagated.
      }
      \sstexamplesubsection{
         rotate f1 f1r 106 nnmeth title="Reoriented features map"
      }{
         This rotates the array components in the NDF called f1 by
         106\dgs\ clockwise, and stores the result in the NDF called
         f1r.  The original axis information is lost.  The resultant
         array components, all of which are propagated, are calculated
         by the nearest-neighbour method.  The title of the output
         NDF is {\tt "Reoriented features map"}.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Bad pixels are ignored in the bi-linear interpolation.  If all
         four pixels are bad, the result is bad.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FLIP, TRANSFORMER; Figaro: IREVX, IREVY, IROT90.
   }
   \sstimplementationstatus{
      The propagation rules depend on parameters ANGLE and NNMETH.
      \ssthitemlist{

         \sstitem
         For rotations that are multiples of 90-degrees, VARIANCE,
         QUALITY, AXIS, HISTORY, LABEL and UNITS components of the input
         NDF are propagated to the output NDF.  The axis components are
         switched and flipped as appropriate.

         \sstitem
         For the nearest-neighbour method VARIANCE, QUALITY, HISTORY,
         LABEL and UNITS components of the input NDF are propagated to the
         output NDF.

         \sstitem
         For the linear interpolation method HISTORY, LABEL and
         UNITS components of the input NDF are propagated to the output
         NDF.  In addition if parameter VARIANCE is {\tt TRUE}, variance
         information is derived from the input variance; and if parameter
         QUALITY is {\tt TRUE}, QUALITY is propagated using the nearest
         neighbour.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric types are supported, though for linear
         interpolation the arithmetic is performed using single- or
         double-precision floating point as appropriate; and for
         90\dgs\ and 270\dgs\ rotations \_INTEGER is used for all
         integer types.
      }
   }
}
\sstroutine{
   SEGMENT
}{
   Copies polygonal segments from one NDF to another
}{
   \sstdescription{
      This routine extracts polygonal segments from an NDF, and
      optionally pastes them into the corresponding positions within
      another NDF.  The application is intended to allow regions of an
      NDF to be removed to another for separate processing.  It may
      also be used to copy bad pixels into a NDF in order to delete a
      region which is not required.

      The vertices of polygonal segments are defined by lists of
      co-ordinate pairs. [For convenience these positions are denoted
      $x$-$y$ positions although they may not be literally $x$ and $y$,
      for example planes in a cube or hypercube.  See the
      {\tt "}Notes{\tt "} for more
      details.]  Polygons are completed by connecting the last position
      in the list to the first.  Pixels within each polygonal segment
      are copied from the first NDF (IN1) to the corresponding position
      in the second NDF (IN2).  A sequence of polygons may be supplied,
      and each is copied in turn.  If a null value is given for either
      NDF, the routine behaves as if an NDF full of bad pixels had been
      specified.  Thus if IN1 is given a null value the inside of each
      polygonal segment will be filled with bad values, and if IN2 is
      given a null value the region outside the polygonal segments will
      be filled with bad values.

      The $x$-$y$ positions may be specified in three ways:

      1) from the parameter system, usually in response to prompting;

      2) within text files (one for each polygon), where the files are free
      format with $x$ co-ordinates in column one and $y$ co-ordinates in
      column two (this is the format produced by other {\footnotesize
      KAPPA} applications such as CURSOR); and

      3) using a graphics cursor of a nominated device, for which an NDF must
      already have been displayed on the device.

      The $x$-$y$ co-ordinates may be given as either data or pixel
      (`world') co-ordinates.  If data co-ordinates are given, the input
      NDFs must contain appropriate AXIS structures to allow the
      corresponding pixel co-ordinates to be found.
   }
   \sstusage{
      segment in1 in2 out
   }
   \sstparameters{
      \sstsubsection{
         AXES( 2 ) = \_INTEGER (Read)
      }{
         The indices of the axes which span the plane containing the
         polygon.  Two values should be given, each less than or equal
         to the minimum of the number of dimensions in the two input
         NDFs. {\tt [1,2]}
      }
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         Whether or not the image display device should be cleared
         before opening it. {\tt [FALSE]}
      }
      \sstsubsection{
         COORDS( 2 ) = \_REAL (Read)
      }{
         A pair of $x$-$y$ co-ordinates representing a single vertex.  It
         is only used if parameter MODE is given the value {\tt "Interface"}.  A
         null value should be given when the final vertex has been
         specified.
      }
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system in which the polygon vertices are
         specified. This can be either {\tt "World"} or {\tt "Data"}.  If COSYS =
         {\tt "Data"} is given, the input co-ordinates (however obtained) are
         presumed to be data co-ordinates (as defined by AXIS structures
         within the NDFs).  Otherwise, they are presumed to be world (or
         `pixel') co-ordinates. {\tt [}Current co-ordinate system{\tt ]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The name of the graphics device on which an image is
         displayed.  This is only used if parameter MODE is given the
         value {\tt "Cursor"}.  Any graphics specified by parameter PLOT will be
         produced on this device.  This device must support cursor
         interaction, and belong to one of the following classes:
         TERMINAL, IMAGE\_DISPLAY, IMAGE\_OVERLAY, WINDOW, and
         WINDOW\_OVERLAY.  {\tt [}Current image-display-overlay device{\tt ]}
      }
      \sstsubsection{
         IN1 = NDF (Read)
      }{
         The input NDF containing the data to be copied to the inside of
         the supplied polygonal segments.  If a null value is supplied,
         the inside of the polygonal segments will be filled with bad
         values.
      }
      \sstsubsection{
         IN2 = NDF (Read)
      }{
         The input NDF containing the data to be copied to the outside
         of the supplied polygonal segments.  If a null value is
         supplied, the outside of the polygonal segments will be filled
         with bad values.
      }
      \sstsubsection{
         LOGFILE = FILENAME (Write)
      }{
         The name of a text file in which the co-ordinates of the
         polygon vertices are to be stored.  A null value ({\tt !}) means that
         no file is created. {\tt [!]}
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The mode by which the vertices of the polygonal segments are
         to be obtained.  The options are as follows: {\tt "Interface"}
         defines via the parameter system, {\tt "Cursor"} enables selection
         by graphics cursor, and {\tt "File"} reads them from a text file.
         {\tt [}Current interaction mode{\tt ]}
      }
      \sstsubsection{
         MAXPOLY = \_INTEGER (Read)
      }{
         The maximum number of polygons which can be used.  For
         instance, this can be set to 1 to ensure that no more than 1
         polygon is used (this sort of thing can be useful when writing
         procedures or scripts).  A null value causes no limit to be
         imposed (unless MODE={\tt "File"} in which case a limit of 20 is
         imposed). {\tt [!]}
      }
      \sstsubsection{
         MINPOLY = \_INTEGER (Read)
      }{
         The minimum number of polygons which can be used.  For
         instance, this can be set to 2 to ensure that at least 2
         polygons are used.  The supplied value must be less than or
         equal to the
         value given for MAXPOLY and must be greater than zero. {\tt [1]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The output NDF.
      }
      \sstsubsection{
         POLY1-POLY20 = FILENAME (Read)
      }{
         Each of the parameters POLY1 to POLY20 are used to access text
         files containing the $x$-$y$ co-ordinates of the vertices of a
         single polygon.  If a value is assigned to POLY1 on the
         command line, you are not prompted for any of the remaining
         parameters in this group; additional polygon files must also
         be supplied on the command line.  Otherwise, you are prompted
         for POLY1, then POLY2, {\it etc.}\ until a null value is given
         or POLY20 is reached.
      }
      \sstsubsection{
         QUALITY = \_LOGICAL (Read)
      }{
         If a {\tt TRUE} value is supplied for parameter QUALITY then quality
         information is copied from the input NDFs to the output NDFs.
         Otherwise, the quality information is not copied.  This
         parameter is only accessed if all supplied input NDFs have
         defined QUALITY components.  If any of the supplied input NDFs
         do not have defined QUALITY components, then no quality is
         copied.  Note, if a null input NDF is given then the
         corresponding output QUALITY values are set to zero. {\tt [TRUE]}
      }
      \sstsubsection{
         VARIANCE = \_LOGICAL (Read)
      }{
         If a {\tt TRUE} value is supplied for parameter VARIANCE then
         variance information is copied from the input NDFs to the
         output NDFs.  Otherwise, the variance information is not
         copied.  This parameter is only accessed if all supplied input
         NDFs have defined VARIANCE components.  If any of the supplied
         input NDFs do not have defined VARIANCE components, then no
         variances are copied.  Note, if a null input NDF is given then
         the corresponding output VARIANCE values are set bad. {\tt [TRUE]}
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         COLOUR = LITERAL (Read)
      }{
         The colour in which to draw any graphics specified by
         parameter PLOT.  The options are described below.

         \begin{description}
         \item {\tt "MAX"}    --- The maximum colour index used for the
                              display of the image.
         \item {\tt "MIN"}    --- The minimum colour index used for the
                              display of the image.
         \item [An integer]   --- The actual colour index.  It is constrained
                              between 0 and the maximum colour index
                              available on the device.
         \item [A named colour] --- Uses the named colour from the palette, and
                            if it is not present, the nearest colour
                            from the palette is selected.
         \end{description}

         If the colour is to remain unaltered as the lookup table is
         manipulated choose an integer between 0 and 15, or a named
         colour.  The suggested default is the current value.

         This parameter is ignored on window overlays, where the
         overlay colour is used.  (Use the PALENTRY command to change
         this colour.)  An overlay has the advantage that the crosses
         or polygon can be erased using OVCLEAR once this task is
         completed.  The parameter is also ignored for terminals.

         {\tt [}The current value, but equals {\tt "Green"} if there is
         no current value.{\tt ]}
      }
      \sstsubsection{
         PLOT = LITERAL (Read)
      }{
         The type of graphics to be used to mark the position of each
         selected vertex.  It is only used if parameter MODE is given
         the value {\tt "Cursor"}.  PLOT can take any of the following values.

         \begin{description}
         \item {\tt "Poly"} --- Causes each vertex to be joined by a straight line
                     to the previous vertex.  The last vertex is joined
                     to the first vertex.

         \item {\tt "Cross"} --- Each vertex is marked by a cross.

         \item {\tt "None"} --- No graphics are produced.
         \end{description}

         The initial default is {\tt "Poly"}, then subsequently it is the
         current value.  {\tt []}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         segment in1=m51a in2=m51b out=m51\_comp poly1=co-ords.lis mode=file
      }{
         Copies a region of the NDF m51a to the corresponding position
         in the output NDF m51\_comp.  The region is defined by the list
         of vertex co-ordinates held in text file {\tt co-ords.lis}.  All
         pixels in the output NDF which fall outside this region are given
         the corresponding pixel values from NDF m51b.
      }
      \sstexamplesubsection{
         segment in1=m51a out=m51\_cut mode=cursor plot=poly accept
      }{
         Copies a region of the NDF m51a to the corresponding position
         in the output NDF m51\_cut.  The region is defined by selecting
         vertices using a graphics cursor.  The image m51a should
         previously have been displayed.  Each vertex is joined to the
         previous vertex by a green line on the graphics device. The
         ACCEPT keyword causes the suggested null default value for IN2
         to be accepted.  This means that all pixels outside the region
         identified using the cursor will be set bad in the output NDF.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The routine can handle NDFs of arbitrary dimensionality.  If
         either input has three or more dimensions then all planes in the
         NDF are processed in the same way, that is the same polygonal
         regions are extracted from each plane and copied to the
         corresponding plane of the output NDF. The polygon is usually
         presumed to lie in the $x$-$y$ plane ({\it i.e.}\ the plane spanned by the
         first two axes of the NDF), but this can be changed by assigning
         appropriate values to parameter AXES so that for instance the
         polygon lies in the $y$-$z$ plane ({\it i.e.}\ the plane spanned by axes 2
         and 3).

         \sstitem
         The output NDF bounds are defined by the overlap region of the
         input NDFs.

         \sstitem
         The log file has a comment line indicating whether the
         co-ordinates are World or Data.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ARDMASK, ERRCLIP, FILLBAD, FFCLEAN, PASTE, SETMAGIC, THRESH.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine will propagate VARIANCE component values so long
         as all supplied input NDFs have defined VARIANCE components, and
         parameter VARIANCE is not {\tt FALSE}.

         \sstitem
         This routine will propagate QUALITY component values so long
         as all supplied input NDFs have defined QUALITY components, and
         parameter QUALITY is not {\tt FALSE}.

         \sstitem
         The UNITS, AXIS, LABEL, TITLE, and HISTORY components are
         propagated from the first supplied input NDF, together with all
         extensions.

         \sstitem
         All non-complex numeric types are supported.  The following
         data types are processed directly: \_WORD,
         \_INTEGER, \_REAL, \_DOUBLE.
      }
   }
}
\sstroutine{
   SETAXIS
}{
   Sets values for an axis array component within an NDF data
   structure
}{
   \sstdescription{
      This routine modifies the values of an axis array component or
      system within an NDF data structure.  There are a number of
      options (see parameter MODE).  They permit the deletion of the
      axis system, or an individual variance or width component; the
      replacement of one or more individual values; assignment of the
      whole array using Fortran-like mathematical expressions, or values
      in a text file, or to pixel co-ordinates.

      If an AXIS structure does not exist, a new one whose centres are
      pixel co-ordinates is created before any modification.
   }
   \sstusage{
      setaxis ndf dim mode [comp]
        $\left\{ {\begin{tabular}{l}
                    file=? \\  
                    index=? newval=? \\
                    exprs=? \\
                   \end{tabular} }
        \right.$
        \newline\hspace*{13.9em}
        \makebox[0mm][c]{\small mode}
   }
   \sstparameters{
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The name of the NDF axis array component to be modified.  The
         choices are: {\tt "Centre"}, {\tt "Data"}, {\tt "Error"}, {\tt "Width"} or {\tt "Variance"}.
         {\tt "Data"} and {\tt "Centre"} are synonyms and selects the axis centres.
         {\tt "Variance"} is the variance of the axis centres, i.e. measures
         the uncertainty of the axis-centre values.  {\tt "Error"} is the
         alternative to {\tt "Variance"} and causes the square of the
         supplied error values to be stored.  {\tt "Width"} selects the axis
         width array.  {\tt ["Data"]}
      }
      \sstsubsection{
         DIM = \_INTEGER (Read)
      }{
         The axis dimension for which the array component is to be
         modified.  There are separate arrays for each NDF dimension.
         The value must lie between 1 and the number of dimensions of
         the NDF.  This defaults to 1 for a 1-dimensional NDF.  DIM is
         not accessed when COMP={\tt "Centre"} and MODE={\tt "Delete"}.  The
         suggested default is the current value. {\tt []}
      }
      \sstsubsection{
         EXPRS = LITERAL (Read)
      }{
         A Fortran-like arithmetic expression giving the value to be
         assigned to each element of the axis array specified by
         parameter COMP.  The expression may just contain a constant
         for the axis widths or variances, but the axis-centre values
         must vary.  In the latter case and whenever a constant value
         is not required, there are two tokens available---INDEX and
         CENTRE---either or both of which may appear in the expression.
         INDEX represents the pixel index of the corresponding array
         element, and CENTRE represents the existing axis centres.
         Either the CENTRE or the INDEX token must appear in the
         expression when modifying the axis centres.  All of the
         standard Fortran-77 intrinsic functions are available for use
         in the expression, plus a few others (see SUN/61 for details
         and an up-to-date list).

         Here are some examples.  Suppose the axis centres are being
         changed, then \linebreak
         EXPRS={\tt "INDEX-0.5"} gives pixel co-ordinates,
         EXPRS={\tt "2.3 $*$ INDEX $+$ 10"} \linebreak
         would give a linear axis at offset 10 and an increment
         of 2.3 per pixel, \linebreak 
         EXPRS={\tt "LOG(INDEX$*$5.2)"} would give a logarithmic axis,
         and \linebreak 
         EXPRS={\tt "CENTRE$+$10"} would add
         ten to all the array centres.  If COMP={\tt "Width"}, EXPRS=0.96
         would set all the widths to 0.96, and EXPRS={\tt "SIND(INDEX-30)$+$2"}
         would assign the widths to two plus the sine of the pixel
         index with respect to index 30 measured in degrees.

         EXPRS is only accessed when MODE={\tt "Expression"}.
      }
      \sstsubsection{
         FILE = FILENAME (Read)
      }{
         Name of the text file containing the free-format axis data.
         This parameter is only accessed if MODE={\tt "File"}.  The
         suggested default is the current value.
      }
      \sstsubsection{
         INDEX = \_INTEGER (Read)
      }{
         The pixel index of the array element to change.  A null value
         ({\tt !}) terminates the loop during multiple replacements.  This
         parameter is only accessed when MODE={\tt "Edit"}.  The suggested
         default is the current value.
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The mode of the modification.  It can be one of the following:

         \begin{description}
         \item {\tt "Delete"} --- Deletes the array, unless COMP={\tt "Data"} or
                           {\tt "Centre"} whereupon the whole axis structure
                           is deleted.
         \item {\tt "Edit"}  --- Allows the modification of individual
                           elements within the array.
         \item {\tt "Expression"} --- Allows a mathematical expression to define
                           the array values.  See parameter EXPRS.
         \item {\tt "File"} --- The array values are read in from a
                           free-format text file.
         \item {\tt "Pixel"} --- The axis centres are set to pixel
                           co-ordinates.  This is only available when
                           COMP={\tt "Data"} or {\tt "Centre"}.
         \end{description}

         The suggested default is the current value.
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF data structure in which an axis array component is to
         be modified.
      }
      \sstsubsection{
         NEWVAL = LITERAL (Read)
      }{
         Value to substitute in the array element.  The range of
         allowed values depends on the data type of the array being
         modified.  NEWVAL={\tt "Bad"} instructs that the bad value
         appropriate for the array data type be substituted.  Placing
         NEWVAL on the command line permits only one element to be
         replaced.  If there are multiple replacements, a null value
         ({\tt !}) terminates the loop.  This parameter is only accessed when
         MODE={\tt "Edit"}.
      }
      \sstsubsection{
         TYPE = LITERAL (Read)
      }{
         The data type of the modified axis array.  TYPE can be either
         {\tt "\_REAL"} or {\tt "\_DOUBLE"}.  It is only accessed for
         MODE={\tt "File"}, {\tt "Expression"}, or {\tt "Pixel"}.  The
         default is the current data
         type of the array component if it exists, otherwise it is
         {\tt "\_REAL"}. {\tt []}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         setaxis ff mode=delete
      }{
         This erases the axis structure from the NDF called ff.
      }
      \sstexamplesubsection{
         setaxis abell4 1 expr exprs="CENTRE $+$ 0.1 $*$ (INDEX-1)"
      }{
         This modifies the axis centres along the first axis in the NDF
         called abell4.  The new centre values are spaced by 0.1 more
         per element than previously.
      }
      \sstexamplesubsection{
         setaxis cube 3 expr error exprs="25.3$+$0.2$*$MOD(INDEX,8)"
      }{
         This modifies the axis errors along the third axis in the NDF
         called cube.  The new errors values are given by the
         expression {\tt "25.3$+$0.2$*$MOD(INDEX,8)"}, in other words the noise
         has a constant term (25.3), and a cyclic ramp component of
         frequency 8 pixels.
      }
      \sstexamplesubsection{
         setaxis spectrum mode=file file=spaxis.dat
      }{
         This assigns the axis centres along the first axis in the
         1-dimensional NDF called spectrum.  The new centre values are
         read from the free-format text file called {\tt spaxis.dat}.
      }
      \sstexamplesubsection{
         setaxis ndf=plate3 dim=2 mode=pixel
      }{
         This assigns pixel co-ordinates to the second axis's centres
         in the NDF called plate3.
      }
      \sstexamplesubsection{
         setaxis datafile 2 expression exprs="centre" type=\_real
      }{
         This modifies the data type of axis centres along the second
         dimension of the NDF called datafile to be \_REAL.
      }
      \sstexamplesubsection{
         setaxis cube 2 edit index=3 newval=129.916
      }{
         This assigns the value 129.916 to the axis centre at index 3
         along the second axis of the NDF called cube.
      }
      \sstexamplesubsection{
         setaxis comp=width ndf=cube dim=1 mode=edit index=-16 newval=1E-05
      }{
         This assigns the value 10$^{-5}$ to the axis width at index $-$16
         along the first axis of the NDF called cube.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         An end-of-file error results when MODE={\tt "File"} and the file
         does not contain sufficient values to assign to the whole array.
         In this case the axis array is unchanged.  A warning is given if
         there are more values in a file record than are needed to complete
         the axis array.

         \sstitem
         An invalid expression when MODE={\tt "Expression"} results in an
         error and the axis array is unchanged.

         \sstitem
         The chapter entitled {\tt "}The Axis Coordinate System{\tt "} in SUN/33
         describes the NDF axis co-ordinate system and is recommended
         reading especially if you are using axis widths.

         \sstitem
         There is no check, apart from constraints on parameter NEWVAL,
         that the variance is not negative and the widths are positive.
      }
   }
   \sstdiytopic{
      File Format
   }{
      The format is quite flexible.  The number of axis-array values
      that may appear on a line is variable; the values are separated
      by at least a space, comma, tab or carriage return.  A line can
      have up to 255 characters.  In addition a record may have
      trailing comments designated by a hash or exclamation mark.  Here
      is an example file, though a more regular format would be clearer
      for the human reader (say 10 values per line with commenting).

      {\tt \begin{verse}
      \# Axis Centres along second dimension \\
      -3.4 -0.81 \\
      .1 3.3 4.52 5.6 9 10.5 12.  15.3   18.1  20.2 \\
      23 25.3 ! a comment \\
      26.8,27.5 29. 30.76  32.1 32.4567 \\
      35.2 37. \\
      <EOF> \\
      \end{verse}}
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: AXCONV, AXLABEL, AXUNITS; Figaro: LXSET, LYSET.
   }
   \sstimplementationstatus{
      Processing is in single- or double-precision floating point.
   }
}
\sstroutine{
   SETBAD
}{
   Sets new bad-pixel flag values for an NDF
}{
   \sstdescription{
      This application sets new logical values for the bad-pixel flags
      associated with an NDF's data and/or variance arrays.  It may
      either be used to test whether bad pixels are actually present in
      these arrays and to set their bad-pixel flags accordingly, or to
      set explicit TRUE or FALSE values for these flags.
   }
   \sstusage{
      setbad ndf [value]
   }
   \sstparameters{
      \sstsubsection{
         DATA = \_LOGICAL (Read)
      }{
         This parameter controls whether the NDF's data array is
         processed. If a {\tt TRUE} value is supplied (the default), then it
         will be processed. Otherwise it will not be processed, so that
         the variance array (if present) may be considered on its own.
         The DATA and VARIANCE parameters should not both be set to
         {\tt FALSE}.  {\tt [TRUE]}
      }
      \sstsubsection{
         MODIFY = \_LOGICAL (Read)
      }{
         If a {\tt TRUE} value is supplied for this parameter (the default),
         then the NDF's bad-pixel flags will be permanently modified if
         necessary. If a {\tt FALSE} value is supplied, then no modifications
         will be made. This latter mode allows the routine to be used
         to check for the presence of bad pixels without changing the
         current state of an NDF's bad-pixel flags.  It also allows the
         routine to be used on NDFs for which write access is not
         available. {\tt [TRUE]}
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF in which bad pixels are to be checked for, and/or
         whose bad-pixel flags are to be modified. (Note that setting
         the MODIFY parameter to {\tt FALSE} makes it possible to check for
         bad pixels without permanently modifying the NDF.)
      }
      \sstsubsection{
         VALUE = \_LOGICAL (Read)
      }{
         If a null ({\tt !}) value is supplied for this parameter (the
         default), then the routine will check to see whether any bad
         pixels are present.  This will only involve testing the value
         of each pixel if the bad-pixel flag value is initially TRUE,
         in which case it will be reset to FALSE if no bad pixels are
         found.  If the bad-pixel flag is initially FALSE, then it will
         remain unchanged.

         If a logical ({\tt TRUE} or {\tt FALSE}) value is supplied for
         this parameter, then it indicates the new bad-pixel flag value
         which is to be set.  Setting a {\tt TRUE} value indicates to later
         applications that there may be bad pixels present in the NDF,
         for which checks must be made. Conversely, setting a {\tt FALSE}
         value indicates that there are definitely no bad pixels
         present, in which case later applications need not check for
         them and should interpret the pixel values in the NDF
         literally.

         The VALUE parameter is not used (a null value is assumed) if
         the MODIFY parameter is set to {\tt FALSE} indicating that the NDF
         is not to be permanently modified. {\tt [!]}
      }
      \sstsubsection{
         VARIANCE = \_LOGICAL (Read)
      }{
         This parameter controls whether the NDF's variance array is
         processed. If a {\tt TRUE} value is supplied (the default), then it
         will be processed. Otherwise it will not be processed, so that
         the data array may be considered on its own.  The DATA and
         VARIANCE parameters should not both be set to {\tt FALSE}. 
         {\tt [TRUE]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         setbad ngc1097
      }{
         Checks the data and variance arrays (if present) in the NDF
         called ngc1097 for the presence of bad pixels. If the initial
         bad-pixel flag values indicate that bad pixels may be present,
         but none are found, then the bad-pixel flags will be reset to
         FALSE. The action taken will be reported.
      }
      \sstexamplesubsection{
         setbad ndf=ngc1368 nomodify
      }{
         Performs the same checks as described above, this time on the
         NDF called ngc1368. The presence or absence of bad pixels is
         reported, but the NDF is not modified.
      }
      \sstexamplesubsection{
         setbad myfile nodata
      }{
         Checks the variance array (if present) in the NDF called
         myfile for the presence of bad pixels, and modifies its
         bad-pixel flag accordingly. Specifying {\tt nodata} inhibits
         processing of the data array, whose bad-pixel flag is left
         unchanged.
      }
      \sstexamplesubsection{
         setbad halpha false
      }{
         Sets the bad-pixel flag for the NDF called halpha to FALSE.
         Any pixel values which might previously have been regarded as
         bad will subsequently be interpreted literally as valid
         pixels.
      }
      \sstexamplesubsection{
         setbad hbeta true
      }{
         Sets the bad-pixel flags for the NDF called hbeta to be TRUE.
         If any pixels have the special `bad' value, then they will
         subsequently be regarded as invalid pixels. Note that if this
         is followed by a further command such as {\tt "setbad hbeta"}, then
         an actual check will be made to see whether any pixels have
         this special value. The bad-pixel flags will be returned to
         FALSE if they do not.
      }
   }
   \sstdiytopic{
      Bad-Pixel Flag Values
   }{
      If a bad-pixel flag is TRUE, it indicates that the associated NDF
      array may contain the special `bad' value and that affected
      pixels are to be regarded as invalid.  Subsequent applications
      will need to check for such pixels and, if found, take account of
      them.

      Conversely, if a bad-pixel flag value is FALSE, it indicates that
      there are no bad pixels present.  In this case, any special `bad'
      values appearing in the array are to be interpreted literally as
      valid pixel values.
   }
   \sstdiytopic{
      Quality Components
   }{
      Bad pixels may also be introduced into an NDF's data and variance
      arrays implicitly through the presence of an associated NDF
      quality component. This application will not take account of such
      a component, nor will it modify it.

      However, if either of the NDF's data or variance arrays do not
      contain any bad pixels themselves, a check will be made to see
      whether a quality component is present. If it is (and its
      associated bad-bits mask is non-zero), then a warning message
      will be issued indicating that bad pixels may be introduced via
      this quality component. If required, these bad pixels may be
      eliminated either by setting the bad-bits mask to zero or by
      erasing the quality component.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: NOMAGIC, SETMAGIC.
   }
}
\sstroutine{
   SETBB
}{
   Sets a new value for the quality bad-bits mask of an NDF
}{
   \sstdescription{
      This application sets a new value for the bad-bits mask
      associated with the quality component of an NDF.  This 8-bit mask
      is used to select which of the bits in the quality array should
      normally be used to generate `bad' pixels when the NDF is
      accessed.

      Wherever a bit is set to 1 in the bad-bits mask, the
      corresponding bit will be extracted from the NDF's quality array
      value for each pixel (the other quality bits being ignored). A
      pixel is then considered `bad' if any of the extracted quality
      bits is set to 1. Effectively, the bad-bits mask therefore allows
      selective activation of any of the eight 1-bit masks which can be
      stored in the quality array.
   }
   \sstusage{
      setbb ndf bb
   }
   \sstparameters{
      \sstsubsection{
         AND = \_LOGICAL (Read)
      }{
         By default, the value supplied via the BB parameter will be
         used literally as the new bad-bits mask value. However, if a
         {\tt TRUE} value is given for the AND parameter, then a bit-wise
         `AND' will first be performed with the old value of the mask.
         This facility allows individual bits in within the mask to be
         cleared ({\it i.e.}\ reset to zero) without affecting the current
         state of other bits (see the {\tt "}Examples{\tt "} section).

         The AND parameter is not used if a {\tt TRUE} value is given for the
         OR parameter. {\tt [FALSE]}
      }
      \sstsubsection{
         BB = LITERAL (Read)
      }{
         The new integer value for the bad-bits mask. This may either
         be specified in normal decimal notation, or may be given using
         binary, octal or hexadecimal notation by adding a {\tt "B"}, 
         {\tt "O"} or {\tt "Z"} prefix (respectively) to the appropriate string of digits.
         The value supplied should lie in the range 0 to 255 decimal (or
         8 bits of binary).

         If the AND and OR parameters are both {\tt FALSE}, then the value
         supplied will be used directly as the new mask value.
         However, if either of these logical parameters is set to {\tt TRUE},
         then an appropriate bit-wise `AND' or `OR' operation with the
         old mask value will first be performed.

         The default value suggested when prompting for this value is
         chosen so as to leave the original mask value unchanged.
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF whose bad-bits mask is to be modified.
      }
      \sstsubsection{
         OR = \_LOGICAL (Read)
      }{
         By default, the value supplied via the BB parameter will be
         used literally as the new bad-bits mask value. However, if a
         {\tt TRUE} value is given for the OR parameter, then a bit-wise `OR'
         will first be performed with the old value of the mask.  This
         facility allows individual bits in within the mask to be set
         to 1 without affecting the current state of other bits (see
         the {\tt "}Examples{\tt "} section). {\tt [FALSE]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         setbb myframe 3
      }{
         Sets the bad-bits mask value for the quality component of the
         NDF called myframe to the value 3. This means that bits 1 and
         2 of the associated quality array will be used to generate bad
         pixels.
      }
      \sstexamplesubsection{
         setbb ndf=myframe bb=b11
      }{
         This example performs the same operation as above, but in this
         case the new mask value has been specified using binary
         notation.
      }
      \sstexamplesubsection{
         setbb xspec b10001000 or
      }{
         Causes the bad-bits mask value in the NDF called xspec to
         undergo a bit-wise `OR' operation with the binary value
         10001000. This causes bits 4 and 8 to be set without changing
         the state of any other bits in the mask.
      }
      \sstexamplesubsection{
         setbb quasar ze7 and
      }{
         Causes the bad-bits mask value in the NDF called quasar to
         undergo a bit-wise `AND' operation with the hexadecimal value
         E7 (binary 11100111). This causes bits 4 and 5 to be cleared
         ({\it i.e.}\  reset to zero) without changing the state of any other
         bits in the mask.
      }
   }
   \sstnotes{
      The bad-bits value will be disregarded if the NDF supplied does
      not have a quality component present. A warning message will be
      issued if this should occur.
   }
   \sstdiytopic{
      Related Applications
   }{
      Figaro: Q2BAD; IRAS90: QUALTOBAD, REMQUAL, SETQUAL, SHOWQUAL.
   }
}
\sstroutine{
   SETBOUND
}{
   Sets new bounds for an NDF
}{
   \sstdescription{
      This application sets new pixel-index bounds for an NDF, either
      trimming it to remove unwanted pixels, or padding it with
      bad pixels to achieve the required shape. The number of dimensions
      may also be altered.  The NDF is accessed in update mode and
      modified {\it in situ}, preserving existing pixel values which lie
      within the new bounds.
   }
   \sstusage{
      setbound ndf
   }
   \sstparameters{
      \sstsubsection{
         LIKE = NDF (Read)
      }{
         This parameter may be used to specify an NDF which is to be
         used as a shape template. If such a template is supplied, then
         its bounds will be used to determine the new shape required
         for the NDF specified via the NDF parameter. By default no
         template will be used and the new shape will be determined
         by means of a section specification applied to the NDF being
         modified (see the {\tt "}Examples{\tt "}).  {\tt [!]}
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF whose bounds are to be modified. In normal use, an
         NDF section will be specified for this parameter (see the
         {\tt "}Examples{\tt "}) and the routine will use the bounds of this section
         to determine the new bounds required for the base NDF from
         which the section is drawn. The base NDF is then accessed in
         update mode and its bounds are modified {\it in situ\/} to make them
         equal to the bounds of the section specified. If a section is
         not specified, then the NDF's shape will only be modified if a
         shape template is supplied via the LIKE parameter.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         setbound datafile(1:512,1:512)
      }{
         Sets the pixel-index bounds of the NDF called datafile to be
         (1:512,1:512), either by trimming off unwanted pixels or by
         padding out with bad pixels, as necessary.
      }
      \sstexamplesubsection{
         setbound alpha(:7,56:)
      }{
         Modifies the NDF called alpha so that its first dimension has
         an upper bound of 7 and its second dimension has a lower bound
         of 56. The lower bound of the first dimension and the upper
         bound of the second dimension remain unchanged.
      }
      \sstexamplesubsection{
         setbound ndf=kg74b(,5500.0$\sim$100.0)
      }{
         Sets new bounds for the NDF called kg74b. The bounds of the
         first dimension are left unchanged, but those of the second
         dimension are changed so that this dimension has an extent of
         100.0 centred on 5500.0, using the physical units in which
         this second dimension is calibrated.
      }
      \sstexamplesubsection{
         setbound newspec like=oldspec
      }{
         Changes the bounds of the NDF newspec so that they are equal
         to the bounds of the NDF called oldspec.
      }
      \sstexamplesubsection{
         setbound xflux(:2048) like=xflux
      }{
         Extracts the section extending from the lower bound of the
         1-dimensional NDF called xflux up to pixel 2048, and then
         modifies the bounds of this section to be equal to the
         original bounds of xflux, replacing xflux with this new NDF.
         This leaves the final shape unchanged, but sets all pixels
         from 2049 onwards to be equal to the bad-pixel value.
      }
      \sstexamplesubsection{
         setbound whole(5:10,5:10) like=whole(0:15,0:15)
      }{
         Extracts the section (5:10,~5:10) from the base NDF called
         whole and then sets its bounds to be equal to those of the
         section whole(0:15,~0:15), replacing whole with this new NDF.
         The effect is to select a 6-pixel-square region from the
         original NDF and then to pad it with a 5-pixel-wide border of
         bad pixels.
      }
   }
   \sstnotes{
      This routine modifies the NDF {\it in situ\/} and will not release unused
      file space if the size of the NDF is reduced. If recovery of
      unused file space is required, then the related application
      NDFCOPY should be used. This will copy the selected region of an
      NDF to a new data structure from which any unused space will be
      eliminated.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: NDFCOPY, SETORIGIN; Figaro: ISUBSET.
   }
}
\sstroutine{
   SETEXT
}{
   Manipulates the contents of a specified NDF extension
}{
   \sstdescription{
      This task enables the contents of a specified NDF extension to be
      edited.  It can create a new extension or delete an existing one,
      can create new scalar components within an extension, or modify
      or display the values of existing scalar components within the
      extension.  The task operates on only one extension at a
      time, and must be closed down and restarted to work on a new
      extension.

      The task may operate in one of two modes, according to the
      LOOP parameter.  When LOOP={\tt FALSE} only a single option is
      executed at a time, making the task suitable for use from an
      {\footnotesize ICL} procedure.  When LOOP={\tt TRUE} several
      options may be executed at once, making it easier to modify
      several extension components interactively in one go.
   }
   \sstusage{
      setext ndf xname option cname 
        $\left\{ {\begin{tabular}{l}
                    ok \\
                    ctype=? shape=? ok \\
                    newname=? \\
                    xtype=? 
                   \end{tabular} }
        \right.$
        \newline\hspace*{14.95em}
        \makebox[0mm][c]{\small option}
   }
   \sstparameters{
      \sstsubsection{
         CNAME = LITERAL (Read)
      }{
         The name of component (residing within the extension) to be
         examined or modified.  It is only accessed when OPTION={\tt "Erase"},
         {\tt "Get"}, {\tt "Put"}, or {\tt "Rename"}.
      }
      \sstsubsection{
         CTYPE = LITERAL (Read)
      }{
         The type of component (residing within the extension) to be
         created.  Allowed values are {\tt "LITERAL"}, {\tt "\_LOGICAL"}, {\tt "\_DOUBLE"},
         {\tt "\_REAL"}, {\tt "\_INTEGER"}, {\tt "\_CHAR"}, {\tt "\_BYTE"}, {\tt "\_UBYTE"}, {\tt "\_UWORD"},
         {\tt "\_WORD"}.  The length of the character type may be defined by
         appending the length, for example, {\tt "\_CHAR$*$32"} is a
         32-character component.  {\tt "LITERAL"} and {\tt "\_CHAR"} generate
         80-character components.  CTYPE is only accessed when
         OPTION={\tt "Put"}.
      }
      \sstsubsection{
         CVALUE = LITERAL (Read)
      }{
         The value(s) for the component.  Each value is converted to the
         appropriate data type for the component.  CVALUE is only
         accessed when OPTION={\tt "Put"}.  Note that for an array of values
         the list must be enclosed in brackets, even in response to a
         prompt.  For convenience, if LOOP={\tt TRUE}, you are prompted for
         each string.
      }
      \sstsubsection{
         LOOP = \_LOGICAL (Read)
      }{
         LOOP={\tt FALSE} requests that only one operation be performed.
         This allows batch and non-interactive processing or use in
         procedures.  LOOP={\tt TRUE} makes SETEXT operate in a looping mode
         that allows several modifications and/or examinations to be
         made to the NDF for one activation.  Setting OPTION to {\tt "Exit"}
         will end the looping.  {\tt [TRUE]}
      }
      \sstsubsection{
         NDF = NDF (Update)
      }{
         The NDF to modify or examine.
      }
      \sstsubsection{
         NEWNAME = LITERAL (Read)
      }{
         The new name of a renamed extension component.  It is only
         accessed when OPTION={\tt "Rename"}.
      }
      \sstsubsection{
         OK = \_LOGICAL (Read)
      }{
         This parameter is used to seek confirmation before a component
         is erased or overwritten.  A {\tt TRUE} value permits the operation.
         A {\tt FALSE} value leaves the existing component unchanged.  This
         parameter is ignored when LOOP={\tt FALSE}.
      }
      \sstsubsection{
         OPTION = LITERAL (Read)
      }{
         The operation to perform on the extension or a component
         therein.  The recognised options are listed below.
         \begin{description}
         \item {\tt "Delete"} --- Delete an existing NDF extension.
         \item {\tt "Erase"}  --- Erase a component within an NDF extension
         \item {\tt "Exit"}   --- Exit from the task (when LOOP={\tt TRUE})
         \item {\tt "Get"}    --- Display the value of a component within an NDF
                                  extension.  The component must exist.
         \item {\tt "Put"}    --- Change the value of a component within an NDF
                         extension or create a new component.
         \item {\tt "Rename"} --- Renames a component.  The component must exist.
         \item {\tt "Select"} --- Selects another extension.  If the extension
                         does not exist a new one is created.  This
                         option is not allowed when LOOP={\tt FALSE}.
         \end{description}

         The suggested default is the current value, except for the
         first option where there is no default.
      }
      \sstsubsection{
         SHAPE( ) = \_INTEGER (Read)
      }{
         The shape of the component.  Thus {\tt 3,2} would be a 2-dimensional
         object with three elements along each of two lines.  {\tt 0} creates
         a scalar.  The suggested default is the shape of the object
         if it already exists, otherwise it is the current value.  It
         is only accessed when OPTION={\tt "Put"}.
      }
      \sstsubsection{
         XNAME = LITERAL (Given)
      }{
         The name of the extension to modify.
      }
      \sstsubsection{
         XTYPE = LITERAL (Given)
      }{
         The type of the extension to create.  The suggested default is
         the current value or {\tt "EXT"} when there is no current value.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         setext hh50 fits delete noloop
      }{
         This deletes the FITS extension in the NDF called hh50.
      }
      \sstexamplesubsection{
         setext myndf select xtype=mytype noloop
      }{
         This creates the extension MYEXT of data type MYTYPE in the
         NDF called myndf.
      }
      \sstexamplesubsection{
         setext xname=ccdpack ndf=abc erase cname=filter noloop
      }{
         This deletes the FILTER component of the CCDPACK extension in
         the NDF called abc.
      }
      \sstexamplesubsection{
         setext abc ccdpack put cname=filter cvalue=B ctype=\_char noloop
      }{
         This assigns the character value {\tt "B"} to the FILTER component
         of the CCDPACK extension a the NDF called abc.
      }
      \sstexamplesubsection{
         setext virgo plate put cname=pitch shape=2 cvalue=[32,16] ctype=\_byte noloop
      }{
         This sets the byte 2-element vector of component PITCH
         of the PLATE extension in the NDF called virgo.  The first
         element of PITCH is set to 32 and the second to 16.
      }
      \sstexamplesubsection{
         setext virgo plate rename cname=filter newname=waveband noloop
      }{
         This renames the FILTER component of the PLATE extension in
         the NDF called virgo to WAVEBAND.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The {\tt "Put"} option allows the creation of extension
         components with any of the primitive data types.

         \sstitem
         The task creates the extension automatically if it does not
         exist and only allows one extension to be modified at a time.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FITSIMP, FITSLIST, NDFTRACE; CCDPACK: CCDEDIT;
      Figaro: FITSKEYS; HDSTRACE; IRAS90: IRASTRACE, PREPARE.
   }
}
\sstroutine{
   SETLABEL
}{
   Sets a new label for an NDF data structure
}{
   \sstdescription{
      This routine sets a new value for the label component of an
      existing NDF data structure. The NDF is accessed in update mode
      and any pre-existing label is over-written with a new value.
      Alternatively, if a `null' value ({\tt !}) is given for the LABEL
      parameter, then the NDF's label will be erased.
   }
   \sstusage{
      setlabel ndf label
   }
   \sstparameters{
      \sstsubsection{
         LABEL = LITERAL (Read)
      }{
         The value to be assigned to the NDF's label component. This
         should describe the type of quantity represented in the NDF's
         data array ({\it e.g.}\ {\tt "Surface Brightness"} or {\tt "Flux Density"}).
         The value may later be used by other applications, for instance to
         label the axes of graphs where the NDF's data values are
         plotted.  The suggested default is the current value.
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF data structure whose label is to be modified.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         setlabel ngc1068 "Surface Brightness"
      }{
         Sets the label component of the NDF structure ngc1068 to be
         {\tt "Surface Brightness"}.
      }
      \sstexamplesubsection{
         setlabel ndf=datastruct label="Flux Density"
      }{
         Sets the label component of the NDF structure datastruct to be
         {\tt "Flux Density"}.
      }
      \sstexamplesubsection{
         setlabel raw\_data label=!
      }{
         By specifying a null value ({\tt !}), this example erases any
         previous value of the label component in the NDF structure
         raw\_data.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: AXLABEL, SETTITLE, SETUNITS.
   }
}
\sstroutine{
   SETMAGIC
}{
   Replaces all occurrences of a given value in an NDF array with
   the bad value
}{
   \sstdescription{
      This application flags all pixels that have a defined value in an
      NDF with the standard bad (`magic') value.  Other values are
      unchanged.  The number of replacements is reported.  SETMAGIC's
      applications include the import of data from software that has a
      different magic value.
   }
   \sstusage{
      setmagic in out repval [comp]
   }
   \sstparameters{
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The components whose values are to be flagged as bad.  It
         may be {\tt "Data"}, {\tt "Variance"}, {\tt "Error"}, or {\tt "All"}.
         The last of the options forces substitution of bad pixels in both
         the data and variance arrays.  This parameter is ignored if the
         data array is the only array component within the NDF. 
         {\tt ["Data"]}
      }
      \sstsubsection{
         IN = NDF  (Read)
      }{
         Input NDF structure containing the data and/or variance array
         to have some of its elements flagged with the magic-value.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF structure containing the data and/or variance array
         that is a copy of the input array, but with bad values flagging
         the replacement value.
      }
      \sstsubsection{
         REPVAL = \_DOUBLE (Read)
      }{
         The element value to be substituted with the bad value.  The
         same value is replaced in both the data and variance arrays
         when COMP={\tt "All"}.  It must lie within the minimum and maximum
         values of the data type of the array with higher precision.
         The replacement value is converted to data type of the array
         being converted before the search begins.  The suggested
         default is the current value.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the input NDF to the output NDF.
         {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         setmagic irasmap aitoff repval=-2000000
      }{
         This copies the NDF called irasmap to the NDF aitoff, except
         that any pixels with the IPAC blank value of $-$2000000 are
         flagged with the standard bad value in aitoff.
      }
      \sstexamplesubsection{
         setmagic saturn saturnb 9999.0 comp=All
      }{
         This copies the NDF called saturn to the NDF saturnb, except
         that any elements in the data and variance arrays that have
         value 9999.0 are flagged with the standard bad value.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The comparison for floating-point values tests that the
         difference between the replacement value and the element value is
         less than their mean times the precision of the data type.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CHPIX, FILLBAD, GLITCH, NOMAGIC, SEGMENT, SUBSTITUTE, ZAPLIN;
      SPECDRE: GOODVAR.

   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         Any number of NDF dimensions is supported.
      }
   }
}

\sstroutine{
   SETNORM
}{
   Sets a new value for one or all of an NDF's axis-normalisation
   flags
}{
   \sstdescription{
      This routine sets a new value for one or all the normalisation
      flags in an NDF AXIS data structure.  The NDF is accessed in
      update mode.  This flag determines how the NDF's data and
      variance arrays behave when the associated axis information is
      modified.

      If an AXIS structure does not exist, a new one whose centres are
      pixel co-ordinates is created.
   }
   \sstusage{
      setnorm ndf dim
   }
   \sstparameters{
      \sstsubsection{
         ANORM = \_LOGICAL (Read)
      }{
         The normalisation flag for the axis.  {\tt TRUE} means that the
         data and variance values in the NDF are normalised to the
         pixel width values for the chosen axis so that the product
         of data value and width, and variance and the squared width
         are constant if the width is altered.

         A {\tt FALSE} value means that the data and variance need not alter
         as the pixel widths are varied.  This is the default for an
         axis.  The suggested default is the current value.
      }
      \sstsubsection{
         DIM = \_INTEGER (Read)
      }{
         The axis dimension for which the normalisation flag is to be
         modified.  There are separate units for each NDF dimension.
         A value of 0 sets the normalisation flag for all the axes.
         The value must lie between 0 and the number of dimensions of
         the NDF.  This defaults to 1 for a 1-dimensional NDF.  The
         suggested default is the current value. {\tt []}
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF data structure in which an axis-normalisation flag
         is to be modified.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         setnorm hd23568 0 anorm
      }{
         This sets the normalisation flags along all axes of the
         NDF structure hd23568 to be true.
      }
      \sstexamplesubsection{
         setnorm ndf=spect noanorm
      }{
         This sets the normalisation flag of the 1-dimensional NDF
         structure spect to be false.
      }
      \sstexamplesubsection{
         setnorm borg 3 anorm
      }{
         This sets the normalisation flag for the third dimension
         in the NDF structure borg.
      }
   }
   \sstdiytopic{
      Axis Normalisation
   }{
      In general, the axis-normalisation property is not needed.  An
      example where it is relevant is a spectrum in which data values
      representing energy per unit wavelength and each pixel has a
      known spread in wavelength.  The sum of each pixel's data value
      multiplied by its width gives the energy in a part of the
      spectrum.  A change to the axis width, say to allow for the
      redshift, necessitates a corresponding modification to the data
      value to retain this property.  In two dimensions an example is
      where the data measure flux per unit area of sky and the pixel
      widths are defined in terms of angular size.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: SETAXIS.
   }
}
\sstroutine{
   SETORIGIN
}{
   Sets a new pixel origin for an NDF
}{
   \sstdescription{
      This application sets a new pixel origin value for an NDF data
      structure.  The NDF is accessed in update mode and the indices of
      the first pixel (the NDF's lower pixel-index bounds) are set to
      specified integer values, which may be positive or negative.  No
      other properties of the NDF are altered.  If required, a template
      NDF may be supplied and the new origin values will be derived
      from it.
   }
   \sstusage{
      setorigin ndf origin
   }
   \sstparameters{
      \sstsubsection{
         LIKE = NDF (Read)
      }{
         This parameter may be used to supply an NDF which is to be
         used as a template.  If such a template is supplied, then its
         origin (its lower pixel-index bounds) will be used as the new
         origin value for the NDF supplied via the NDF parameter.  By
         default, no template will be used and the new origin will be
         specified via the ORIGIN parameter. {\tt [!]}
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF data structure whose pixel origin is to be modified.
      }
      \sstsubsection{
         ORIGIN() = \_INTEGER (Read)
      }{
         A 1-dimensional array specifying the new pixel origin values,
         one for each NDF dimension.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         setorigin image\_2d [1,1]
      }{
         Sets the indices of the first pixel in the 2-dimensional image
         image\_2d to be (1,1).  The image pixel values are unaltered.
      }
      \sstexamplesubsection{
         setorigin ndf=starfield
      }{
         A new pixel origin is set for the NDF structure called
         starfield. SETORIGIN will prompt for the new origin values,
         supplying the existing values as defaults.
      }
      \sstexamplesubsection{
         setorigin ndf=cube origin=[-128,-128]
      }{
         Sets the pixel origin values for the first two dimensions of
         the 3-dimensional NDF called cube to be ($-$128,$-$128). A value
         for the third dimension is not specified, so the origin of
         this dimension will remain unchanged.
      }
      \sstexamplesubsection{
         setorigin betapic like=alphapic
      }{
         Sets the pixel origin of the NDF called betapic to be equal to
         that of the NDF called alphapic.
      }
   }
   \sstnotes{
      If the number of new pixel origin values is less than the number
      of NDF dimensions, then the pixel origin of the extra dimensions
      will remain unchanged. If the number of values exceeds the number
      of NDF dimensions, then the excess values will be ignored.
   }
   \sstdiytopic{
      Timing
   }{
      Setting a new pixel origin is a quick operation whose timing does
      not depend on the size of the NDF.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: SETBOUND.
   }
}
\sstroutine{
   SETSKY
}{
   Makes an IRAS astrometry extension
}{
   \sstdescription{
      This application makes an IRAS astrometry extension within a
      two-dimensional NDF, therefore allowing sky co-ordinate
      information to be stored with an arbitrary image.  This
      information is used by certain IRAS90 applications (those with
      the SKY prefix) to perform various astrometric operations.  These
      include annotation of a displayed image with a grid of celestial
      co-ordinates, marking the location of a given celestial position
      given a pixel position, and aligning a group of images.  See
      SUN/163 for details.

      The astrometry is determined either by you supplying explicit
      values for certain projection parameters, or by you providing the
      sky and corresponding image co-ordinates for a set of positions
      (see parameter POSITIONS).  In the latter case, the projection
      parameters are determined automatically by searching through
      parameter space in order to minimise the sum of the squared
      residuals between the supplied pixel co-ordinates and the
      transformed sky co-ordinates.  You may force particular
      projection parameters to take certain values by assigning an
      explicit value to the corresponding application parameter listed
      below.  The individual residuals at each position can be written
      out to a logfile so that you can identify any aberrant points.
      The RMS residual (in pixels) implied by the best-fitting
      parameters is displayed.
   }
   \sstusage{
      setsky ndf positions coords epoch [projtype] [lon] [lat] [refcode]
             \newline\hspace*{1.5em}
             [pixelsize] [orient] [tilt] [logfile]
   }
   \sstparameters{
      \sstsubsection{
         COORDS = LITERAL (Read)
      }{
         The sky co-ordinate system to use.  Valid values include
         {\tt "Ecliptic"} (IAU 1980), {\tt "Equatorial"} (FK4 and FK5), and
         {\tt "Galactic"} (IAU 1958).  Ecliptic and equatorial co-ordinates
         are referred to the mean equinox of a given epoch.  This epoch
         is specified by appending it to the system name, in
         parentheses, such as, {\tt "Equatorial(1994.5)"}.  The epoch may
         be preceded by a single character, {\tt "B"} or {\tt "J"}, indicating that
         the epoch is Besselian or Julian respectively.  If this letter
         is missing, a Besselian epoch is assumed.
      }
      \sstsubsection{
         EPOCH = DOUBLE PRECISION (Read)
      }{
         The Julian epoch at which the observation was made ({\it e.g.}
         {\tt "1994.0"}).
      }
      \sstsubsection{
         LAT = LITERAL (Read)
      }{
         The latitude of the reference point, in the co-ordinate system
         specified by parameter COORDS.  For example, if COORDS is
         {\tt "Equatorial"}, LAT is the Declination.
         See SUN/163, Section 4.7.2 for full details of the allowed syntax for specifying
         this position.  For convenience here are some examples how you
         may specify the Declination $-$45\dgs~12\arcm$\,$:
         {\tt "-45 12 00"}, {\tt "-45 12"}, {\tt "-45d 12m"}, {\tt "-45.2d"},
         {\tt "-451200"}, {\tt "-0.78888r"}.
         The last of these is a radians value.  A null value causes the
         latitude of the reference point to be estimated automatically
         from the data supplied for parameter POSITIONS. {\tt [!]}
      }
      \sstsubsection{
         LOGFILE = FILENAME (Read)
      }{
         Name of the text file to log the final projection parameter
         values and the residual at each supplied position.  If null,
         there will be no logging.  This parameter is ignored if a null
         value is given to parameter POSITIONS. {\tt [!]}
      }
      \sstsubsection{
         LON= LITERAL (Read)
      }{
         The longitude of the reference point, in the co-ordinate
         system specified by parameter COORDS.  For example, if COORDS
         is {\tt "Equatorial"}, LON is the Right Ascension.
         See SUN/163, Section 4.7.2 for full details of the allowed syntax for
         specifying this position.  For convenience here are some
         examples how you may specify the Right Ascension
         11\hr~34\mn~56.\us2$\,$: {\tt "11 34 56.2"}, {\tt "11h 34m 56.2s"},
         {\tt "11 34.9366"}, {\tt "11.58228"}, {\tt "113456.2"}.  See
         parameter LAT for examples of specifying a non-equatorial longitude.
         A null value causes the longitude of the reference point to be
         estimated automatically from the data supplied for parameter
         POSITIONS. {\tt [!]}
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF that is to have an IRAS astrometry extension.
      }
      \sstsubsection{
         ORIENT = LITERAL (Read)
      }{
         The position angle of the NDF's $y$ axis on the celestial
         sphere, measured from north through east.  North is defined as
         the direction of increasing sky latitude, and east is the
         direction of increasing sky longitude.  Values are constrained
         to the range 0 to 2$\pi$ radians.  A null value causes the
         position angle to be estimated automatically from the data
         supplied for parameter POSITIONS. {\tt [!]}
      }
      \sstsubsection{
         PIXELREF( 2 ) = REAL (Read)
      }{
         The pixel co-ordinates of the reference pixel ($x$ then $y$).
         This parameter is ignored unless REFCODE = {\tt "Pixel"}.  Remember that
         the centre of a pixel at indices $i$,$\,j$ is ($i-0.5$,$\,j-0.5$).  A null
         value causes the pixel co-ordinates of the reference point to
         be estimated automatically from the data supplied for
         parameter POSITIONS. {\tt [!]}
      }
      \sstsubsection{
         PIXELSIZE( 2 ) = \_REAL (Read)
      }{
         The $x$ and $y$ pixel sizes at the reference position.  If only
         one value is given, the pixel is deemed to be square.  Values
         may be given in a variety of units (see parameter LAT).  For
         example, 0.54 arcseconds could be specified as {\tt "0.54s"} or
         {\tt "0.009m"} or {\tt "2.618E-6r"}.  A null value causes the pixel
         dimensions to be estimated automatically from the data
         supplied for parameter POSITIONS. {\tt [!]}
      }
      \sstsubsection{
         POSITIONS = LITERAL (Read)
      }{
         A list of sky co-ordinates and corresponding image
         co-ordinates for the set of positions which are to be used to
         determine the astrometry.  If a null value is supplied then
         the astrometry is determined by the explicit values you give
         for each of the other parameters.  Each position
         is defined by four values, the sky longitude (in the same
         format as for parameter LON), the sky latitude (in the same
         format as for parameter LAT), the image pixel $x$ co-ordinate
         and the image pixel $y$ co-ordinate (both decimal values).
         These should be supplied (in the order stated) for each position.
         These values are given in the form of a `group expression'
         (see SUN/150).  This means that values can be
         either typed in directly or supplied in a text file.  If typed
         in directly, the items in the list should be separated by
         commas, and you are re-prompted for further values if the
         last supplied value ends in a minus sign.  If conveyed in a
         text file, they should again be separated by commas, but can
         be split across lines.  The name of the text file is given in
         response to the prompt, preceded by an `up arrow' symbol ($\wedge$).
      }
      \sstsubsection{
         PROJTYPE = LITERAL (Read)
      }{
         The type of projection to use.  The options are:
         \begin{description}
         \item {\tt "Aitoff"} --- Aitoff equal-area,
         \item {\tt "Gnomonic"} --- Gnomonic or tangent plane,
         \item {\tt "Lambert"}  --- Lambert normal equivalent cylindrical,
         \item {\tt "Orthographic"} --- Orthographic.
         \end{description}

         The following synonyms are also recognised:
         \begin{description}
         \item {\tt "All\_sky"} --- Aitoff,
         \item {\tt "Cylindrical"} ---- Lambert,
         \item {\tt "Tangent\_plane"} --- Gnomonic.
         \end{description}

         See SUN/163 for descriptions of these projections.  A null
         value causes the projection to be determined automatically
         from the data supplied for parameter POSITIONS. {\tt [!]}
      }
      \sstsubsection{
         REFCODE = LITERAL (Read)
      }{
         The code for the reference pixel.  If it has value {\tt "Pixel"}
         this requests that pixel co-ordinates for the reference point
         be obtained through parameter PIXELREF.  The other options are
         locations specified by two characters, the first corresponding
         to the vertical position and the second the horizontal.  For
         the vertical, valid positions are T(op), B(ottom), or
         C(entre); and for the horizontal the options are L(eft),
         R(ight), or C(entre).  Thus REFCODE = {\tt "CC"} means the reference
         position is at the centre of the NDF image, and {\tt "BL"} specifies
         that the reference position is at the centre of the
         bottom-left pixel in the image.  A null value causes the pixel
         co-ordinates of the reference point to be estimated
         automatically from the data supplied for parameter POSITIONS.
         {\tt [!]}
      }
      \sstsubsection{
         TILT = LITERAL (Read)
      }{
         The angle through which the celestial sphere is to be rotated
         prior to doing the projection.  The axis of rotation is a
         radius passing through the reference point.  The rotation is
         in an anti-clockwise sense when looking from the reference
         point towards the centre of the celestial sphere.  In common
         circumstances this can be set to zero.  Values may be given in
         a variety of units (see parameter LAT).  Values are
         constrained to the range 0 to 2$\pi$ radians.  A null value
         causes the latitude of the reference point to be estimated
         automatically from the data supplied for parameter POSITIONS.
         {\tt ["0.0"]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         setsky m51 \^{}stars.lis ecl(j1994.0) 1994.0 logfile=m51.log
      }{
         This creates an astrometry extension within the two-dimensional
         NDF called m51.  The values for parameters PROJTYPE, LON, LAT,
         PIXELREF, PIXELSIZE and ORIENT are determined automatically so
         that they minimised the sum of the squared residuals (in
         pixels) at each of the positions specified in the file
         {\tt stars.lis}.  This file contains a line for each position, each
         line containing an ecliptic longitude and latitude, followed
         by a pair of image co-ordinates.  These values should be
         separated by commas.  The ecliptic co-ordinates were
         determined at Julian epoch 1994.0, and are referred to the
         mean equinox at Julian epoch 1994.0.  The determined parameter
         values together with the residual at each position are logged
         to file {\tt m51.log}.
      }
      \sstexamplesubsection{
         setsky m51 \^{}stars.lis ecl(j1994.0) 1994.0 orient=0 projtype=orth
      }{
         This creates an astrometry extension within the two-dimensional
         NDF called m51.  The values for parameters PROJTYPE, LON, LAT,
         PIXELREF and PIXELSIZE are determined automatically as in the
         previous example.  In this example however, an Orthographic
         projection is forced, and the value zero is assigned to
         parameter ORIENT, resulting in north being `upwards' in the image.
      }
      \sstexamplesubsection{
         setsky virgo "!" eq(j2000.0) 1989.3 gn "12 29" "$+$12 30" bl 1.1s 0.0d
      }{
         This creates an astrometry extension within the two-dimensional
         NDF called virgo.  It is a gnomonic projection in the
         equatorial system at Julian epoch 2000.0.  The bottom-left
         pixel of the image is located at Right Ascension 12\hr~29\mn,
         Declination $+$12\dgs~30\arcm.  A pixel at that
         position is square and has angular size of 1.\uarcs1.
         The image was observed at epoch 1989.3.  At the bottom-left of
         the image, north is at the top, parallel to the $y$-axis of the
         image.
      }
      \sstexamplesubsection{
         setsky map "!" galactic(1950.0) 1993.8 aitoff 90 0 cc
         [0.5d,0.007r] 180.0d
      }{
          This creates an astrometry extension within the two-dimensional
          NDF called map.  It is an Aitoff projection in the galactic
          system at Besselian epoch 1950.0.  The centre of the image is
          located at galactic longitude 90\dgs, latitude 0\dgs.
          A pixel at that position is rectangular and has angular size
          of 0.\udeg5 by 0.007 radians.  The image was made at epoch
          1993.8.  At the image centre, south is at the top and is
          parallel to the $y$-axis of the image.
      }
      \sstexamplesubsection{
         setsky zodiac "!" ec 1983.4 or 10.3 -5.6 Pixel 20m 0.3d
         pixelref=[9.5,-11.2]
      }{
          This creates an astrometry extension within the
          two-dimensional NDF called zodiac.  It is an orthographic
          projection in the Ecliptic system at Besselian epoch 1950.0.
          The reference point at pixel co-ordinates (9.5,$-$11.2)
          corresponds to ecliptic longitude \mbox{10.\udeg3}, latitude
          $-$5.\udeg6.  A pixel at that position is square and has
          angular size of 20\arcm.  The image was observed at
          epoch 1983.4.  At the reference point the $y$-axis of the image
          points to 0.\udeg3 east of north.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         SETSKY overwrites an existing astrometry extension within
         the NDF.

         \sstitem
         WARNING: As is standard for NDF extensions, the transformation
         stored in the NDF will be propagated to new NDFs derived from it.
         However, certain operations will invalidate the transformation.
         These include configuration change, a shift of origin, and
         resampling.  Once there is a standard astrometry extension,
         {\footnotesize KAPPA} applications will be made to process that
         extension correctly, by modifying it where that's possible
         otherwise not copying it.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      ASTROM; IRAS90: SKYALIGN, SKYBOX, SKYGRID, SKYLINE, SKYMARK,
      SKYPOS, SKYWRITE. 
   }
}
\sstroutine{
   SETTITLE
}{
   Set a new title for an NDF data structure
}{
   \sstdescription{
      This routine sets a new value for the title component of an
      existing NDF data structure. The NDF is accessed in update mode
      and any pre-existing title is over-written with a new value.
      Alternatively, if a `null' value ({\tt !}) is given for the TITLE
      parameter, then the NDF's title will be erased.
   }
   \sstusage{
      settitle ndf title
   }
   \sstparameters{
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF data structure whose title is to be modified.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         The value to be assigned to the NDF's title component ({\it e.g.}
         {\tt "NGC1068 with a B filter"} or {\tt "Ice band in HD123456"}).  This
         value may later be used by other applications as a heading for
         graphs and other forms of display where the NDF's data values
         are plotted.  The suggested default is the current value.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         settitle ngc1068 "NGC1068 with a B filter"
      }{
         Sets the title component of the NDF structure ngc1068 to be
         {\tt "NGC1068 with a B filter"}.
      }
      \sstexamplesubsection{
         settitle ndf=myspec title="Ice band, short integration"
      }{
         Sets the title component of the NDF structure myspec to be
         {\tt "Ice band, short integration"}.
      }
      \sstexamplesubsection{
         settitle dat123 title=!
      }{
         By specifying a null value ({\tt !}), this example erases any
         previous value of the title component in the NDF structure
         dat123.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: SETLABEL, SETUNITS.
   }
}
\sstroutine{
   SETTYPE
}{
   Sets a new numeric type for the data and variance components of
   an NDF
}{
   \sstdescription{
      This application allows the numeric type of the data and variance
      components of an NDF to be changed. The NDF is accessed in update
      mode and the values stored in these components are converted
      {\it in situ}\ to the new type. No other attributes of the NDF are
      changed.
   }
   \sstusage{
      settype ndf type
   }
   \sstparameters{
      \sstsubsection{
         COMPLEX = \_LOGICAL (Read)
      }{
         If a {\tt TRUE} value is given for this parameter, then the NDF's
         array components will be altered so that they hold complex
         values, an imaginary part containing zeros being created if
         necessary.  If a {\tt FALSE} value is given, then the components will
         be altered so that they hold non-complex values, any imaginary
         part being deleted if necessary.  The dynamic default for this
         parameter is chosen so that no change is made to the current
         state. {\tt []}
      }
      \sstsubsection{
         DATA = \_LOGICAL (Read)
      }{
         If a {\tt TRUE} value is given for this parameter, then the numeric
         type of the NDF's data array will be changed.  Otherwise, this
         component's type will remain unchanged. {\tt [TRUE]}
      }
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF data structure whose array components are to have
         their numeric type changed.
      }
      \sstsubsection{
         TYPE = LITERAL (Read)
      }{
         The new numeric type to which the NDF's array components are
         to be converted. The value given should be one of the
         following: \_DOUBLE, \_REAL, \_INTEGER, \_WORD, \_UWORD, \_BYTE or
         \_UBYTE (note the leading underscore). Existing pixel values
         stored in the NDF will not be lost, but will be converted to
         the new type. Any values which cannot be represented using the
         new type will be replaced with the bad-pixel value.
      }
      \sstsubsection{
         VARIANCE = \_LOGICAL (Read)
      }{
         If a {\tt TRUE} value is given for this parameter, then the numeric
         type of the NDF's variance array will be changed.  Otherwise,
         this component's type will remain unchanged. {\tt [TRUE]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         settype rawdata \_real
      }{
         Converts the data and variance values held in the NDF data
         structure rawdata to have a numeric type of \_REAL ({\it i.e.}\ to be
         stored as single-precision floating-point numbers).
      }
      \sstexamplesubsection{
         settype inst.run1 \_word novariance
      }{
         Converts the data array in the NDF structure inst.run1 to be
         stored as word ({\it i.e.}\ Fortran INTEGER$*$2) values. No change is
         made to the variance component.
      }
      \sstexamplesubsection{
         settype hd26571 \_double complex
      }{
         Causes the data and variance components of the NDF structure
         hd26571 to be altered so as to hold complex values using
         double precision numbers. The existing pixel values are
         converted to this new type.
      }
   }
   \sstdiytopic{
      Timing
   }{
      The execution time is approximately proportional to the number of
      pixel values to be converted.
   }
   \sstdiytopic{
      Related Applications
   }{
      Figaro: RETYPE.
   }
}
\sstroutine{
   SETUNITS
}{
   Sets a new units value for an NDF data structure
}{
   \sstdescription{
      This routine sets a new value for the units component of an
      existing NDF data structure. The NDF is accessed in update mode
      and any pre-existing units component is over-written with a new
      value.  Alternatively, if a `null' value ({\tt !}) is given for the
      UNITS parameter, then the NDF's units component will be erased.
   }
   \sstusage{
      setunits ndf units
   }
   \sstparameters{
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF data structure whose units component is to be
         modified.
      }
      \sstsubsection{
         UNITS = LITERAL (Read)
      }{
         The value to be assigned to the NDF's units component ({\it e.g.}
         {\tt "J/(m**2*Ang*s)"} or {\tt "count/s"}).  This value
         may later be used by other applications for labelling graphs
         and other forms of display where the NDF's data values are
         shown.  The suggested default is the current value.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         setunits ngc1342 "count/s"
      }{
         Sets the units component of the NDF structure ngc1342 to have
         the value {\tt "count/s"}.
      }
      \sstexamplesubsection{
         setunits ndf=spect units="J/(m**2*Ang*s)"
      }{
         Sets the units component of the NDF structure spect to have
         the value \linebreak {\tt "J/(m**2*Ang*s)"}.
      }
      \sstexamplesubsection{
         setunits datafile units=!
      }{
         By specifying a null value ({\tt !}), this example erases any
         previous value of the units component in the NDF structure
         datafile.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: AXUNITS, SETLABEL, SETTITLE.
   }
}
\sstroutine{
   SETVAR
}{
   Sets new values for the variance component of an NDF data
   structure
}{
   \sstdescription{
      This routine sets new values for the variance component of an NDF
      data structure. The data structure is accessed in `update' mode,
      and new variance values are generated from the NDF's data array
      by means of a Fortran-like arithmetic expression.  Any previous
      variance information is over-written with the new values.
      Alternatively, if a `null' value ({\tt !}) is given for the
      variance, then any pre-existing variance information is erased.
   }
   \sstusage{
      setvar ndf variance
   }
   \sstparameters{
      \sstsubsection{
         NDF = NDF (Read and Write)
      }{
         The NDF data structure whose variance values are to be
         modified.
      }
      \sstsubsection{
         VARIANCE = LITERAL (Read)
      }{
         A Fortran-like arithmetic expression giving the variance value
         to be assigned to each pixel in terms of the variable DATA,
         which represents the value of the corresponding data array
         pixel. For example, VARIANCE = {\tt "DATA"} implies normal
         $\surd N$ error estimates, whereas VARIANCE ={\tt "DATA + 50.7"}
         might be used if a sky background of 50.7 units had previously
         been subtracted.

         If a `null' value ({\tt !}) is given for this parameter, then
         no new variance component will be created and any pre-existing
         variance values will be erased.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         setvar ngc4709 data
      }{
         This sets the variance component within the NDF structure
         ngc4709 to equal its corresponding data-array component.
      }
      \sstexamplesubsection{
         setvar ndf=arcspec "data - 0.31"
      }{
         This sets the variance component within the NDF structure
         arcspec to be its corresponding data-array component less a
         constant 0.31.
      }
      \sstexamplesubsection{
         setvar cube4 Variance=!
      }{
         This erases the values of the variance component within
         the NDF structure cube4, if it exists.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         All of the standard Fortran 77 intrinsic functions are
         available for use in the variance expression, plus a few others
         (see SUN/61 for details and an up-to-date list).

         \sstitem
         Calculations are performed using real arithmetic (or double
         precision if appropriate) and are constrained to be non-negative.

         \sstitem
         The data type of the variance component is set to match that of
         the data component.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ERRCLIP; Figaro: GOODVAR.
   }
}
\sstroutine{
   SHADOW
}{
   Enhances edges in a 2-dimensional NDF using a shadow effect
}{
   \sstdescription{
      This routine enhances a 2-dimensional NDF by creating a
      bas-relief or shadow effect, that causes features in an array to
      appear as though they have been illuminated from the side by some
      imaginary light source.  The enhancement is useful in locating
      edges and fine detail in an array.
   }
   \sstusage{
      shadow in out
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         The 2-dimensional NDF to be enhanced.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The output NDF containing the enhanced image.
      }
      \sstsubsection{
         SHIFT( 2 )  =  \_INTEGER (Given)
      }{
         The shift in $x$ and $y$ pixel indices to be used in the
         enhancement.  If the $x$ shift is positive, positive features
         in the original array will appear to be lit from the positive
         $x$ direction, {\it i.e.}\ from the right.  Similarly, if the $y$ shift
         is positive, the light source will appear to be shining from
         the top of the array.  A one- or two-pixel shift is normally
         adequate. {\tt [1,1]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the NDF supplied for parameter IN to be used
         instead. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         shadow horse horse\_bas
      }{
         This enhances the NDF called horse by making it appear to be
         illuminated from the top right, and stores the result in the
         NDF called horse\_bas.
      }
      \sstexamplesubsection{
         shadow out=aash in=aa [-1,-1] title="Bas relief"
      }{
         This enhances the NDF called aa by making it appear to be
         illuminated from the bottom left, and stores the result in the
         NDF called aash, which has the title {\tt "Bas relief"}.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: LAPLACE, MEDIAN; Figaro: ICONV3.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF data
         structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         The output NDF will be trimmed compared with the input NDF
         by the shifts applied.
      }
   }
}
\begin{htmlonly}
Click \htmladdnormallink{here}{shadow_exam.gif} to see an example plot
(120k).
\end{htmlonly}

\manroutine {{\manheadstyle{SLIDE}}}{ Realigns a 2-d data array via an
{$x$}-{$y$} shift.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  The data array in the input {\mantt{IMAGE}} structure is shifted, in either
  or both of the {$x$} and {$y$} axes, to produce the new array, in the
  output image structure. The shifts in {$x$} and {$y$} are either input as
  absolute {$x$} and {$y$} shifts by the user, or alternatively, are
  calculated from the co-ordinates of two points provided by the
  user. These are a fiducial point, with co-ordinates {\mantt {\%FIDX}},
  {\mantt {\%FIDY}}, and a standard object, with co-ordinates {\mantt {\%OBJX}},
  {\mantt {\%OBJY}}. The shift in {$x$} is then given by
  {\mantt {\%FIDX}}{$-$}{\mantt {\%OBJX}} and the shift in {$y$} is given by
  {\mantt {\%FIDY}}{$-$}{\mantt {\%OBJY}}. The output data array is padded with
  zeros in the regions not occupied by the shifted input array.  Fractional
  shifts are computed by bilinear interpolation.

  The magic-value method is used for processing bad data.

\manroutineitem {Invocation }{}
  SLIDE

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  {\mantt{IMAGE}} structure containing the 2-d data array to be shifted.
\manparameterentry {{\mantt{READ}} }{{\mantt{STYPE}}  }{{\mantt{\_CHAR}}}
  The sort of shift is to be used. The choice is {\mantt{'Relative'}} or
  {\mantt{'Absolute'}}.
\manparameterentry {{\mantt{READ}} }{{\mantt{ABSX}}  }{{\mantt{\_REAL}}}
  Absolute {$x$} shift in pixels. ({\mantt{Absolute}} shift)
\manparameterentry {{\mantt{READ}} }{{\mantt{ABSY}}  }{{\mantt{\_REAL}}}
  Absolute {$y$} shift in pixels. ({\mantt{Absolute}} shift)
\end{manparametertable}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{FIDX}}  }{{\mantt{\_REAL}}}
  {$x$}-co-ordinate of the fiducial point. ({\mantt{Relative}} shift)
\manparameterentry {{\mantt{READ}} }{{\mantt{FIDY}}  }{{\mantt{\_REAL}}}
  {$y$}-co-ordinate of the fiducial point. ({\mantt{Relative}} shift)
\manparameterentry {{\mantt{READ}} }{{\mantt{OBJX}}  }{{\mantt{\_REAL}}}
  {$x$}-co-ordinate of the standard object. ({\mantt{Relative}} shift)
\manparameterentry {{\mantt{READ}} }{{\mantt{OBJY}}  }{{\mantt{\_REAL}}}
  {$y$}-co-ordinate of the standard object. ({\mantt{Relative}} shift)
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}} }{{\mantt{IMAGE}}}
  {\mantt{IMAGE}} structure to contain the 2-d data array after being
  shifted.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}} }{{\mantt{\_CHAR}}}
  Will be used as the {\mantt{TITLE}} component for the output
  {\mantt{IMAGE}} structure. \mbox{{\mantt ['KAPPA - Slide']}}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
  Dave Baines ({\mantt{ROE}}::{\mantt{ASOC5}})
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
\end{manroutinedescription}

 
\sstroutine{
   SNAPSHOT
}{
   Dumps an image-display memory to a graphics hardcopy and
   optionally to an NDF
}{
   \sstdescription{
      This routine captures the data in the memory of an image-display
      device, and writes these data to a different GKS device.  For
      example, the contents of an X-windows memory might be captured and
      sent to a PostScript laser printer.

      Various options are available:
      \begin{itemize}
         \item you may choose to capture a whole or part of what is visible
           on the screen, or the entire contents of the memory.  For the
           former you adjust a rubber-band region until the desired
           area is enclosed. Instructions for controlling the
           rubber-band are given at run time.
         \item A title may be included in the output.
         \item The array may be output to an NDF.
      \end{itemize}
   }
   \sstusage{
      snapshot odevice [out] [whole] [scale] [negativ] [title] [planes]
   }
   \sstparameters{
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         Input image-display device.  {\tt [}Current image-display 
         device{\tt ]}
      }
      \sstsubsection{
         NEGATIVE = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, the output hardcopy is a negative version of what is
         stored in the image display.  On some output devices a
         constant dark background can give a non-uniform result, and
         so a negative representation is the default. {\tt [TRUE]}
      }
      \sstsubsection{
         ODEVICE = DEVICE (Read)
      }{
         Name of the output device.  The suggested default is the
         graphics device last used in SNAPSHOT, and if there is not one,
         the suggested default is the global current graphics device.
         The device must be in the GNS category MATRIX\_PRINTER, and
         have at least 24 greyscale intensities.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Name given to the output NDF data structure used to store the
         contents of the image-display memory.  If it is null ({\tt !}) no
         NDF will be created. {\tt [!]}
      }
      \sstsubsection{
         PLANES = \_INTEGER (Read)
      }{
         The numbers of the image memory planes not be output.  All
         unspecified planes become visible.  If PLANES is null ({\tt !}), all
         memory planes will be used to form the snapshot.  The base
         memory is 0 and overlays are numbered consecutively from 1.
         The value must be between 0 and the number of image memories
         minus 1.  For an Ikon the only overlay plane is 1.  {\tt [!]}
      }
      \sstsubsection{
         SCALE = \_REAL (Read)
      }{
         Scale factor for output.  Unity gives the largest possible
         output, but it takes longest to compute and print (goes as the
         square of the scale factor).  On the other hand unity does
         provide maximum resolution. SCALE must be between 0 and 1.
         {\tt [0.707]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title of the plot and the output NDF.  There is only space on
         the plot for about 25 characters in the title.  If it is null
         ({\tt !}) no title will be plotted, and the title in the output NDF
         becomes {\tt "KAPPA - Snapshot"}. {\tt [!]}
      }
      \sstsubsection{
         WHOLE = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, the whole image-display memory is recorded, otherwise
         a selected region of what is visible on the screen is plotted.
         Dumping the whole memory can require considerable disc space
         for work arrays and the output NDF. {\tt [FALSE]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         snapshot whole odevice=canon\_l
      }{
         This will dump the whole of the current image display's 
         memories to the canon\_l device.  The plot will occupy half 
         of the maximum area available on the device, {\it i.e.}
         $\surd2/2$ magnification.
      }
      \sstexamplesubsection{
         snapshot scale=1.0 $\backslash$
      }{
         This will capture a the whole or part of what is visible on
         the screen of the current image display and dump it to the
         current snapshot device at the largest magnification.
      }
      \sstexamplesubsection{
         snapshot postscript views device=xw whole
      }{
         This dumps the whole of the xw device's memories to the postscript
         device, and also to a NDF called views.  The area magnification
         is a half.
      }
      \sstexamplesubsection{
         snapshot ps\_l device=ikon whole planes=0 title="Hardcopy Base"
      }{
         This dumps the whole of the Ikon's base memory to the ps\_l
         device.  The plot is entitled {\tt "Hardcopy Base"}.  The entire
         output plot occupies half of the maximum area available on the
         device.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The whole of the screen and the whole of the memory may be
         different, for example, the image may have been zoomed or panned.

         \sstitem
         Files are not spooled to laserprinters.  They must be printed
         outside this application.
      }
   }
   \sstdiytopic{
       Related Applications
   }{
       KAPPA: DISPLAY, GREYPLOT.
   }
   \sstimplementationstatus{
      No origin information is passed to the output NDF.
   }
}
\manroutine {{\manheadstyle{SQORST}}}{ Squashes or stretches a 2-d
  data array in either or both axes.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  The output data array, written to an {\mantt{IMAGE}} structure, is produced
  by either squashing or stretching the 2-d data array, in the input
  {\mantt{IMAGE}} structure, in either or both of the {$x$} and {$y$} axes.
  The dimensions of the output data array, are given by the user. The
  stretching is performed by keeping the edge pixels fixed and
  calculating the intervening pixels by bi-linear interpolation. The
  squashing is performed by calculating each pixel in the output
  array as the mean of the corresponding pixels in the input array.

  The magic-value method is used for processing bad data.

\manroutineitem {Invocation }{}
  SQORST

\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}}  }{{\mantt{IMAGE}}}
  {\mantt{IMAGE}} structure containing the 2-d data array to be squashed
  or stretched.
\manparameterentry {{\mantt{READ}} }{{\mantt{XDIM}}  }{{\mantt{\_INTEGER}}}
  First dimension for the output 2-d data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{YDIM}}  }{{\mantt{\_INTEGER}}}
  Second dimension for the output 2-d data array.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}} }{{\mantt{IMAGE}}}
  {\mantt{IMAGE}} structure to contain the 2-d data array after being
  squashed or stretched.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}} }{{\mantt{\_CHAR}}}
  Will form the {\mantt{TITLE}} component for the output {\mantt{IMAGE}}
  structure. \mbox{{\mantt ['KAPPA - Sqorst']}}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
  Dave Baines ({\mantt{ROE}}::{\mantt{ASOC5}})
\end{manroutinedescription}

\sstroutine{
   STATS
}{
   Computes simple statistics for an NDF's pixels
}{
   \sstdescription{
      This application computes and displays simple statistics for the
      pixels in an NDF's data, quality or variance array. The
      statistics available are:
      \ssthitemlist{

         \sstitem
         the pixel sum,

         \sstitem
         the pixel mean,

         \sstitem
         the pixel standard deviation,

         \sstitem
         the value and position of the minimum- and maximum-valued
         pixels,

         \sstitem
         the total number of pixels in the NDF,

         \sstitem
         the number of pixels used in the statistics, and

         \sstitem
         the number of pixels omitted.

      }
      Iterative $\kappa$-sigma clipping may also be applied as an option.
   }
   \sstusage{
      stats ndf [comp] [clip] [logfile]
   }
   \sstparameters{
      \sstsubsection{
         CLIP( ) = \_REAL (Read)
      }{
         An optional 1-dimensional array of clipping levels to be
         applied, expressed as standard deviations.  If a null value is
         supplied for this parameter (the default), then no iterative
         clipping will take place and the statistics computed will
         include all the valid NDF pixels.

         If an array of clipping levels is given, then the routine will
         first compute statistics using all the available pixels. It
         will then reject all those pixels whose values lie outside
         $\kappa$ standard deviations of the mean (where $\kappa$ is
         the first value
         supplied) and will then re-evaluate the statistics. This
         rejection iteration is repeated in turn for each value in the
         CLIP array.  A maximum of 5 values may be supplied, all of
         which must be positive. {\tt [!]}
      }
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The name of the NDF array component for which statistics are
         required: {\tt "Data"}, {\tt "Error"}, {\tt "Quality"} or
         {\tt "Variance"} (where {\tt "Error"} is the alternative
         to {\tt "Variance"} and causes the square root of the variance
         values to be taken before computing the statistics). If
         {\tt "Quality"} is specified, then the quality values are treated
         as numerical values (in the range 0 to 255). {\tt ["Data"]}
      }
      \sstsubsection{
         LOGFILE = FILENAME (Write)
      }{
         A text file into which the results should be logged.  If a null
         value is supplied (the default), then no logging of results
         will take place. {\tt [!]}
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         The NDF data structure to be analysed.
      }
      \sstsubsection{
         NUMBAD = \_INTEGER (Write)
      }{
         The number of pixels which were either not valid or were
         rejected from the statistics during iterative $\kappa$-sigma
         clipping.
      }
   }
   \sstresparameters{
      \sstsubsection{
         MAXCOORD( ) = \_DOUBLE (Write)
      }{
         A 1-dimensional array of values giving the data co-ordinates of
         the centre of the (first) maximum-valued pixel found in the
         NDF array. The number of co-ordinates is equal to the number of
         NDF dimensions.
      }
      \sstsubsection{
         MAXIMUM = \_DOUBLE (Write)
      }{
         The maximum pixel value found in the NDF array.
      }
      \sstsubsection{
         MAXPOS( ) = \_INTEGER (Write)
      }{
         A 1-dimensional array of pixel indices identifying the (first)
         maximum-valued pixel found in the NDF array. The number of
         indices is equal to the number of NDF dimensions.
      }
      \sstsubsection{
         MEAN = \_DOUBLE (Write)
      }{
         The mean value of all the valid pixels in the NDF array.
      }
      \sstsubsection{
         MINCOORD( ) = \_DOUBLE (Write)
      }{
         A 1-dimensional array of values giving the data co-ordinates of
         the centre of the (first) minimum-valued pixel found in the
         NDF array.  The number of co-ordinates is equal to the number of
         NDF dimensions.
      }
      \sstsubsection{
         MINIMUM = \_DOUBLE (Write)
      }{
         The minimum pixel value found in the NDF array.
      }
      \sstsubsection{
         MINPOS( ) = \_INTEGER (Write)
      }{
         A 1-dimensional array of pixel indices identifying the (first)
         minimum-valued pixel found in the NDF array. The number of
         indices is equal to the number of NDF dimensions.
      }
      \sstsubsection{
         NUMGOOD = \_INTEGER (Write)
      }{
         The number of NDF pixels which actually contributed to the
         computed statistics.
      }
      \sstsubsection{
         NUMPIX = \_INTEGER (Write)
      }{
         The total number of pixels in the NDF (both good and bad).
      }
      \sstsubsection{
         SIGMA = \_DOUBLE (Write)
      }{
         The standard deviation of the pixel values in the NDF array.
      }
      \sstsubsection{
         TOTAL = \_DOUBLE (Write)
      }{
         The sum of the pixel values in the NDF array.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         stats image
      }{
         Computes and displays simple statistics for the data array in
         the NDF called image.
      }
      \sstexamplesubsection{
         stats ndf=spectrum variance
      }{
         Computes and displays simple statistics for the variance array
         in the NDF called spectrum.
      }
      \sstexamplesubsection{
         stats spectrum error
      }{
         Computes and displays statistics for the variance array in the
         NDF called spectrum, but takes the square root of the variance
         values before doing so.
      }
      \sstexamplesubsection{
         stats halley logfile=stats.dat
      }{
         Computes statistics for the data array in the NDF called
         halley, and writes the results to a logfile called {\tt stats.dat}.
      }
      \sstexamplesubsection{
         stats ngc1333 clip=[3.0,2.8,2.5]
      }{
         Computes statistics for the data array in the NDF called
         ngc1333, applying three iterations of $\kappa$-sigma clipping. The
         statistics are first calculated for all the valid pixels in
         the data array.  Those pixels with values lying more than 3.0
         standard deviations from the mean are then rejected, and the
         statistics are re-computed. This process is then repeated
         twice more, rejecting pixel values lying more than 2.8 and 2.5
         standard deviations from the mean.  The final statistics are
         displayed.
      }
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, VARIANCE,
         QUALITY, TITLE, and HISTORY components of the NDF.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using double-precision floating point.

         \sstitem
         Any number of NDF dimensions is supported.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: HISTAT, INSPECT, NDFTRACE; Figaro: ISTAT.
   }
}
\sstroutine{
   SUB
}{
   Subtracts one NDF data structure from another
}{
   \sstdescription{
      The routine subtracts one NDF data structure from another
      pixel-by-pixel to produce a new NDF.
   }
   \sstusage{
      sub in1 in2 out
   }
   \sstparameters{
      \sstsubsection{
         IN1 = NDF (Read)
      }{
         First NDF, from which the second NDF is to be subtracted.
      }
      \sstsubsection{
         IN2 = NDF (Read)
      }{
         Second NDF, to be subtracted from the first NDF.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF to contain the difference of the two input NDFs.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the NDF supplied for parameter IN1 to be used
         instead. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         sub a b c
      }{
         This subtracts the NDF called b from the NDF called a, to make
         the NDF called c.  NDF c inherits its title from a.
      }
      \sstexamplesubsection{
         sub out=c in1=a in2=b title="Background subtracted"
      }{
         This subtracts the NDF called b from the NDF called a, to make
         the NDF called c.  NDF c has the title {\tt "Background subtracted"}.
      }
   }
   \sstnotes{
      If the two input NDFs have different pixel-index bounds, then
      they will be trimmed to match before being subtracted.  An error
      will result if they have no pixels in common.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ADD, CADD, CDIV, CMULT, CSUB, DIV, MATHS, MULT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, HISTORY, and VARIANCE components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Units processing is not supported at present and therefore the
         UNITS component is not propagated.

         \sstitem
         Processing of bad pixels and automatic quality masking are supported.

         \sstitem
         All non-complex numeric data types can be handled.

      }
   }
}
\sstroutine{
   SUBSTITUTE
}{
   Replaces all occurrences of a given value in an NDF array with
   another value
}{
   \sstdescription{
      This application changes all pixels that have a defined value in
      an NDF with an alternate value.  Other values are unchanged.  The
      number of replacements is reported.
   }
   \sstusage{
      substitute in out oldval newval [comp]
   }
   \sstparameters{
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The components whose values are to be substituted.  It may
         be {\tt "Data"}, {\tt "Error"}, {\tt "Variance"}, or {\tt "All"}.  The last of the
         options forces substitution in both the data and variance
         arrays.  This parameter is ignored if the data array is the
         only array component within the NDF.  {\tt ["Data"]}
      }
      \sstsubsection{
         IN = NDF  (Read)
      }{
         Input NDF structure containing the data and/or variance array
         to have some of its elements substituted.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF structure containing the data and/or variance array
         that is a copy of the input array, but with replacemeent values
         substituted.
      }
      \sstsubsection{
         NEWVAL = \_DOUBLE (Read)
      }{
         The value to replace occurrences of OLDVAL.  It must lie
         within the minimum and maximum values of the data type of the
         array with higher precision.  The new value is converted to
         data type of the array being converted before the search
         begins.  The suggested default is the current value.
      }
      \sstsubsection{
         OLDVAL = \_DOUBLE (Read)
      }{
         The element value to be replaced.  The same value is
         substituted in both the data and variance arrays when
         COMP={\tt "All"}.  It must lie within the minimum and maximum values
         of the data type of the array with higher precision.  The
         replacement value is converted to data type of the array being
         converted before the search begins.  The suggested default is
         the current value.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the input NDF to the output NDF. {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         substitute aa bb 1 0
      }{
         This copies the NDF called aa to the NDF bb, except
         that any pixels with value 1 in aa are altered to have value
         0 in bb.
      }
      \sstexamplesubsection{
         substitute aa bb oldval=1 newval=0 comp=v
      }{
         As above except the substitution occurs to the variance
         values.
      }
      \sstexamplesubsection{
         substitute in=saturn out=saturn5 oldval=2.5 newval=5 comp=All
      }{
         This copies the NDF called saturn to the NDF saturn5, except
         that any elements in the data and variance arrays that have
         value 2.5 are altered to have value 5 in saturn5.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The comparison for floating-point values tests that the
         difference between the replacement value and the element value is
         less than their mean times the precision of the data type.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CHPIX, FILLBAD, GLITCH, NOMAGIC, SEGMENT, SETMAGIC, ZAPLIN;
      SPECDRE: GOODVAR.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF
         data structure and propagates all extensions.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         Any number of NDF dimensions is supported.
      }
   }
}

\sstroutine{
   SURFIT
}{
   Fits a polynomial or bi-cubic spline surface to 2-dimensional
   data array
}{
   \sstdescription{
      The background of a 2-dimensional data array in the supplied NDF
      structure is estimated by condensing the array into equally sized
      rectangular bins, fitting a spline or polynomial surface to the
      bin values, and finally evaluating the surface for each pixel in
      the data array.

      There is a selection of estimators by which representative
      values for each bin are determined.  There are several options to
      make the fit more accurate.  Values beyond upper and lower
      thresholds may be excluded from the binning.  Bad pixels are also
      excluded, so prior masking may help to find the background more
      rapidly.  Kappa-sigma clipping of the fitted bins is available
      so that the fit is not biased by anomalous bins, such as those
      entirely within an extended object.  If a given bin contains more
      than a prescribed fraction of bad pixels, it is excluded from the
      fit.

      The data array representing the background is evaluated at each
      pixel by one of two methods.  It is written to the output NDF
      structure.

      The raw binned data, the weights, the fitted binned data and the
      residuals to the fit may be written to a logfile.  This also
      keeps a record of the input parameters and the rms error of the
      fit.
   }
   \sstusage{
      surfit in out [fittype] [estimator] [bindim] [evaluate]
   }
   \sstparameters{
      \sstsubsection{
         BINDIM() = \_INTEGER (Read)
      }{
         The $x$-$y$ dimensions of a bin used to estimate the local
         background.  If you supply only one value, it is used for
         both dimensions.  The minimum value is 2.  The maximum may be
         constrained by the number of polynomial terms, such that in
         each direction there are at least as many bins as terms.  The
         default is dynamic such that 32 bins are created along each
         axis. {\tt []}
      }
      \sstsubsection{
         CLIP() = \_REAL (Read)
      }{
         Array of limits for progressive clipping of pixel values
         during the binning process in units of standard deviation.  A
         null value means only unclipped statistics are computed and
         presented.  Between 1 and 5 values may be supplied. {\tt [2,3]}
      }
      \sstsubsection{
         ESTIMATOR = LITERAL (Read)
      }{
         The estimator for the bin.  It must one of the following
         values: {\tt "Mean"} for the mean value, {\tt "Ksigma"} for the mean with
         kappa-sigma clipping; {\tt "Mode"} for the mode, and {\tt "Median"} for
         the median.  {\tt "Mode"} is only available when there are at least
         twelve pixels in a bin.  It is also the default when this
         criterion is met, other the default is {\tt "Median"}. {\tt []}
      }
      \sstsubsection{
         EVALUATE = LITERAL (Read)
      }{
         The method by which the resulting data array is to be
         evaluated from the surface-fit.  It must be either
         {\tt "Interpolate"} where the values at the corners of the bins are
         derived first, and then the pixel values are found by linear
         interpolation within those bins; or {\tt "All"} where the
         surface-fit is evaluated for every pixel.  The latter is
         slower, but can produce more-accurate results, unless the
         surface is well behaved.  The default is the current
         value, which is initially set to {\tt "Interpolate"}. {\tt []}
      }
      \sstsubsection{
         FITCLIP() = \_REAL (Read)
      }{
         Array of limits for progressive clipping of the binned array
         in units of the rms deviation of the fit.  A null value ({\tt !})
         means no clipping of the binned array will take place.
         Between 1 and 5 values may be supplied.  The default is the
         current value, which is {\tt !} initially. {\tt []}
      }
      \sstsubsection{
         FITTYPE = LITERAL (Read)
      }{
         The type of fit.  It must be either {\tt "Polynomial"} for a
         Chebyshev polynomial or {\tt "Spline"} for a bi-cubic spline.  The
         default is the current value, which initially is {\tt "Spline"}.
         {\tt []}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         NDF containing the 2-dimensional data array to be fitted.
      }
      \sstsubsection{
         KNOTS( 2 ) = \_INTEGER (Read)
      }{
         The number of interior knots used for the bi-cubic-spline fit
         along the $x$ and $y$ axes.  These knots are equally spaced
         within the image.  Both values must be in the range 0 to
         11.  If you supply a single value, it applies to both axes.
         Thus {\tt 1} creates one interior knot, {\tt [5,4]} gives 5
         along the $x$
         axis and 4 along the $y$ direction.  Increasing this
         parameter values increases the flexibility of the surface.
         Normally, {\tt 4} is a reasonable value.  The upper limit of
         acceptable values will be reduced along each axis when its
         binned array dimension is less than 29.  KNOTS is only
         accessed when FITTYPE={\tt "Spline"}.  The default is the current
         value, which is {\tt 4} initially. {\tt []}
      }
      \sstsubsection{
         LOGFILE = FILENAME (Read)
      }{
         Name of the file to log the binned array and errors before and
         after fitting.  If null, there will be no logging. {\tt [!]}
      }
      \sstsubsection{
         ORDER( 2 ) = \_INTEGER (Read)
      }{
         The orders of the fits along the $x$ and $y$ directions.  Both
         values must be in the range 0 to 14.  If you supply a single
         single value, it applies to both axes.  Thus {\tt 0} gives a
         constant, {\tt [3,1]} gives a cubic along the $x$ direction and
         a linear fit along the $y$.  Increasing this parameter values
         increases the flexibility of the surface.  The upper limit of
         acceptable values will be reduced along each axis when its
         binned array dimension is less than 29.  ORDER is only
         accessed when FITTYPE={\tt "Polynomial"}.  The default is the
         current value, which is {\tt 4} initially. {\tt []}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         NDF to contain the fitted 2-dimensional data array.
      }
      \sstsubsection{
         THRHI = \_REAL (Read)
      }{
         Upper threshold above which values will be excluded from the
         analysis to derive representative values for the bins.  If it
         is null ({\tt !}) there will be no upper threshold. {\tt [!]}
      }
      \sstsubsection{
         THRLO = \_REAL (Read)
      }{
         Lower threshold below which values will be excluded from the
         analysis to derive representative values for the bins.  If it
         is null ({\tt !}) there will be no lower threshold. {\tt [!]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Value for the title of the output NDF.  A null value will cause
         the title of the NDF supplied for parameter IN to be used
         instead. {\tt [!]}
      }
      \sstsubsection{
         WLIM = \_REAL (Read)
      }{
         The minimum fraction of good pixels in a bin that permits the
         bin to be included in the fit.  Here good pixels are ones that
         participated in the calculation of the bin's representative
         value. So they exclude both bad pixels and ones rejected
         during estimation ({\em{e.g.}}\ ones beyond the thresholds or were
         clipped). {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         surfit comaB comaB\_bg
      }{
         This calculates the surface fit to the 2-dimensional NDF
         called comaB using the current defaults.  The evaluated fit is
         stored in the NDF called comaB\_bg.
      }
      \sstexamplesubsection{
         surfit comaB comaB\_bg poly median order=5 bindim=[24,30]
      }{
         As above except that 5th-order polynomial fit is chosen,
         the median is used to derive the representative value for each
         bin, and the binning size is 24 pixels along the first axis,
         and 32 pixels along the second.
      }
      \sstexamplesubsection{
         surfit comaB comaB\_bg fitclip=[2,3] logfile=comaB\_fit.lis
      }{
         As the first example except that the binned array is clipped at
         2 then 3 standard deviations to remove outliers before the
         final fit is computed.  The text file {\tt comaB\_fit.lis} records a
         log of the surface fit.
      }
      \sstexamplesubsection{
         surfit comaB comaB\_bg estimator=ksigma clip=[2,2,3]
      }{
         As the first example except that the representative value of
         each bin is the mean after clipping twice at 2 then once at
         3 standard deviations.
      }
      \sstexamplesubsection{
         surfit in=irasorion out=sback evaluate=all fittype=s knots=7
      }{
         This calculates the surface fit to the 2-dimensional NDF called
         irasorion.  The fit is evaluated at every pixel and the
         resulting array stored in the NDF called sback.  A spline with
         seven knots along each axis is used to fit the surface.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ARDMASK, FITSURFACE, MAKESURFACE.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, UNITS, and HISTORY components of the input NDF.
         There is no support for VARIANCE processing.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using single- or double-precision floating point for
         FITTYPE = {\tt "Spline"} or {\tt "Polynomial"} respectively.
      }
   }
}
\begin{htmlonly}
Click \htmladdnormallink{here}{surfit_exam.gif} to see an example plot
(67k).  It shows IRAS data, raw to the left, and sky-subtracted to the
right.
\end{htmlonly}

\sstroutine{
   THRESH
}{
   Edits an NDF such that array values below and above two thresholds
   take constant values
}{
   \sstdescription{
      This application creates from an input NDF structure an NDF with
      an array component whose values are edited as follows.  Array
      values between and including the upper and lower thresholds are
      copied from the input to output array.  Any values in the input
      array greater than the upper threshold will be set to one
      specified value, and anything less than the lower threshold will
      be set to another specified value, in the output data array.
      Thus if the replacement values equal their respective thresholds
      this application creates an NDF constrained to lie between two
      bounds.  Each replacement value may be the bad-pixel value for
      masking.
   }
   \sstusage{
      thresh in out thrlo thrhi newlo newhi [comp]
   }
   \sstparameters{
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The components whose values are to be constrained between
         thresholds.  It may be {\tt "Data"}, {\tt "Variance"}, or
         {\tt "Quality"}.  If {\tt "Quality"} is specified, then the
         quality values are treated as numerical values in the range
         0 to 255. {\tt ["Data"]}
      }
      \sstsubsection{
         IN = NDF  (Read)
      }{
         Input NDF structure containing the array to have thresholds
         applied.
      }
      \sstsubsection{
         NEWLO = LITERAL (Read)
      }{
         This defines the value to which all input array-element values
         less than the lower threshold are set.  If this is set to
         {\tt "Bad"}, the bad value is substituted.  Numerical values of
         NEWLO must lie in within the minimum and maximum values of the
         data type of the array being processed.  The suggested default
         is the lower threshold.
      }
      \sstsubsection{
         NEWHI = LITERAL (Read)
      }{
         This defines the value to which all input array-element values
         greater than the upper threshold are set.  If this is set to
         {\tt "Bad"}, the bad value is substituted.  Numerical values of
         NEWHI must lie in within the minimum and maximum values of the
         data type of the array being processed.  The suggested default
         is the upper threshold.
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         Output NDF structure containing the thresholded version of
         the array.
      }
      \sstsubsection{
         THRHI = \_DOUBLE (Read)
      }{
         The upper threshold value within the input array.  It must lie
         in within the minimum and maximum values of the data type of
         the array being processed.  The suggested default is the
         current value.
      }
      \sstsubsection{
         THRLO = \_DOUBLE (Read)
      }{
         The lower threshold value within the input array.  It must lie
         in within the minimum and maximum values of the data type of
         the array being processed.  The suggested default is the
         current value.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the input NDF to the output NDF.
         {\tt [!]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         thresh zzcam zzcam2 100 500 0 0
      }{
         This copies the data array in the NDF called zzcam to the NDF
         called zzcam2.  Any data value less than 100 and greater than
         500 in zzcam is set to 0 in zzcam2.
      }
      \sstexamplesubsection{
         thresh zzcam zzcam2 100 500 0 0 comp=Variance
      }{
         As above except that the data array is copied unchanged and the
         thresholds apply to the variance array.
      }
      \sstexamplesubsection{
         thresh n253 n253cl thrlo=$-$0.5 thrhi=10.1 $\backslash$
      }{
         This copies the data array in the NDF called n253 to the NDF
         called n253cl.  Any data value less than $-$0.5 in n253 is set
         to $-$0.5 in n253cl, and any value greater than 10.1 in n253
         becomes 10.1 in n253cl.
      }
      \sstexamplesubsection{
         thresh pavo pavosky $-$0.02 0.02 bad bad
      }{
         All data values outside the range $-$0.02 to 0.02 in the NDF
         called pavo become bad in the NDF called pavosky.  All values
         within this range are copied from pavo to pavosky.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: HISTEQ, MATHS; Figaro: CLIP, IDIFF, RESCALE.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of an NDF
         data structure and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         Any number of NDF dimensions is supported.
      }
   }
}


\sstroutine{
   TRANDAT
}{
   Converts free-format text data into an NDF
}{
   \sstdescription{
      This application takes grid data contained in a free-format text
      file and stores them in the data array of an NDF.  The data file
      could contain, for example, mapping data  or results from
      simulations which are to be converted into an image for analysis.

      There are two modes of operation which depend on whether the
      text file contains co-ordinate information, or solely data
      values (determined by parameter AUTO).

      a) {\bf AUTO=FALSE}  ~~If the file contains co-ordinate information
      the format of the data is tabular; the positions and values are
      arranged in columns and a record may contain information for only
      a single point.  Where data points are duplicated only the last
      value appears in the NDF.  Comment lines can be given, and are
      indicated by a hash or exclamation mark in the first column.
      Here is an example file (the vertical ellipses indicate missing
      lines in the file):
{\tt \begin{verse}
          \# Model 5, phi = 0.25,  eta = 1.7 \\
          1 -40.0   40.0   1121.9 \\
          2  0.0   30.0     56.3 \\
          3 100.0   20.0   2983.2 \\
          4 120.0   85.0    339.3 \\
          . ~. ~~. ~ .   \\
          . ~. ~~. ~ .   \\
          . ~. ~~. ~ .   \\
          <EOF>
\end{verse}}

      The records do not need to be ordered (but see the warning in the
      Notes), as the application searches for the maximum and minimum
      co-ordinates in each dimension so that it can define the size of
      the output image.  Also, each record may contain other data
      fields (separated by one or more spaces), which need not be all
      the same data type.  In the example above only columns 2, 3 and 4
      are required.  There are parameters (POSCOLS, VALCOL) which
      select the co-ordinate and value columns.

      The distance between adjacent pixels (given by parameter PSCALE)
      defaults to 1, and is in the same units as the read-in
      co-ordinates.  The pixel index of a data value is calculated
      using the expression

      \[   index = {\rm IFIX}( ( x - xmin ) / scale ) + 1 \]

      where $x$ is the supplied co-ordinate and $xmin$ is the minimum
      supplied co-ordinate along an axis, $scale$ is the value of
      parameter PSCALE, and IFIX converts from real to integer.

      You are informed of the number of points found and the maximum
      and minimum co-ordinate values for each dimension.  There is no
      limit imposed by the application on the number of points or the
      maximum output array size, though there may be external
      constraints.  The derived array size is reported in case you have
      made a typing error in the text file.  If you realise that this
      has indeed occurred just abort ({\tt !!}) when prompted for the output
      NDF.

      b) {\bf AUTO=TRUE}  ~~If the text file contains no co-ordinates, the
      format is quite flexible, however, the data are read into the data array
      in Fortran order, {\it i.e.}\ the first dimension is the most rapidly
      varying, followed by the second dimension and so on.  The number
      of data values that may appear on a line is variable; data values
      are separated by at least a space, comma, tab or carriage return.
      A line can have up to 255 characters.  In addition a record may
      have trailing comments designated by a hash or exclamation mark.
      Here is an example file, though a more regular format would be
      clearer for the human reader.
{\tt \begin{verse}
          \# test for the new TRANDAT \\
          23 45.3 ! a comment \\
          50.7,47.5 120. 46.67  47.89 42.4567 \\
          .1 23.3 45.2 43.2  56.0 30.9 29. 27. 26. 22.4 20. 18. -12. 8. \\
           9.2 11. \\
          <EOF>
\end{verse}}

      Notice that the shape of the NDF is defined by a parameter rather
      than explicitly in the file.
   }
   \sstusage{
      trandat freename out [poscols] [valcol] [pscale] [dtype] [title]
   }
   \sstparameters{
      \sstsubsection{
         AUTO = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the text file does not contain co-ordinate
         information. {\tt [FALSE]}
      }
      \sstsubsection{
         BAD = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the output NDF data array is initialised with the
         bad value, otherwise it is filled with zeroes. {\tt [TRUE]}
      }
      \sstsubsection{
         DTYPE = LITERAL (Read)
      }{
         The HDS type of the data values within the text file, and
         the type of the data array in the output NDF. The options
         are: {\tt '\_REAL'}, {\tt '\_DOUBLE'}, {\tt '\_INTEGER'},
         {\tt '\_BYTE'}, {\tt '\_UBYTE'}, {\tt '\_WORD'}, {\tt '\_UWORD'}.
         (Note the leading underscore.) {\tt ['\_REAL']}
      }
      \sstsubsection{
         FREENAME = FILENAME (Read)
      }{
         Name of the text file containing the free-format data.
      }
      \sstsubsection{
         LBOUND( ) = \_INTEGER (Read)
      }{
         The lower bounds of the NDF to be created.  The number of
         values must match the number supplied to parameter SHAPE.  It
         is only accessed in automatic mode.  It defaults to 1 along
         each axis. {\tt []}
      }
      \sstsubsection{
         POSCOLS() = \_INTEGER (Read)
      }{
         Column positions of the co-ordinates in an input record
         of the text file, starting from $x$ to higher dimensions.  It
         is only used in co-ordinate mode.  The columns must be
         different amongst themselves and also different from the
         column containing the values.  If there is duplication,
         new values for both POSCOLS and VALCOL will be requested.
         {\tt [1,2]}
      }
      \sstsubsection{
         PSCALE() = \_REAL (Read)
      }{
         Pixel-to-pixel distance in co-ordinate units for each
         dimension.  It is only used in co-ordinate mode.  Its purpose
         is to permit linear scaling from some arbitrary units to
         pixels. {\tt [}1.0 in each co-ordinate dimension{\tt ]}
      }
      \sstsubsection{
         QUANTUM = \_INTEGER (Read)
      }{
         You can safely ignore this parameter.  It is used for
         fine-tuning performance in the co-ordinate mode.

         The application obtains work space to store the position-value
         data before they can be copied into the output NDF so that the
         array bounds can be computed.  Since the number of lines in
         the text file is unknown, the application obtains chunks of
         work space whose size is three times this parameter whenever
         it runs out of storage.  (Three because the parameter
         specifies the number of lines in the file rather than the
         number of data items.)  If you have a large number of points
         there are efficiency gains if you make this parameter either
         about 20--30 per cent or slightly greater than or equal to the
         number of lines your text file.  A value slightly less than
         the number of lines is inefficient as it creates nearly 50 per
         cent unused space.  A value that is too small can cause
         unnecessary unmapping, expansion and re-mapping of the work
         space.  For most purposes the default should give acceptable
         performance.  It must lie between 32 and 2097152. {\tt [2048]}
      }
      \sstsubsection{
         SHAPE( ) = \_INTEGER (Read)
      }{
         The shape of the NDF to be created.  For example, {\tt [50,30,20]}
         would create 50 columns by 30 lines by 10 bands.  It is only
         accessed in automatic mode.
      }
      \sstsubsection{
         NDF = NDF (Write)
      }{
         Output NDF for the generated data array.
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF. {\tt ["KAPPA - Trandat"]}
      }
      \sstsubsection{
         VALCOL = \_INTEGER (Read)
      }{
         Column position of the array values in an input record of
         the text file.  It is only used in co-ordinate mode.  The
         column position must be different from those specified for
         the co-ordinate columns.  If there is duplication, new values
         for both POSCOLS and VALCOL will be requested. {\tt [3]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         trandat simdata.dat model
      }{
         Reads the text file {\tt simdata.dat} and stores the data into the
         data array of a two-dimensional, \_REAL NDF called model.  The
         input file should have the co-ordinates and real values
         arranged in columns, with the $x$-$y$ positions in columns 1 and 2
         respectively, and the real data in column 3.
      }
      \sstexamplesubsection{
         trandat freename=simdata out=model auto shape=[50,40,9]
      }{
         Reads the text file {\tt simdata} and stores the data into the
         data array of a three-dimensional, \_REAL NDF called model.
         Its $x$ dimension is 50, $y$ is 40 and $z$ is 9.  The input file only
         contains real values and comments.
      }
      \sstexamplesubsection{
         trandat freename=simdata out=model auto shape=[50,40,9] dtype=\_i
      }{
         As the previous example except an \_INTEGER NDF is created, and
         the text file must contain integer data.
      }
      \sstexamplesubsection{
         trandat simdata.dat model [6,3,4] 2
      }{
         Reads the text file {\tt simdata.dat} and stores the data into the
         data array of a three-dimensional, \_REAL NDF called model.  The
         input file should have the co-ordinates and real values
         arranged in columns, with the $x$-$y$-$z$ positions in columns 6, 3
         and 4 respectively, and the real data in column 2.
      }
      \sstexamplesubsection{
         trandat spectrum.dat lacertid noauto poscols=2 valcol=4 pscale=2.3
      }{
         Reads the text file {\tt spectrum.dat} and stores the data into the
         data array of a one-dimensional, \_REAL NDF called lacertid. 
         The input file should have the co-ordinate and real values
         arranged in columns, with its co-ordinates in columns 2, and
         the real data in column 4.  A one-pixel step in the NDF
         corresponds to 2.3 in units of the supplied co-ordinates.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         All non-complex numeric data types can be handled.  However,
         byte, unsigned byte, word and unsigned word require data
         conversion, and therefore involve additional processing.
         to a vector element (for $n$-d generality).

         \sstitem
         {\bf WARNING:} In non-automatic mode it is strongly advisable for
         large output NDFs to place the data in Fortran order, {\it i.e.}\ the
         first dimension is the most rapidly varying, followed by the
         second dimension and so on.  This gives optimum performance.  The
         meaning of `large' will depend on working-set quotas on your
         system, but a few megabytes gives an idea.  If you jump randomly
         backwards and forwards, or worse, have a text file in
         reverse-Fortran order, this can have disastrous performance
         consequences for you and other users.

         \sstitem
         In non-automatic mode, the co-ordinates for each dimension are
         stored in the NDF axis structure.  The first centre is at the
         minimum value found in the list of positions for the dimension
         plus half of the scale factor.  Subsequent centres are
         incremented by the scale factor.

         \sstitem
         The output NDF may have between one and seven dimensions.

         \sstitem
         In automatic mode, an error is reported if the shape does not
         use all the data points in the file.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      CONVERT: ASCII2NDF, NDF2ASCII; SPECDRE: ASCIN, ASCOUT.
   }
}
\sstroutine{
   TRANINVERT
}{
   Inverts a transformation
}{
   \sstdescription{
      This inverts a transformation stored in the TRANSFORM (SUN/61)
      format within an existing HDS file.
   }
   \sstusage{
      traninvert transform
   }
   \sstparameters{
      \sstsubsection{
         TRANSFORM = TRN (Read and Write)
      }{
         The transformation structure to be inverted.  This may be an
         HDS container file, in which case the transformation structure
         is assumed to be called TRANSFORM at the top level of the
         file; or a path to the HDS object.  The suggested default is
         the current transformation structure.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         traninvert shear.transform
      }{
         This inverts the transformation structure stored in object
         TRANSFORM within the HDS file called shear.
      }
      \sstexamplesubsection{
         traninvert shear
      }{
         This does the same as above.
      }
      \sstexamplesubsection{
         traninvert $\backslash$
      }{
         This inverts the current transformation structure.
      }
      \sstexamplesubsection{
         traninvert m51.more.polar
      }{
         This inverts the transformation structure called POLAR in
         the extension of the NDF called m51.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         On completion, the destination structure for the
         transformation information equates to the current transformation
         global parameter.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: TRANSFORMER, TRANJOIN, TRANMAKE, TRANTRACE;
      CCDPACK: \linebreak CCDEDIT, TRANLIST, TRANNDF.
   }
}

\sstroutine{
   TRANJOIN
}{
   Joins two transformations
}{
   \sstdescription{
      This joins two transformations stored in the TRANSFORM (SUN/61)
      format.  The concatenated transformation can be stored with either
      original transformation or be placed in a new file.
   }
   \sstusage{
      tranjoin in1 in2 out dest=?
   }
   \sstparameters{
      \sstsubsection{
         DEST = LITERAL (Read)
      }{
         The destination for the concatenated transformations.  This can
         be one of the following:
         \begin{description}
         \item {\tt "First"} --- Appends the second transformation in the first.
                        The second transformation is unchanged.
         \item {\tt "Second"} --- Prefixes the first transformation in the second.
                        The first transformation is unchanged.
         \item {\tt "New"}  --- Creates a new transformation structure using
                        parameter OUT.  The input transformations are
                        unchanged.
         \end{description}
         {\tt ["New"]}
      }
      \sstsubsection{
         IN1 = TRN (Read and Write)
      }{
         The first transformation structure to be concatenated.  It
         prefixes the second supplied transformation.  This may be an
         HDS container file, in which case the transformation structure
         is assumed to be called TRANSFORM at the top level of the
         file; or a path to the HDS object.  The suggested default is
         the current value.
      }
      \sstsubsection{
         IN2 = TRN (Read and Write)
      }{
         The second transformation structure to be concatenated.  It
         appends to the first supplied transformation.  This may be an
         HDS container file, in which case the transformation structure
         is assumed to be called TRANSFORM at the top level of the
         file; or a path to the HDS object.  The suggested default is
         the current value.
      }
      \sstsubsection{
         OUT = TRN (Write)
      }{
         The path to the new transformation structure created when
         DEST={\tt "NEW"} to hold the concatenated transformations.  If only
         an HDS container filename is supplied, the transformation is
         placed within a structure called TRANSFORM at the top-level of
         the file.  So for instance, if OUT={\tt warp9}, the transformation
         will be placed in the top-level structure TRANSFORM within the HDS
         file {\tt warp9.sdf}.  In this case the container file may already
         exist.  If, on the other hand, an explicit structure is named,
         the transformation information will be placed there.  For
         example, to place the transformation in the extension GALPHOT
         of the NDF called NGC253, OUT would be
         {\tt NGC253.MORE.GALPHOT}.  The structure name is limited to 15
         printing characters.  Note that the structure must not already
         exist.  If it does, an error condition results.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         tranjoin tr1 tr2 tr3
      }{
         This prefixes the transformation in the HDS file called
         {\tt tr1.sdf} to that in file {\tt tr2.sdf}, and stores the
         result in HDS file {\tt tr3.sdf}.  All three transformations
         are located within objects called TRANSFORM at the top-level.
      }
      \sstexamplesubsection{
         tranjoin offset shear.tr1 shape.rotate
      }{
         This prefixes the transformation in the structure TRANSFORM at
         the top level of the HDS container file called {\tt offset.sdf}
         ({\it i.e.} OFFSET.TRANSFORM) to the transformation in the structure
         TR1 in the HDS file {\tt shear.sdf}.  The resulting transformation is in
         the file called {\tt shape.sdf} and is named ROTATE.
      }
      \sstexamplesubsection{
         tranjoin norm.scale1 polar dest=S
      }{
         This prefixes the transformation structure NORM.SCALE1 to \linebreak
         POLAR.TRANSFORM, the concatenation being stored in
         POLAR.TRANSFORM.
      }
      \sstexamplesubsection{
         tranjoin norm.scale1 polar dest=f
      }{
         This appends the transformation structure POLAR.TRANSFORM to \linebreak
         NORM.SCALE1, the concatenation being stored in NORM.SCALE1.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The number of output variables of the first transformation must
         equal the number of input variables of the second.  Also it is not
         permitted to concatenate a transformation in which only the
         forward mapping is defined with another in which only the inverse
         mapping is specified.

         \sstitem
         On completion, the destination structure for the
         transformation information equates to the current transformation
         global parameter.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: TRANSFORMER, TRANINVERT, TRANMAKE, TRANTRACE;
      CCDPACK: CCDEDIT, TRANLIST, TRANNDF.
   }
}

\sstroutine{
   TRANMAKE
}{
   Makes a transformation structure given its co-ordinate mappings
}{
   \sstdescription{
      This application creates a transformation data structure from
      forward and inverse mappings that you supply.  The
      TRANSFORMER application uses such a structure to transform an NDF
      by resampling.  The structure can have classification qualifiers
      and a comment.

      For convenience, TRANMAKE can create a two-dimensional linear
      transformation merely from the six coefficients, or
      two-dimensional Cartesian-to-polar transformation given the
      origin position and angle.
   }
   \sstusage{
      tranmake transform trtype comment
        \newline\hspace*{1.5em}
        $\left\{ {\begin{tabular}{l}
                  nvin=? nvout=? class=? for1-for7=? inv1-inv7=? fa-fz=? pa-pz=? \\
                  tr \\
                  \end{tabular} }
        \right.$
        \newline\hspace*{1.9em}
        \makebox[0mm][c]{\small trtype}
   }
   \sstparameters{
      \sstsubsection{
         CLASS( ) = LITERAL (Read)
      }{
         A list of classifications that describe the properties of the
         transformation.  This is optional, but the information can be
         used to make other applications run more efficiently.  This
         applies particularly to linear or constant determinants.
         Valid values are:
         \begin{description}
         \item {\tt "Linear"}       --- linear and preserves straight lines,
         \item {\tt "Independent"}  --- preserves the independence of the axes,
         \item {\tt "Diagonal"}     --- preserves the axes themselves,
         \item {\tt "Isotropic"}    --- preserves angles and shapes,
         \item {\tt "Positive\_det"} --- a component of reflection is absent,
         \item {\tt "Negative\_det"} --- a component of reflection is present,
         \item {\tt "Constant\_det"} --- the scale factor is constant,
         \item {\tt "Unit\_det"}     --- areas (or volumes {\it etc.}) are preserved.
         \end{description}

         See SUN/61 Appendix~B for more details of transformation
         classification and a table of the classifications of common
         mappings.  The suggested default is null ({\tt !}) meaning unknown,
         and no classification is written to the transformation
         structure.  This parameter is ignored unless TRTYPE={\tt "Expres"}.
      }
      \sstsubsection{
         COMMENT = LITERAL (Read)
      }{
         The comment string associated with the transformation.
         A null value ({\tt !}) causes a blank comment to be written into
         the transformation.  Use the {\tt "--$>$"} symbol to indicate the
         forward transformation.  The suggested default is the null value.
      }
      \sstsubsection{
         FA-FZ = LITERAL (Read)
      }{
         These parameters supply the values of `sub-expressions' used
         in the expressions FOR1-FOR7, and INV1-INV7.  Any of the 26
         may appear; there is no restriction on order.  These
         parameters should be used when repeated expressions are
         present in complex transformations.  Sub-expressions may
         contain references to other sub-expressions and constants
         (PA-PZ).  An example of using sub-expressions is:
         \begin{description}
         \item FOR1 $>$ {\tt "XX=PA$*$ASIND(FA/PA)$*$X/FA"}
         \item FOR2 $>$ {\tt "YY=PA$*$ASIND(FA/PA)$*$Y/FA"}
         \item INV1 $>$ {\tt "X=PA$*$SIND(FB/PA)$*$XX/FB"}
         \item INV2 $>$ {\tt "Y=PA$*$SIND(FB/PA)$*$YY/FB"}
         \item FA $>$ {\tt SQRT(X$*$X$+$Y$*$Y)}
         \item PA $>$ {\tt 100D0}
         \item FB $>$ {\tt SQRT(XX$*$XX$+$YY$*$YY)}
         \end{description}
         where the parameter name is to the left of $>$ and its value is
         to the right of the $>$.  This parameter is ignored unless
         TRTYPE={\tt "Expres"}.
      }
      \sstsubsection{
         FITTYPE = \_INTEGER (Read)
      }{
         The type of fit specified by coefficients supplied via the
         TR parameter.  Appropriate values are:
         \begin{description}
         \item 1 -- shift of origin,
         \item 2 -- shift of origin and rotation,
         \item 3 -- shift of origin and magnification,
         \item 4 -- shift of origin, rotation, and magnification
                   (solid body), and
         \item 5 -- a full six-parameter fit.
         \end{description}

         The value of this parameter is used to classify the
         transformation (see the CLASS parameter).  This parameter is
         ignored unless TRTYPE={\tt "Bilinear"}.  {\tt [5]}
      }
      \sstsubsection{
         FOR1-FOR7 = LITERAL (Read)
      }{
         The NVIN expressions that define the forward mapping or
         mappings of the transformation.  FOR1 applies to first
         output variable, and so on through to FOR7 for the seventh
         input variable.  The expressions are written in Fortran-like
         syntax.  The arithmetic operators ({\tt $+$,$-$,/,$*$,$*$$*$})
         follow the normal order of precedence.  Using matching (nested)
         parentheses will explicitly define the order of expression
         evaluation.  The expression may contain constants and the
         built-in functions (LOG10, SQRT, SIN, TAND {\it etc.}) described in
         SUN/61 Appendix~A.

         For a null forward transformation there must still be NVOUT
         expressions, each just containing the name of the output
         variable, for example, {\tt "X"}.  An example expression is
         {\tt "Z=PA$*$(NINT(ZP)$+$0.5)"}.

         This parameter is ignored unless TRTYPE={\tt "Expres"}.
      }
      \sstsubsection{
         INV1-INV7 = LITERAL (Read)
      }{
         The NVOUT expressions that define the inverse mapping or
         mappings of the transformation.  INV1 applies to first
         input variable, and so on through to INV7 for the seventh
         input variable.  The expressions are written in Fortran-like
         syntax.  The arithmetic operators ({\tt $+$,$-$,/,$*$,$*$$*$})
         follow the normal order of precedence.  Using matching (nested)
         parentheses will explicitly define the order of expression
         evaluation.  The expression may contain constants and the
         built-in functions (LOG10, SQRT, SIN, TAND {\it etc.}) described in
         SUN/61 Appendix~A.

         For a null inverse mapping there must still be NVIN
         expressions, each just containing the name of the input
         variable, for example, {\tt "XX"}.  Generally, it is the
         inverse mapping that is required. An example expression is
         {\tt "X=MOD(10$*$SQRT((XX$+$YY)$*$ZZ),360)"}.

         This parameter is ignored unless TRTYPE={\tt "Expres"}.
      }
      \sstsubsection{
         NVIN = INTEGER (Read)
      }{
         The number of input variables in the transformation.  It must
         lie in the range 1 to 7.  The suggested default is the current
         value, and 2 initially.   This parameter is ignored unless
         TRTYPE={\tt "Expres"}.
      }
      \sstsubsection{
         NVOUT = INTEGER (Read)
      }{
         The number of output variables in the transformation.  It must
         lie in the range 1 to 7.  The suggested default is the number
         of input variables.   This parameter is ignored unless
         TRTYPE={\tt "Expres"}.
      }
      \sstsubsection{
         PA-PZ = \_DOUBLE (Read)
      }{
         These parameters supply the values of constants used in the
         expressions FOR1-FOR7, and INV1-INV7.  Any of the 26 may
         appear; there is no restriction on order.  Using parameters
         allows the substitution of repeated constants using one
         reference.  This is especially convenient for constants with
         many significant digits.  It also allows easy modification of
         parameterised expressions (expressions say with an adjustable
         centre) provided the application has not been used in the
         interim.  The parameter PI has a default value of
         3.14159265359D0.  An example of using parameters is:
         \begin{description}
         \item FOR1 $>$ {\tt "XX=SQRT(FX$*$FX$+$FY$*$FY)"}
         \item FOR2 $>$ {\tt "YY=ATAN2D(-FY,FX)"}
         \item INV1 $>$ {\tt "X=XX$*$SIND(YY)$+$PA"}
         \item INV2 $>$ {\tt "Y=-YY$*$COSD(XX)$+$PB"}
         \item FX $>$ {\tt X-PA}
         \item FY $>$ {\tt Y-PB}
         \item PA $>$ X-centre-value
         \item PB $>$ Y-centre-value
         \end{description}
         where the parameter name is to the left of $>$ and its value is
         to the right of the $>$.  This example maps Cartesian
         co-ordinates ($x$,$y$) to polar ($r$,$\theta$) about a specified
         centre.
         This parameter is ignored unless TRTYPE={\tt "Expres"}.
      }
      \sstsubsection{
         PREC = LITERAL (Read)
      }{
         The arithmetic precision with which the transformation
         functions will be evaluated when used.  This may be either
         {\tt "\_REAL"} for single precision, {\tt "\_DOUBLE"} for
         double precision, or {\tt "\_INTEGER"} for integer precision.
         Elastic precisions are used, such that a higher precision will
         be used if the input
         data warrant it.  So for example if PREC = {\tt "\_REAL"}, but
         double-precision data were to be transformed, double-precision
         arithmetic would actually be used.  This parameter is
         ignored unless TRTYPE={\tt "Expres"}.  {\tt ["\_REAL"]}
      }
      \sstsubsection{
         TR( 6 ) = \_DOUBLE (Read)
      }{
         If TRTYPE={\tt "Bilinear"} is chosen then the values of this
         parameter are the 6 coefficients of a linear transformation of
         the type.
         \begin{description}
         \item X$^\prime$ = TR(1) $+$ TR(2)$*$X $+$ TR(3)$*$Y
         \item Y$^\prime$ = TR(4) $+$ TR(5)$*$X $+$ TR(6)$*$Y
         \end{description}
         The initial suggested default is the identity transformation.
         ({\tt [0,1,0,0,0,1]}).

         If TRTYPE={\tt "Polar"}, only three values are needed.  TR(1) and
         TR(2) are the $x$-$y$ co-ordinates of the origin of the centre of
         the polar co-ordinate system.  An optional third value is
         the angular origin measured in degrees starting from the
         $x$-axis in an anticlockwise direction.  If this is omitted, it
         defaults to 0.  The initial suggested default is {\tt [0,0,0]}.

         This parameter is ignored unless TRTYPE={\tt "Bilinear"} or
         {\tt "Polar"}.
      }
      \sstsubsection{
         TRANSFORM = TRN (Write)
      }{
         The actual or implied HDS object to store the created
         transformation.  It may be an HDS container file, in which
         case the transformation structure is placed within a structure
         called TRANSFORM at the top level of the file; or a path to
         the HDS object.  So for instance, if parameter
         TRANSFORM={\tt warp9}, the transformation will be placed in
         the top-level structure TRANSFORM within the HDS file
         {\tt warp9.sdf}.  In this case the container file may already
         exist.  If, on the other hand, the explicit structure is
         named, the transformation information will be placed there. 
         For example, to place the transformation in the extension
         GALPHOT of the NDF called NGC253, parameter TRANSFORM would be
         {\tt NGC253.MORE.GALPHOT}.  The structure name is limited to 15
         printing characters.  Note that in either case the structure
         must not already exist.  If it does, an error condition
         results.
 
         This has parameter no suggested default.
      }
      \sstsubsection{
         TRTYPE = LITERAL (Read)
      }{
         The type of transform which will be supplied.  Valid values are
         {\tt "Bilinear"}, {\tt "Expres"}, and {\tt "Polar"}.

         {\tt "Bilinear"} requests that the transform will be generated from
         the six coefficients specified by parameter TR in the
         equations:
         \begin{description}
         \item X$^\prime$ = TR(1) $+$ TR(2)$*$X $+$ TR(3)$*$Y
         \item Y$^\prime$ = TR(4) $+$ TR(5)$*$X $+$ TR(6)$*$Y
         \end{description}
         that define a linear two-dimensional transformation.

         {\tt "Expres"} lets an arbitrary transformation be specified
         using algebraic-like statements of the type:
         \begin{description}
         \item  FOR1 $>$ {\tt "XX=PA$+$PC$*$X"}
         \item  FOR2 $>$ {\tt "YY=PD$+$PE$*$Y"}
         \item  INV1 $>$ {\tt "X=(XX-PA)/PC"}
         \item  INV2 $>$ {\tt "Y=(YY-PD)/PE"}
         \end{description}
         where the parameter name is to the left of $>$ and its value is
         to the right of the $>$.  The PA-PZs are reserved for constants
         (FA-FZ are also reserved for repeated expressions).  This
         example allows independent offsets and scales in $x$ and $y$.  The
         inverse mapping must be supplied.

         {\tt "Polar"} makes a 2-dimensional Cartesian-to-polar
         transformation, where the origin in Cartesian co-ordinates, and
         polar angle are specified by parameter TR.

         {\tt ["Expres"]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         tranmake xyz nvin=1 nvout=1 for1="xd=1.01$*$x-0.34"
         inv1="x=(xd$+$0.34)/1.01" $\backslash$
      }{
          This creates a transformation structure TRANSFORM in the HDS  
          file called {\tt xyz.sdf}.  It specifies mappings between one input and
          one output variable.  The output variable is 1.01 the output
          variable less 0.34. 
      }
      \sstexamplesubsection{
         tranmake xyz nvin=1 nvout=1 for1="xd=1.01$*$x-0.34" class=linear
         inv1="x=(xd$+$0.34)/1.01" comment="Copier correction"
      }{
          As above, except that because the transformation is linear
          (it is a magnification and translation), the classification is
          set to {\tt "Linear"}.  {\tt "Copier correction"} is written
          as the comment in the structure.
      }
      \sstexamplesubsection{
         tranmake transform=turn.more.rot45 nvin=2 nvout=2
         class=["linear","isotropic","unit\_det"] pa=45
         for1="xo=cosd(pa)$*$xi-sind(pa)$*$yi"
         for2="yo=sind(pa)$*$xi$+$cosd(pa)$*$yi"
         inv1="xi=cosd(pa)$*$xo$+$sind(pa)$*$yo"
         inv2="yi=-sind(pa)$*$xo$+$cosd(pa)$*$yo"
      }{
          This creates a transformation structure TURN.MORE.ROT45
          (in HDS file {\tt turn.sdf}) that rotates
          a two-dimensional co-ordinate system by 45\dgs\ clockwise.
          Three classes---linear, isotropic, unit determinant---are
          assigned for this transformation.  (As it is
          a rotation, positive and constant determinants are also
          applicable.)
      }
      \sstexamplesubsection{
         tranmake shiftim trtype=lin tr=[8.73,1,0,-64.6,0,1] fittype=1 $\backslash$
      }{
         This creates an HDS file called {\tt raw\_origin.sdf} containing a
         transformation structure called TRANSFORM at the top-level.
         This transformation defines a shift of 8.73 of the first
         variable (usually $x$ in an image) and a negative shift of 64.6
         in the second variable (normally $y$).  The shift is specified
         using the appropriate linear-transformation coefficients
         [XSHIFT,1,0,YSHIFT,0,1] and is correctly classified as a
         fit type of 1.  There is no comment.
      }
      \sstexamplesubsection{
         tranmake my\_ndf.more.my\_extension.tran1 bilinear "15-deg rotation"
         [0,0.965926,-0.258819,0,0.258819,0.965926] fittype=2
      }{
         This creates a transformation structure called TRAN1
         in the extension MY\_EXTENSION of the NDF called my\_ndf.  The
         structure defines a rotation by 15\dgs\ in about the (0,~0)
         position in a plane (say $x$-$y$ of an image).  The rotation
         is specified using the appropriate linear transformation
         coefficients [0,COS,$-$SIN,0,SIN,COS].  The comment stored in the
         structure is {\tt "15-deg rotation"}.
      }
      \sstexamplesubsection{
         tranmake polish.origin1 trtype=p tr=[100.0,21.3] $\backslash$
      }{
         This creates an HDS file called {\tt polish.sdf} containing a
         transformation structure called ORIGIN1 at the top-level.
         This structure defines a Cartesian-to-polar transformation
         about an origin at (100.3,~21.3) in pixel co-ordinates.
         There is no comment stored in ORIGIN1.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         This routine does not check that the forward and inverse
         expressions actually define a pair of complementary mappings.

         \sstitem
         On completion, the destination structure for the
         transformation information equates to the current transformation
         global parameter.

         \sstitem
         Expressions supplied for parameters FOR1-FOR7 and INV1-INV7 
         should be enclosed in double quotes (even when given in response 
         to a prompt) to protect the equals sign from interpretation by 
         the shell or parameter system.

      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FLIP, ROTATE, SLIDE, TRANINVERT, TRANJOIN, TRANSFORMER, \linebreak
      TRANTRACE; CCDPACK: CCDEDIT, TRANLIST, TRANNDF.
   }
}


\sstroutine{
   TRANSFORMER
}{
   Applies a transformation to an NDF
}{
   \sstdescription{
      This application performs an arbitrary transformation on an NDF to
      create an output NDF.  There is full control of the shape, origin,
      and co-ordinate limits of the output NDF.  The output NDF is
      calculated by resampling within the input NDF.  Output array
      elements are set to the bad value if their inverse-transformed
      co-ordinates lie outside the input NDF's co-ordinate limits.
   }
   \sstusage{
      transformer in transform out [method] [shape]
        \newline\hspace*{1.5em}
        $\left\{ {\begin{tabular}{l}
                  lcoord=? ucoord=? \\
                  lbound=? ubound=? \\
                  \end{tabular} }
        \right.$
        \newline\hspace*{1.9em}
        \makebox[0mm][c]{\small shape}
   }
   \sstparameters{
      \sstsubsection{
         CONSERVE = \_LOGICAL (Read)
      }{
         If CONSERVE is {\tt TRUE}, the output values are normalised by the
         ratio of the output-to-input pixel areas.  In other words this
         conserves flux.  If CONSERVE is {\tt FALSE}, there is no
         normalisation.  {\tt [FALSE]}
      }
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either {\tt "World"}
         or {\tt "Data"}.  {\tt "World"} inputs pixel co-ordinates to the supplied
         transformation to derive the co-ordinates in the output NDF.
         {\tt "Data"} causes the NDF's axis information to be the input
         co-ordinates to the transformation.  See the SHAPE parameter.
         {\tt [}Current co-ordinate system{\tt ]}
      }
      \sstsubsection{
         FULL = \_LOGICAL (Read)
      }{
         When the number of input variables is less than the number
         of dimensions of input NDF (but not less than the number of
         output variables), FULL set to {\tt TRUE} applies the transformation
         to all the higher dimensions.  For example, FULL = {\tt TRUE}
         would apply a 2-dimensional transformation to all the planes
         along the third dimension of a cube NDF.  FULL = {\tt FALSE} would
         only transformed the first plane.  {\tt [FALSE]}
      }
      \sstsubsection{
         IN = NDF (Read)
      }{
         The NDF to be transformed.
      }
      \sstsubsection{
         LBOUND() = \_INTEGER (Read)
      }{
         The lower pixel-index bounds of the output NDF.  The number of
         values should equal the number of output variables in the
         transformation.  This parameter is only used when SHAPE is
         {\tt "Full"} or {\tt "Bounds"}.  The suggested defaults are the lower
         bounds of the input NDF, and where there are more output than
         input dimensions they are set to 1.
      }
      \sstsubsection{
         LCOORD() = \_DOUBLE (Read)
      }{
         The lower co-ordinate limits of the output NDF.  The number of
         values should equal the number of output variables in the
         transformation.  This parameter is only used when SHAPE is
         {\tt "Full"} or {\tt "Limits"}.  The suggested defaults are the lower
         co-ordinate limits determined from applying the transformation
         to a series of test points.  Where there are more output than
         input dimensions they are set to 0.0.
      }
      \sstsubsection{
         METHOD = LITERAL (Read)
      }{
         The interpolation method used to resample the input array.
         Permitted values are {\tt "Nearest"} for nearest-neighbour, and
         {\tt "Linint"} for linear interpolation.  {\tt ["Nearest"]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The transformed NDF.
      }
      \sstsubsection{
         SHAPE = LITERAL (Read)
      }{
         The method by which to define the bounds and co-ordinate limits
         of the output NDF.  See Section {\tt "}Co-ordinate Limits and
         Bounds{\tt "}.  The options for SHAPE are described below.
         \begin{description}
         \item {\tt "Bounds"} --- Specify the output bounds with LBOUND and UBOUND.
                       Use the default co-ordinate limits derived from
                       the transformation of test points in the input
                       NDF.
         \item {\tt "Full"} --- Specify the output co-ordinate limits and bounds
                       with LCOORD, UCOORD, LBOUND and UBOUND.
         \item {\tt "Limits"} --- Use the bounds of the input NDF and specify the
                       output co-ordinate limits with LCOORD and UCOORD.
         \item {\tt "Match"} --- Use the co-ordinate limits from transformed test
                       points of the input NDF, and make a co-ordinate
                       unit equivalent to one pixel.  The bounds are the
                       integer-rounded co-ordinate limits.  This option
                       results in an output NDF that is not clipped and
                       unlike the other options guarantees no further
                       linear compression or expansion.
         \item {\tt "Same"} --- Use the bounds of the input NDF and take the
                       co-ordinate limits from transformed test points
                       of the input NDF.
         \end{description}
         The first three also cause the co-ordinate limits to be
         reported before obtaining the limits and/or bounds.

         Not all of these are permitted simultaneously.  {\tt "Same"} is not
         allowed when the number of input and output transformation
         variables are not equal.  Otherwise it is the value of
         COSYS that controls the options.  When COSYS = {\tt "Data"} all but
         {\tt "Match"} are allowed, and COSYS = {\tt "World"} excludes {\tt "Limits"} and
         {\tt "Full"}.  There is a special case where SHAPE is fixed to be
         {\tt "Bounds"}.  This is when the number of output variables exceeds
         the number of input variables, and that in turn equals the
         number of dimensions in the input NDF.

         SHAPE defaults to the dynamic default.  When COSYS = {\tt "Data"}
         this is {\tt "Bounds"}, and when COSYS = {\tt "World"} the default is
         {\tt "Match"}.  The suggested default is current value, or the
         dynamic default if there is not one.  {\tt []}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value ({\tt !})
         propagates the title from the input NDF to the output NDF.
         {\tt [!]}
      }
      \sstsubsection{
         TRANSFORM = TRN (Read)
      }{
         The transformation to be applied.  This may be an HDS
         container file, in which case the transformation structure is
         assumed to be called TRANSFORM at the top level of the file;
         or a path to the HDS object.  For example, a value of
         {\tt distort.mapping} would use the transformation structure called
         MAPPING in the HDS file {\tt distort.sdf}; and a value of
         {\tt aitoff} would make the routine look for the transformation
         in the top-level object TRANSFORM within the HDS file {\tt aifoff.sdf}.
         Normally the object name is TRANSFORM.  The structure must
         contain both the forward and inverse mappings.

         Structures can be made using CCDEDIT in {\footnotesize CCDPACK} or TRANMAKE.
      }
      \sstsubsection{
         UBOUND() = \_INTEGER (Read)
      }{
         The upper pixel-index bounds of the output NDF.  The number of
         values should equal the number of output variables in the
         transformation.  This parameter is only used when SHAPE is
         {\tt "Full"} or {\tt "Bounds"}.  Each suggested-default value is the
         maximum of the input upper bound and the output lower bound.
      }
      \sstsubsection{
         UCOORD() = \_DOUBLE (Read)
      }{
         The upper co-ordinate limits of the output NDF.  The number of
         values should equal the number of output variables in the
         transformation.  This parameter is only used when SHAPE is
         {\tt "Full"} or {\tt "Limits"}.  Each suggested-default value is 
         the upper co-ordinate limit determined from applying the
         transformation to a series of test points.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         transformer curved sdist straight
      }{
         This transforms the NDF called curved into an NDF called
         {\tt straight.sdf}, using the transformation TRANSFORM in the
         HDS file called {\tt sdist.sdf}.  It uses nearest-neighbour
         resampling.  Assuming the current co-ordinate system is world,
         the transformation is performed in pixel co-ordinates, setting
         the bounds to just enclose the transformed input array.
      }
      \sstexamplesubsection{
         transformer curved sdist.transform straight linint same
      }{
         As above, except linear interpolation is used, and the array
         of NDF straight array uses the bounds of NDF curved.
      }
      \sstexamplesubsection{
         transformer a119 proj.merc a119s shape=bounds lbound=[1,-20]
         ubound=[256,172]
      }{
         This transforms the NDF called a119, using the transformation
         MERC in the HDS file called {\tt proj.sdf}, into an NDF called a119s.
         It uses nearest-neighbour resampling.  a119s just encloses
         the transformed arrays from a119, and has 256 $\times$ 192 pixels
         from origin (1,$-$20).
      }
      \sstexamplesubsection{
         transformer spec2d scrunch.trn full method=l out=spec2d\_l
         shape=limits lcoord=5000 ucoord=6500
      }{
         This transforms the 2-dimensional NDF called spec2d, using
         the 1-dimensional transformation TRN in the HDS file called
         {\tt scrunch.sdf}, into an NDF called spec2d\_l.  (NDF spec2d might be a
         set of spectra before scrunching.)  The linear-interpolation
         resampling is applied to all the lines in spec2d\_l.  The NDFs
         have the same pixel-index bounds.  spec2d\_l is constrained
         to contain elements whose transformed co-ordinates lie between
         5000 to 6500.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         In general the test points to calculate the co-ordinate limits
         for LCOORD and UCOORD are situated at the corners of each pixel,
         assuming spaced axes.  Thus for a 2-dimensional array of 9-by-7
         pixels there are 80 (10$\times$8) test points.  For linear
         transformations there is a smaller set of test points for
         improved efficiency.  These are the corners of each axis and the
         midpoints between them.

         \sstitem
         On completion, the current transformation global parameter
         takes the value of parameter TRANSFORM.
      }
   }
   \sstdiytopic{
      Co-ordinate Limits and Bounds
   }{
      The limits are the lower co-ordinates of the first element, and
      the upper co-ordinates of the last element of the NDF.  Using
      these limits, TRANSFORMER derives the co-ordinates of the output
      NDF's pixel centres by linear interpolation.  Therefore, the
      co-ordinate limits define the region of the input NDF that will
      appear in the output.

      The bounds of the output NDF define its shape and origin.  So
      an additional linear scaling transformation can be applied along
      each axis by adjusting the shape independently of the co-ordinate
      limits.
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FLIP, ROTATE, SLIDE, TRANINVERT, TRANJOIN, TRANMAKE,\linebreak
      TRANTRACE; CCDPACK: CCDEDIT, TRANLIST, TRANNDF.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         Flux conservation can only be applied to constant-determinant
         or linear transformations.

         \sstitem
         The NDF components are processed by this application as
         follows.
         \ssthitemlist{
            \item LABEL, UNITS, HISTORY, and extensions are propagated.
            \item TITLE is controlled by the TITLE parameter.
            \item QUALITY is not derived from the input NDF for a linearly
            interpolated NDF.  The DATA and VARIANCE arrays are resampled.
            \item Axis centre arrays are created using the co-ordinate
            limits for COSYS = {\tt "Data"}.
         }

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types are handled.

         \sstitem
         There can be an arbitrary number of NDF dimensions.
      }
   }
}
\sstroutine{
   TRANTRACE
}{
   Lists the contents of a transformation structure
}{
   \sstdescription{
      This application reports or write to a text file the contents of
      a TRANSFORM structure.  Items listed include:

      \ssthitemlist{

         \sstitem
            the structure's name;

         \sstitem
            the version of the TRANSFORM software used to write the
              structure;

         \sstitem
            the number of input and output variables for the nett
            transformation and for each transformation where the structure
            contains more than one;

         \sstitem
            the classification of the forward and inverse mappings; and

         \sstitem
            for each transformation its precision, comment, forward and
            inverse functions.
      }
   }
   \sstusage{
      trantrace transform [logfile]
   }
   \sstparameters{
      \sstsubsection{
         LOGFILE = FILENAME (Write)
      }{
         The name of the text file to store a list of the
         transformation structure.  If it is null ({\tt !}) the list of the
         transformation structure is reported directly to you.
         {\tt [!]}
      }
      \sstsubsection{
         TRANSFORM = TRN (Read)
      }{
         The transformation structure to be listed.  This may be an HDS
         container file, in which case the transformation structure is
         assumed to be called TRANSFORM at the top level of the file;
         or a path to the HDS object.  The suggested default is the
         current transformation structure.
      }
   }
   \sstresparameters{
      \sstsubsection{
         CLASSFOR = LITERAL (Write)
      }{
         A comma-separated list of classifications that describe the
         properties of the forward mapping of the transformation.  The
         possible values in the list are:
         \begin{description}
         \item {\tt "Linear"}       --- linear and preserves straight lines,
         \item {\tt "Independent"}  --- preserves the independence of the axes,
         \item {\tt "Diagonal"}     --- preserves the axes themselves,
         \item {\tt "Isotropic"}    --- preserves angles and shapes,
         \item {\tt "Positive\_det"} --- a component of reflection is absent,
         \item {\tt "Negative\_det"} --- a component of reflection is present,
         \item {\tt "Constant\_det"} --- the scale factor is constant,
         \item {\tt "Unit\_det"}     --- areas (or volumes {\it etc.}) are preserved.
         \end{description}

         See SUN/61 Appendix~B for more details of transformation
         classification and a table of the classifications of common
         mappings.
      }
      \sstsubsection{
         CLASSINV = LITERAL (Write)
      }{
         A comma-separated list of classifications that describe the
         properties of the inverse mapping of the transformation.  See
         parameter CLASSFOR for further details.
      }
      \sstsubsection{
         COMMENT = LITERAL (Write)
      }{
         The comment string associated with the transformation.  A
         {\tt "--$>$"} symbol, if present, indicates the forward
         transformation.
      }
      \sstsubsection{
         FORWARD = LITERAL (Write)
      }{
         The expression that defines the last forward mapping of the
         transformation.
      }
      \sstsubsection{
         INVERSE = LITERAL (Write)
      }{
         The expression that defines the last inverse mapping of the
         transformation.
      }
      \sstsubsection{
         PREC = LITERAL (Write)
      }{
         The arithmetic precision of the transformation.  This may be
         either {\tt "\_REAL"} for single precision, {\tt "\_DOUBLE"} for double
         precision, or {\tt "\_INTEGER"} for integer precision.
      }
      \sstsubsection{
         VERSION = LITERAL (Write)
      }{
         The version number of the TRANSFORM software used to write the
         transformation structure.
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         trantrace rot45.transform
      }{
         This reports the contents of the transformation structure within
         the HDS container file {\tt rot45.sdf}, component TRANSFORM.
      }
      \sstexamplesubsection{
         trantrace rot45
      }{
         This has the same affect as the previous example.
      }
      \sstexamplesubsection{
         trantrace $\backslash$
      }{
         This reports the contents of the current TRANSFORM structure.
      }
      \sstexamplesubsection{
         trantrace jkt256.more.ccdpack.transform trn.lis
      }{
         This lists to the text file {\tt trn.lis} the contents of the
         transformation structure located within the HDS file
         {\tt jkt256.sdf}, as component MORE.CCDPACK.TRANSFORM.
      }
      \sstexamplesubsection{
         trantrace stretch.limit nvin=(nvi) comment=(trncom)
      }{
         This reports the contents of the transformation structure within
         the HDS container file {\tt stretch.sdf}, component LIMIT.  The
         number of input transformation variables is written to the
         {\footnotesize ICL} variable called NVI, and the
         transformation comment is stored in {\footnotesize ICL}
         variable TRNCOM.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         Where a value is not optional but is absent, {\tt "$<$undefined$>$"} appears
         in the listing.

         \sstitem
         TRANTRACE attempts to compile the forward and inverse mappings
         to check that it is a TRANSFORM structure, and will exit with an
         error if both of these compilations fail.

         \sstitem
         On completion, the current transformation global parameter
         takes the value of parameter TRANSFORM.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: TRANSFORMER, TRANINVERT, TRANJOIN, TRANMAKE;
      CCDPACK: CCDEDIT, TRANLIST, TRANNDF.
   }
}

\manroutine {{\manheadstyle{TRIG}}}{ Performs a trigonometric
  transformation on a data array.}
\begin{manroutinedescription}
\manroutineitem {Description }{}
  This routine allows the user to select one of a set of several
  basic trigonometrical functions (sine, cosine, tangent, arcsine,
  {\it etc.}) and operate on each pixel of the data array, in the input
  {\mantt{IMAGE}} structure, with this function, and then to output a
  transformed version of the array. The trigonometric functions can
  be selected to act as if the input data are to be treated as
  radians or degrees. If a scalar value rather than a data array is
  input, the application acts purely on that scalar value.

  The magic-value method is used for processing bad data.  Undefined
  results are set to the magic value.

\manroutineitem {Invocation }{}
  TRIG

\newpage
\manroutineitem {Parameters }{}
\begin{manparametertable}
\manparameterentry {{\mantt{READ}} }{{\mantt{INPIC}} }{{\mantt{IMAGE}}}
  Input {\mantt{IMAGE}} structure containing the data array to be
  transformed.
\manparameterentry {{\mantt{READ}} }{{\mantt{TRIGFUNC}} }{{\mantt{\_CHAR}}}
  Trigonometrical function to be applied.  The options are {\mantt {SIN}},
  {\mantt {COS}}, {\mantt {TAN}}, {\mantt {SIND}}, {\mantt {COSD}},
  {\mantt {TAND}}, {\mantt {ASIN}}, {\mantt {ACOS}}, {\mantt {ATAN}},
  {\mantt {ASIND}}, {\mantt {ACOSD}}, {\mantt {ATAND}}.
\manparameterentry {{\mantt{WRITE}} }{{\mantt{OUTPIC}} }{{\mantt{IMAGE}}}
  Output {\mantt{IMAGE}} structure containing the transformed data array.
\manparameterentry {{\mantt{READ}} }{{\mantt{OTITLE}} }{{\mantt{\_CHAR}}}
  Title string for the output {\mantt{IMAGE}} structure.
 \mbox{{\mantt ['KAPPA - Trig']}}
\end{manparametertable}
\manroutineitem {Bugs }{}
  None known.

\manroutineitem {Authors }{}
  Mark McCaughrean UoE ( {\mantt REVA}::{\mantt MJM} )
  Malcolm J. Currie ~STARLINK \mbox{( {\mantt RAL}::{\mantt CUR} )}
\end{manroutinedescription}

 
\sstroutine{
   TURBOCONT
}{
   Contours a 2-d NDF quickly
}{
   \sstdescription{
      This application draws a contour plot of a 2-dimensional NDF on the
      current graphics device via an efficient algorithm.  The image may be
      part or whole of the data array, but also the variance or quality
      can be shown.  The plot is situated within the current
      graphics-database picture.

      The contour plot resides within optional, annotated and enumerated
      axes.  An optional, but recommended, key may be drawn to the
      right of the contour plot.  It reports the NDF's units if there
      are any, and only contour heights actually plotted are included.
      There are seven methods for selecting contours.
   }
   \sstusage{
      turbocont ndf [comp] mode ncont [key] [device]
        $\left\{ {\begin{tabular}{l}
                    firstcnt=? stepcnt=? \\
                    heights=? \\
                    percentiles=?
                   \end{tabular} }
        \right.$
        \newline\hspace*{23.3em}
        \makebox[0mm][c]{\small mode}
   }
   \sstparameters{
      \sstsubsection{
         ANNOTA = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the contour lines will be annotated with a contour
         number corresponding to the key entry.  It is ignored and there
         are no annotations when KEY = {\tt FALSE}.  {\tt [FALSE]}
      }
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the graphics device is to be cleared before display
         of the array. {\tt [TRUE]}
      }
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The NDF component to be contoured.  It may be {\tt "Data"},
         {\tt "Quality"}, {\tt "Variance"}, or {\tt "Error"} (where
         {\tt "Error"} is the alternative to {\tt "Variance"} and causes
         the square root of the variance values to be taken before
         plotting contours).  If {\tt "Quality"} is specified, then
         the quality values are treated as numerical values (in the
         range 0 to 255).  {\tt ["Data"]}
      }
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either {\tt "World"}
         or {\tt "Data"}.  {\tt "World"} makes pixel co-ordinates to appear on axes.
         If COSYS = {\tt "Data"} the NDF's axis information is used to
         annotate axes.  {\tt [}Current co-ordinate system{\tt ]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The plotting device. {\tt [}Current image-display device{\tt ]}
      }
      \sstsubsection{
         FILL = \_LOGICAL (Read)
      }{
         The contour plot normally has square pixels, in other words
         a length along each axis corresponds to the same number of
         pixels.  However, for images with markedly different
         dimensions this default behaviour may not be suitable or give
         the clearest plot.  When FILL is {\tt TRUE}, the square-pixel
         constraint is relaxed and the contour plot is the largest
         possible within the current picture.  When FILL is {\tt FALSE}, the
         pixels are square.  The suggested default is the current
         value.  {\tt [FALSE]}
      }
      \sstsubsection{
         FIRSTCNT = \_REAL (Read)
      }{
         Height of the first contour (Linear and Magnitude modes).
         The suggested value is the current value.
      }
      \sstsubsection{
         HEIGHTS() = \_REAL (Read)
      }{
         Contour levels (Free mode).  The suggested default is the
         current value.
      }
      \sstsubsection{
         KEY = \_LOGICAL (Read)
      }{
         When KEY is {\tt TRUE}, a key of the contour level versus
         pixel value is to be produced. {\tt [TRUE]}
      }
      \sstsubsection{
         MAXRES = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the contours are interpolated to the resolution of the
         plotting device, {\it i.e.}\ provides sub-pixel resolution, otherwise
         straight-line segments at pixel resolution are drawn.  The
         latter does not give smooth contours, but this makes the
         processing much faster.  The former draws smoother contours to
         the resolution of the graphics workstation, but they still
         have vertices.  If you require smooth well-rounded contours try
         the slower CONTOUR. {\tt [FALSE]}
      }
      \sstsubsection{
         MODE = LITERAL (Read)
      }{
         The method used to select the contour levels.  The options are
         described below.
         \begin{description}
         \item {\tt "Area"} --- The contours enclose areas of the array for
                         which the equivalent radius increases by equal
                         increments.  You specify the number of levels.
         \item {\tt "Automatic"} --- The contour levels are equally spaced between the maximum
                        the maximum and minimum pixel values in the
                        array.  You supply the number of contour levels.
         \item {\tt "Equalised"} --- You define the number of equally spaced
                          percentiles.
         \item {\tt "Free"} --- You specify a series of contour values
                        explicitly.
         \item {\tt "Linear"} --- You define the number of contours, the start
                        contour level and linear step between contours.
         \item {\tt "Magnitude"} --- You define the number of contours, the start
                        contour level and step between contours.  The
                        step size is in magnitudes so the $n^{\rm th}$
                        contour is 10$^{-0.4*(n-1)*{\rm step}}$ times the
                        start contour level.
         \item {\tt "Percentiles"} --- You specify a series of percentiles.
         \end{description}

         The suggested default is the current value, which is initially
         {\tt "Free"}.
      }
      \sstsubsection{
         NCONT = \_INTEGER (Read)
      }{
         The number of contours required (all modes except Free and
         Percentiles).  It must be between 1 and 50.  If the number is
         large, the plot may be cluttered and take longer to produce.
         {\tt 6}, the initial suggested default, gives reasonable results.
         The current value becomes the suggested default.
      }
      \sstsubsection{
         NDF = NDF (Read)
      }{
         NDF structure containing the 2-dimensional image to be contoured.
      }
      \sstsubsection{
         PERCENTILES() = \_REAL (Read)
      }{
         Contour levels given as percentiles.  The values must lie
         between 0.0 and 100.0. (Percentiles mode).  The suggested
         default is the current value.
      }
      \sstsubsection{
         PXSIZE = \_REAL (Read)
      }{
         The length ($x$ axis) of the plot in metres.  There is an upper
         limit given by the $x$ size of the current picture.
         {\tt [}Maximum that can fit in the current picture whilst
         preserving square pixels{\tt ]}
      }
      \sstsubsection{
         PYSIZE = \_REAL (Read)
      }{
         The length ($y$ axis) of the plot in metres.  There is an upper
         limit given by the $y$ size of the current picture.
         {\tt [}Maximum that can fit in the current picture whilst
         preserving square pixels{\tt ]}
      }
      \sstsubsection{
         RESOLUTION = \_REAL (Read)
      }{
         The resolution factor. The actual plotting resolution is this
         times the $x$ and $y$ theoretical resolutions in world
         co-ordinates.  In GKS, whether or not a given `lamp' is
         illuminated or pen position is marked with ink cannot be
         determined, so a factor of unity is too small for the most
         efficient processing. It must lie between 2.0 and 10.0. {\tt [2.0]}
      }
      \sstsubsection{
         STEPCNT = \_REAL (Read)
      }{
         Separation between contour levels, linear for Linear mode
         and in magnitudes for Magnitude mode.  The suggested value is
         the current value.
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         ABSLAB  =  LITERAL (Read)
      }{
         Label for the plot abscissa, in which NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  If axis information is
         present the suggested default is the NDF's axis label
         followed by the units, in parentheses.  If an error occurs
         obtaining the label the suggested default is {\tt "X"}. {\tt []}
      }
      \sstsubsection{
         AXES = \_LOGICAL (Read)
      }{
         {\tt TRUE} if labelled and annotated axes are to be drawn around the
         contour plot.  The annotations are either the data
         co-ordinates from the NDF axis components, provided these are
         present and linear and COSYS = {\tt "Data"}; otherwise pixel
         co-ordinates are used.  {\tt [TRUE]}
      }
      \sstsubsection{
         BORDER = \_LOGICAL (Read)
      }{
         BORDER is {\tt TRUE} if a box is to be drawn about the contour
         plot.  This is only accessed when there are no axes required.
         {\tt [TRUE]}
      }
      \sstsubsection{
         CONCOL = LITERAL (Read)
      }{
        The colour of the contour lines on devices that support colour.
        The options are described below.

         \begin{description}
         \item {\tt "MAX"}  --- The maximum colour index in the image
                          display colour lookup table.
         \item {\tt "MIN"}  --- The minimum (non-reserved) colour index in
                          the image-display colour lookup table.
         \item {\bf An integer} --- The actual colour index.  It is
                          constrained between 0 and the maximum colour
                          index available on the device.
         \item {\bf A named colour} --- Uses the named colour from the
                          palette, and if it is not present, the nearest
                          colour from the palette is selected.
         \end{description}

         If the colour is to remain unaltered as the lookup table is
         manipulated choose an integer between 0 and 15, or a named
         colour.  This parameter will be ignored if PENROT = {\tt TRUE}.
         {\tt [}The current value, but equals {\tt 1} (the foreground
         colour) if there is no current value.{\tt ]}
      }
      \sstsubsection{
         DASHED = \_REAL (Read)
      }{
         The height below which the contours will be drawn with dashed
         lines.  A null value ({\tt !}) means all contours are drawn with
         solid lines.  This facility is only available when ANNOTA =
         {\tt FALSE}. {\tt [!]}
      }
      \sstsubsection{
         FONT = LITERAL (Read)
      }{
         The fount to be used for the line graphics.  It can be either
         {\tt "NCAR"} for the NCAR fancy characters and {\tt "GKS"} for the standard
         GKS san-serif fount.   The former is intended for hardcopy
         publication-quality plots, since it is relatively slow; the
         latter is intended for normal interactive graphics requiring
         rapid plotting, and it is clearer on small plots. The
         suggested default is the current value. {\tt ["GKS"]}
      }
      \sstsubsection{
         LABELFREQ = \_INTEGER (Read)
      }{
         The frequency with which contour levels are annotated.  {\tt 1}
         means every level will be labelled.  This may be excessive in
         plots where the contours are closely packed.  This parameter
         is ignored
         unless contour annotation has been selected.  It must be between
         one and the number of contour heights. {\tt [1]}
      }
      \sstsubsection{
         MAJTIC( 2 ) = \_REAL (Read)
      }{
         The parameter controlling the numbers of major tick marks
         for the $x$ and $y$ axes.  (Number used is between MAJTIC$+$2 and
         5$*$MAJTIC/2$+$4.) {\tt [3.,3.]}
      }
      \sstsubsection{
         MINTIC( 2 ) = \_REAL (Read)
      }{
         The number of minor tick marks between each major tick mark
         for the $x$ and $y$ axes.  A negative value forces the graphics
         package to compute appropriate values. {\tt [-1.,-1.]}
      }
      \sstsubsection{
         NOISY = \_LOGICAL (Read)
      }{
         If {\tt TRUE} the contour lines will alternately be annotated with
         a contour number corresponding to the key entry, but at
         twice the frequency.  It is ignored unless annotated contours
         have been selected. {\tt [FALSE]}
      }
      \sstsubsection{
         ORDLAB  =  LITERAL (Read)
      }{
         Label for the plot ordinate, in which NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  If axis information is
         present the suggested default is the NDF's axis label followed
         by the units, in parentheses.  If an error occurs obtaining
         the label the suggested default is {\tt "Y"}. {\tt []}
      }
      \sstsubsection{
         OUTTIC = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the axis tick marks are to appear on the outside of
         the axes instead of inside.  By default, the tick marks are
         drawn outside the contouring region to eliminate
         intersections of ticks with the contours. {\tt [TRUE]}
      }
      \sstsubsection{
         PENROT = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, the plotting pens are cycled through the contours to
         aid identification of the contour heights.  It is ignored
         when annotation is selected. {\tt [FALSE]}
      }
      \sstsubsection{
         PLTITL = LITERAL (Read)
      }{
         The title of the plot.  Up to about 40 characters can be
         accommodated, and NCAR fancy founts may be embedded when
         FONT = {\tt "NCAR"}.  If an error occurs obtaining the title, it
         is defaulted to {\tt "Contour plot"}. {\tt [}The NDF title{\tt ]}
      }
      \sstsubsection{
         THICK = \_REAL (Read)
      }{
         The thickness of the lines and NCAR-fount characters in the plot, where
         {\tt 1.0} is the normal thickness.  Currently, this is only available
         on a few devices.  It must take a value in the range 0.5--10.0.
         {\tt [1.0]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         turbocont myfile D $\backslash$
      }{
         Contours the data array in the NDF called myfile on the current
         graphics device.  All other settings are defaulted, so for
         example the current method for determining heights is used, and
         a key is plotted.
      }
      \sstexamplesubsection{
         turbocont taurus1(100:199,150:269,4) $\backslash$
      }{
         Contours a 2-dimensional portion of current array component in
         the NDF cube called taurus1 on the current graphics device.
         The portion extends from pixel (100,~150,~4) to pixel
         (199,~269,~4).  All other settings are defaulted, so for example
         coarse contours are drawn, using the current mode for
         determining heights, and a key is plotted.
      }
      \sstexamplesubsection{
         turbocont ndf=ngc6872 mode=au ncont=5 device=ps\_l concol=white
      }{
         Contours the data array in the NDF called ngc6872 on the
         ps\_l graphics device.  Five equally spaced contours between
         the maximum and minimum data values are drawn in white.  The
         NDF's title adorns the plot.  A key is plotted.
      }
      \sstexamplesubsection{
         turbocont ngc6872 mode=au ncont=5 annota labelfreq=2 cosys=w
         device=ps\_l concol=white
      }{
         As above.  In addition the contours are annotated at
         alternate heights.  The axes are annotated with pixel
         co-ordinates.
      }
      \sstexamplesubsection{
         turbocont ngc6872 mode=li firstcnt=10 stepcnt=2 ncont=4 noaxes
      }{
         Contours the data array in the NDF called ngc6872 on the
         current graphics device.  Four contours at heights 10, 12, 14,
         and 16 are drawn.  A key is plotted, but no axes surround the
         contour plot.
      }
      \sstexamplesubsection{
         turbocont ss443 mode=pe percentiles=[80,90,95,98,99,99.9] annota
      }{
         Contours the data array in the NDF called ss443 on the
         current graphics device.  Annotated contours at heights
         corresponding to the 80, 90, 95, 98, 99, and 99.9 percentiles
         are drawn.  A key is plotted.
      }
      \sstexamplesubsection{
         turbocont mode=eq ncont=5 dashed=0 pencol=red ndf=skyflux
      }{
         Contours the data array in the NDF called skyflux on the
         current graphics device.  Contours at heights corresponding to
         the 10, 30, 50, 70 and 90 percentiles are drawn in red.  Those
         contours whose values are negative will appear as dashed
         lines.  A key is plotted.
      }
      \sstexamplesubsection{
         turbocont comp=d nokey penrot $\backslash$
      }{
         Contours the portion of the data array in the current NDF on
         the current graphics device using the current method for height
         selection.  The NDF's title adorns the plot.  No key is drawn.
         The appearance of the contours cycles every third contour.
      }
      \sstexamplesubsection{
         turbocont comp=v mode=fr heights=[10,20,40,80] title=Variance
      }{
         Contours the variance array in the current NDF on the
         current graphics device.  Contours at 10, 20, 40 and 80 are
         drawn.  {\tt "Variance"} is the title of the plot.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The application stores a number of pictures in the graphics
         database in the following order: a FRAME of the specified size
         containing the title, annotated axes, contours and key; a DATA
         picture which is stored with world co-ordinates in units of data
         pixels; and a KEY picture to store the key if present.  The DATA
         picture also may have double-precision data co-ordinates derived
         from the NDF axis components provided these are linear and
         different from pixel co-ordinates; the data co-ordinates are
         stored via a linear transformation.  The NDF associated with the
         plot is stored by reference with the DATA picture.  On exit the
         current database picture for the chosen device reverts to the
         input picture.

         \sstitem
         There are some options for setting the characteristics of the
         contour lines.  By default, solid lines are drawn with the same
         colour as the axes and key, namely the foreground colour.  The
         colour will depend on the graphics device chosen, but it is often
         black for printers or white for terminals.  The alternatives to
         override this default behaviour are listed below.

         \begin{enumerate}
         \item Set a colour for all contours using parameter CONCOL.
         \item Request dashed contours below some threshold given by
               parameter DASHED and solid lines for other heights.  All
               contours have either the foreground colour or that
               prescribed by parameter CONCOL.
         \item Cycle the pens modulo 3 for each contour height actually
               plotted by setting PENROT = {\tt TRUE}.  The characteristics of
               the second and third line styles will depend on the chosen
               graphics device.  An image display or pen plotter will draw
               coloured lines using palette entries 1 to 3; whereas a
               window overlay, or monochrome laser printer or terminal
               will draw a variety of dashed or thicker lines.
         \item Combine options 2 and 3.  However, palette colours 1 to 3
               will always be used and CONCOL ignored.  The contours below
               the threshold continue the cycle through the three colours.
               There may be some confusion on devices that already use
               dashed lines, so this is only suitable for devices
               supporting at least three colours simultaneously.
         \item Annotate the contours using the number of the contour height
               corresponding to the key entries rather than the values
               themselves.  (Set parameter ANNOTA = {\tt TRUE}.)  The frequency
               of labelling may be defined (LABELFREQ).  The key option
               must be chosen (KEY = {\tt TRUE}) in conjunction with annotated
               contours.  Annotation is not recommended should the data
               array have a large number of bad pixels, or if the contours
               are closely packed.  There is an additional parameter
               (NOISY) to select double annotations for short or noisy
               contours in option 2.
         \end{enumerate}

         Annotation takes precedence over pen rotation, which in turn
         overrides colour control through CONCOL. 
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CONTOUR, CONTOVER; Figaro: ICONT; SPECDRE: SPECCONT.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         Only real data can be processed directly.  Other non-complex
         numeric data types will undergo a type conversion before the
         contour plot is drawn.

         \sstitem
         Bad pixels and automatic quality masking are supported.
      }
   }
}
\sstroutine{
   VECPLOT
}{
   Plots a 2-dimensional vector map
}{
   \sstdescription{
      This application plots vectors defined by the values contained
      within a pair of 2-dimensional NDFs, the first holding the
      magnitude of the vector quantity at each pixel, and the second
      holding the corresponding vector orientations.  The number of
      vectors in the plot is kept to a manageable value by only plotting
      vectors for pixels on a sparse regular matrix.  The increment (in
      pixels) between plotted vectors is given by parameter STEP.  Zero
      orientation may be fixed at any position angle within the plot by
      specifying an appropriate value for parameter ANGROT.  Each
      vector may be represented either by an arrow or by a simple line,
      as selected by parameter VTYPE.

      The plot is situated within the current graphics-database
      picture, and may reside within optional, annotated and enumerated
      axes.  An optional, but recommended, key may be drawn to the
      right of the plot.  It reports the data units if there are any
      (taken from the NDF associated with parameter NDF1) and gives the
      scale used for drawing the vectors in data units per centimetre.
      It also displays a typical vector and the corresponding data
      value.  The justification of the vector is indicated by a small
      circle placed at the position of the corresponding pixel centre.
   }
   \sstusage{
      vecplot ndf1 ndf2 [comp] [step] [vscale] [vtype] [just] [device]
   }
   \sstparameters{
      \sstsubsection{
         ANGROT = \_REAL (Read)
      }{
         A rotation angle in degrees to be added to each vector
         orientation before plotting the vectors (see parameter NDF2).
         It should be in the range 0--360. {\tt [0.0]}
      }
      \sstsubsection{
         CLEAR = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the graphics device is to be cleared before display
         of the array. {\tt [TRUE]}
      }
      \sstsubsection{
         COMP = LITERAL (Read)
      }{
         The component of NDF1 which is to be used to define the vector
         magnitudes.  It may be {\tt "Data"}, {\tt "Error"} or {\tt "Variance"}.  The
         last two are not available if NDF1 does not contain a VARIANCE
         component.  The vector orientations are always defined by the
         {\tt "Data"} component of NDF2. {\tt ["Data"]}
      }
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either {\tt "World"}
         or {\tt "Data"}.  {\tt "World"} makes pixel co-ordinates to appear on axes.
         If COSYS = {\tt "Data"} the axis information from NDF1 is used to
         annotate axes (if it exists).
         {\tt [}Current co-ordinate system{\tt ]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The plotting device. {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         KEY = \_LOGICAL (Read)
      }{
         {\tt TRUE} if a key is to be produced. {\tt [TRUE]}
      }
      \sstsubsection{
         KEYVEC = \_REAL (Read)
      }{
         Length of the vector to be displayed in the key, in data units.
         A default value is generated based on the spread of vector
         lengths in the plot. {\tt []}
      }
      \sstsubsection{
         NDF1 = NDF (Read)
      }{
         NDF structure containing the 2-dimensional image giving the
         vector magnitudes.
      }
      \sstsubsection{
         NDF2 = NDF (Read)
      }{
         NDF structure containing the 2-dimensional image giving the
         vector orientations.  The values are considered to be in units
         of degrees unless the UNITS component of the NDF has the
         value {\tt "Radians"} (case insensitive).  The positive $y$ axis
         defines zero orientation, and rotation from the $x$ axis to the
         $y$ axis is considered positive.  The suggested default is the
         current value.
      }
      \sstsubsection{
         PXSIZE = \_REAL (Read)
      }{
         The length ($x$ axis) of the plot in metres. {\tt [}Maximum that can
         fit in the current picture whilst preserving square pixels{\tt ]}
      }
      \sstsubsection{
         PYSIZE = \_REAL (Read)
      }{
         The length ($y$ axis) of the plot in metres. {\tt [}Maximum that can
         fit in the current picture whilst preserving square pixels{\tt ]}
      }
      \sstsubsection{
         STEP = \_INTEGER (Read)
      }{
         The number of pixels between adjacent displayed vectors (along
         both axes).  Increasing this value reduces the number of
         displayed vectors.  The default value gives about 30 vectors
         along the longest axis of the plot. {\tt []}
      }
      \sstsubsection{
         VSCALE = \_REAL (Read)
      }{
         The scale to be used for the vectors.  The supplied value
         should give the data value corresponding to a vector length of
         one centimetre.  The default makes 5\% of all displayed vectors
         larger than the interval between adjacent vectors. {\tt []}
      }
      \sstsubsection{
         VTYPE = LITERAL (Read)
      }{
         The type of vector to be plotted; it can take the value {\tt "Arrow"}
         or {\tt "Line"}.  Vectors are drawn as arrows or lines accordingly.
         {\tt ["Line"]}
      }
   }
   \sstgraphparameters{
      \sstsubsection{
         ABSLAB  =  LITERAL (Read)
      }{
         Label for the plot abscissa, in which NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  If axis information is present
         in NDF1 the suggested default is the axis label from NDF1
         followed by the units, in parentheses.  If an error occurs
         obtaining the label the suggested default is {\tt "X"}. {\tt []}
      }
      \sstsubsection{
         AXES = \_LOGICAL (Read)
      }{
         {\tt TRUE} if labelled and annotated axes are to be drawn around the
         plot.  The annotations are either the data co-ordinates from
         the axis components of NDF1, provided these are present and
         linear and COSYS = {\tt "Data"}; otherwise pixel co-ordinates are
         used. {\tt [TRUE]}
      }
      \sstsubsection{
         BORDER = \_LOGICAL (Read)
      }{
         {\tt TRUE} if a box is to be drawn about the plot.  This is only
         accessed if no axes are drawn. {\tt [TRUE]}
      }
      \sstsubsection{
         FONT = LITERAL (Read)
      }{
         The fount to be used for the line graphics.  It can be either
         {\tt "NCAR"} for the NCAR fancy characters and {\tt "GKS"} for
         the standard
         GKS san-serif fount.  The former is intended for hardcopy
         publication-quality plots, since it is relatively slow; the
         latter is intended for normal interactive graphics requiring
         rapid plotting, and it is clearer on small plots.  The
         suggested default is the current value. {\tt ["GKS"]}
      }
      \sstsubsection{
         JUST = LITERAL (Read)
      }{
         The justification for each vector; it can take any of the
         following values:

         \begin{description}
         \item {\tt "Centre"} --- the vectors are drawn centred on the
                     corresponding pixel,

         \item {\tt "Start"}  --- the vectors are drawn starting at the
                     corresponding pixel,

         \item {\tt "End"} --- the vectors are drawn ending at the corresponding
                     pixel.
         \end{description}

         {\tt ["Centre"]}
      }
      \sstsubsection{
         MAJTIC( 2 ) = \_REAL (Read)
      }{
         The parameter controlling the numbers of major tick marks
         for the $x$ and $y$ axes.  (Number used is between MAJTIC$+$2 and
         5$*$MAJTIC/2$+$4.) {\tt [3.,3.]}
      }
      \sstsubsection{
         MINTIC( 2 ) = \_REAL (Read)
      }{
         The number of minor tick marks between each major tick mark
         for the $x$ and $y$ axes.  A negative value forces the graphics
         package to compute appropriate values. {\tt [-1.,-1.]}
      }
      \sstsubsection{
         ORDLAB  =  LITERAL (Read)
      }{
         Label for the plot ordinate, in which NCAR fancy founts may be
         embedded when FONT = {\tt "NCAR"}.  If axis information is present
         in NDF1 the suggested default is the axis label from NDF1
         followed by the units, in parentheses.  If an error occurs
         obtaining the label the suggested default is {\tt "Y"}. {\tt []}
      }
      \sstsubsection{
         OUTTIC = \_LOGICAL (Read)
      }{
         {\tt TRUE} if the axis tick marks are to appear on the outside of
         the axes instead of inside.  By default, the tick marks are
         drawn outside the plotting region to eliminate
         intersections of ticks with the vectors. {\tt [TRUE]}
      }
      \sstsubsection{
         PLTITL = LITERAL (Read)
      }{
         The title of the plot.  Up to about 40 characters can be
         accommodated, and NCAR fancy founts may be embedded when FONT =
         {\tt "NCAR"}.  If an error occurs obtaining the title, it is
         defaulted to {\tt "VECPLOT map"}.
         {\tt [}The title from NDF1{\tt ]}
      }
      \sstsubsection{
         THICK = \_REAL (Read)
      }{
         The thickness of the axes and annotations in the plot, where
         1.0 is the normal thickness.  Currently, this is only available
         on a few devices.  It must take a value in the range 0.5--5.0.
         {\tt [1.0]}
      }
      \sstsubsection{
         VECCOL = LITERAL (Read)
      }{
         The colour for the vectors.  The options are described below.
         \begin{description}
         \item {\tt "MAX"}  --- The maximum colour index in the image
                            display colour lookup table.
         \item {\tt "MIN"}  --- The minimum (non-reserved) colour index in
                            the image display colour lookup table.
         \item [An integer] --- The actual colour index. It is constrained
                            between 0 and the maximum colour index
                            available on the device.
         \item [A named colour] --- Uses the named colour from the palette, and
                            if it is not present, the nearest colour
                            from the palette is selected.
         \end{description}

         If the colour is to remain unaltered as the lookup table is
         manipulated choose an integer between 0 and 15, or a named
         colour.  The suggested default is the current value.
         {\tt [}The current value, but equals {\tt "MIN"} if there is
         no current value.{\tt ]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         vecplot polint polang
      }{
         Produces a vector map on the current graphics device with
         vector magnitude taken from the NDF called polint and vector
         orientation taken from NDF polang.  All other settings are
         defaulted, so for example about 20 vectors are displayed along
         the longest axis, and a key is plotted.
      }
      \sstexamplesubsection{
         vecplot polint polang angrot=23.4
      }{
         Produces a vector map in which the primary axis of the vectors
         (as defined by the value zero in the NDF polang) is at the
         position angle 23.$\udeg$4 (measured anti-clockwise from the
         positive $y$ axis) in the displayed map.
      }
      \sstexamplesubsection{
         vecplot stack(,,2) stack(,,1) vtype=arrow just=start nokey
      }{
         Produces a vector map in which the vectors are defined by two
         planes in the 3-dimensional NDF called stack.  There is no
         need to copy the two planes into two separate NDFs before
         running VECPLOT.  Each vector is represented by an arrow,
         starting at the position of the corresponding pixel.  No key
         to the vector scale and justification is produced.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         If the current graphics-database picture is a DATA picture or
         contains a DATA picture (such as created by the applications
         DISPLAY, CONTOUR, {\it etc.}), then the vector plot is overlaid on top
         of the existing DATA plot.  In this case any requested annotation
         and key are drawn outside the DATA picture but within the current
         picture.  If there is insufficient room within the current
         picture, then the annotation and/or key may not be drawn.  You
         are warned if this happens but the application continues.  If no
         DATA picture can be found within the current picture then you
         specify the total size of the plot frame using parameters PXSIZE
         and PYSIZE, but the application itself chooses how to position
         the vector plot and key within this frame.

         \sstitem
         The application stores a number of pictures in the graphics
         database in the following order: a FRAME of the specified size
         containing the title, annotated axes, vector map and key; a DATA
         picture which is stored with world co-ordinates in units of data
         pixels; and a KEY picture to store the key if present.  The DATA
         picture may also have double-precision data co-ordinates derived
         from the axis components of NDF1 provided these are linear and
         different from pixel co-ordinates; the data co-ordinates are
         stored via a linear transformation.  A reference to NDF1 is
         stored with the DATA picture.  On exit the current database
         picture for the chosen device reverts to the input picture.

         \sstitem
         The units string in the key may be truncated with an ellipsis
         if it cannot be accommodated in full at the smallest allowed
         character height.  Generally the maximum length will be between
         18 and 27 characters, the exact value depending on the number of
         digits in the scale factors.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: CALPOL.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         Only real data can be processed directly.  Other non-complex
         numeric data types will undergo a type conversion before the
         vector plot is drawn.

         \sstitem
         Bad pixels and automatic quality masking are supported.
      }
   }
}
\begin{htmlonly}
Click \htmladdnormallink{here}{vecplot_exam.gif} to see an example
plot (7k).
\end{htmlonly}

\sstroutine{
   WIENER
}{
   Applies a Wiener filter to a 1- or 2- dimensional array
}{
   \sstdescription{
      This application filters the supplied 1- or 2-dimensional array
      using a Wiener filter.  It takes an array holding observed data
      and another holding a Point-Spread Function as input and produces
      an output restored array with potentially higher resolution and
      lower noise.  Generally superior results can be obtained using
      applications MEM2D or LUCY, but at the cost of much more
      processing time.

      The Wiener filter attempts to minimise the mean squared difference
      between the undegraded image and the restored image.  To do this it
      needs to know the power spectrum of the undegraded image
      ({\it i.e.}\ the power at each spatial frequency before the
      instrumental blurring and the addition of noise).  Obviously, this
      is not usually available, and instead the power spectrum of some
      other image must be used (the `model' image).  The idea is that a
      model image should be chosen for which there is some {\it a
      priori\/} reason for believing it to have a power spectrum similar
      to the undegraded image.  Many different suggestions have been made
      for the best way to make this choice and the literature should be
      consulted for a detailed discussion (for instance, see the paper
      {\it Wiener Restoration of HST Images: Signal Models and Photometric
      Behavior\/} by I.C. Busko in the proceedings of the first Annual
      Conference on Astronomical Data Analysis Software and Systems,
      Tucson).  By default, this application uses a `white' model image,
      {\it i.e.}\ one in which there is equal power at all spatial
      frequencies.  The default value for this constant power is the
      mean power per pixel in the input image.  There is also an option
      to use the power spectrum of a supplied model image.

      The filter also depends on a model of the noise in the supplied
      image. This application assumes that the noise is `white' and is
      constant across the image.  You can specify the noise power to
      use.  If a noise power of zero is supplied, then the Wiener filter
      just becomes a normal inverse filter which will tend to amplify noise
      in the supplied image.

      The filtering is done by multiplying the Fourier transform of the
      supplied image by the Fourier transform of the filter function.
      The output image is then created by taking the inverse Fourier
      transform of the product. The Fourier transform of the filter
      function is given by:

         {\Large
         \[ \frac{H^{\ast}}{ \left | H \right |^{2} + \frac{P_{n}}{P_{g}}} \]
         }

      where $H$ is the Fourier transform of the supplied Point-Spread
      Function, $P_{n}$ is the noise power, $P_{g}$ is the power in the model
      image, and $H^{\ast}$ is the complex conjugate of $H$. If the
      supplied model includes noise (as indicated by parameter QUIET)
      then $P_{n}$ is subtracted from $P_{g}$ before evaluating the above
      expression.
   }
   \sstusage{
      wiener in psf out xcentre ycentre
   }
   \sstparameters{
      \sstsubsection{
         IN = NDF (Read)
      }{
         The input NDF containing the observed data.  This image may
         contain bad values, in which case the bad values will be
         replaced by zero before applying the filter.  The resulting
         filtered image is normalised by dividing each pixel value by
         the corresponding weight of the good input pixels.  These
         weights are found by filtering a mask image which holds the
         value one at every good input pixel, and zero at every bad
         input pixel.
      }
      \sstsubsection{
         MODEL = NDF (Read)
      }{
         An NDF containing an image to use as the model for the power
         spectrum of the restored image.  Any bad values in this image are
         replaced by the mean of the good values.  If a null value is supplied
         then the model power spectrum is taken to be uniform with a
         value specified by parameter PMODEL. {\tt [!]}
      }
      \sstsubsection{
         OUT = NDF (Write)
      }{
         The restored output array.  An extension named WIENER is added
         to the output NDF to indicate that the image was created by
         this application (see parameter QUIET).
      }
      \sstsubsection{
         PMODEL = \_REAL (Read)
      }{
         The mean power per pixel in the model image.  This parameter is
         only accessed if a null value is supplied for parameter MODEL.
         If a value is obtained for PMODEL then the model image is
         assumed to have the specified constant power at all spatial
         frequencies.  The run-time default is the mean power per pixel
         in the input image. {\tt []}
      }
      \sstsubsection{
         PNOISE = \_REAL (Read)
      }{
         The mean noise power per pixel in the observed data. For
         Gaussian noise this is equal to the variance. The run-time
         default is an estimate of the noise variance based on the
         difference between adjacent pixel values in the observed
         data. {\tt []}
      }
      \sstsubsection{
         PSF = NDF (Read)
      }{
         An NDF holding an estimate of the Point-Spread Function (PSF)
         of the input array.  This could, for instance, be produced
         using the {\footnotesize KAPPA} application {\tt "}PSF{\tt "}.  There should be no bad
         pixels in the PSF otherwise an error will be reported.  The PSF
         can be centred anywhere within the array, but the location of
         the centre must be specified using parameters XCENTRE and
         YCENTRE.  The PSF is assumed to have the value zero outside the
         supplied NDF.
      }
      \sstsubsection{
         QUIET = \_LOGICAL (Read)
      }{
         This specifies whether or not the image given for parameter
         MODEL (or the value given for parameter PMODEL), includes noise.
         If the model does not include any noise then a {\tt TRUE} value
         should be supplied for QUIET.  If there is any noise in the
         model then QUIET should be supplied {\tt FALSE}.  The run-time default
         is {\tt FALSE}, unless the image given for parameter MODEL was
         created by a previous run of WIENER (as indicated by the
         presence of a WIENER extension in the NDF), in which case the run
         time default is {\tt TRUE} ({\it i.e.}\ the previous run of
         WIENER is assumed to have removed the noise). {\tt []}
      }
      \sstsubsection{
         THRESH = \_REAL (Read)
      }{
         The fraction of the PSF peak amplitude at which the extents of
         the PSF are determined.  These extents are used to determine
         the size of the margins used to pad the supplied input array.
         Lower values of THRESH will result in larger margins being
         used.  THRESH must be positive and less than 0.5.  {\tt [0.0625]}
      }
      \sstsubsection{
         TITLE = LITERAL (Read)
      }{
         A title for the output NDF.  A null ({\tt !}) value means using the
         title of the input NDF. {\tt [!]}
      }
      \sstsubsection{
         WLIM = \_REAL (Read)
      }{
         If the input array contains bad values, then this parameter
         may be used to determine the minimum weight of good input
         values required to create a good output value.  It can be used,
         for example, to prevent output pixels from being generated in
         regions where there are relatively few good input values to
         contribute to the restored result.  It can also be used to
         `fill in' small areas ({\it i.e.}\ smaller than the PSF) of bad
         pixels.

         The numerical value given for WLIM specifies the minimum total
         weight associated with the good pixels in a smoothing box
         required to generate a good output pixel (weights for each
         pixel are defined by the normalised PSF).  If this specified
         minimum weight is not present, then a bad output pixel will
         result, otherwise a smoothed output value will be calculated.
         The value of this parameter should lie between 0.0 and
         1.0.  WLIM = {\tt 0} causes a good output value to be created even if
         there is only one good input value, whereas WLIM = {\tt 1} causes a
         good output value to be created only if all input values are
         good. {\tt [0.001]}
      }
      \sstsubsection{
         XCENTRE = \_INTEGER (Read)
      }{
         The $x$ pixel index of the centre of the PSF within the supplied
         PSF array.  The suggested default is the middle pixel (rounded
         down if there are an even number of pixels per line).
      }
      \sstsubsection{
         YCENTRE = \_INTEGER (Read)
      }{
         The $y$ pixel index of the centre of the PSF within the supplied
         PSF array.  The suggested default is the middle line (rounded
         down if there are an even number of lines).
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         wiener cenA star cenA\_hires 11 13
      }{
         This example deconvolves the array in the NDF called cenA,
         putting the resulting array in the NDF called cenA\_hires.
         The PSF is defined by the array in NDF star, and the centre
         of the PSF is at pixel (11,13).
      }
      \sstexamplesubsection{
         wiener cenA star cenA\_hires 11 13 pnoise=0
      }{
         This example performs the same function as the previous
         example, except that the noise power is given as zero.  This
         causes the Wiener filter to reduce to a standard inverse
         filter, which will result in more high frequencies being
         present in the restored image.
      }
      \sstexamplesubsection{
         wiener cenA star cenA\_hires 11 13 model=theory quiet
      }{
         This example performs the same function as the first example,
         except that the power spectrum of the restored image is
         modelled on that of NDF theory, which may for instance
         contain a theoretical model of the object in NDF cenA,
         together with a simulated star field.  The parameter QUIET is
         set to a {\tt TRUE} value to indicate that the theoretical model
         contains no noise.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         The convolutions required by the Wiener filter are performed by
         the multiplication of Fourier transforms.  The supplied input
         array is extended by a margin along each edge to avoid problems
         of wrap-around between opposite edges of the array.  The width of
         this margin is about equal to the width of the significant part
         of the PSF (as determined by parameter THRESH).  The application
         displays the width of these margins.  The margins are filled by
         replicating the edge pixels from the supplied input NDFs.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: FOURIER, LUCY, MEM2D.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         LABEL, TITLE, UNITS, and HISTORY components of the
         input NDF and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.  Arithmetic
         is performed using single-precision floating point.
      }
   }
}

\sstroutine{
   ZAPLIN
}{
   Replaces regions in a 2-d NDF by bad values or by linear
   interpolation
}{
   \sstdescription{
      This routine allows you to mask or clean whole columns or
      lines, or regions from a 2-dimensional data and variance arrays
      in an NDF structure.  The cleaned arrays are written to an output NDF.
      At present the cleaning process is one of the following selection.
      \begin{enumerate}
      \item Flagging by substitution of the bad value.
      \item Bi-linear interpolation across a region, using the nearest
         non-bad pixels on each of the four sides.
      \item Linear interpolation across whole lines or columns, using
         the nearest non-bad pixels to either side.
      \end{enumerate}
      The magic value is also substituted in processes 2. and 3. where
      interpolation is not possible, such as at the edge of the array.

      Co-ordinates may either be in pixels or in data co-ordinates.
      Three methods are available for obtaining the lines or columns
      or the region:
      \begin{enumerate}
      \item From the parameter system, usually in response to prompting.
      \item By a placing a graphics cursor of a nominated device either
         side of the defect.  If columns are being zapped then the
         line position of the cursor is ignored, and vice versa.  To
         use this mode the data array must already be displayed as an
         image or contour plot and the picture stored in the graphics
         database.
      \item By reading a free-format text file in which each record
         defines a zapping instruction.  Each record must contain
         either a) a pair of column or line positions followed by {\tt L} or
         {\tt C} to indicate whether it is lines or columns being specified
         respectively; or b) the lower followed by the upper bound of
         a region ({\it i.e.}\ a pair of $x$-$y$ positions.)  There may be
         commentary lines in the file beginning with {\tt \#} or {\tt !}.  For
         example,
      {\tt \begin{verse}
         \# University of Madrugada  CCD Mark III  defects \\
         23  23  L \\
         157 158 C \\
         40 23 45 25 \\
         <EOF>
      \end{verse}}

        would zap line 23, columns 157 and 158, and a region from
        (40,~23) to (45,~25).
      \end{enumerate}

      In the first two modes the application loops asking for new
      columns, lines or regions to zap, until told to quit or it
      encounters an error.  An output co-ordinate-list file may also be
      produced; it may be recycled in later processing as the input to
      the third mode.  In the last mode processing stops when the end
      of file is found.
   }
   \sstusage{
      zaplin in out [title] colin=? lincol=? columns=? lines=?
   }
   \sstparameters{
      \sstsubsection{
         COLIN =  FILENAME (Read)
      }{
         Name of the text file containing the column and line bounds
         of areas to be cleaned. It is only used when MODE = {\tt "File"}.
      }
      \sstsubsection{
         COLOUT =  FILENAME (Read)
      }{
         Name of the file to store the areas cleaned.  It has the same
         format as an input text file.  It may be used as input via
         parameter COLIN for processing of other NDFs in the same way,
         without the drudgery of repeating the commands by hand.
         It is not available if MODE = {\tt "File"}.  If COLOUT is null ({\tt !}),
         there will be no logging to an output text file. {\tt [!]}
      }
      \sstsubsection{
         COLUMNS( 2 ) = \_DOUBLE (Read)
      }{
         Columns that define the inclusive bounds of the region to be
         zapped.  These are given either pixel indices if COSYS =
         {\tt "World"} or the NDF has no axis information, or data
         co-ordinates if COSYS = {\tt "Data"}.  The application constrains
         the column bounds to be within the bounds of the NDF.  This
         parameter is only required when MODE = {\tt "Interface"}.
      }
      \sstsubsection{
         COSYS = LITERAL (Read)
      }{
         The co-ordinate system to be used.  This can be either {\tt "World"}
         or {\tt "Data"}.  If COSYS = {\tt "Data"} the input co-ordinates,
         either in the text file (File mode) or parameter values (Interface
         mode) are to be expressed in data co-ordinates, otherwise
         pixel indices (the world co-ordinates) are used.  In all modes
         the results are written in data co-ordinates.  The data values
         are converted to and from pixel indices via the NDF's axis
         values; if there is no axis information within the NDF, world
         co-ordinates are then used, except in Cursor mode where the
         transformation, if present, is taken from the last DATA
         picture in the graphics database.  If COSYS = {\tt "World"} pixel
         co-ordinates are used throughout.  {\tt [}Current co-ordinate
         system{\tt ]}
      }
      \sstsubsection{
         DEVICE = DEVICE (Read)
      }{
         The graphics device whose the cursor is used to select the
         columns or lines that are to be zapped.  It is only used
         when MODE = {\tt "Cursor"}.  {\tt [}Current graphics device{\tt ]}
      }
      \sstsubsection{
         IN  =  NDF (Read)
      }{
         Input NDF structure containing the 2-dimensional data array to be
         cleaned.
      }
      \sstsubsection{
         LINCOL  =  LITERAL (Read)
      }{
         The type of area is to be cleaned.  The options are {\tt "Lines"},
         {\tt "Columns"} or a {\tt "Region"}.   {\tt "Lines"} cleans all the columns
         between two line limits; likewise {\tt "Columns"} cleans all the
         lines between two column limits; {\tt "Region"} cleans an area
         given by pairs of column and line limits.  This parameter is
         not used if MODE = {\tt "File"}.  If it is specified on the command
         line in interface mode only one zap operation will be
         performed; otherwise a series of changes may be made until
         terminated by setting LINCOL to null ({\tt !}).
      }
      \sstsubsection{
         LINES( 2 ) =  \_DOUBLE (Read)
      }{
         Lines that define the inclusive bounds of the region to be
         zapped.  These are given either pixel indices if COSYS =
         {\tt "World"} or the NDF has no axis information, or data
         co-ordinates if COSYS = {\tt "Data"}.  The application constrains
         the line bounds to be within the bounds of the NDF.  This
         parameter is only required when MODE = {\tt "Interface"}.
      }
      \sstsubsection{
         MARK = \_LOGICAL (Read)
      }{
         If {\tt TRUE}, each point selected by the cursor will be marked by a
         cross when MODE = {\tt "Cursor"}.  {\tt [FALSE]}
      }
      \sstsubsection{
         MODE  =  LITERAL (Read)
      }{
         The mode by which the bounds of the region to be cleaned
         are to be obtained.  The options are as follows: {\tt "Interface"}
         defines via the parameter system, {\tt "Cursor"} enables selection
         by graphics cursor, and {\tt "File"} reads them from a text file.
         {\tt [}Current interaction mode{\tt ]}
      }
      \sstsubsection{
         NOISE  =  \_LOGICAL (Read)
      }{
         If NOISE is {\tt TRUE} random noise is added to each substituted
         pixel unless ZAPTYPE = {\tt "Bad"}.  The variance of the noise is
         equal to that of the data variance of the substituted data
         value.  If the data variance is bad for a pixel, no noise is
         added to that pixel.  This facility is provided for cosmetic
         use. {\tt [FALSE]}
      }
      \sstsubsection{
         OUT  =  NDF (Write)
      }{
         Output NDF structure containing cleaned version of the
         input data and variance arrays.
      }
      \sstsubsection{
         TITLE  =  LITERAL (Read)
      }{
         Title for the output NDF structure.  A null value (!)
         propagates the title from the input NDF to the output NDF.
         {\tt [!]}
      }
      \sstsubsection{
         ZAPTYPE  =  LITERAL (Read)
      }{
         The type of the cleaning.  The options are {\tt "Linear"} for linear
         interpolation across the line or column using the values that
         abut the pixels to be zapped, or {\tt "Bad"} for substitution by the
         bad-pixel value. {\tt ["Linear"]}
      }
   }
   \sstexamples{
      \sstexamplesubsection{
         zaplin out=cleaned colout=fudge.dat
      }{
         Assuming the current interaction mode is cursor this will copy
         the NDF associated with the last DATA picture to an NDF called
         cleaned, ready to be zapped interactively using the
         current graphics device.  The cleaning is via linear interpolation.
         A record of the areas cleaned will be stored in the text file
         named {\tt fudge.dat}.
      }
      \sstexamplesubsection{
         zaplin grubby cleaned i cosys=w lincol=r columns=[188,190] lines=[15,16]
      }{
          This zaps a region from pixel (188,15) to (190,16) within the
          NDF called grubby and stores the result in the NDF called
          cleaned.  The zapping is via linear interpolation.
      }
      \sstexamplesubsection{
         zaplin grubby(6,,) cleaned i cosys=w lincol=r columns=[188,190]
      }{
         This zaps columns 188 to 190 in the sixth $y$-$z$ plane region
         within the NDF called grubby and stores the result in the NDF
         called cleaned.  The zapping is via linear interpolation.
      }
      \sstexamplesubsection{
         zaplin m42 m42c f colin=aaoccd1.dat zaptype=b
      }{
         This flags with the bad pixel value the regions in the NDF
         called m42 defined in the text file called {\tt aaoccd1.dat}, and
         stores the result in an NDF called m42c.
      }
      \sstexamplesubsection{
         zaplin m42 m42c f colin=aaoccd1.dat noise
      }{
         As above except that linear interpolation plus cosmetic noise
         are used to replace the areas to be cleaned rather than bad
         pixels.
      }
   }
   \sstnotes{
      \sstitemlist{

         \sstitem
         If there is no variance array in the NDF, the absolute data
         value is used instead to apply noise.  This variance is not
         written to the output NDF.

         \sstitem
         When using input files care should be taken to ensure that
         the co-ordinate system used in the file matches that of the NDF
         in the current co-ordinate system.

         \sstitem
         Data co-ordinates are stored and output in single precision
         except when the axis array is type \_DOUBLE or \_INTEGER, or
         in cursor mode when there is no axis information in the NDF.

         \sstitem
         If the input NDF is a section of an NDF with a higher
         dimensionality, the {\tt "}lines{\tt "} and
         {\tt "}columns{\tt "} are with respect to the 2-dimensional
         section, and do not necessarily refer to the first and second
         dimensions of the NDF as a whole.  See the
         {\tt "}Examples{\tt "}.
      }
   }
   \sstdiytopic{
      Related Applications
   }{
      KAPPA: ARDMASK, CHPIX, FILLBAD, GLITCH, NOMAGIC, SEGMENT, SETMAGIC;
      Figaro: CSET, ICSET, NCSET, TIPPEX.
   }
   \sstimplementationstatus{
      \sstitemlist{

         \sstitem
         This routine correctly processes the AXIS, DATA, QUALITY,
         VARIANCE, LABEL, TITLE, UNITS, and HISTORY components of the
         input NDF and propagates all extensions.

         \sstitem
         Processing of bad pixels and automatic quality masking are
         supported.

         \sstitem
         All non-complex numeric data types can be handled.

         \sstitem
         There could be a false precision in the data co-ordinates
         when the transformation is obtained from the AGI database.  This
         only occurs when there is no axis information in the NDF.
      }
   }
}

\newpage
\markboth{\stardocname}{\stardocname}
\section{\xlabel{ap_colset}Standard Named Colours\label{ap:colset}}
The standard set of named colours recognised by {\footnotesize KAPPA} is
tabulated below together with their red, green, and blue relative
intensities.  It is the X-windows standard colour set so don't blame
{\footnotesize KAPPA} if you think some of them are anomalous.  In addition to
those tabulated, there are grey levels at each percentage between
``Black'' and ``White''.  These are called ``Grey1'', ``Grey2'', \dots,
``Grey99''.  All the names containing ``Grey'' have synonyms spelt with
``Gray''.  \medskip

\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\multicolumn{8}{|c|}{{\large Standard Colour Set}} \\ \hline
\multicolumn{1}{|c|}{Name} & \multicolumn{1}{|c|}{R} & \multicolumn{1}{c|}{G} &
\multicolumn{1}{c|}{B} & \multicolumn{1}{|c|}{Name} & \multicolumn{1}{c|}{R} &
\multicolumn{1}{c|}{G} & \multicolumn{1}{c|}{B}  \\ \hline
AliceBlue           & 0.941 & 0.973 & 1.000 & AntiqueWhite        & 0.980 & 0.922 & 0.843 \\ 
AntiqueWhite1       & 1.000 & 0.937 & 0.859 & AntiqueWhite2       & 0.933 & 0.875 & 0.800 \\ 
AntiqueWhite3       & 0.804 & 0.753 & 0.690 & AntiqueWhite4       & 0.545 & 0.514 & 0.471 \\ 
Aquamarine          & 0.498 & 1.000 & 0.831 & Aquamarine1         & 0.498 & 1.000 & 0.831 \\ 
Aquamarine2         & 0.463 & 0.933 & 0.776 & Aquamarine3         & 0.400 & 0.804 & 0.667 \\ 
Aquamarine4         & 0.271 & 0.545 & 0.455 & Azure               & 0.941 & 1.000 & 1.000 \\ 
Azure1              & 0.941 & 1.000 & 1.000 & Azure2              & 0.878 & 0.933 & 0.933 \\ 
Azure3              & 0.757 & 0.804 & 0.804 & Azure4              & 0.514 & 0.545 & 0.545 \\ 
Beige               & 0.961 & 0.961 & 0.863 & Bisque              & 1.000 & 0.894 & 0.769 \\ 
Bisque1             & 1.000 & 0.894 & 0.769 & Bisque2             & 0.933 & 0.835 & 0.718 \\ 
Bisque3             & 0.804 & 0.718 & 0.620 & Bisque4             & 0.545 & 0.490 & 0.420 \\ 
Black               & 0.000 & 0.000 & 0.000 & BlanchedAlmond      & 1.000 & 0.922 & 0.804 \\ 
Blue                & 0.000 & 0.000 & 1.000 & Blue1               & 0.000 & 0.000 & 1.000 \\ 
Blue2               & 0.000 & 0.000 & 0.933 & Blue3               & 0.000 & 0.000 & 0.804 \\ 
Blue4               & 0.000 & 0.000 & 0.545 & BlueViolet          & 0.541 & 0.169 & 0.886 \\ 
Brown               & 0.647 & 0.165 & 0.165 & Brown1              & 1.000 & 0.251 & 0.251 \\ 
Brown2              & 0.933 & 0.231 & 0.231 & Brown3              & 0.804 & 0.200 & 0.200 \\ 
Brown4              & 0.545 & 0.137 & 0.137 & Burlywood           & 0.871 & 0.722 & 0.529 \\ 
Burlywood1          & 1.000 & 0.827 & 0.608 & Burlywood2          & 0.933 & 0.773 & 0.569 \\ 
Burlywood3          & 0.804 & 0.667 & 0.490 & Burlywood4          & 0.545 & 0.451 & 0.333 \\ 
CadetBlue           & 0.373 & 0.620 & 0.627 & CadetBlue1          & 0.596 & 0.961 & 1.000 \\ 
CadetBlue2          & 0.557 & 0.898 & 0.933 & CadetBlue3          & 0.478 & 0.773 & 0.804 \\ 
CadetBlue4          & 0.325 & 0.525 & 0.545 & Chartreuse          & 0.498 & 1.000 & 0.000 \\ 
Chartreuse1         & 0.498 & 1.000 & 0.000 & Chartreuse2         & 0.463 & 0.933 & 0.000 \\ 
Chartreuse3         & 0.400 & 0.804 & 0.000 & Chartreuse4         & 0.271 & 0.545 & 0.000 \\ 
Chocolate           & 0.824 & 0.412 & 0.118 & Chocolate1          & 1.000 & 0.498 & 0.141 \\ 
Chocolate2          & 0.933 & 0.463 & 0.129 & Chocolate3          & 0.804 & 0.400 & 0.114 \\ 
Chocolate4          & 0.545 & 0.271 & 0.075 & Coral               & 1.000 & 0.498 & 0.314 \\ 
Coral1              & 1.000 & 0.447 & 0.337 & Coral2              & 0.933 & 0.416 & 0.314 \\ 
Coral3              & 0.804 & 0.357 & 0.271 & Coral4              & 0.545 & 0.243 & 0.184 \\ 
CornflowerBlue      & 0.392 & 0.584 & 0.929 & Cornsilk            & 1.000 & 0.973 & 0.863 \\ 
Cornsilk1           & 1.000 & 0.973 & 0.863 & Cornsilk2           & 0.933 & 0.910 & 0.804 \\ 
Cornsilk3           & 0.804 & 0.784 & 0.694 & Cornsilk4           & 0.545 & 0.533 & 0.471 \\ 
Cyan                & 0.000 & 1.000 & 1.000 & Cyan1               & 0.000 & 1.000 & 1.000 \\ 
Cyan2               & 0.000 & 0.933 & 0.933 & Cyan3               & 0.000 & 0.804 & 0.804 \\ 
Cyan4               & 0.000 & 0.545 & 0.545 & DarkGoldenrod       & 0.722 & 0.525 & 0.043 \\ 
DarkGoldenrod1      & 1.000 & 0.725 & 0.059 & DarkGoldenrod2      & 0.933 & 0.678 & 0.055 \\ 
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\multicolumn{8}{|c|}{{\large Standard Colour Set}} \\ \hline
\multicolumn{1}{|c|}{Name} & \multicolumn{1}{|c|}{R} & \multicolumn{1}{c|}{G} &
\multicolumn{1}{c|}{B} & \multicolumn{1}{|c|}{Name} & \multicolumn{1}{c|}{R} &
\multicolumn{1}{c|}{G} & \multicolumn{1}{c|}{B}  \\ \hline
DarkGoldenrod3      & 0.804 & 0.584 & 0.047 & DarkGoldenrod4      & 0.545 & 0.396 & 0.031 \\ 
DarkGreen           & 0.000 & 0.392 & 0.000 & DarkKhaki           & 0.741 & 0.718 & 0.420 \\ 
DarkOliveGreen      & 0.333 & 0.420 & 0.184 & DarkOliveGreen1     & 0.792 & 1.000 & 0.439 \\ 
DarkOliveGreen2     & 0.737 & 0.933 & 0.408 & DarkOliveGreen3     & 0.635 & 0.804 & 0.353 \\ 
DarkOliveGreen4     & 0.431 & 0.545 & 0.239 & DarkOrange          & 1.000 & 0.549 & 0.000 \\ 
DarkOrange1         & 1.000 & 0.498 & 0.000 & DarkOrange2         & 0.933 & 0.463 & 0.000 \\ 
DarkOrange3         & 0.804 & 0.400 & 0.000 & DarkOrange4         & 0.545 & 0.271 & 0.000 \\ 
DarkOrchid          & 0.600 & 0.196 & 0.800 & DarkOrchid1         & 0.749 & 0.243 & 1.000 \\ 
DarkOrchid2         & 0.698 & 0.227 & 0.933 & DarkOrchid3         & 0.604 & 0.196 & 0.804 \\ 
DarkOrchid4         & 0.408 & 0.133 & 0.545 & DarkSalmon          & 0.914 & 0.588 & 0.478 \\ 
DarkSeaGreen        & 0.561 & 0.737 & 0.561 & DarkSeaGreen1       & 0.757 & 1.000 & 0.757 \\ 
DarkSeaGreen2       & 0.706 & 0.933 & 0.706 & DarkSeaGreen3       & 0.608 & 0.804 & 0.608 \\ 
DarkSeaGreen4       & 0.412 & 0.545 & 0.412 & DarkSlateBlue       & 0.282 & 0.239 & 0.545 \\ 
DarkSlateGrey       & 0.184 & 0.310 & 0.310 & DarkSlateGrey1      & 0.592 & 1.000 & 1.000 \\ 
DarkSlateGrey2      & 0.553 & 0.933 & 0.933 & DarkSlateGrey3      & 0.475 & 0.804 & 0.804 \\ 
DarkSlateGrey4      & 0.322 & 0.545 & 0.545 & DarkTurquoise       & 0.000 & 0.808 & 0.820 \\ 
DarkViolet          & 0.580 & 0.000 & 0.827 & DeepPink            & 1.000 & 0.078 & 0.576 \\ 
DeepPink1           & 1.000 & 0.078 & 0.576 & DeepPink2           & 0.933 & 0.071 & 0.537 \\ 
DeepPink3           & 0.804 & 0.063 & 0.463 & DeepPink4           & 0.545 & 0.039 & 0.314 \\ 
DeepSkyBlue         & 0.000 & 0.749 & 1.000 & DeepSkyBlue1        & 0.000 & 0.749 & 1.000 \\ 
DeepSkyBlue2        & 0.000 & 0.698 & 0.933 & DeepSkyBlue3        & 0.000 & 0.604 & 0.804 \\ 
DeepSkyBlue4        & 0.000 & 0.408 & 0.545 & DimGrey             & 0.412 & 0.412 & 0.412 \\ 
DodgerBlue          & 0.118 & 0.565 & 1.000 & DodgerBlue1         & 0.118 & 0.565 & 1.000 \\ 
DodgerBlue2         & 0.110 & 0.525 & 0.933 & DodgerBlue3         & 0.094 & 0.455 & 0.804 \\ 
DodgerBlue4         & 0.063 & 0.306 & 0.545 & Firebrick           & 0.698 & 0.133 & 0.133 \\ 
Firebrick1          & 1.000 & 0.188 & 0.188 & Firebrick2          & 0.933 & 0.173 & 0.173 \\ 
Firebrick3          & 0.804 & 0.149 & 0.149 & Firebrick4          & 0.545 & 0.102 & 0.102 \\ 
FloralWhite         & 1.000 & 0.980 & 0.941 & ForestGreen         & 0.133 & 0.545 & 0.133 \\ 
Gainsboro           & 0.863 & 0.863 & 0.863 & GhostWhite          & 0.973 & 0.973 & 1.000 \\ 
Gold                & 1.000 & 0.843 & 0.000 & Gold1               & 1.000 & 0.843 & 0.000 \\ 
Gold2               & 0.933 & 0.788 & 0.000 & Gold3               & 0.804 & 0.678 & 0.000 \\ 
Gold4               & 0.545 & 0.459 & 0.000 & Goldenrod           & 0.855 & 0.647 & 0.125 \\ 
Goldenrod1          & 1.000 & 0.757 & 0.145 & Goldenrod2          & 0.933 & 0.706 & 0.133 \\ 
Goldenrod3          & 0.804 & 0.608 & 0.114 & Goldenrod4          & 0.545 & 0.412 & 0.078 \\ 
Green               & 0.000 & 1.000 & 0.000 & Green1              & 0.000 & 1.000 & 0.000 \\ 
Green2              & 0.000 & 0.933 & 0.000 & Green3              & 0.000 & 0.804 & 0.000 \\ 
Green4              & 0.000 & 0.545 & 0.000 & GreenYellow         & 0.678 & 1.000 & 0.184 \\ 
Grey                & 0.753 & 0.753 & 0.753 & Honeydew            & 0.941 & 1.000 & 0.941 \\ 
Honeydew1           & 0.941 & 1.000 & 0.941 & Honeydew2           & 0.878 & 0.933 & 0.878 \\ 
Honeydew3           & 0.757 & 0.804 & 0.757 & Honeydew4           & 0.514 & 0.545 & 0.514 \\ 
HotPink             & 1.000 & 0.412 & 0.706 & HotPink1            & 1.000 & 0.431 & 0.706 \\ 
HotPink2            & 0.933 & 0.416 & 0.655 & HotPink3            & 0.804 & 0.376 & 0.565 \\ 
HotPink4            & 0.545 & 0.227 & 0.384 & IndianRed           & 0.804 & 0.361 & 0.361 \\ 
IndianRed1          & 1.000 & 0.416 & 0.416 & IndianRed2          & 0.933 & 0.388 & 0.388 \\ 
IndianRed3          & 0.804 & 0.333 & 0.333 & IndianRed4          & 0.545 & 0.227 & 0.227 \\ 
Ivory               & 1.000 & 1.000 & 0.941 & Ivory2              & 0.933 & 0.933 & 0.878 \\ 
Ivory3              & 0.804 & 0.804 & 0.757 & Ivory4              & 0.545 & 0.545 & 0.514 \\ 
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\multicolumn{8}{|c|}{{\large Standard Colour Set}} \\ \hline
\multicolumn{1}{|c|}{Name} & \multicolumn{1}{|c|}{R} & \multicolumn{1}{c|}{G} &
\multicolumn{1}{c|}{B} & \multicolumn{1}{|c|}{Name} & \multicolumn{1}{c|}{R} &
\multicolumn{1}{c|}{G} & \multicolumn{1}{c|}{B}  \\ \hline
Khaki               & 0.941 & 0.902 & 0.549 & Khaki1              & 1.000 & 0.965 & 0.561 \\ 
Khaki2              & 0.933 & 0.902 & 0.522 & Khaki3              & 0.804 & 0.776 & 0.451 \\ 
Khaki4              & 0.545 & 0.525 & 0.306 & Lavender            & 0.902 & 0.902 & 0.980 \\ 
LavenderBlush       & 1.000 & 0.941 & 0.961 & LavenderBlush1      & 1.000 & 0.941 & 0.961 \\ 
LavenderBlush2      & 0.933 & 0.878 & 0.898 & LavenderBlush3      & 0.804 & 0.757 & 0.773 \\ 
LavenderBlush4      & 0.545 & 0.514 & 0.525 & LawnGreen           & 0.486 & 0.988 & 0.000 \\ 
LemonChiffon        & 1.000 & 0.980 & 0.804 & LemonChiffon1       & 1.000 & 0.980 & 0.804 \\ 
LemonChiffon2       & 0.933 & 0.914 & 0.749 & LemonChiffon3       & 0.804 & 0.788 & 0.647 \\ 
LemonChiffon4       & 0.545 & 0.537 & 0.439 & LightBlue           & 0.678 & 0.847 & 0.902 \\ 
LightBlue1          & 0.749 & 0.937 & 1.000 & LightBlue2          & 0.698 & 0.875 & 0.933 \\ 
LightBlue3          & 0.604 & 0.753 & 0.804 & LightBlue4          & 0.408 & 0.514 & 0.545 \\ 
LightCoral          & 0.941 & 0.502 & 0.502 & LightCyan           & 0.878 & 1.000 & 1.000 \\ 
LightCyan1          & 0.878 & 1.000 & 1.000 & LightCyan2          & 0.820 & 0.933 & 0.933 \\ 
LightCyan3          & 0.706 & 0.804 & 0.804 & LightCyan4          & 0.478 & 0.545 & 0.545 \\ 
LightGoldenrod      & 0.933 & 0.867 & 0.510 & LightGoldenrod1     & 1.000 & 0.925 & 0.545 \\ 
LightGoldenrod2     & 0.933 & 0.863 & 0.510 & LightGoldenrod3     & 0.804 & 0.745 & 0.439 \\ 
LightGoldenrod4     & 0.545 & 0.506 & 0.298 & LightGoldenrodYellow& 0.980 & 0.980 & 0.824 \\ 
LightGrey           & 0.827 & 0.827 & 0.827 & LightPink           & 1.000 & 0.714 & 0.757 \\ 
LightPink1          & 1.000 & 0.682 & 0.725 & LightPink2          & 0.933 & 0.635 & 0.678 \\ 
LightPink3          & 0.804 & 0.549 & 0.584 & LightPink4          & 0.545 & 0.373 & 0.396 \\ 
LightSalmon         & 1.000 & 0.627 & 0.478 & LightSalmon1        & 1.000 & 0.627 & 0.478 \\ 
LightSalmon2        & 0.933 & 0.584 & 0.447 & LightSalmon3        & 0.804 & 0.506 & 0.384 \\ 
LightSalmon4        & 0.545 & 0.341 & 0.259 & LightSeaGreen       & 0.125 & 0.698 & 0.667 \\ 
LightSkyBlue        & 0.529 & 0.808 & 0.980 & LightSkyBlue1       & 0.690 & 0.886 & 1.000 \\ 
LightSkyBlue2       & 0.643 & 0.827 & 0.933 & LightSkyBlue3       & 0.553 & 0.714 & 0.804 \\ 
LightSkyBlue4       & 0.376 & 0.482 & 0.545 & LightSlateBlue      & 0.518 & 0.439 & 1.000 \\ 
LightSlateGrey      & 0.467 & 0.533 & 0.600 & LightSteelBlue      & 0.690 & 0.769 & 0.871 \\ 
LightSteelBlue1     & 0.792 & 0.882 & 1.000 & LightSteelBlue2     & 0.737 & 0.824 & 0.933 \\ 
LightSteelBlue3     & 0.635 & 0.710 & 0.804 & LightSteelBlue4     & 0.431 & 0.482 & 0.545 \\ 
LightYellow         & 1.000 & 1.000 & 0.878 & LightYellow1        & 1.000 & 1.000 & 0.878 \\ 
LightYellow2        & 0.933 & 0.933 & 0.820 & LightYellow3        & 0.804 & 0.804 & 0.706 \\ 
LightYellow4        & 0.545 & 0.545 & 0.478 & LimeGreen           & 0.196 & 0.804 & 0.196 \\ 
Linen               & 0.980 & 0.941 & 0.902 & Magenta             & 1.000 & 0.000 & 1.000 \\ 
Magenta1            & 1.000 & 0.000 & 1.000 & Magenta2            & 0.933 & 0.000 & 0.933 \\ 
Magenta3            & 0.804 & 0.000 & 0.804 & Magenta4            & 0.545 & 0.000 & 0.545 \\ 
Maroon              & 0.690 & 0.188 & 0.376 & Maroon1             & 1.000 & 0.204 & 0.702 \\ 
Maroon2             & 0.933 & 0.188 & 0.655 & Maroon3             & 0.804 & 0.161 & 0.565 \\ 
Maroon4             & 0.545 & 0.110 & 0.384 & MediumAquamarine    & 0.400 & 0.804 & 0.667 \\ 
MediumBlue          & 0.000 & 0.000 & 0.804 & MediumOrchid        & 0.729 & 0.333 & 0.827 \\ 
MediumOrchid1       & 0.878 & 0.400 & 1.000 & MediumOrchid2       & 0.820 & 0.373 & 0.933 \\ 
MediumOrchid3       & 0.706 & 0.322 & 0.804 & MediumOrchid4       & 0.478 & 0.216 & 0.545 \\ 
MediumPurple        & 0.576 & 0.439 & 0.859 & MediumPurple1       & 0.671 & 0.510 & 1.000 \\ 
MediumPurple2       & 0.624 & 0.475 & 0.933 & MediumPurple3       & 0.537 & 0.408 & 0.804 \\ 
MediumPurple4       & 0.365 & 0.278 & 0.545 & MediumSeaGreen      & 0.235 & 0.702 & 0.443 \\ 
MediumSlateBlue     & 0.482 & 0.408 & 0.933 & MediumSpringGreen   & 0.000 & 0.980 & 0.604 \\ 
MediumTurquoise     & 0.282 & 0.820 & 0.800 & MediumVioletRed     & 0.780 & 0.082 & 0.522 \\ 
MidnightBlue        & 0.098 & 0.098 & 0.439 & MintCream           & 0.961 & 1.000 & 0.980 \\ 
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\multicolumn{8}{|c|}{{\large Standard Colour Set}} \\ \hline
\multicolumn{1}{|c|}{Name} & \multicolumn{1}{|c|}{R} & \multicolumn{1}{c|}{G} &
\multicolumn{1}{c|}{B} & \multicolumn{1}{|c|}{Name} & \multicolumn{1}{c|}{R} &
\multicolumn{1}{c|}{G} & \multicolumn{1}{c|}{B}  \\ \hline
MistyRose           & 1.000 & 0.894 & 0.882 & MistyRose1          & 1.000 & 0.894 & 0.882 \\ 
MistyRose2          & 0.933 & 0.835 & 0.824 & MistyRose3          & 0.804 & 0.718 & 0.710 \\ 
MistyRose4          & 0.545 & 0.490 & 0.482 & Moccasin            & 1.000 & 0.894 & 0.710 \\ 
NavajoWhite         & 1.000 & 0.871 & 0.678 & NavajoWhite1        & 1.000 & 0.871 & 0.678 \\ 
NavajoWhite2        & 0.933 & 0.812 & 0.631 & NavajoWhite3        & 0.804 & 0.702 & 0.545 \\ 
NavajoWhite4        & 0.545 & 0.475 & 0.369 & Navy                & 0.000 & 0.000 & 0.502 \\ 
NavyBlue            & 0.000 & 0.000 & 0.502 & OldLace             & 0.992 & 0.961 & 0.902 \\ 
OliveDrab           & 0.420 & 0.557 & 0.137 & OliveDrab1          & 0.753 & 1.000 & 0.243 \\ 
OliveDrab2          & 0.702 & 0.933 & 0.227 & OliveDrab3          & 0.604 & 0.804 & 0.196 \\ 
OliveDrab4          & 0.412 & 0.545 & 0.133 & Orange              & 1.000 & 0.647 & 0.000 \\ 
Orange1             & 1.000 & 0.647 & 0.000 & Orange2             & 0.933 & 0.604 & 0.000 \\ 
Orange3             & 0.804 & 0.522 & 0.000 & Orange4             & 0.545 & 0.353 & 0.000 \\ 
OrangeRed           & 1.000 & 0.271 & 0.000 & OrangeRed1          & 1.000 & 0.271 & 0.000 \\ 
OrangeRed2          & 0.933 & 0.251 & 0.000 & OrangeRed3          & 0.804 & 0.216 & 0.000 \\ 
OrangeRed4          & 0.545 & 0.145 & 0.000 & Orchid              & 0.855 & 0.439 & 0.839 \\ 
Orchid1             & 1.000 & 0.514 & 0.980 & Orchid2             & 0.933 & 0.478 & 0.914 \\ 
Orchid3             & 0.804 & 0.412 & 0.788 & Orchid4             & 0.545 & 0.278 & 0.537 \\ 
PaleGoldenrod       & 0.933 & 0.910 & 0.667 & PaleGreen           & 0.596 & 0.984 & 0.596 \\ 
PaleGreen1          & 0.604 & 1.000 & 0.604 & PaleGreen2          & 0.565 & 0.933 & 0.565 \\ 
PaleGreen3          & 0.486 & 0.804 & 0.486 & PaleGreen4          & 0.329 & 0.545 & 0.329 \\ 
PaleTurquoise       & 0.686 & 0.933 & 0.933 & PaleTurquoise1      & 0.733 & 1.000 & 1.000 \\ 
PaleTurquoise2      & 0.682 & 0.933 & 0.933 & PaleTurquoise3      & 0.588 & 0.804 & 0.804 \\ 
PaleTurquoise4      & 0.400 & 0.545 & 0.545 & PaleVioletRed       & 0.859 & 0.439 & 0.576 \\ 
PaleVioletRed1      & 1.000 & 0.510 & 0.671 & PaleVioletRed2      & 0.933 & 0.475 & 0.624 \\ 
PaleVioletRed3      & 0.804 & 0.408 & 0.537 & PaleVioletRed4      & 0.545 & 0.278 & 0.365 \\ 
PapayaWhip          & 1.000 & 0.937 & 0.835 & PeachPuff           & 1.000 & 0.855 & 0.725 \\ 
PeachPuff1          & 1.000 & 0.855 & 0.725 & PeachPuff2          & 0.933 & 0.796 & 0.678 \\ 
PeachPuff3          & 0.804 & 0.686 & 0.584 & PeachPuff4          & 0.545 & 0.467 & 0.396 \\ 
Peru                & 0.804 & 0.522 & 0.247 & Pink                & 1.000 & 0.753 & 0.796 \\ 
Pink1               & 1.000 & 0.710 & 0.773 & Pink2               & 0.933 & 0.663 & 0.722 \\ 
Pink3               & 0.804 & 0.569 & 0.620 & Pink4               & 0.545 & 0.388 & 0.424 \\ 
Plum                & 0.867 & 0.627 & 0.867 & Plum1               & 1.000 & 0.733 & 1.000 \\ 
Plum2               & 0.933 & 0.682 & 0.933 & Plum3               & 0.804 & 0.588 & 0.804 \\ 
Plum4               & 0.545 & 0.400 & 0.545 & PowderBlue          & 0.690 & 0.878 & 0.902 \\ 
Purple              & 0.627 & 0.125 & 0.941 & Purple1             & 0.608 & 0.188 & 1.000 \\ 
Purple2             & 0.569 & 0.173 & 0.933 & Purple3             & 0.490 & 0.149 & 0.804 \\ 
Purple4             & 0.333 & 0.102 & 0.545 & Red                 & 1.000 & 0.000 & 0.000 \\ 
Red1                & 1.000 & 0.000 & 0.000 & Red2                & 0.933 & 0.000 & 0.000 \\ 
Red3                & 0.804 & 0.000 & 0.000 & Red4                & 0.545 & 0.000 & 0.000 \\ 
RosyBrown           & 0.737 & 0.561 & 0.561 & RosyBrown1          & 1.000 & 0.757 & 0.757 \\ 
RosyBrown2          & 0.933 & 0.706 & 0.706 & RosyBrown3          & 0.804 & 0.608 & 0.608 \\ 
RosyBrown4          & 0.545 & 0.412 & 0.412 & RoyalBlue           & 0.255 & 0.412 & 0.882 \\ 
RoyalBlue1          & 0.282 & 0.463 & 1.000 & RoyalBlue2          & 0.263 & 0.431 & 0.933 \\ 
RoyalBlue3          & 0.227 & 0.373 & 0.804 & RoyalBlue4          & 0.153 & 0.251 & 0.545 \\ 
SaddleBrown         & 0.545 & 0.271 & 0.075 & Salmon              & 0.980 & 0.502 & 0.447 \\ 
Salmon1             & 1.000 & 0.549 & 0.412 & Salmon2             & 0.933 & 0.510 & 0.384 \\ 
Salmon3             & 0.804 & 0.439 & 0.329 & Salmon4             & 0.545 & 0.298 & 0.224 \\ 
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\multicolumn{8}{|c|}{{\large Standard Colour Set}} \\ \hline
\multicolumn{1}{|c|}{Name} & \multicolumn{1}{|c|}{R} & \multicolumn{1}{c|}{G} &
\multicolumn{1}{c|}{B} & \multicolumn{1}{|c|}{Name} & \multicolumn{1}{c|}{R} &
\multicolumn{1}{c|}{G} & \multicolumn{1}{c|}{B}  \\ \hline
SandyBrown          & 0.957 & 0.643 & 0.376 & SeaGreen            & 0.180 & 0.545 & 0.341 \\ 
SeaGreen1           & 0.329 & 1.000 & 0.624 & SeaGreen2           & 0.306 & 0.933 & 0.580 \\ 
SeaGreen3           & 0.263 & 0.804 & 0.502 & SeaGreen4           & 0.180 & 0.545 & 0.341 \\ 
Seashell            & 1.000 & 0.961 & 0.933 & Seashell1           & 1.000 & 0.961 & 0.933 \\ 
Seashell2           & 0.933 & 0.898 & 0.871 & Seashell3           & 0.804 & 0.773 & 0.749 \\ 
Seashell4           & 0.545 & 0.525 & 0.510 & Sienna              & 0.627 & 0.322 & 0.176 \\ 
Sienna1             & 1.000 & 0.510 & 0.278 & Sienna2             & 0.933 & 0.475 & 0.259 \\ 
Sienna3             & 0.804 & 0.408 & 0.224 & Sienna4             & 0.545 & 0.278 & 0.149 \\ 
SkyBlue             & 0.529 & 0.808 & 0.922 & SkyBlue1            & 0.529 & 0.808 & 1.000 \\ 
SkyBlue2            & 0.494 & 0.753 & 0.933 & SkyBlue3            & 0.424 & 0.651 & 0.804 \\ 
SkyBlue4            & 0.290 & 0.439 & 0.545 & SlateBlue           & 0.416 & 0.353 & 0.804 \\ 
SlateBlue1          & 0.514 & 0.435 & 1.000 & SlateBlue2          & 0.478 & 0.404 & 0.933 \\ 
SlateBlue3          & 0.412 & 0.349 & 0.804 & SlateBlue4          & 0.278 & 0.235 & 0.545 \\ 
SlateGrey           & 0.439 & 0.502 & 0.565 & SlateGrey1          & 0.776 & 0.886 & 1.000 \\ 
SlateGrey2          & 0.725 & 0.827 & 0.933 & SlateGrey3          & 0.624 & 0.714 & 0.804 \\ 
SlateGrey4          & 0.424 & 0.482 & 0.545 & Snow                & 1.000 & 0.980 & 0.980 \\ 
Snow1               & 1.000 & 0.980 & 0.980 & Snow2               & 0.933 & 0.914 & 0.914 \\ 
Snow3               & 0.804 & 0.788 & 0.788 & Snow4               & 0.545 & 0.537 & 0.537 \\ 
SpringGreen         & 0.000 & 1.000 & 0.498 & SpringGreen1        & 0.000 & 1.000 & 0.498 \\ 
SpringGreen2        & 0.000 & 0.933 & 0.463 & SpringGreen3        & 0.000 & 0.804 & 0.400 \\ 
SpringGreen4        & 0.000 & 0.545 & 0.271 & SteelBlue           & 0.275 & 0.510 & 0.706 \\ 
SteelBlue1          & 0.388 & 0.722 & 1.000 & SteelBlue2          & 0.361 & 0.675 & 0.933 \\ 
SteelBlue3          & 0.310 & 0.580 & 0.804 & SteelBlue4          & 0.212 & 0.392 & 0.545 \\ 
Tan                 & 0.824 & 0.706 & 0.549 & Tan1                & 1.000 & 0.647 & 0.310 \\ 
Tan2                & 0.933 & 0.604 & 0.286 & Tan3                & 0.804 & 0.522 & 0.247 \\ 
Tan4                & 0.545 & 0.353 & 0.169 & Thistle             & 0.847 & 0.749 & 0.847 \\ 
Thistle1            & 1.000 & 0.882 & 1.000 & Thistle2            & 0.933 & 0.824 & 0.933 \\ 
Thistle3            & 0.804 & 0.710 & 0.804 & Thistle4            & 0.545 & 0.482 & 0.545 \\ 
Tomato              & 1.000 & 0.388 & 0.278 & Tomato1             & 1.000 & 0.388 & 0.278 \\ 
Tomato2             & 0.933 & 0.361 & 0.259 & Tomato3             & 0.804 & 0.310 & 0.224 \\ 
Tomato4             & 0.545 & 0.212 & 0.149 & Turquoise           & 0.251 & 0.878 & 0.816 \\ 
Turquoise1          & 0.000 & 0.961 & 1.000 & Turquoise2          & 0.000 & 0.898 & 0.933 \\ 
Turquoise3          & 0.000 & 0.773 & 0.804 & Turquoise4          & 0.000 & 0.525 & 0.545 \\ 
Violet              & 0.933 & 0.510 & 0.933 & VioletRed           & 0.816 & 0.125 & 0.565 \\ 
VioletRed1          & 1.000 & 0.243 & 0.588 & VioletRed2          & 0.933 & 0.227 & 0.549 \\ 
VioletRed3          & 0.804 & 0.196 & 0.471 & VioletRed4          & 0.545 & 0.133 & 0.322 \\ 
Vory1               & 1.000 & 1.000 & 0.941 & Wheat               & 0.961 & 0.871 & 0.702 \\ 
Wheat1              & 1.000 & 0.906 & 0.729 & Wheat2              & 0.933 & 0.847 & 0.682 \\ 
Wheat3              & 0.804 & 0.729 & 0.588 & Wheat4              & 0.545 & 0.494 & 0.400 \\ 
White               & 1.000 & 1.000 & 1.000 & WhiteSmoke          & 0.961 & 0.961 & 0.961 \\ 
Yellow              & 1.000 & 1.000 & 0.000 & Yellow1             & 1.000 & 1.000 & 0.000 \\ 
Yellow2             & 0.933 & 0.933 & 0.000 & Yellow3             & 0.804 & 0.804 & 0.000 \\ 
Yellow4             & 0.545 & 0.545 & 0.000 & YellowGreen         & 0.604 & 0.804 & 0.196 \\ 
\hline
\end{tabular}
\end{center}

\newpage
\section{\xlabel{ap_NDFformat}Standard Components in an NDF
\label{ap:NDFformat}}

An NDF comprises a main data array plus a collection of objects drawn
from a set of standard items and extensions (see \xref{SUN/33}{sun33}{}).
Only the main data array must be present; all the other items are optional.

{\tt \$KAPPA\_DIR/example.sdf} is an NDF which contains all the standard 
NDF components, except a FITS extension; it also has a FIGARO extension.
The structure of the file (as revealed by
{\tt \%~hdstrace \$KAPPA\_DIR/example}) is shown below.  The layout is
\begin{verbatim} 
     NAME(dimensions)    <TYPE>     VALUE(S)
\end{verbatim}
Note that scalar objects have no dimensions and that each level down the
hierarchy is indented.

\begin{verbatim}

   EXAMPLE  <NDF>

      DATA_ARRAY(856)  <_REAL>       *,0.2284551,-2.040089,45.84504,56.47374,
                                     ... 746.2602,820.8976,570.0729,*,449.574
      TITLE          <_CHAR*30>      'HR6259 - AAT fibre data'
      LABEL          <_CHAR*20>      'Flux'
      UNITS          <_CHAR*20>      'Counts/s'
      QUALITY        <QUALITY>       {structure}
         BADBITS        <_UBYTE>        1
         QUALITY(856)   <_UBYTE>        1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,
                                     ... 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0

      VARIANCE(856)  <_REAL>         2.1,0.1713413,1.5301,34.38378,42.35531,
                                     ... 615.6732,427.5547,353.9127,337.1805
      AXIS(1)        <AXIS>          {structure}

      Contents of AXIS(1)
         DATA_ARRAY(856)  <_REAL>       3847.142,3847.672,3848.201,3848.731,
                                        ... 4298.309,4298.838,4299.368,4299.897
         LABEL          <_CHAR*20>      'Wavelength'
         UNITS          <_CHAR*20>      'Angstroms'

      HISTORY        <HISTORY>       {structure}
         CREATED        <_CHAR*30>      '1990-DEC-12 08:21:02.324'
         CURRENT_RECORD  <_INTEGER>     3
         RECORDS(10)    <HIST_REC>      {array of structures}

         Contents of RECORDS(1)
            TEXT           <_CHAR*40>      'Extracted spectrum from fibre data.'
            DATE           <_CHAR*25>      '1990-DEC-19 08:43:03.08'
            COMMAND        <_CHAR*30>      'FIGARO V2.4 FINDSP command'


      MORE           <EXT>           {structure}
         FIGARO         <EXT>           {structure}
            TIME           <_REAL>         1275
            SECZ           <_REAL>         2.13

   End of Trace.
\end{verbatim}

Of course, this is only an example format.  There are various ways of
representing some of the components.  These {\em variants\/} are
described in \xref{SGP/38}{sgp38}{}, but not all are currently supported.

The components are considered in detail below.  The names (in bold
typeface) are significant as they are used by the NDF access routines to
identify the components. 

\begin{description}

\item[{\bf DATA}] -- the main data array is the only component which
must be present in an NDF.  In the case of {\tt example.sdf}, the data
component is a 1-dimensional array of real type with 856 elements.  It
can have up to seven dimensions.  It is particularly referenced via
parameter names IN, OUT, and NDF.

\label{apndf:title}
\item[{\bf TITLE}] -- the  character string {\tt "HR6259 - AAT fibre
data"} describes the contents of the NDF.  The NDF's TITLE might be
used as the title of a graph {\it etc.} It may be set with task
\htmlref{SETTITLE}{SETTITLE}.  Applications that create an NDF assign
a TITLE to the NDF via a parameter, called TITLE unless the
application generates several NDFs.

\label{apndf:label}
\item[{\bf LABEL}] -- the character string {\tt "Flux"} describes the
quantity represented in the NDF's main data array.  The LABEL is
intended for use on the axis of graphs {\it etc.}  It may be set
using the task \htmlref{SETLABEL}{SETLABEL}.

\label{apndf:units}
\item[{\bf UNITS}] -- this character string describes the physical units
of the quantity stored in the main data array, in this case,
{\tt "Counts/s"}.  It may be set via the command
\htmlref{SETUNITS}{SETUNITS}.  

\label{apndf:quality}
\item[{\bf QUALITY}] -- this component is used to indicate the quality
of each element in the main data array, for example whether each pixel
is vignetted or not.  The quality structure contains a quality array
and a BADBITS value, both of which {\em must\/} be of type
\htmlref{\_UBYTE}{ap:HDStypes}.  The quality array has the same shape
and size as the main data array and is used in conjunction with the
BADBITS value to decide the quality of a pixel in the main data array.
In {\tt example.sdf} the BADBITS component has value 1.  This means
that a value of 1 in the quality array indicates a bad pixel in the
main data array, whereas any other value indicates that the associated
pixel is good.  (Note that the pixel is bad if the bit-wise comparison
QUALITY {\tt{"}}AND{\tt{"}} BADBITS is non-zero).  The meanings of the
QUALITY bits are arbitrary.  See the command \htmlref{SETBB}{SETBB}.
To enter new quality information, use the SETQUAL command of \IRASref\@.

\label{apndf:variance}
\item[{\bf VARIANCE}] -- the variance array is the same shape and size
as the main data array and contains the errors associated with the
individual data values.  These are stored as {\em variance\/}
estimates for each pixel.  VARIANCE may be set with the
\htmlref{SETVAR}{SETVAR} command.

\label{apndf:axis}
\item[{\bf AXIS}] -- the AXIS structure may contain axis information for
any dimension of the NDF's main array.  In this case, the main data
array is only 1-dimensional, therefore only the AXIS(1) structure is
present.  This structure contains the actual axis data array of pixel
centres, and also label and units information.  {\footnotesize KAPPA} uses the
label and units for axis annotations.  Not shown in this example
are optional array components for storing pixel widths and the variance
of the axis centres.  All axes or none must be present.  Use 
\htmlref{SETAXIS}{SETAXIS} to set the values of an AXIS array component;
\htmlref{AXLABEL}{AXLABEL} and \htmlref{AXUNITS}{AXUNITS} to set an
axis LABEL or UNITS component; and \htmlref{SETNORM}{SETNORM} to set
an axis normalisation flag.

\label{apndf:history}
\item[{\bf HISTORY}] -- this component provides a record of the
processing history of the NDF.  Only the first of three records is
shown for {\tt example.sdf}.  This indicates that the spectrum was
extracted from fibre data using the {\footnotesize FIGARO} FINDSP command on
1990 December 19. The history recording level is set by task 
\htmlref{HISSET}{HISSET}.  This task also allows you to switch off
history recording or delete the history records.
\htmlref{HISLIST}{HISLIST} lists an NDF's history.  You can add
commentary with \htmlref{HISCOM}{HISCOM}.

\label{apndf:extensions}
\item[{\bf EXTENSIONs}] -- the purpose of extensions is to store
non-standard items.  These auxiliary data could be information about
the original observing setup, such as the airmass during the
observation or the temperature of the detector; they may be
calibration data or results produced during processing of the data
array, {\it{e.g.}}\ spectral-line fits. {\tt example.sdf} began life as
a \Figaroref\ file.  It was converted to an NDF using the command
\xref{DST2NDF}{sun55}{DST2NDF}\latexonly{ (see SUN/55)}.  It contains
values for the airmass and exposure time associated with the
observations.  These are stored in the FIGARO extension, and the
intention is that the {\footnotesize FIGARO} applications which use these
values will know where to find them.  Task \htmlref{SETEXT}{SETEXT}
lets the contents of extensions be listed, created, deleted, renamed
and assigned new values.

One extension that is used by {\footnotesize KAPPA} is the
\htmlref{FITS extension}{se:fitsairlock}.  This holds the FITS headers
as an array of 80-character elements, {\it{i.e.}}\ one FITS card image
per array element.  You can extract the values of ancillary items from
the FITS extension to a non-standard extension via
\htmlref{FITSIMP}{FITSIMP}.  Use \htmlref{FITSEXP}{FITSEXP} to do the
reverse operation.  The extension can be listed via the command
\htmlref{FITSLIST}{FITSLIST}.  \htmlref{FITSEDIT}{FITSEDIT} allows you
to edit the headers prior to export of the dataset to another format
such as \FITSref\ or \IRAFref\@.

\end{description}

\newpage
\section{\xlabel{ap_IMAGEformat}IMAGE data format\label{ap:IMAGEformat}}

The IMAGE format as used by some of {\footnotesize KAPPA} is a simple
\HDSref\ structure, comprising a floating-point data array, a character title,
and the maximum and minimum data values.  It is variant of the
original Wright-Giddings IMAGE structure.  There are others is use
that contain more items. An example structure is shown schematically
below using the \HDSTRACEref\latexonly{ (SUN/102)} notation; see
\latexelsehtml{Appendix~\ref{ap:NDFformat}}{\htmlref{the NDF format}{ap:NDFformat}}.
\begin{verbatim}
   HORSEHEAD  <IMAGE>

      DATA_ARRAY(384,512)  <_REAL>   100.5,102.6,110.1,109.8,105.3,107.6,
                                     ... 123.1,117.3,119,120.5,127.3,108.4
      TITLE          <_CHAR*72>      'KAPPA - Flip'
      DATA_MIN       <_REAL>         28.513
      DATA_MAX       <_REAL>         255.94

   End of Trace.
\end{verbatim}

Currently, the DATA\_ARRAY may have up to seven dimensions.
IMAGE structures are associated with parameters like INPIC and OUTPIC.
The TITLE object of new IMAGE structures takes the value of the
parameter OTITLE.  DATA\_MIN and DATA\_MAX are now ignored.

The IMAGE format is not too dissimilar from a {\em primitive\/} NDF
with no extensions.  Indeed if it did not have DATA\_MAX and DATA\_MIN
it would be a {\it bona fide}\ NDF.  Thus applications that handle the
IMAGE format can follow the rules of \xref{SGP/38}{sgp38}{} and
process it like an NDF.  In effect this means that all extensions are
propagated to output files, and a quality array is propagated where
the processing does not invalidate its values.  IMAGE applications
also handle most {\em simple\/} NDFs correctly (those where the data
array is an array of numbers at the second level of the hierarchical
structure).  This similarity in formats enables NDF and IMAGE
applications to work in co-operation, and the conversion within
{\footnotesize KAPPA} can be undertaken piecemeal.  Note that the primitive
variant is no longer the norm for NDFs, since for example, it does not
support origin information.

\section{\xlabel{ap_HDStypes}Supported HDS Data Types\label{ap:HDStypes}}

{\footnotesize KAPPA} applications can process \NDFref{NDFs}\ in one or more of the
following \HDSref\ data types.  The correspondence between Fortran types
and HDS data types is as follows:

\begin{center}
\begin{tabular}{|l|c|l|} \hline
{\bf HDS Type} & {\bf Number of bytes} & {\bf FORTRAN Type}\\ \hline
\_DOUBLE & 8 & DOUBLE PRECISION \\
\_INTEGER & 4 & INTEGER \\
\_REAL & 8 & REAL \\
\_UBYTE & 1 & BYTE \\
\_BYTE & 1 & BYTE \\
\_UWORD & 2 & INTEGER*2 \\
\_WORD & 2 & INTEGER*2\\
\hline
\end{tabular}
\end{center}

(\_UBYTE) and (\_UWORD) types are unsigned and so permit data ranges of
0--255 and 0--65535 respectively.

\newpage
\begin{htmlonly}
\section{Release Notes---V0.8-5}

\subsection{Global changes}

\begin{itemize}
  \item  All the graphics modules are linked shareably with AGI, thus any
    bug fixes in AGI can be applied without relinking KAPPA.
 
  \item  Applications using axis data will now detect if the axis centres
    are not monotonic, and then take appropriate action, {\it{e.g.}}
    DISPLAY would use world co-ordinates for axes, and NDFTRACE
    would flag the axis as being non-monotonic.

  \item  Typographical errors in the online documentation are corrected.

\end{itemize}

\subsection{New applications}

Although only an incremental release, the following applications
have been made available because they are simple to use, and AXCONV
is needed urgently.

\begin{description}

  \item [\htmlref{AXCONV}{AXCONV}] Expands spaced axes in an NDF into the primitive form.
    (This is to enable certain \ASTERIXref\ files to be useable in
    {\footnotesize KAPPA}).

  \item [\htmlref{FITSDIN}{FITSDIN}]  The disc-FITS reader is now available on UNIX
     platforms.

  \item [\htmlref{FITSIN}{FITSIN}]  The FITS reader is now available on UNIX platforms.

  \item [\htmlref{MEM2D}{MEM2D}]  The maximum-entropy deconvolution application is
     available under SunOS.

  \item [\htmlref{NOGLOBALS}{NOGLOBALS}]  Resets the KAPPA global parameters.

  \item [\htmlref{OVCLEAR}{OVCLEAR}]  Clears the current image-overlay device (without
    purging the graphics database), with an option to clear just the
    current picture.

\end{description}

\subsection{Modified applications}
\begin{itemize}
  \item  CLEANER writes the bad-pixel flag.

  \item  \htmlref{CONTOUR}{CONTOUR}, \htmlref{DISPLAY}{DISPLAY},
    \htmlref{GREYPLOT}{GREYPLOT}, \htmlref{TURBOCONT}{TURBOCONT}
    each has a new FILL parameter that permits the maximum-sized
    plots, ignoring any restriction on square pixels.  It is useful
    for displaying images with markedly different dimensions, such as
    spectra.  FILL defaults to {\tt FALSE}.

  \item  CONTOUR and TURBOCONT each has a new BORDER parameter that
    defaults to {\tt TRUE}.  When it and parameter AXES are both
    {\tt FALSE}, no border is drawn around the contour plot.

  \item  DISPLAY and GREYPLOT pass the linear scaling limits to global
    parameters that \linebreak LUTVIEW now uses by default.  (Note that
    this is a temporary arrangement, since a means to pass non-linear
    scalings is planned.) 

  \item  GAUSS permits arbitrarily oriented elliptical Gaussian
    smoothing kernels, though the default still produces a
    circularly symmetric filter.

  \item  \htmlref{INSPECT}{INSPECT} uses the existing palette for
    its annotations in cursor mode.

  \item  \htmlref{MLINPLOT}{MLINPLOT} has a KEY parameter so that the
    key may be removed.  It is defaulted to {\tt TRUE}, giving the
    previous behaviour.

  \item  \htmlref{PALENTRY}{PALENTRY} works for X-window overlays, so
    that it is now possible to alter the foreground colour of the overlay. 

  \item  \htmlref{PICLIST}{PICLIST} indicates the current picture.

  \item  \htmlref{SNAPSHOT}{SNAPSHOT} parameter ODEVICE suggested default is the current
    value of the parameter.  There is a new defaulted parameter
    PLANES that enables snapshots of just the base or the overlay
    for X-windows.  The default behaviour is to capture all planes,
    as happened prior to this version.

  \item  \htmlref{SURFIT}{SURFIT} has more constraints to prevent problems with small
    arrays.  It obtains the value of ESTIMATOR after the number of
    values in a bin is known, its default becomes the median when
    the bins comprise less than 12 pixels.

\end{itemize}

\newpage
\section{Release Notes---V0.9}

The main features of this release are 39 new tasks, 6 new synonym
commands for easier use, 15 applications converted to use NDFs, numerous
enhancements to existing facilities, and updated documentation for UNIX
and hypertext.

\subsection{Global changes}
\begin{itemize}
  \item  There is no longer support for VMS.
  
  \item  The KAPPA monolith has been fragmented.  It now comprises three
    monoliths: KAPVIEW for the graphics tasks; NDFPACK for the NDF and
    HDS-object manipulations; and KAPPA itself for the remainder (mostly
    image-processing tasks).  All are linked statically for efficiency
    and ease of export.

  \item In addition to describing new and revised commands in the
    Appendices, the documentation has been greatly expanded featuring
    more examples and tutorials; new sections on C-shell scripts, NDF
    history, masking, the FITS airlock, composite hardcopy plots, and
    various aspects of parameters including a summary of usage; and
    numerous improved sections and figures.  VMS references have been
    removed.  Many of the examples show usage from the shell,
    including how to escape metacharacters.  The reference section has
    been standardised for the non-IMAGE tasks.  This includes Usage
    and Examples sections with lowercase invocations, a
    more-consistent notation, and a Related Applications section.
    Applications released in version 0.8-4U are documented in paper
    form for the first time. Several typographical and editing errors
    have been corrected in the documentation.

  \item There is hypertext documentation based upon this SUN.  It
  includes links to other documents and more graphics.

  \item  KAPHELP and its navigational aids work from within \ICLref\@.

  \item  There are improved instructions for cursor interactions,
    including using the mouse for X-window devices.

  \item  There is more-efficient workspace management scheme available
    in most tasks where work arrays are needed.

  \item  COMP parameters that obtain array components, now support a
    value of {\tt "Error"} to use the square root of the variance.

  \item  The TITLE parameters largely default to null, so that an
    output NDF inherits the title of the input NDF.  Previously, the
    TITLE defaulted to {\tt "KAPPA - }{\it $<$task$>$}{\tt{"}}.

  \item  An improved algorithm is used to test whether axes are linear
    or not.

  \item  The bad-pixel flag is now updated throughout {\footnotesize KAPPA}.
    Previously, it had been commented out so that it would not cause
    problems with the IMAGE-format tasks.

  \item  A dot in the foreground colour that could appear at 
    co-ordinate (0,0) in plots with axes has been removed.

\end{itemize}

\subsection{New applications}
\begin{description}
  \item [\htmlref{ARDGEN}{ARDGEN}]  Creates a text file which describes
    selected regions of a previously displayed image.  The file is
    in the ARD format.  There are eleven shapes, and 5 logical
    operators for combining regions.  There are options, amongst others,
    for listing and deleting regions .

  \item [\htmlref{ARDMASK}{ARDMASK}]  Uses an ARD file, such as created
    by ARDGEN, to define regions of an NDF to mask.  It therefore
    allows regions to be excluded from subsequent data processing.

  \item [\htmlref{AXLABEL}{AXLABEL}]  Sets a new label component for an
    NDF axis.

  \item [\htmlref{AXUNITS}{AXUNITS}]  Sets a new units component for an
    NDF axis.

  \item [\htmlref{CALC}{CALC}]  Evaluates a Fortran-like mathematical
    expression.  This is particularly useful for non-integer
    arithmetic in C-shell scripts.  Parameters and sub-expressions are
    supported.

  \item [\htmlref{CALPOL}{CALPOL}]  Calculates various polarisation
    parameters from four NDFs containing intensity arrays analysed at
    0\dgs, 45\dgs, 90\dgs, and 135\dgs\ from a reference direction.
    The resultant parameters are stored in NDFs, of which there are
    eight possible.

  \item [\htmlref{CONVOLVE}{CONVOLVE}]  Convolves a 1- or 2-dimensional
    NDF using a function given by a second NDF.  The latter is
    normally a point-spread function, and need not have the same
    dimensions as the convolved NDF.

  \item [\htmlref{ELPROF}{ELPROF}]  Creates a radial or azimuthal
    profile of a 2-dimensional NDF.  It bins the image into elliptical
    annuli or a fan-shaped region of adjacent sectors.  The mean
    values in each bin are stored in a 1-dimensional output NDF.
    There are options to restrict the range of radial distance and
    azimuthal angle.

  \item [\htmlref{ERRCLIP}{ERRCLIP}]  Removes pixels with errors larger
    than some limit from an NDF by flagging them with the bad value in
    a copy NDF.

  \item [\htmlref{FILLBAD}{FILLBAD}]  Removes regions of bad values
    from a 2-dimensional NDF using a smooth function which matches the
    surrounding data.

  \item [\htmlref{FITSEXP}{FITSEXP}]  Moves ancillary data within an
    NDF from arbitrary extensions to the FITS extension.  It uses a
    keyword translation table held in a text file.

  \item [\htmlref{FITSTEXT}{FITSTEXT}]  Creates an NDF FITS extension
   from a text file.  There is limited validation of the headers.

  \item [\htmlref{FITSURFACE}{FITSURFACE}]  Fits a polynomial surface
    to 2-dimensional NDF.  It does not bin the data and does not reject
    outliers to the fit. It stores the result within a POLYNOMIAL data
    structure called SURFACEFIT within the fitted NDF.

  \item [\htmlref{HISCOM}{HISCOM}]  Adds commentary to the history
    records of an NDF.  The text can be supplied through a parameter, or
    a text file.  There is optional control of paragraph wrapping.

  \item [\htmlref{HISLIST}{HISLIST}]  Lists all the history records in
    an NDF.  The output comprises the date, time, and application name, and
    optionally the history text.

  \item [\htmlref{HISSET}{HISSET}]  Sets the history update mode for
    an NDF.  History recording disabled, or set to record at three
    levels of verbosity.  Existing history records can also be deleted.

  \item [\htmlref{LUCY}{LUCY}]  Performs a Lucy-Richardson deconvolution
    of a 1- or 2-dimensional NDF with a point-spread function.  It uses an
    iterative process to provide new estimates of the restored,
    higher-resolution array.  The closeness of the fit is determined
    by the $\chi^2$ per pixel.  The algorithm terminates
    after a specified number of iterations or when a certain
    $\chi^2$ has been reached.  The Synder correction may be applied
    using the data variance.

  \item [\htmlref{MAKESURFACE}{MAKESURFACE}]  Creates a 2-dimensional NDF
    from the coefficients of a polynomial surface stored in a SURFACEFIT
    extension (as written by FITSURFACE).  The size and extent of the
    output NDF may be defined by a template NDF.

  \item [\htmlref{PARGET}{PARGET}]  Obtains the value or values of an
    application parameter.  This offers an easier way to pass parameter
    values between tasks in shell scripts.

  \item [\htmlref{PASTE}{PASTE}]  Pastes a series of NDFs upon each
    other.  Bad values may be transparent or opaque.  The NDFs need not
    be congruent or even have the same number of dimensions.

  \item [\htmlref{PICCUR}{PICCUR}]  Uses a cursor to select the current
    picture and to report the co-ordinates of points.  Some of the
    functions of CURSOR have been moved to this task.

  \item [\htmlref{PICEMPTY}{PICEMPTY}]  Finds the first empty FRAME
    picture in the \htmlref{graphics database}{se:agitate}.  This is
    most useful with PICGRID to fill in a grid of FRAME pictures
    without having to know or compute their labels.

  \item [\htmlref{PICENTIRE}{PICENTIRE}]  Finds the first unobscured
    and unobscuring FRAME picture in the graphics database.  This tries
    to locate a FRAME picture such that additional plotting within it
    will not affect earlier plots.

  \item [\htmlref{PICTRANS}{PICTRANS}]  Transforms co-ordinates between
    the current and BASE pictures of the graphics database.

  \item [\htmlref{PICVIS}{PICVIS}]  Finds the first unobscured FRAME
    picture in the graphics database.

  \item [\htmlref{RIFT}{RIFT}]  Adds a constant to a sub-section of an
    NDF to correct rift-valley defects.

  \item [\htmlref{SETAXIS}{SETAXIS}]  Sets the values for an axis array
    component of an NDF.  There are several options which permit the
    deletion of the axis system, or an individual variance or width
    array; the replacement of one or more individual values; the
    assignment of the whole array using a Fortran-like mathematical
    expression, or from a text file, or set to pixel co-ordinates.

  \item [\htmlref{SETEXT}{SETEXT}]  Manipulates the contents of a
    specified NDF extension. There are six options including erasure,
    renaming, and assignment of extension components.

  \item [\htmlref{SETNORM}{SETNORM}]  Sets one or more of the
    axis-normalisation flags in an NDF.

  \item [\htmlref{SETSKY}{SETSKY}]  Makes an IRAS astrometry extension
    for a 2-dimensional NDF, thereby allowing sky co-ordinate
    information to be stored with an arbitrary image.  The co-ordinate
    system can be defined by suppling explicit projection parameters, or
    by providing a list of sky and corresponding image co-ordinates, or
    a combination of both. This task enables non-IRAS data to use the
    SKY$\lsk$ tasks in the \IRASref\ package which perform
    various astrometric tasks such as plotting axes and overlay grids in
    sky co-ordinates.

  \item [\htmlref{TRANINVERT}{TRANINVERT}]  Inverts a transformation
   stored in a \xref{TRANSFORM}{sun61}{} structure within an existing HDS file.

  \item [\htmlref{TRANJOIN}{TRANJOIN}]  Joins two transformation stored
    in the TRANSFORM format.

  \item [\htmlref{TRANMAKE}{TRANMAKE}]  Makes a transformation structure
    from supplied forward and inverse mappings.  The structure can
    have classification qualifiers and a comment.  The mappings are
    defined using Fortran-like expressions, and may include
    sub-expressions and constants.  There are also preset
    2-dimensional transformations: polar about a supplied origin, and
    the six-term linear which encompasses shift of origin, rotation,
    magnification, and shear.

  \item [\htmlref{TRANSFORMER}{TRANSFORMER}]  Applies a transformation
    stored in TRANSFORM structure to an NDF by resampling.  There is
    full control of the shape, origin, and co-ordinate limits of the
    transformed NDF.  The resampling use either linear interpolation or
    the nearest-neighbour method.

  \item [\htmlref{TRANTRACE}{TRANTRACE}]  Lists the contents of a
    TRANSFORM structure.

  \item [\htmlref{VECPLOT}{VECPLOT}]  Plots a 2-dimensional vector map
    of an NDF.  The vectors may be arrows or lines, and there is
    control of the frequency, justification, and scaling of the
    vectors.  There is an optional key, and many parameters for
    customising the appearance of the plot.

  \item [\htmlref{WIENER}{WIENER}]  Applies a Wiener filter to a 1- or 2-dimensional
    array.  It offers a much faster, but cruder, deconvolution method.
    This task uses a `white' noise model or a power spectrum supplied
    in another NDF.  The noise power is adjustable.
\end{description}

\subsection{New synonyms}
\begin{description}
  \item [\htmlref{PICBASE}{PICBASE}]  Selects the BASE picture from the
    \htmlref{graphics database}{se:agitate}.

  \item [\htmlref{PICDATA}{PICDATA}]  Selects the last DATA picture
    from the graphics database.

  \item [\htmlref{PICFRAME}{PICFRAME}]  Selects the last FRAME picture
    from the graphics database.

  \item [\htmlref{PICGRID}{PICGRID}]  Creates an array of FRAME
    pictures in the graphics database.

  \item [\htmlref{PICLAST}{PICLAST}]  Selects the last picture from the
    graphics database.

  \item [\htmlref{PICXY}{PICXY}] Creates a new FRAME picture defined
    by co-ordinate bounds.
\end{description}

\subsection{New procedures}
\begin{description}
  \item [\htmlref{FITSEDIT}{FITSEDIT}]  Edits the FITS extension of an
    NDF.  You can use your favourite editor to modify the FITS headers
    in a NDF's \htmlref{FITS extension}{se:fitsairlock}.  There is
    limited validation of the edited headers.

  \item [\htmlref{FITSHEAD}{FITSHEAD}]  Lists the primary and extension
    headers of a list of FITS files or a range of FITS files on tape.
\end{description}

\subsection{Withdrawn and renamed applications}

  The following applications have been withdrawn at V0.9.
  \begin{description}
  \item [COMPRESS]  The NDF\_ version of \htmlref{COMPAVE}{COMPAVE}
     provides COMPRESS's functionality.
  \item [NUMBA]  The NDF\_ version of \htmlref{NUMB}{NUMB} provides
     NUMBA's functionality.
  \item [PICK2D]  \htmlref{NDFCOPY}{NDFCOPY} or
     \htmlref{SETBOUND}{SETBOUND} provides the functionality in $n$-d.
  \item [STATS2D]  Use \htmlref{STATS}{STATS} with \latexonly{NDF
     sectioning (see SUN/33.3 Section 16 or Section~\ref{se:ndfsect})}
     \htmlref{NDF sectioning}{se:ndfsect}
     replaces the capability of STATS2D.
  \item [THRESH0]  The THRESH0 operation can easily be performed
     by \htmlref{THRESH}{THRESH}.
  \item [TWEAK]  Use \htmlref{LUTTWEAK}{LUTTWEAK}.
  \end{description}

  To prevent command-name clashes with \Figaroref\ and C-shell
  built-in functions three applications are renamed.  GAUSS has
  become \htmlref{GAUSMOOTH}{GAUSMOOTH}, CLEANER reverts to
  \htmlref{FFCLEAN}{FFCLEAN}, and SHIFT becomes \htmlref{SLIDE}{SLIDE}.

\subsection{Deprecated features}

  The following commands, which do not work for X-window devices,
  will be withdrawn at V0.10.
  \begin{description}
  \item [BLINK]  We hope to provide this facility for X-windows through
    a graphical-user-interface (GUI) image-processing tool.
  \item [IDUNZOOM]  Full control of zoom and unzooming should be a
    feature of the planned GUI tool.
  \item [IDVISIBLE]  This functionality might be part of the planned
    GUI.
  \end{description}

\subsection{Modified applications}  

Here is a summary of the main modifications.  All those converted to use
the NDF\_ library feature much-improved documentation.
\begin{itemize}
  \item  The accuracy of \htmlref{APERADD}{APERADD} has been improved
    by using pixel co-ordinates rather than indices.  There is a warning
    in the documentation concerning its limited algorithm and accuracy.

  \item  \htmlref{BLOCK}{BLOCK} permits smoothing of 1-dimensional
    \NDFref{NDFs}.  There is an ESTIMATOR parameter for forming the
    output values.  At present it may select the mean or median of the
    neighbourhood.  There is no restriction on maximum box size.

  \item  \htmlref{CHPIX}{CHPIX} has been converted to use NDF\_, and works
    in $n$ dimensions.  Sections as well as individual pixels may be
    edited in any of the NDF array components.

  \item  COLUMNAR and HIDE have a THRESH parameter to set lower and upper
    data values to appear in the plot.

  \item  \htmlref{COMPADD}{COMPADD}, \htmlref{COMPAVE}{COMPAVE}, and
    \htmlref{COMPICK}{COMPICK} now use NDF\_ and operate on
    $n$-dimensional NDFs.  They can have different compression factors
    along each axis.  COMPADD and COMPAVE have the WLIM parameter for
    deciding whether each output pixel is good or bad, and permit
    weighting using the variance information of the input NDF.

  \item \htmlref{CONTOVER}{CONTOVER} works with MATRIX\_PRINTER class
    devices so can be used to generate composite hardcopy plots.

  \item  \htmlref{CURSOR}{CURSOR} no longer changes the current picture
    in the graphics database.  The chosen points may be marked with
    crosses or joined by polygon lines in a nominated colour.  Points
    are also erasable.  The final set of co-ordinates and the number
    of points are written to output parameters XP, YP, and NUMBER
    respectively.

  \item  \htmlref{FFCLEAN}{FFCLEAN} (formerly CLEANER) can filter
   1-dimensional NDFs.

  \item  A bug in \htmlref{FITSDIN}{FITSDIN} and
    \htmlref{FITSIN}{FITSIN}, where BITPIX=8 FITS data would generate
    a \_BYTE type data array instead of the correct
    \htmlref{\_UBYTE}{ap:HDStypes} type, has been cured.  FITSIN no
    longer rewinds the tape before reading it.  The suggested default
    for parameter FMTCNV in both FITSIN and FITSDIN has been changed
    from {\tt FALSE} to {\tt TRUE}.

  \item  \htmlref{FOURIER}{FOURIER} no longer uses NAG and calls the
    FFTPACK routines of the \PDAref\ library.  As a result it is slightly
    faster, and is now available on all supported systems.  It can now
    operate upon 1-dimensional NDFs too.

  \item  \htmlref{GAUSMOOTH}{GAUSMOOTH} (formerly GAUSS) will now
    smooth 1-dimensional NDFs.

  \item  When reading from a text file, \htmlref{GLITCH}{GLITCH}
    interprets integer positions as pixel indices, and floating-point
    positions as pixel co-ordinates.

  \item  \htmlref{GLOBALS}{GLOBALS} reports the current transformation
    structure.

  \item  \htmlref{HISTAT}{HISTAT} has been converted to NDF\_.  It
    supports all NDF array components, and operates on NDFs of
    arbitrary dimensionality.

  \item  \htmlref{HISTEQ}{HISTEQ} has been converted to NDF\_.  It
    operates on NDFs of arbitrary dimensionality.

  \item  \htmlref{HISTOGRAM}{HISTOGRAM} has been converted to NDF\_.  It
    supports all NDF array components, and operates on NDFs of
    arbitrary dimensionality.  The histogram may be saved to an NDF
    (with the bad-pixel flag set to {\tt FALSE}); and/or plotted, with
    several parameters available for controlling the appearance of the
    graphics.

  \item  In \htmlref{INSPECT}{INSPECT} the defaults for parameters
    HISTOGRAM and SLICE have changed to null, meaning do not produce
    an output NDF.  Cursor mode using a workstation or X-terminal is
    fully controllable using the mouse buttons; pressing the mouse
    middle button allows sampling of the region or slice, or erasure
    of points in the {\tt XYcur} mode.

  \item  \htmlref{LUTABLE}{LUTABLE} requires fewer required pens (8 if
    FULL={\tt TRUE} or 24 if FULL={\tt FALSE}).  Works directly on
    double-precision NDFs for the histogram equalisation.

  \item  \htmlref{LUTHILITE}{LUTHILITE}, \htmlref{LUTROT}{LUTROT},
    \htmlref{LUTTWEAK}{LUTTWEAK} have a FULL parameter to allow
    manipulation of the full colour table.

  \item  \htmlref{LUTSAVE}{LUTSAVE} now creates NDF with data array
    having the simple variant, and the bad-pixel flag set to FALSE. 

  \item  \htmlref{LUTVIEW}{LUTVIEW} has new NN and LUT parameters, to
    read in a lookup table NDF into the colour table.

  \item  \htmlref{MATHS}{MATHS} supports tokens for pixel and data
     co-ordinates, and so can generate test data of virtually
     arbitrary functions.  When there are no input NDFs, the bounds of
     the output NDF is controlled either by parameters or a template
     NDF.  There are also new tokens for variance and sub-expressions.

  \item  \htmlref{MEDIAN}{MEDIAN} has been converted to NDF\_.

  \item  \htmlref{MEM2D}{MEM2D} no longer uses NAG and calls the
     FFTPACK routines of the PDA library.  As a result it is slightly
     faster, and is now available on all supported systems.  The 
     limitation on the array size has been relaxed.

  \item  \htmlref{NDFTRACE}{NDFTRACE} has 28 output parameters.  There
     is now also a QUIET flag to suppress output for procedures.

  \item  \htmlref{NOGLOBALS}{NOGLOBALS} erases the current transformation
    structure.

  \item  \htmlref{NOMAGIC}{NOMAGIC} has been converted to NDF\_.  There
    is a new SIGMA parameter, to allow the replacement values to be
    replaced with random numbers of a Gaussian distribution about the
    replacement value.  All array components may be processed.

  \item \htmlref{NUMB}{NUMB} has been converted to NDF\_.  All array
    components may be processed.  There are new parameters ABS and
    ABOVE.  {\tt NUMB ABS} is equivalent to the defunct NUMBA: it finds the
    number of pixels whose absolute value exceeds or is less than a
    threshold.  The ABOVE parameter controls the sense of the comparison.

  \item  \htmlref{PALENTRY}{PALENTRY} and \htmlref{PALDEF}{PALDEF}
    permit the DEVICE to be of class MATRIX\_PRINTER and have the
    reset attribute.  In other words you can call these for output to
    coloured PostScript.

  \item  \htmlref{PICDEF}{PICDEF} has an ASPECT parameter for
    controlling the aspect ratio of pictures defined by the TBC and
    LRC positioning modes. Parameter FRACTION can have different
    values for the two axes. The order of the positional parameters
    has changed: DEVICE and CURRENT were removed, XPIC and YPIC were
    added at positions 3 and 4, and LBOUND and UBOUND at positions 5
    and 6.  These modifications were needed to make
    \htmlref{PICGRID}{PICGRID} and \htmlref{PICXY}{PICXY} work.

  \item  \htmlref{PICIN}{PICIN} writes the bounds of the picture in
    raster co-ordinates to parameters RCX1, RCX2, RCY1, and RCY2.

  \item  \htmlref{PICLABEL}{PICLABEL} documentation explains what
    happens if the new label already exists in the database.

  \item  \htmlref{PICLIST}{PICLIST} does not report the list of pictures when
    a value for parameter PICNUM is supplied on the command line.
    PICNUM may take the value {\tt "Last"} to select the last picture
    in the graphics database.

  \item  \htmlref{PIXDUPE}{PIXDUPE} has been converted to NDF\_.  It
    operates on NDFs of arbitrary dimensions.  The expansion factors
    may be different along each axis.

  \item  \htmlref{PSF}{PSF} allows greater control of the plotting
    style, including the option to plot along the major axis of an
    elliptical point-spread function, and scaling of the abscissa from
    pixels to physical units.  It now aborts correctly at a prompt for
    the POSCOLS parameter, and computes the correct dimensions for the
    output point-spread function NDF.

  \item  \htmlref{ROTATE}{ROTATE} has been converted to NDF\_.
    Variance and quality information may be rotated.

  \item  \htmlref{SEGMENT}{SEGMENT} has been converted to NDF\_.  The
    $x$-$y$ positions may be specified through parameters, from a text
    file, and by using the graphics cursor.  The chosen points may be
    marked with crosses, or connected with polygon lines in a
    nominated colour.  Variance and quality information may be copied.
    The number of polygons can be constrained between limits.

  \item  \htmlref{SETMAGIC}{SETMAGIC} has been converted to NDF\_.  All
    array components may be processed.

  \item  \htmlref{SHADOW}{SHADOW} has been converted to NDF\_.  The
  shifts have been combined into a single SHIFT parameter

  \item  \htmlref{THRESH}{THRESH} has been converted to the NDF\_.  All
    array components may be processed.

  \item  \htmlref{TRANDAT}{TRANDAT} has a new parameter LBOUND for
    setting the array lower bounds in automatic mode.  The created NDF
    uses the simple variant for the data array.
\end{itemize}
\end{htmlonly}


\section{Release Notes---V0.10}
This release includes four new applications, three new synonyms, and
enhancements to existing applications.  Five applications have been
withdrawn, two to enable {\footnotesize KAPPA} to be NAG-free.

\subsection{Global changes}

\begin{itemize}
  \item  The NAG library is no longer used, and thus {\footnotesize
    KAPPA} is available via the Starlink Software Store.
 
  \item  Standardized the implementation status in the reference section,
    and corrected the HISTORY processing.

  \item  Improved the documentation: clarified and reduced the usage of
    the term ``magic pixel''; added more warnings about metacharacters in
    C-shell usage; updated the related applications; extended the FITS
    airlock to include the new editing commands; added a new section
    on bad-pixel removal; and corrected various typographical errors,
    mostly in the reference section.

  \item  The reference dataset in the AGI database may be a foreign file
    and/or include an NDF section.

  \item Several tasks use a faster method of managing workspace.

\end{itemize}

\subsection{New applications}
\begin{description}

  \item [\htmlref{DRAWSIG}{DRAWSIG}] 
     Draws ${\pm}n$ standard-deviation lines on a line plot.  There is
     clipping, and the statistics may be reported too.  It is
     expected to be used in conjunction with LINPLOT or HISTOGRAM.

  \item [\htmlref{FITSMOD}{FITSMOD}]  
     Edits an NDF FITS extension via a text file or parameters.  This
     allows you to manipulate ancillary data not in a special extension.

  \item [\htmlref{KSTEST}{KSTEST}]
     Compares datasets against a reference dataset using the 
     Kolmogorov-Smirnov test, and combines statistically similar data
     into an output vector dataset.  You can supply a single dataset,
     which is divided for the comparisons.

  \item [\htmlref{SUBSTITUTE}{SUBSTITUTE}] 
     Replaces all occurrences of a given value in an NDF array with
     another value.

\end{description}

\subsection{New synonyms}
\begin{description}

  \item [\htmlref{FITSEXIST}{FITSEXIST}]
     Inquires whether or not a keyword exists in an NDF's FITS extension.
     It is intended for scripts.

  \item [\htmlref{FITSVAL}{FITSVAL}]
     Reports the value of a keyword in the FITS extension.

  \item [\htmlref{FITSWRITE}{FITSWRITE}]
     Writes a new keyword and optional comment to the FITS extension.

\end{description}

\subsection{Withdrawn applications}
\begin{description}

   \item [BLINK]
     Does not work with X-windows.  Use \GAIAref\@.

   \item [COLUMNAR]
     No suitable public-domain equivalent plotting could be found to
     replace the NAG graphics.  This command may reappear if an
     adequate replacement subroutine can be located.

   \item [HIDE]
     See COLUMNAR.

   \item [IDUNZOOM]
     Does not work with X-windows.  Use \GAIAref\@.

   \item [IDVISIBLE]
     Does not work with X-windows.

\end{description}

\subsection{Modified applications}
\begin{itemize}
  \item  Examples added to \htmlref{CHPIX}{CHPIX} documentation.

  \item  \htmlref{CONTOVER}{CONTOVER}, \htmlref{CONTOUR}{CONTOUR},
    and \htmlref{TURBOCONT}{TURBOCONT} have had several improvements.  There are
    two new methods for selecting the contour heights using percentiles
    (MODE={\tt "Equalised"} and {\tt "Percentiles"}).  There is a CONCOL parameter
    for selecting the colour of the contours.  Dashed contours may be
    plotted for those with heights below the value of a new DASHED
    parameter.  There is a new THICK parameter for CONTOVER akin to
    those in other tasks.  The maximum thickness has been increased to
    10.  The tessellation size has been increased to 512 pixels for
    CONTOVER and TURBOCONT.  There are new examples.

  \item  The smoothed contouring and annotation facilities in CONTOUR
    have been withdrawn, possibly temporarily, as they called NAG
    graphics routines.  Thus parameters ANNOTA, LABELFREQ, MAXRES,
    NOISY, RESOLUTION, and SMOOTHING are no longer available.

  \item  CONTOUR has a new parameter---STATS---which, if set to
    {\tt TRUE}, will compute the number of closed contours and the
    total contour length at each height.  The statistics are written
    to new output parameters called NUMBER and LENGTH respectively.

  \item  \htmlref{FITSIN}{FITSIN} has a REWIND parameter to enable
    tape rewinding between invocations.  \htmlref{FITSDIN}{FITSDIN}
    and FITSIN are available from Linux.

  \item  \htmlref{FITSURFACE}{FITSURFACE} computes and records the
    variances of the polynomial coefficients.

  \item  \htmlref{FITSTEXT}{FITSTEXT} has improved validation of the FITS
    headers.

  \item  \htmlref{LINPLOT}{LINPLOT} has had a big overhaul to make it
    easier to use, especially for composite plots, and to offer more
    facilities. 

    There is an option through parameter ERRBAR to plot error bars
    with a choice of styles (SHAPE) and including co-ordinate errors.
    To avoid clutter the frequency of errors bars is controllable
    using parameter FREQ.  There is a choice of graph styles via
    parameter MODE; these include the histogram form, or points marked
    with a symbol from a selection of five.  Parameters ERRCOL,
    LINCOL, and SYMCOL allow you to select the colours of the error
    bars, graph lines, and symbols respectively.  Thick lines now work
    for all parts of the plot, and the maximum thickness is 10.
    There are new examples.

    LINPLOT was redesigned to allow easy production of composite
    plots, controlled using the CLEAR parameter.  When CLEAR is {\tt
    FALSE} the axis limits are inquired from the graphics database.
    The abscissa limits are now adjustable, so there is a new
    parameter called AXLIM which replaces ORDLIM and applies to both
    axes.  ORDLIM now specifies the ordinate limits, replacing ORDLOW
    and ORDUPP.  The abscissa limits may be adjusted through parameter
    ABSLIM.  Both ABSLIM and ORDLIM have sensible (dynamic) defaults.

    Using the new facilities it is possible to create colour hardcopy
    graphs combining graphs from multiple datasets, with each plotted
    in a different colour and/or linestyle.

  \item  \htmlref{MAKESURFACE}{MAKESURFACE} can create a variance
    array when the new parameter VARIANCE is set to {\tt TRUE}.

  \item  In \htmlref{MLINPLOT}{MLINPLOT}, the LNINDX parameter prompts
    with the current value as the suggested default.

  \item  The \htmlref{SETAXIS}{SETAXIS} documentation has an improved
    description of parameter EXPRS and additional examples.

  \item  \htmlref{SURFIT}{SURFIT} has been rewritten to use the NDF
    library and thus can take advantage of NDF facilities including
    sections and history, and can accept foreign-format files.

    Several of the parameters have been renamed to their standard
    values (INPIC to IN, OUTPIC to OUT, OTITLE to TITLE, WLIMIT to
    WLIM), or combined to make array parameters (IX and IY have become
    BINDIM, NXPAR and NYPAR have become ORDER or KNOTS depending on
    wheher a polyomial or spline fit is selected).  If you use SURFIT
    in scripts, you should check the usage carefully.

  \item  The \htmlref{TRANDAT}{TRANDAT} description has been improved
    and there is an additional example.

  \item  Relaxed the limits of parameter UCOORD in
    \htmlref{TRANSFORMER}{TRANSFORMER}.

\end{itemize}


\begin{latexonly}
\newpage
\pagestyle{empty}

\begin{center}
{\Large{\bf Classified KAPPA commands}}
\end{center}

{\footnotesize KAPPA} applications may be classified in terms of their
functions as follows.

\begin{small}
{\large
\begin{center}
{\bf DATA IMPORT \& EXPORT}
\end{center}
}

\begin{description}
%
\item {\large\bf Image generation and input}

\begin{description}
\classitem{CREFRAME}
 Generates a test 2-d data array from a selection of several types.
\classitem{FITSDIN}
 Reads a FITS disc file composed of simple, group or table objects.
\classitem{FITSHEAD}
 Lists the headers of FITS files.
\classitem{FITSIMP}
 Imports FITS information into an NDF extension.
\classitem{FITSIN}
 Reads a FITS tape composed of simple, group or table files.
\classitem{MATHS}
 Evaluates mathematical expressions applied to NDF data structures.
\classitem{TRANDAT}
 Converts free-format data into an NDF.
\end {description}
\vspace*{0.7ex}
%
\item {\large\bf Preparation for output}
\begin{description}
\classitem{FITSEDIT}
 Edits the FITS extension of an NDF.
\classitem{FITSEXP}
 Exports NDF-extension information into an NDF FITS extension.
\classitem{FITSMOD}
 Edits an NDF FITS extension via a text file or parameters.
\classitem{FITSTEXT}
 Creates an NDF FITS extension from a text file.
\classitem{FITSWRITE}
 Writes a new keyword to the FITS extension.
\end {description}
\end {description}

\vspace*{-1ex}

{\large
\begin{center}
{\bf DATA DISPLAY}
\end{center}
}

\begin{description}
%
\item {\large\bf Detail enhancement}
\begin{description}
\classitem{HISTEQ}
 Performs an histogram equalisation on an NDF.
\classitem{LAPLACE}
 Performs a Laplacian convolution as an edge detector in a 2-d data array.
\classitem{SHADOW}
 Enhances edges in a 2-dimensional NDF using a shadow effect.
\classitem{THRESH}
 Edits an NDF such that array values below and above two thresholds take
 constant values.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Device selection}
\begin{description}
\classitem{GDNAMES}
 Shows which graphics devices are available.
\classitem{GDSET}
 Selects a current graphics device.
\classitem{IDSET}
 Selects a current image-display device.
\classitem{OVSET}
 Selects a current image-display overlay.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Display control}
\begin{description}
\classitem{CURSOR}
 Reports the co-ordinates of points selected using the cursor.
\classitem{GDCLEAR}
 Clears a graphics device and purges its database entries.
\classitem{GDSTATE}
 Shows the current status of a graphics device.
\classitem{IDCLEAR}
 Clears an image display and purges its database entries.
\classitem{IDINVISIBLE}
 Makes memory planes of an image-display device invisible.
\classitem{IDPAZO}
 Pans and zooms an image-display device.
\classitem{IDSTATE}
 Shows the current status of an image display.
\classitem{OVCLEAR}
 Clears an image-display overlay.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Graphics Database}
\begin{description}
\classitem{PICBASE}
 Selects the BASE picture from the graphics database.
\classitem{PICCUR}
 Uses a cursor to select the current picture and to report the co-ordinates of points.
\classitem{PICDATA}
 Selects the last DATA picture from the graphics database.
\classitem{PICDEF}
 Defines a new graphics-database FRAME picture or an array of FRAME pictures.
\classitem{PICEMPTY}
 Finds the first empty FRAME picture in the graphics database.
\classitem{PICENTIRE}
 Finds the first unobscured and unobscuring FRAME picture in the graphics database.
\classitem{PICFRAME}
 Selects the last FRAME picture from the graphics database.
\classitem{PICGRID}
 Creates an array of FRAME pictures.
\classitem{PICIN}
 Finds the attributes of a picture interior to the current picture.
\classitem{PICLABEL}
 Labels the current graphics-database picture.
\classitem{PICLAST}
 Selects the last picture from the graphics database.
\classitem{PICLIST}
 Lists the pictures in the graphics database for a device.
\classitem{PICSEL}
 Selects a graphics-database picture by its label.
\classitem{PICTRANS}
 Transforms co-ordinates between the current and BASE pictures.
\classitem{PICVIS}
 Finds the first unobscured FRAME picture in the graphics database.
\classitem{PICXY}
 Creates a new picture defined by co-ordinate bounds.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Lookup/Colour tables}
\begin{description}
\classitem{CRELUT}
 Creates or manipulates an image-display lookup table using a palette.
\classitem{LUTABLE}
 Manipulates an image-display colour table.
\classitem{LUTBGYRW}
 Loads the {\it BGYRW\/} lookup table.
\classitem{LUTCOL}
 Loads the standard colour lookup table.
\classitem{LUTCONT}
 Loads a lookup table to give the display the appearance of a contour plot.
\classitem{LUTFC}
 Loads the standard false-colour lookup table.
\classitem{LUTFLIP}
 Flips the colour table of an image-display device.
\classitem{LUTGREY}
 Loads the standard greyscale lookup table.
\classitem{LUTHEAT}
 Loads the {\it heat\/} lookup table.
\classitem{LUTHILITE}
 Highlights a colour table of an image-display device.
\classitem{LUTIKON}
 Loads the default {\it Ikon\/} lookup table.
\classitem{LUTNEG}
 Loads the standard negative greyscale lookup table.
\classitem{LUTRAMPS}
 Loads the coloured-ramps lookup table.
\classitem{LUTREAD}
 Loads an image-display lookup table from an NDF.
\classitem{LUTROT}
 Rotates the colour table of an image-display device.
\classitem{LUTSAVE}
 Saves the current colour table of an image-display device in an NDF.
\classitem{LUTSPEC}
 Loads a spectrum-like lookup table.
\classitem{LUTTWEAK}
 Tweaks a colour table of an image-display device.
\classitem{LUTVIEW}
 Draws a colour-table key.
\classitem{LUTZEBRA}
 Loads a pseudo-contour lookup table.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Output}
\begin{description}
\classitem{CONTOUR}
 Contours a 2-d NDF.
\classitem{CONTOVER}
 Contours a 2-d data array overlaid on an image displayed previously.
\classitem{DISPLAY}
 Displays a 1-d or 2-d NDF.
\classitem{DRAWSIG}
 Draws ${\pm}n$ standard-deviation lines on a line plot.
\classitem{ELPROF}
 Creates a radial or azimuthal profile of a 2-dimensional image.
\classitem{GREYPLOT}
 Produces a greyscale plot of a 1-d or 2-d NDF.
\classitem{INSPECT}
 Inspects a 2-d NDF in a variety of ways.
\classitem{LINPLOT}
 Draws a line plot of a 1-d NDF's data values against their axis co-ordinates.
\classitem{LOOK}
 Outputs the values of a sub-array of a 2-d data array to the screen or a
 text file.
\classitem{MLINPLOT}
 Draws a multi-line plot of a 2-d NDF's data values against their axis
 co-ordinates.
\classitem{SNAPSHOT}
 Dumps an image-display memory to a graphics hardcopy and
 optionally to an NDF.
\classitem{TURBOCONT}
 Contours a 2-d NDF quickly.
\classitem{VECPLOT}
 Plots a 2-dimensional vector map.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Palette}
\begin{description}
\classitem{PALDEF}
 Loads the default palette to a colour table.
\classitem{PALENTRY}
 Enters a colour into an image display's palette.
\classitem{PALREAD}
 Fills the palette of a colour table from an NDF.
\classitem{PALSAVE}
 Saves the current palette of a colour table to an NDF.
\end{description}
\end{description}

\vspace*{-1ex}

{\large
\begin{center}
{\bf DATA MANIPULATION}
\end{center}
}

\begin{description}
%
\item {\large\bf Arithmetic}
\begin{description}
\classitem{ADD}
 Adds two NDF data structures.
\classitem{CADD}
 Adds a scalar to an NDF data structure.
\classitem{CDIV}
 Divides an NDF by a scalar.
\classitem{CMULT}
 Multiplies an NDF by a scalar.
\classitem{CSUB}
 Subtracts a scalar from an NDF data structure.
\classitem{DIV}
 Divides one NDF data structure by another.
\classitem{EXP10}
 Takes the base-10 exponential of each pixel of a data array.
\classitem{EXPE}
 Takes the exponential of each pixel of a data array (base $e$).
\classitem{EXPON}
 Takes the exponential of each pixel of a data array (specified base).
\classitem{LOG10}
 Takes the base-10 logarithm of each pixel of a data array.
\classitem{LOGAR}
 Takes the logarithm of each pixel of a data array (specified base).
\classitem{LOGE}
 Takes the natural logarithm of each pixel of a data array.
\classitem{MATHS}
 Evaluates mathematical expressions applied to NDF data structures.
\classitem{MULT}
 Multiplies two NDF data structures.
\classitem{POW}
 Takes the specified power of each pixel of a data array.
\classitem{SUB}
 Subtracts one NDF data structure from another.
\classitem{TRIG}
 Performs a trigonometric transformation on a data array.
\end {description}
\vspace*{0.7ex}
%
\item {\large\bf Combination}
\begin{description}
\classitem{CALPOL}
 Calculates polarisation parameters.
\classitem{KSTEST}
 Compares data sets using the Kolmogorov-Smirnov test.
\classitem{MOSAIC}
 Merges several non-congruent 2-d data arrays into one output data array.
\classitem{NORMALIZE}
 Normalises one NDF to a similar NDF by calculating a scale factor and zero
 difference.
\classitem{QUILT}
 Generates a mosaic from equally sized 2-d data arrays, optionally specified
 from an ASCII file.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Compression and expansion}
\begin{description}
\classitem{COMPADD}
 Reduces the size of an NDF by adding values in rectangular boxes.
\classitem{COMPAVE}
 Reduces the size of an NDF by averaging values in rectangular boxes.
\classitem{COMPICK}
 Reduces the size of an NDF by picking equally spaced pixels.
\classitem{PIXDUPE}
 Expands an NDF by pixel duplication.
\classitem{SQORST}
 Squashes or stretches a 2-d data array in either or both axes.
\classitem{TRANSFORMER}
 Applies a transformation to an NDF.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Configuration change}
\begin{description}
\classitem{FLIP}
 Reverses an NDF's pixels along a specified dimension.
\classitem{INSPECT}
 Inspects a 2-d NDF in a variety of ways.
\classitem{MANIC}
 Converts all or part of a data array from one dimensionality to another.
\classitem{NDFCOPY}
 Copies an NDF (or NDF section) to a new location.
\classitem{ROTATE}
 Rotates a 2-dimensional NDF about its centre through any angle.
\classitem{SETBOUND}
 Sets new bounds for an NDF.
\classitem{SLIDE}
 Realigns a 2-d data array via an $x$-$y$ shift.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Filtering}
\begin{description}
\classitem{BLOCK}
 Smooths a 1- or 2-dimensional image using a square or rectangular box filter.
\classitem{CONVOLVE}
 Convolves a pair of 1- or 2-dimensional NDFs together.
\classitem{FFCLEAN}
 Removes defects from a substantially flat 1- or 2-dimensional NDF.
\classitem{FOURIER}
 Performs forward and inverse Fourier transforms of 1- or 2-dimensional NDFs.
\classitem{GAUSMOOTH}
 Smooths a 1- or 2-dimensional image using a Gaussian filter.
\classitem{LUCY}
 Performs a Richardson-Lucy deconvolution of a 1- or 2-dimensional array.
\classitem{MEDIAN}
 Smooths a 2-dimensional data array using a weighted median filter.
\classitem{MEM2D}
 Performs a Maximum-Entropy deconvolution of a 2-dimensional NDF.
\classitem{WIENER}
 Applies a Wiener filter to a 1- or 2-dimensional array.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf HDS components}
\begin{description}
\classitem{ERASE}
 Erases an HDS object.
\classitem{NATIVE}
 Converts an HDS object to native machine data representation.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf NDF array components}
\begin{description}
\classitem{NDFCOPY}
 Copies an NDF (or NDF section) to a new location.
\classitem{SETBAD}
 Sets new bad-pixel flag values for an NDF.
\classitem{SETBB}
 Sets a new value for the quality bad-bits mask of an NDF.
\classitem{SETBOUND}
 Sets new bounds for an NDF.
\classitem{SETTYPE}
 Sets a new numeric type for the data and variance components of an NDF.
\classitem{SETVAR}
 Sets new values for the variance component of an NDF data structure.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf NDF axis components}
\begin{description}
\classitem{AXCONV}
 Expands spaced axes in an NDF into the primitive form.
\classitem{AXLABEL}
 Sets a new label value for an axis within an NDF data structure.
\classitem{AXUNITS}
 Sets a new units value for an axis within an NDF data structure.
\classitem{SETAXIS}
 Sets values for an axis array component within an NDF data structure.
\classitem{SETNORM}
 Sets a new value for one or all of an NDF's axis-normalisation flags.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf NDF character components}
\begin{description}
\classitem{SETLABEL}
 Sets a new label for an NDF data structure.
\classitem{SETTITLE}
 Sets a new title for an NDF data structure.
\classitem{SETUNITS}
 Sets a new units value for an NDF data structure.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf NDF extensions}
\begin{description}
\classitem{FITSEDIT}
 Edits the FITS extension of an NDF.
\classitem{FITSEXIST}
 Inquires whether or not a keyword exists in a FITS extension.
\classitem{FITSEXP}
 Exports NDF-extension information into an NDF FITS extension.
\classitem{FITSLIST}
 Lists the FITS extension of an NDF.
\classitem{FITSMOD}
 Edits an NDF FITS extension via a text file or parameters.
\classitem{FITSTEXT}
 Creates an NDF FITS extension from a text file.
\classitem{FITSVAL}
 Reports the value of a keyword in the FITS extension.
\classitem{FITSWRITE}
 Writes a new keyword to the FITS extension.
\classitem{SETEXT}
 Manipulates the contents of a specified NDF extension.
\classitem{SETSKY}
 Makes an IRAS astrometry extension.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf NDF History}
\begin{description}
\classitem{HISCOM}
 Adds commentary to the history of an NDF.
\classitem{HISLIST}
 Lists NDF history records.
\classitem{HISSET}
 Sets the NDF history update mode.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Pixel editing and masking}
\begin{description}
\classitem{ARDGEN}
 Creates a text file describing selected regions of an image.
\classitem{ARDMASK}
 Uses an ARD file to set some pixels of an NDF to be bad.
\classitem{CHPIX}
 Replaces the values of selected pixels in an NDF.
\classitem{ERRCLIP}
 Removes pixels with large errors from an NDF.
\classitem{FFCLEAN}
 Removes defects from a substantially flat 1- or 2-dimensional NDF.
\classitem{FILLBAD}
 Removes regions of bad values from a 2-dimensional NDF.
\classitem{GLITCH}
 Replaces bad pixels in a 2-d data array with the local median.
\classitem{NOMAGIC}
 Replaces all occurrences of magic value pixels in an NDF array with
 a new value.
\classitem{OUTSET}
 Sets pixels outside a specified circle in a 2-d data array to a specified
 value.
\classitem{PASTE}
 Pastes a series of NDFs upon each other.
\classitem{RIFT}
 Adds a scalar to a section of an NDF data structure to correct rift-valley defects.
\classitem{SEGMENT}
 Copies polygonal segments from one NDF to another.
\classitem{SETMAGIC}
 Replaces all occurrences of a given value in an NDF array
 with the bad value.
\classitem{SUBSTITUTE}
 Replaces all occurrences of a given value in an NDF array with another value.
\classitem{ZAPLIN}
 Replaces regions in a 2-d NDF by bad values or by linear
 interpolation.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Polarimetry}
\begin{description}
\classitem{CALPOL}
 Calculates polarisation parameters.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Resampling and transformations}
\begin{description}
\classitem{TRANINVERT}
 Inverts a transformation.
\classitem{TRANJOIN}
 Joins two transformations.
\classitem{TRANMAKE}
 Makes a transformation structure given its co-ordinate mappings.
\classitem{TRANSFORMER}
 Applies a transformation to an NDF.
\classitem{TRANTRACE}
 Lists the contents of a transformation structure.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Surface fitting}
\begin{description}
\classitem{FITSURFACE}
 Fits a polynomial surface to 2-dimensional data array.
\classitem{MAKESURFACE}
 Creates a 2-dimensional NDF from the coefficients of a polynomial surface.
\classitem{SURFIT}
 Fits a polynomial or spline surface to a 2-d data array using blocking.
\end{description}
\end{description}

\vspace*{-1ex}

{\large
\begin{center}
{\bf DATA ANALYSIS}
\end{center}
}

\begin{description}
%
\item {\large\bf Statistics}
\begin{description}
\classitem{APERADD}
 Derives statistics of pixels within a specified circle of a 2-d data array.
\classitem{HISTAT}
 Computes ordered statistics for an NDF's pixels using an histogram.
\classitem{HISTOGRAM}
 Computes an histogram of an NDF's values.
\classitem{INSPECT}
 Inspects a 2-d NDF in a variety of ways.
\classitem{MSTATS}
 Does cumulative statistics on a 2-d sub-array over a sequence of data arrays.
\classitem{NUMB}
 Counts the number of elements of an NDF with values or absolute values above
 or below a threshold.
\classitem{STATS}
 Computes simple statistics for an NDF's pixels.
\end{description}
\vspace*{0.7ex}
%
\item {\large\bf Other}
\begin{description}
\classitem{CENTROID}
 Finds the centroids of star-like features in an NDF.
\classitem{NORMALIZE}
 Normalises one NDF to a similar NDF by calculating a scale factor
 and zero-point difference.
\classitem{PSF}
 Determines the parameters of a model star profile by fitting star images
 in a two-dimensional NDF.
\classitem{SURFIT}
 Fits a polynomial or spline surface to a 2-d data array.
\end{description}
\end{description}

\vspace*{-1ex}

{\large
\begin{center}
{\bf SCRIPTING TOOLS}
\end{center}
}

\begin{description}
\item{~}
\vspace*{-5.6ex}
%
\begin{description}
\classitem{CALC}
 Evaluates a mathematical expression.
\classitem{PARGET}
 Obtains the value or values of an application parameter.
\end{description}
\end{description}

\vspace*{-1ex}

{\large
\begin{center}
{\bf INQUIRIES \& STATUS}
\end{center}
}

\begin{description}
\item{~}
\vspace*{-5.6ex}
%
\begin{description}
\classitem{GLOBALS}
 Displays the values of the {\footnotesize KAPPA} global parameters.
\classitem{FITSEXIST}
 Inquires whether or not a keyword exists in a FITS extension.
\classitem{FITSLIST}
 Lists the FITS extension of an NDF.
\classitem{FITSVAL}
 Reports the value of a keyword in the FITS extension.
\classitem{NDFTRACE}
 Displays the attributes of an NDF data structure.
\classitem{NOGLOBALS}
 Resets the {\footnotesize KAPPA} global parameters.
\end{description}
\end{description}

\vspace*{-1ex}

{\large
\begin{center}
{\bf MISCELLANEOUS}
\end{center}
}

\begin{description}
\item{~}
\vspace*{-5.6ex}
%
\begin{description}
\classitem{KAPHELP}
 Gives help about {\footnotesize KAPPA}.
\end{description}
\end{description}
\end{small}

\end{latexonly}

\end{document}
