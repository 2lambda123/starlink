<H1>Super-Applications in Fortran</H1>

I have been doing some experimenting with ways to run ADAM
applications as "slave" tasks from other ADAM applications, so as to
be able to write "super-applications" in Fortran -- rather similar to
the way (say) ICL scripts work, but with a "proper" parameter
interface. (The motivation for this is described in the "alternatives
to shell scripts" reply to the IRAF integration topic.)
<P>

I have made quite good progress and am encouraged by the result so
far, but it has proved a bit more difficult than I imagined. Mainly,
this is because almost none of the low-level ADAM facilities you need
are even slightly documented, and I've had to rely almost exclusively
on discussions with others to find out what's required. I suspect we
might have had this capability some time ago but for this.
<P>

There have also been two main technical obstacles: (1 - not
surprisingly) handling parameter requests from slave tasks has
required quite a bit of detailed coding, and (2 - more surprisingly)
being able to kill slave tasks off reliably so they don't hang around
if you kill the main super-application (e.g. with ctrl-C) has been
rather difficult (neither ICL nor StarTcl suffer the same predations
from the user, so neither uses a sufficiently reliable method).
<P>

I believe I've now cracked both of these problems, although it adds up
to rather more code than I'd expected. What I have at present is a
demonstration prototype, coded in C (but callable from a Fortran
A-task), which shows what is possible and where we might go next if it
looks worthwhile. Writing this has also helped clarify in my mind how
ADAM tasks might fit into the idea of using distributed objects to
provide data reduction services, so the work has certainly been
valuable. However, what's needed now is some feedback to help decide
whether continuing down this road is sensible, so I'd encourage people
to give it a look (or even a try out) and let me know what you think.

<H2>An Example</H2>

To whet your appetite, here's a simple super-app that runs a (somewhat
arbitrary) sequence of KAPPA applications on an NDF - NDFTRACE, STATS
and DISPLAY (in that order). It has two parameters: NDF and MODE (the
display scaling mode) - all the others are hidden from the user:
<PRE>
      SUBROUTINE SLAVETEST( STATUS )

      IMPLICIT NONE              ! No implicit typing
      INCLUDE 'SAE_PAR'          ! Standard SAE constants

      INTEGER KAPPAPID
      INTEGER NDFPID
      INTEGER SLV_LOADW
      INTEGER STATUS
      INTEGER TIMEOUT
      INTEGER VIEWID
      LOGICAL DETACH

*  Check inherited global status.
      IF ( STATUS .NE. SAI__OK ) RETURN

      DETACH = .FALSE.
      TIMEOUT = 15

*  Load tasks.
      NDFPID = SLV_LOADW( 'NDFPACK', 'ndfpack_mon', DETACH, TIMEOUT,
     :                    STATUS )
      KAPPAPID = SLV_LOADW( 'KAPPA', 'kappa_mon', DETACH, TIMEOUT,
     :                      STATUS )
      VIEWID = SLV_LOADW( 'KAPVIEW', 'kapview_mon', DETACH, TIMEOUT,
     :                    STATUS )
      
*  Run the applications.
      CALL SLV_OBEYW( 'NDFPACK', 'ndftrace', 'prompt', 'NDF&lt;NDF',
     :                STATUS )
      CALL SLV_OBEYW( 'KAPPA', 'stats', 'prompt', 'NDF&lt;NDF',
     :                STATUS )
      CALL SLV_OBEYW( 'KAPVIEW', 'display', 'prompt',
     :                'IN&lt;NDF,MODE&lt;MODE', STATUS )

*  You could kill the tasks here (like this), but they die anyway when
*  the main application task terminates.
c      CALL SLV_KILL( NDFPID, STATUS )
c      CALL SLV_WAITK( NDFPID, TIMEOUT, STATUS )

*  Report a contextual error message.
      IF ( STATUS .NE. SAI__OK ) THEN
         CALL ERR_REP( 'SLAVETEST_ERR',
     :                 'SLAVETEST: Error occurred in test program.',
     :                 STATUS )
      END IF
      END
</PRE>

Here is the (very rudimentary) interface file:

<PRE>
interface SLAVETEST
   parameter NDF
      position 1
      ppath dynamic
   endparameter

   parameter MODE
      position 2
      ppath dynamic
   endparameter

endinterface
</PRE>
This is what the SLV_ routines do:

<H2>SLV_LOADW( TASK, FILE, DETACH, TIMEOUT, STATUS )</H2>

This loads an ADAM task called TASK from a file called FILE (using
$PATH to find it if necessary), or attaches to an existing task of
that name if it is already running. If DETACH is .TRUE., the new task
has an existence independent of its parent and will need to be
explicitly killed before you log out and go home (otherwise it dies
when its parent dies). TIMEOUT is the time in seconds to wait to allow
the task to load. SLV_LOADW returns a process ID if it loads a new
task (or zero if it doesn't) - this can be used to kill it later.

<H2>SLV_OBEYW( TASK, ACTION, OPTIONS, ASSOC, STATUS )</H2>

Runs the action (i.e. application) called ACTION in the
previously-loaded task called TASK and waits for it to finish. OPTIONS
is a set of ADAM command-line options in the usual format (and can
include "prompt", "reset", etc). If the actions fails, the return
status comes back in STATUS with appropriate error messages stacked
using EMS (so you could choose to annul them, for example).
<P>

ASSOC is the interesting argument - it specifies associations between
the slave's parameters and those of the super-app (the "master"). At
present only input parameters are supported, in the form of a
comma-separated list with elements like 'SLAVE&lt;MASTER'. This indicates
that if the slave task asks the master for a value for parameter
"SLAVE", it should use the master task's "MASTER" parameter to obtain
it (possibly resulting in a prompt, depending on the master's
interface file). If the slave parameter doesn't appear in this list,
it behaves as if "accept" had been given in the OPTIONS argument, so
the parameter is hidden from the user. You can control which
parameters the slave asks the master for by using "prompt", "accept",
"reset" and specific parameter assignments in the OPTIONS argument.
<P>

In the example above, when NDFTRACE runs, it requests a value for its
NDF parameter from the master. This causes a prompt for the associated
parameter (NDF) from the master (unless over-ridden on the command
line) with its dynamic default set to the suggested value from the
slave task. The resulting value is returned to NDFTRACE. Any other
requests from NDFTRACE (produced by the "prompt" option) cause the
master to respond with the suggested value (as if a user had pressed
RETURN when prompted). When STATS runs, it also requests an NDF value,
and the value of the master's NDF parameter (which is now set) is
returned. Similarly, the request from DISPLAY for its IN parameter
(which is associated wth the master's NDF parameter) uses the same
value. However, DISPLAY's MODE parameter is also associated with one
of the master's parameters (also called MODE) which doesn't yet have a
value, so this results in another prompt.
<P>

What I have been trying to do is devise a way for super-apps to
"inherit" the properties of their slaves - in particular their
parameters. I've gone some way down this road, but I'd like it (for
example) if you didn't have to specify an elaborate interface file
entry for every master parameter, but could let the slave's interface
file entry over-ride anything you hadn't specified explicitly in the
master's. This would save a lot of work on things like prompt strings,
help files, etc., because the slave's behaviour would be 90% OK in
most cases. To make this work properly needs more access to parameter
system internals, however, which I don't want to get into just yet!!

<H2>SLV_KILL, SLV_WAITK</H2>

These routines do what you'd expect - kill a task and (SLV_WAITK)
wait for it to terminate (with an optional timeout). This may be
needed if you create detached tasks, but with attached tasks the
slaves are killed automatically when the master task that created them
terminates. This is the simplest option, but detached tasks do offer
performance improvements (no repeated task startup overheads) if you
can be bothereed to cope with keeping track of the tasks. Of course,
from ICL (and maybe from IRAF?), the performance improvement comes for
free because nothing dies until you exit ICL. I have tested this stuff
with ICL, but not yet with IRAF (although it should work). Bear in
mind that super-apps can also ask other super-apps to do work for
them, so there are a lot of task arrangements possible.

<H2>Deficiencies</H2>

The main deficiencies in the demonstration system I have at present are:
<OL>
<LI>Output parameters aren't supported.
<P>
<LI>Routines are probably needed to get and set (and maybe reset) parameters.
<P>
<LI>Quoting of parameter values isn't really handled properly.
<P>
<LI>Parameters names are a bit case sensitive.
<P>
<LI>Run-away slaves (but only very naughty ones) can cause hangs.
<P>
<LI>Error handling needs to be more robust in places.
<P>
<LI>You need a way of intercepting messages to/from tasks so you can (for
     example) capture or suppress textual output.
<P>
<LI>The interface could probably be designed better to allow control
     in future of other (non-ADAM) services, such as WWW services.
<P>
<LI>The code needs a general tidy and a makefile, etc. is needed.
<P>
<LI>Documentation.
</OL>

However, I think we could have a workable system with perhaps another
2 or 3 weeks work, which should still allow time for submission for
release this autumn (but only if it seems worthwhile).

<H2>How to Get a Copy</H2>

If you'd like to give this stuff a try out, there is a tar file
(slv.tar) in my ~rfws/export directory at RAL. The system consists
simply of a "slv.c" and a "slv.h" file which you compile -- and then link
with your A-task using "alink" (or with a monolith, I guess). This
lets you write super-apps like the one above.
<P>
If you want diagnostics, then define the PRINT macro at the top if the
slv.c file to be 1. This writes stuff to STDOUT to let you see the
detail of what's going on.
<P>
Comments please...
<P>
Rod.
